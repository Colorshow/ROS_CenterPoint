&&&& RUNNING TensorRT.trtexec [TensorRT v8401] # /usr/src/tensorrt/bin/trtexec --onnx=model/rpn_centerhead_sim.onnx --saveEngine=model/rpn_centerhead_sim.plan.8503 --workspace=4096 --fp16 --outputIOFormats=fp16:chw --inputIOFormats=fp16:chw --verbose --dumpLayerInfo --dumpProfile --separateProfileRun --profilingVerbosity=detailed
[08/10/2023-11:16:15] [W] --workspace flag has been deprecated by --memPoolSize flag.
[08/10/2023-11:16:15] [I] === Model Options ===
[08/10/2023-11:16:15] [I] Format: ONNX
[08/10/2023-11:16:15] [I] Model: model/rpn_centerhead_sim.onnx
[08/10/2023-11:16:15] [I] Output:
[08/10/2023-11:16:15] [I] === Build Options ===
[08/10/2023-11:16:15] [I] Max batch: explicit batch
[08/10/2023-11:16:15] [I] Memory Pools: workspace: 4096 MiB, dlaSRAM: default, dlaLocalDRAM: default, dlaGlobalDRAM: default
[08/10/2023-11:16:15] [I] minTiming: 1
[08/10/2023-11:16:15] [I] avgTiming: 8
[08/10/2023-11:16:15] [I] Precision: FP32+FP16
[08/10/2023-11:16:15] [I] LayerPrecisions: 
[08/10/2023-11:16:15] [I] Calibration: 
[08/10/2023-11:16:15] [I] Refit: Disabled
[08/10/2023-11:16:15] [I] Sparsity: Disabled
[08/10/2023-11:16:15] [I] Safe mode: Disabled
[08/10/2023-11:16:15] [I] DirectIO mode: Disabled
[08/10/2023-11:16:15] [I] Restricted mode: Disabled
[08/10/2023-11:16:15] [I] Build only: Disabled
[08/10/2023-11:16:15] [I] Save engine: model/rpn_centerhead_sim.plan.8503
[08/10/2023-11:16:15] [I] Load engine: 
[08/10/2023-11:16:15] [I] Profiling verbosity: 2
[08/10/2023-11:16:15] [I] Tactic sources: Using default tactic sources
[08/10/2023-11:16:15] [I] timingCacheMode: local
[08/10/2023-11:16:15] [I] timingCacheFile: 
[08/10/2023-11:16:15] [I] Input(s): fp16:chw
[08/10/2023-11:16:15] [I] Output(s): fp16:chw
[08/10/2023-11:16:15] [I] Input build shapes: model
[08/10/2023-11:16:15] [I] Input calibration shapes: model
[08/10/2023-11:16:15] [I] === System Options ===
[08/10/2023-11:16:15] [I] Device: 0
[08/10/2023-11:16:15] [I] DLACore: 
[08/10/2023-11:16:15] [I] Plugins:
[08/10/2023-11:16:15] [I] === Inference Options ===
[08/10/2023-11:16:15] [I] Batch: Explicit
[08/10/2023-11:16:15] [I] Input inference shapes: model
[08/10/2023-11:16:15] [I] Iterations: 10
[08/10/2023-11:16:15] [I] Duration: 3s (+ 200ms warm up)
[08/10/2023-11:16:15] [I] Sleep time: 0ms
[08/10/2023-11:16:15] [I] Idle time: 0ms
[08/10/2023-11:16:15] [I] Streams: 1
[08/10/2023-11:16:15] [I] ExposeDMA: Disabled
[08/10/2023-11:16:15] [I] Data transfers: Enabled
[08/10/2023-11:16:15] [I] Spin-wait: Disabled
[08/10/2023-11:16:15] [I] Multithreading: Disabled
[08/10/2023-11:16:15] [I] CUDA Graph: Disabled
[08/10/2023-11:16:15] [I] Separate profiling: Enabled
[08/10/2023-11:16:15] [I] Time Deserialize: Disabled
[08/10/2023-11:16:15] [I] Time Refit: Disabled
[08/10/2023-11:16:15] [I] Inputs:
[08/10/2023-11:16:15] [I] === Reporting Options ===
[08/10/2023-11:16:15] [I] Verbose: Enabled
[08/10/2023-11:16:15] [I] Averages: 10 inferences
[08/10/2023-11:16:15] [I] Percentile: 99
[08/10/2023-11:16:15] [I] Dump refittable layers:Disabled
[08/10/2023-11:16:15] [I] Dump output: Disabled
[08/10/2023-11:16:15] [I] Profile: Enabled
[08/10/2023-11:16:15] [I] Export timing to JSON file: 
[08/10/2023-11:16:15] [I] Export output to JSON file: 
[08/10/2023-11:16:15] [I] Export profile to JSON file: 
[08/10/2023-11:16:15] [I] 
[08/10/2023-11:16:15] [I] === Device Information ===
[08/10/2023-11:16:15] [I] Selected Device: Orin
[08/10/2023-11:16:15] [I] Compute Capability: 8.7
[08/10/2023-11:16:15] [I] SMs: 14
[08/10/2023-11:16:15] [I] Compute Clock Rate: 0.93 GHz
[08/10/2023-11:16:15] [I] Device Global Memory: 30588 MiB
[08/10/2023-11:16:15] [I] Shared Memory per SM: 164 KiB
[08/10/2023-11:16:15] [I] Memory Bus Width: 128 bits (ECC disabled)
[08/10/2023-11:16:15] [I] Memory Clock Rate: 0.93 GHz
[08/10/2023-11:16:15] [I] 
[08/10/2023-11:16:15] [I] TensorRT version: 8.4.1
[08/10/2023-11:16:15] [V] [TRT] Registered plugin creator - ::GridAnchor_TRT version 1
[08/10/2023-11:16:15] [V] [TRT] Registered plugin creator - ::GridAnchorRect_TRT version 1
[08/10/2023-11:16:15] [V] [TRT] Registered plugin creator - ::NMS_TRT version 1
[08/10/2023-11:16:15] [V] [TRT] Registered plugin creator - ::Reorg_TRT version 1
[08/10/2023-11:16:15] [V] [TRT] Registered plugin creator - ::Region_TRT version 1
[08/10/2023-11:16:15] [V] [TRT] Registered plugin creator - ::Clip_TRT version 1
[08/10/2023-11:16:15] [V] [TRT] Registered plugin creator - ::LReLU_TRT version 1
[08/10/2023-11:16:15] [V] [TRT] Registered plugin creator - ::PriorBox_TRT version 1
[08/10/2023-11:16:15] [V] [TRT] Registered plugin creator - ::Normalize_TRT version 1
[08/10/2023-11:16:15] [V] [TRT] Registered plugin creator - ::ScatterND version 1
[08/10/2023-11:16:15] [V] [TRT] Registered plugin creator - ::RPROI_TRT version 1
[08/10/2023-11:16:15] [V] [TRT] Registered plugin creator - ::BatchedNMS_TRT version 1
[08/10/2023-11:16:15] [V] [TRT] Registered plugin creator - ::BatchedNMSDynamic_TRT version 1
[08/10/2023-11:16:15] [V] [TRT] Registered plugin creator - ::BatchTilePlugin_TRT version 1
[08/10/2023-11:16:15] [V] [TRT] Registered plugin creator - ::FlattenConcat_TRT version 1
[08/10/2023-11:16:15] [V] [TRT] Registered plugin creator - ::CropAndResize version 1
[08/10/2023-11:16:15] [V] [TRT] Registered plugin creator - ::CropAndResizeDynamic version 1
[08/10/2023-11:16:15] [V] [TRT] Registered plugin creator - ::DetectionLayer_TRT version 1
[08/10/2023-11:16:15] [V] [TRT] Registered plugin creator - ::EfficientNMS_TRT version 1
[08/10/2023-11:16:15] [V] [TRT] Registered plugin creator - ::EfficientNMS_ONNX_TRT version 1
[08/10/2023-11:16:15] [V] [TRT] Registered plugin creator - ::EfficientNMS_Explicit_TF_TRT version 1
[08/10/2023-11:16:15] [V] [TRT] Registered plugin creator - ::EfficientNMS_Implicit_TF_TRT version 1
[08/10/2023-11:16:15] [V] [TRT] Registered plugin creator - ::ProposalDynamic version 1
[08/10/2023-11:16:15] [V] [TRT] Registered plugin creator - ::Proposal version 1
[08/10/2023-11:16:15] [V] [TRT] Registered plugin creator - ::ProposalLayer_TRT version 1
[08/10/2023-11:16:15] [V] [TRT] Registered plugin creator - ::PyramidROIAlign_TRT version 1
[08/10/2023-11:16:15] [V] [TRT] Registered plugin creator - ::ResizeNearest_TRT version 1
[08/10/2023-11:16:15] [V] [TRT] Registered plugin creator - ::Split version 1
[08/10/2023-11:16:15] [V] [TRT] Registered plugin creator - ::SpecialSlice_TRT version 1
[08/10/2023-11:16:15] [V] [TRT] Registered plugin creator - ::InstanceNormalization_TRT version 1
[08/10/2023-11:16:15] [V] [TRT] Registered plugin creator - ::InstanceNormalization_TRT version 2
[08/10/2023-11:16:15] [V] [TRT] Registered plugin creator - ::CoordConvAC version 1
[08/10/2023-11:16:15] [V] [TRT] Registered plugin creator - ::DecodeBbox3DPlugin version 1
[08/10/2023-11:16:15] [V] [TRT] Registered plugin creator - ::GenerateDetection_TRT version 1
[08/10/2023-11:16:15] [V] [TRT] Registered plugin creator - ::MultilevelCropAndResize_TRT version 1
[08/10/2023-11:16:15] [V] [TRT] Registered plugin creator - ::MultilevelProposeROI_TRT version 1
[08/10/2023-11:16:15] [V] [TRT] Registered plugin creator - ::NMSDynamic_TRT version 1
[08/10/2023-11:16:15] [V] [TRT] Registered plugin creator - ::PillarScatterPlugin version 1
[08/10/2023-11:16:15] [V] [TRT] Registered plugin creator - ::VoxelGeneratorPlugin version 1
[08/10/2023-11:16:15] [V] [TRT] Registered plugin creator - ::MultiscaleDeformableAttnPlugin_TRT version 1
[08/10/2023-11:16:16] [I] [TRT] [MemUsageChange] Init CUDA: CPU +218, GPU +0, now: CPU 242, GPU 5604 (MiB)
[08/10/2023-11:16:19] [I] [TRT] [MemUsageChange] Init builder kernel library: CPU +351, GPU +461, now: CPU 612, GPU 6082 (MiB)
[08/10/2023-11:16:19] [I] Start parsing network model
[08/10/2023-11:16:19] [I] [TRT] ----------------------------------------------------------------
[08/10/2023-11:16:19] [I] [TRT] Input filename:   model/rpn_centerhead_sim.onnx
[08/10/2023-11:16:19] [I] [TRT] ONNX IR version:  0.0.6
[08/10/2023-11:16:19] [I] [TRT] Opset version:    11
[08/10/2023-11:16:19] [I] [TRT] Producer name:    pytorch
[08/10/2023-11:16:19] [I] [TRT] Producer version: 1.11.0
[08/10/2023-11:16:19] [I] [TRT] Domain:           
[08/10/2023-11:16:19] [I] [TRT] Model version:    0
[08/10/2023-11:16:19] [I] [TRT] Doc string:       
[08/10/2023-11:16:19] [I] [TRT] ----------------------------------------------------------------
[08/10/2023-11:16:20] [V] [TRT] Plugin creator already registered - ::GridAnchor_TRT version 1
[08/10/2023-11:16:20] [V] [TRT] Plugin creator already registered - ::GridAnchorRect_TRT version 1
[08/10/2023-11:16:20] [V] [TRT] Plugin creator already registered - ::NMS_TRT version 1
[08/10/2023-11:16:20] [V] [TRT] Plugin creator already registered - ::Reorg_TRT version 1
[08/10/2023-11:16:20] [V] [TRT] Plugin creator already registered - ::Region_TRT version 1
[08/10/2023-11:16:20] [V] [TRT] Plugin creator already registered - ::Clip_TRT version 1
[08/10/2023-11:16:20] [V] [TRT] Plugin creator already registered - ::LReLU_TRT version 1
[08/10/2023-11:16:20] [V] [TRT] Plugin creator already registered - ::PriorBox_TRT version 1
[08/10/2023-11:16:20] [V] [TRT] Plugin creator already registered - ::Normalize_TRT version 1
[08/10/2023-11:16:20] [V] [TRT] Plugin creator already registered - ::ScatterND version 1
[08/10/2023-11:16:20] [V] [TRT] Plugin creator already registered - ::RPROI_TRT version 1
[08/10/2023-11:16:20] [V] [TRT] Plugin creator already registered - ::BatchedNMS_TRT version 1
[08/10/2023-11:16:20] [V] [TRT] Plugin creator already registered - ::BatchedNMSDynamic_TRT version 1
[08/10/2023-11:16:20] [V] [TRT] Plugin creator already registered - ::BatchTilePlugin_TRT version 1
[08/10/2023-11:16:20] [V] [TRT] Plugin creator already registered - ::FlattenConcat_TRT version 1
[08/10/2023-11:16:20] [V] [TRT] Plugin creator already registered - ::CropAndResize version 1
[08/10/2023-11:16:20] [V] [TRT] Plugin creator already registered - ::CropAndResizeDynamic version 1
[08/10/2023-11:16:20] [V] [TRT] Plugin creator already registered - ::DetectionLayer_TRT version 1
[08/10/2023-11:16:20] [V] [TRT] Plugin creator already registered - ::EfficientNMS_TRT version 1
[08/10/2023-11:16:20] [V] [TRT] Plugin creator already registered - ::EfficientNMS_ONNX_TRT version 1
[08/10/2023-11:16:20] [V] [TRT] Plugin creator already registered - ::EfficientNMS_Explicit_TF_TRT version 1
[08/10/2023-11:16:20] [V] [TRT] Plugin creator already registered - ::EfficientNMS_Implicit_TF_TRT version 1
[08/10/2023-11:16:20] [V] [TRT] Plugin creator already registered - ::ProposalDynamic version 1
[08/10/2023-11:16:20] [V] [TRT] Plugin creator already registered - ::Proposal version 1
[08/10/2023-11:16:20] [V] [TRT] Plugin creator already registered - ::ProposalLayer_TRT version 1
[08/10/2023-11:16:20] [V] [TRT] Plugin creator already registered - ::PyramidROIAlign_TRT version 1
[08/10/2023-11:16:20] [V] [TRT] Plugin creator already registered - ::ResizeNearest_TRT version 1
[08/10/2023-11:16:20] [V] [TRT] Plugin creator already registered - ::Split version 1
[08/10/2023-11:16:20] [V] [TRT] Plugin creator already registered - ::SpecialSlice_TRT version 1
[08/10/2023-11:16:20] [V] [TRT] Plugin creator already registered - ::InstanceNormalization_TRT version 1
[08/10/2023-11:16:20] [V] [TRT] Plugin creator already registered - ::InstanceNormalization_TRT version 2
[08/10/2023-11:16:20] [V] [TRT] Plugin creator already registered - ::CoordConvAC version 1
[08/10/2023-11:16:20] [V] [TRT] Plugin creator already registered - ::DecodeBbox3DPlugin version 1
[08/10/2023-11:16:20] [V] [TRT] Plugin creator already registered - ::GenerateDetection_TRT version 1
[08/10/2023-11:16:20] [V] [TRT] Plugin creator already registered - ::MultilevelCropAndResize_TRT version 1
[08/10/2023-11:16:20] [V] [TRT] Plugin creator already registered - ::MultilevelProposeROI_TRT version 1
[08/10/2023-11:16:20] [V] [TRT] Plugin creator already registered - ::NMSDynamic_TRT version 1
[08/10/2023-11:16:20] [V] [TRT] Plugin creator already registered - ::PillarScatterPlugin version 1
[08/10/2023-11:16:20] [V] [TRT] Plugin creator already registered - ::VoxelGeneratorPlugin version 1
[08/10/2023-11:16:20] [V] [TRT] Plugin creator already registered - ::MultiscaleDeformableAttnPlugin_TRT version 1
[08/10/2023-11:16:20] [V] [TRT] Adding network input: input with dtype: float32, dimensions: (1, 256, 180, 180)
[08/10/2023-11:16:20] [V] [TRT] Registering tensor: input for ONNX tensor: input
[08/10/2023-11:16:20] [V] [TRT] Importing initializer: model.neck.deblocks.1.0.weight
[08/10/2023-11:16:20] [V] [TRT] Importing initializer: model.neck.deblocks.1.1.weight
[08/10/2023-11:16:20] [V] [TRT] Importing initializer: model.neck.deblocks.1.1.bias
[08/10/2023-11:16:20] [V] [TRT] Importing initializer: model.neck.deblocks.1.1.running_mean
[08/10/2023-11:16:20] [V] [TRT] Importing initializer: model.neck.deblocks.1.1.running_var
[08/10/2023-11:16:20] [V] [TRT] Importing initializer: model.bbox_head.tasks.0.reg.3.weight
[08/10/2023-11:16:20] [V] [TRT] Importing initializer: model.bbox_head.tasks.0.reg.3.bias
[08/10/2023-11:16:20] [V] [TRT] Importing initializer: model.bbox_head.tasks.0.height.3.weight
[08/10/2023-11:16:20] [V] [TRT] Importing initializer: model.bbox_head.tasks.0.height.3.bias
[08/10/2023-11:16:20] [V] [TRT] Importing initializer: model.bbox_head.tasks.0.dim.3.weight
[08/10/2023-11:16:20] [V] [TRT] Importing initializer: model.bbox_head.tasks.0.dim.3.bias
[08/10/2023-11:16:20] [V] [TRT] Importing initializer: model.bbox_head.tasks.0.rot.3.weight
[08/10/2023-11:16:20] [V] [TRT] Importing initializer: model.bbox_head.tasks.0.rot.3.bias
[08/10/2023-11:16:20] [V] [TRT] Importing initializer: model.bbox_head.tasks.0.vel.3.weight
[08/10/2023-11:16:20] [V] [TRT] Importing initializer: model.bbox_head.tasks.0.vel.3.bias
[08/10/2023-11:16:20] [V] [TRT] Importing initializer: model.bbox_head.tasks.0.hm.3.weight
[08/10/2023-11:16:20] [V] [TRT] Importing initializer: model.bbox_head.tasks.0.hm.3.bias
[08/10/2023-11:16:20] [V] [TRT] Importing initializer: model.bbox_head.tasks.1.reg.3.weight
[08/10/2023-11:16:20] [V] [TRT] Importing initializer: model.bbox_head.tasks.1.reg.3.bias
[08/10/2023-11:16:20] [V] [TRT] Importing initializer: model.bbox_head.tasks.1.height.3.weight
[08/10/2023-11:16:20] [V] [TRT] Importing initializer: model.bbox_head.tasks.1.height.3.bias
[08/10/2023-11:16:20] [V] [TRT] Importing initializer: model.bbox_head.tasks.1.dim.3.weight
[08/10/2023-11:16:20] [V] [TRT] Importing initializer: model.bbox_head.tasks.1.dim.3.bias
[08/10/2023-11:16:20] [V] [TRT] Importing initializer: model.bbox_head.tasks.1.rot.3.weight
[08/10/2023-11:16:20] [V] [TRT] Importing initializer: model.bbox_head.tasks.1.rot.3.bias
[08/10/2023-11:16:20] [V] [TRT] Importing initializer: model.bbox_head.tasks.1.vel.3.weight
[08/10/2023-11:16:20] [V] [TRT] Importing initializer: model.bbox_head.tasks.1.vel.3.bias
[08/10/2023-11:16:20] [V] [TRT] Importing initializer: model.bbox_head.tasks.1.hm.3.weight
[08/10/2023-11:16:20] [V] [TRT] Importing initializer: model.bbox_head.tasks.1.hm.3.bias
[08/10/2023-11:16:20] [V] [TRT] Importing initializer: model.bbox_head.tasks.2.reg.3.weight
[08/10/2023-11:16:20] [V] [TRT] Importing initializer: model.bbox_head.tasks.2.reg.3.bias
[08/10/2023-11:16:20] [V] [TRT] Importing initializer: model.bbox_head.tasks.2.height.3.weight
[08/10/2023-11:16:20] [V] [TRT] Importing initializer: model.bbox_head.tasks.2.height.3.bias
[08/10/2023-11:16:20] [V] [TRT] Importing initializer: model.bbox_head.tasks.2.dim.3.weight
[08/10/2023-11:16:20] [V] [TRT] Importing initializer: model.bbox_head.tasks.2.dim.3.bias
[08/10/2023-11:16:20] [V] [TRT] Importing initializer: model.bbox_head.tasks.2.rot.3.weight
[08/10/2023-11:16:20] [V] [TRT] Importing initializer: model.bbox_head.tasks.2.rot.3.bias
[08/10/2023-11:16:20] [V] [TRT] Importing initializer: model.bbox_head.tasks.2.vel.3.weight
[08/10/2023-11:16:20] [V] [TRT] Importing initializer: model.bbox_head.tasks.2.vel.3.bias
[08/10/2023-11:16:20] [V] [TRT] Importing initializer: model.bbox_head.tasks.2.hm.3.weight
[08/10/2023-11:16:20] [V] [TRT] Importing initializer: model.bbox_head.tasks.2.hm.3.bias
[08/10/2023-11:16:20] [V] [TRT] Importing initializer: model.bbox_head.tasks.3.reg.3.weight
[08/10/2023-11:16:20] [V] [TRT] Importing initializer: model.bbox_head.tasks.3.reg.3.bias
[08/10/2023-11:16:20] [V] [TRT] Importing initializer: model.bbox_head.tasks.3.height.3.weight
[08/10/2023-11:16:20] [V] [TRT] Importing initializer: model.bbox_head.tasks.3.height.3.bias
[08/10/2023-11:16:20] [V] [TRT] Importing initializer: model.bbox_head.tasks.3.dim.3.weight
[08/10/2023-11:16:20] [V] [TRT] Importing initializer: model.bbox_head.tasks.3.dim.3.bias
[08/10/2023-11:16:20] [V] [TRT] Importing initializer: model.bbox_head.tasks.3.rot.3.weight
[08/10/2023-11:16:20] [V] [TRT] Importing initializer: model.bbox_head.tasks.3.rot.3.bias
[08/10/2023-11:16:20] [V] [TRT] Importing initializer: model.bbox_head.tasks.3.vel.3.weight
[08/10/2023-11:16:20] [V] [TRT] Importing initializer: model.bbox_head.tasks.3.vel.3.bias
[08/10/2023-11:16:20] [V] [TRT] Importing initializer: model.bbox_head.tasks.3.hm.3.weight
[08/10/2023-11:16:20] [V] [TRT] Importing initializer: model.bbox_head.tasks.3.hm.3.bias
[08/10/2023-11:16:20] [V] [TRT] Importing initializer: model.bbox_head.tasks.4.reg.3.weight
[08/10/2023-11:16:20] [V] [TRT] Importing initializer: model.bbox_head.tasks.4.reg.3.bias
[08/10/2023-11:16:20] [V] [TRT] Importing initializer: model.bbox_head.tasks.4.height.3.weight
[08/10/2023-11:16:20] [V] [TRT] Importing initializer: model.bbox_head.tasks.4.height.3.bias
[08/10/2023-11:16:20] [V] [TRT] Importing initializer: model.bbox_head.tasks.4.dim.3.weight
[08/10/2023-11:16:20] [V] [TRT] Importing initializer: model.bbox_head.tasks.4.dim.3.bias
[08/10/2023-11:16:20] [V] [TRT] Importing initializer: model.bbox_head.tasks.4.rot.3.weight
[08/10/2023-11:16:20] [V] [TRT] Importing initializer: model.bbox_head.tasks.4.rot.3.bias
[08/10/2023-11:16:20] [V] [TRT] Importing initializer: model.bbox_head.tasks.4.vel.3.weight
[08/10/2023-11:16:20] [V] [TRT] Importing initializer: model.bbox_head.tasks.4.vel.3.bias
[08/10/2023-11:16:20] [V] [TRT] Importing initializer: model.bbox_head.tasks.4.hm.3.weight
[08/10/2023-11:16:20] [V] [TRT] Importing initializer: model.bbox_head.tasks.4.hm.3.bias
[08/10/2023-11:16:20] [V] [TRT] Importing initializer: model.bbox_head.tasks.5.reg.3.weight
[08/10/2023-11:16:20] [V] [TRT] Importing initializer: model.bbox_head.tasks.5.reg.3.bias
[08/10/2023-11:16:20] [V] [TRT] Importing initializer: model.bbox_head.tasks.5.height.3.weight
[08/10/2023-11:16:20] [V] [TRT] Importing initializer: model.bbox_head.tasks.5.height.3.bias
[08/10/2023-11:16:20] [V] [TRT] Importing initializer: model.bbox_head.tasks.5.dim.3.weight
[08/10/2023-11:16:20] [V] [TRT] Importing initializer: model.bbox_head.tasks.5.dim.3.bias
[08/10/2023-11:16:20] [V] [TRT] Importing initializer: model.bbox_head.tasks.5.rot.3.weight
[08/10/2023-11:16:20] [V] [TRT] Importing initializer: model.bbox_head.tasks.5.rot.3.bias
[08/10/2023-11:16:20] [V] [TRT] Importing initializer: model.bbox_head.tasks.5.vel.3.weight
[08/10/2023-11:16:20] [V] [TRT] Importing initializer: model.bbox_head.tasks.5.vel.3.bias
[08/10/2023-11:16:20] [V] [TRT] Importing initializer: model.bbox_head.tasks.5.hm.3.weight
[08/10/2023-11:16:20] [V] [TRT] Importing initializer: model.bbox_head.tasks.5.hm.3.bias
[08/10/2023-11:16:20] [V] [TRT] Importing initializer: onnx::Conv_797
[08/10/2023-11:16:20] [V] [TRT] Importing initializer: onnx::Conv_798
[08/10/2023-11:16:20] [V] [TRT] Importing initializer: onnx::Conv_800
[08/10/2023-11:16:20] [V] [TRT] Importing initializer: onnx::Conv_801
[08/10/2023-11:16:20] [V] [TRT] Importing initializer: onnx::Conv_803
[08/10/2023-11:16:20] [V] [TRT] Importing initializer: onnx::Conv_804
[08/10/2023-11:16:20] [V] [TRT] Importing initializer: onnx::Conv_806
[08/10/2023-11:16:20] [V] [TRT] Importing initializer: onnx::Conv_807
[08/10/2023-11:16:20] [V] [TRT] Importing initializer: onnx::Conv_809
[08/10/2023-11:16:20] [V] [TRT] Importing initializer: onnx::Conv_810
[08/10/2023-11:16:20] [V] [TRT] Importing initializer: onnx::Conv_812
[08/10/2023-11:16:20] [V] [TRT] Importing initializer: onnx::Conv_813
[08/10/2023-11:16:20] [V] [TRT] Importing initializer: onnx::Conv_815
[08/10/2023-11:16:20] [V] [TRT] Importing initializer: onnx::Conv_816
[08/10/2023-11:16:20] [V] [TRT] Importing initializer: onnx::Conv_818
[08/10/2023-11:16:20] [V] [TRT] Importing initializer: onnx::Conv_819
[08/10/2023-11:16:20] [V] [TRT] Importing initializer: onnx::Conv_821
[08/10/2023-11:16:20] [V] [TRT] Importing initializer: onnx::Conv_822
[08/10/2023-11:16:20] [V] [TRT] Importing initializer: onnx::Conv_824
[08/10/2023-11:16:20] [V] [TRT] Importing initializer: onnx::Conv_825
[08/10/2023-11:16:20] [V] [TRT] Importing initializer: onnx::Conv_827
[08/10/2023-11:16:20] [V] [TRT] Importing initializer: onnx::Conv_828
[08/10/2023-11:16:20] [V] [TRT] Importing initializer: onnx::Conv_830
[08/10/2023-11:16:20] [V] [TRT] Importing initializer: onnx::Conv_831
[08/10/2023-11:16:20] [V] [TRT] Importing initializer: onnx::Conv_833
[08/10/2023-11:16:20] [V] [TRT] Importing initializer: onnx::Conv_834
[08/10/2023-11:16:20] [V] [TRT] Importing initializer: onnx::Conv_836
[08/10/2023-11:16:20] [V] [TRT] Importing initializer: onnx::Conv_837
[08/10/2023-11:16:20] [V] [TRT] Importing initializer: onnx::Conv_839
[08/10/2023-11:16:20] [V] [TRT] Importing initializer: onnx::Conv_840
[08/10/2023-11:16:20] [V] [TRT] Importing initializer: onnx::Conv_842
[08/10/2023-11:16:20] [V] [TRT] Importing initializer: onnx::Conv_843
[08/10/2023-11:16:20] [V] [TRT] Importing initializer: onnx::Conv_845
[08/10/2023-11:16:20] [V] [TRT] Importing initializer: onnx::Conv_846
[08/10/2023-11:16:20] [V] [TRT] Importing initializer: onnx::Conv_848
[08/10/2023-11:16:20] [V] [TRT] Importing initializer: onnx::Conv_849
[08/10/2023-11:16:20] [V] [TRT] Importing initializer: onnx::Conv_851
[08/10/2023-11:16:20] [V] [TRT] Importing initializer: onnx::Conv_852
[08/10/2023-11:16:20] [V] [TRT] Importing initializer: onnx::Conv_854
[08/10/2023-11:16:20] [V] [TRT] Importing initializer: onnx::Conv_855
[08/10/2023-11:16:20] [V] [TRT] Importing initializer: onnx::Conv_857
[08/10/2023-11:16:20] [V] [TRT] Importing initializer: onnx::Conv_858
[08/10/2023-11:16:20] [V] [TRT] Importing initializer: onnx::Conv_860
[08/10/2023-11:16:20] [V] [TRT] Importing initializer: onnx::Conv_861
[08/10/2023-11:16:20] [V] [TRT] Importing initializer: onnx::Conv_863
[08/10/2023-11:16:20] [V] [TRT] Importing initializer: onnx::Conv_864
[08/10/2023-11:16:20] [V] [TRT] Importing initializer: onnx::Conv_866
[08/10/2023-11:16:20] [V] [TRT] Importing initializer: onnx::Conv_867
[08/10/2023-11:16:20] [V] [TRT] Importing initializer: onnx::Conv_869
[08/10/2023-11:16:20] [V] [TRT] Importing initializer: onnx::Conv_870
[08/10/2023-11:16:20] [V] [TRT] Importing initializer: onnx::Conv_872
[08/10/2023-11:16:20] [V] [TRT] Importing initializer: onnx::Conv_873
[08/10/2023-11:16:20] [V] [TRT] Importing initializer: onnx::Conv_875
[08/10/2023-11:16:20] [V] [TRT] Importing initializer: onnx::Conv_876
[08/10/2023-11:16:20] [V] [TRT] Importing initializer: onnx::Conv_878
[08/10/2023-11:16:20] [V] [TRT] Importing initializer: onnx::Conv_879
[08/10/2023-11:16:20] [V] [TRT] Importing initializer: onnx::Conv_881
[08/10/2023-11:16:20] [V] [TRT] Importing initializer: onnx::Conv_882
[08/10/2023-11:16:20] [V] [TRT] Importing initializer: onnx::Conv_884
[08/10/2023-11:16:20] [V] [TRT] Importing initializer: onnx::Conv_885
[08/10/2023-11:16:20] [V] [TRT] Importing initializer: onnx::Conv_887
[08/10/2023-11:16:20] [V] [TRT] Importing initializer: onnx::Conv_888
[08/10/2023-11:16:20] [V] [TRT] Importing initializer: onnx::Conv_890
[08/10/2023-11:16:20] [V] [TRT] Importing initializer: onnx::Conv_891
[08/10/2023-11:16:20] [V] [TRT] Importing initializer: onnx::Conv_893
[08/10/2023-11:16:20] [V] [TRT] Importing initializer: onnx::Conv_894
[08/10/2023-11:16:20] [V] [TRT] Importing initializer: onnx::Conv_896
[08/10/2023-11:16:20] [V] [TRT] Importing initializer: onnx::Conv_897
[08/10/2023-11:16:20] [V] [TRT] Importing initializer: onnx::Conv_899
[08/10/2023-11:16:20] [V] [TRT] Importing initializer: onnx::Conv_900
[08/10/2023-11:16:20] [V] [TRT] Importing initializer: onnx::Conv_902
[08/10/2023-11:16:20] [V] [TRT] Importing initializer: onnx::Conv_903
[08/10/2023-11:16:20] [V] [TRT] Importing initializer: onnx::Conv_905
[08/10/2023-11:16:20] [V] [TRT] Importing initializer: onnx::Conv_906
[08/10/2023-11:16:20] [V] [TRT] Importing initializer: onnx::Conv_908
[08/10/2023-11:16:20] [V] [TRT] Importing initializer: onnx::Conv_909
[08/10/2023-11:16:20] [V] [TRT] Importing initializer: onnx::Conv_911
[08/10/2023-11:16:20] [V] [TRT] Importing initializer: onnx::Conv_912
[08/10/2023-11:16:20] [V] [TRT] Importing initializer: onnx::Conv_914
[08/10/2023-11:16:20] [V] [TRT] Importing initializer: onnx::Conv_915
[08/10/2023-11:16:20] [V] [TRT] Importing initializer: onnx::Conv_917
[08/10/2023-11:16:20] [V] [TRT] Importing initializer: onnx::Conv_918
[08/10/2023-11:16:20] [V] [TRT] Importing initializer: onnx::Conv_920
[08/10/2023-11:16:20] [V] [TRT] Importing initializer: onnx::Conv_921
[08/10/2023-11:16:20] [V] [TRT] Importing initializer: onnx::Conv_923
[08/10/2023-11:16:20] [V] [TRT] Importing initializer: onnx::Conv_924
[08/10/2023-11:16:20] [V] [TRT] Importing initializer: onnx::Conv_926
[08/10/2023-11:16:20] [V] [TRT] Importing initializer: onnx::Conv_927
[08/10/2023-11:16:20] [V] [TRT] Importing initializer: onnx::Conv_929
[08/10/2023-11:16:20] [V] [TRT] Importing initializer: onnx::Conv_930
[08/10/2023-11:16:20] [V] [TRT] Importing initializer: onnx::Conv_932
[08/10/2023-11:16:20] [V] [TRT] Importing initializer: onnx::Conv_933
[08/10/2023-11:16:20] [V] [TRT] Importing initializer: onnx::Conv_935
[08/10/2023-11:16:20] [V] [TRT] Importing initializer: onnx::Conv_936
[08/10/2023-11:16:20] [V] [TRT] Importing initializer: onnx::Conv_938
[08/10/2023-11:16:20] [V] [TRT] Importing initializer: onnx::Conv_939
[08/10/2023-11:16:20] [V] [TRT] Importing initializer: onnx::Conv_941
[08/10/2023-11:16:20] [V] [TRT] Importing initializer: onnx::Conv_942
[08/10/2023-11:16:20] [V] [TRT] Importing initializer: onnx::Conv_944
[08/10/2023-11:16:20] [V] [TRT] Importing initializer: onnx::Conv_945
[08/10/2023-11:16:20] [V] [TRT] Parsing node: Conv_15 [Conv]
[08/10/2023-11:16:20] [V] [TRT] Searching for input: input
[08/10/2023-11:16:20] [V] [TRT] Searching for input: onnx::Conv_797
[08/10/2023-11:16:20] [V] [TRT] Searching for input: onnx::Conv_798
[08/10/2023-11:16:20] [V] [TRT] Conv_15 [Conv] inputs: [input -> (1, 256, 180, 180)[FLOAT]], [onnx::Conv_797 -> (128, 256, 3, 3)[FLOAT]], [onnx::Conv_798 -> (128)[FLOAT]], 
[08/10/2023-11:16:20] [V] [TRT] Convolution input dimensions: (1, 256, 180, 180)
[08/10/2023-11:16:20] [V] [TRT] Registering layer: Conv_15 for ONNX node: Conv_15
[08/10/2023-11:16:20] [V] [TRT] Using kernel: (3, 3), strides: (1, 1), prepadding: (1, 1), postpadding: (1, 1), dilations: (1, 1), numOutputs: 128
[08/10/2023-11:16:20] [V] [TRT] Convolution output dimensions: (1, 128, 180, 180)
[08/10/2023-11:16:20] [V] [TRT] Registering tensor: input.8 for ONNX tensor: input.8
[08/10/2023-11:16:20] [V] [TRT] Conv_15 [Conv] outputs: [input.8 -> (1, 128, 180, 180)[FLOAT]], 
[08/10/2023-11:16:20] [V] [TRT] Parsing node: Relu_16 [Relu]
[08/10/2023-11:16:20] [V] [TRT] Searching for input: input.8
[08/10/2023-11:16:20] [V] [TRT] Relu_16 [Relu] inputs: [input.8 -> (1, 128, 180, 180)[FLOAT]], 
[08/10/2023-11:16:20] [V] [TRT] Registering layer: Relu_16 for ONNX node: Relu_16
[08/10/2023-11:16:20] [V] [TRT] Registering tensor: input.12 for ONNX tensor: input.12
[08/10/2023-11:16:20] [V] [TRT] Relu_16 [Relu] outputs: [input.12 -> (1, 128, 180, 180)[FLOAT]], 
[08/10/2023-11:16:20] [V] [TRT] Parsing node: Conv_17 [Conv]
[08/10/2023-11:16:20] [V] [TRT] Searching for input: input.12
[08/10/2023-11:16:20] [V] [TRT] Searching for input: onnx::Conv_800
[08/10/2023-11:16:20] [V] [TRT] Searching for input: onnx::Conv_801
[08/10/2023-11:16:20] [V] [TRT] Conv_17 [Conv] inputs: [input.12 -> (1, 128, 180, 180)[FLOAT]], [onnx::Conv_800 -> (128, 128, 3, 3)[FLOAT]], [onnx::Conv_801 -> (128)[FLOAT]], 
[08/10/2023-11:16:20] [V] [TRT] Convolution input dimensions: (1, 128, 180, 180)
[08/10/2023-11:16:20] [V] [TRT] Registering layer: Conv_17 for ONNX node: Conv_17
[08/10/2023-11:16:20] [V] [TRT] Using kernel: (3, 3), strides: (1, 1), prepadding: (1, 1), postpadding: (1, 1), dilations: (1, 1), numOutputs: 128
[08/10/2023-11:16:20] [V] [TRT] Convolution output dimensions: (1, 128, 180, 180)
[08/10/2023-11:16:20] [V] [TRT] Registering tensor: input.20 for ONNX tensor: input.20
[08/10/2023-11:16:20] [V] [TRT] Conv_17 [Conv] outputs: [input.20 -> (1, 128, 180, 180)[FLOAT]], 
[08/10/2023-11:16:20] [V] [TRT] Parsing node: Relu_18 [Relu]
[08/10/2023-11:16:20] [V] [TRT] Searching for input: input.20
[08/10/2023-11:16:20] [V] [TRT] Relu_18 [Relu] inputs: [input.20 -> (1, 128, 180, 180)[FLOAT]], 
[08/10/2023-11:16:20] [V] [TRT] Registering layer: Relu_18 for ONNX node: Relu_18
[08/10/2023-11:16:20] [V] [TRT] Registering tensor: input.24 for ONNX tensor: input.24
[08/10/2023-11:16:20] [V] [TRT] Relu_18 [Relu] outputs: [input.24 -> (1, 128, 180, 180)[FLOAT]], 
[08/10/2023-11:16:20] [V] [TRT] Parsing node: Conv_19 [Conv]
[08/10/2023-11:16:20] [V] [TRT] Searching for input: input.24
[08/10/2023-11:16:20] [V] [TRT] Searching for input: onnx::Conv_803
[08/10/2023-11:16:20] [V] [TRT] Searching for input: onnx::Conv_804
[08/10/2023-11:16:20] [V] [TRT] Conv_19 [Conv] inputs: [input.24 -> (1, 128, 180, 180)[FLOAT]], [onnx::Conv_803 -> (128, 128, 3, 3)[FLOAT]], [onnx::Conv_804 -> (128)[FLOAT]], 
[08/10/2023-11:16:20] [V] [TRT] Convolution input dimensions: (1, 128, 180, 180)
[08/10/2023-11:16:20] [V] [TRT] Registering layer: Conv_19 for ONNX node: Conv_19
[08/10/2023-11:16:20] [V] [TRT] Using kernel: (3, 3), strides: (1, 1), prepadding: (1, 1), postpadding: (1, 1), dilations: (1, 1), numOutputs: 128
[08/10/2023-11:16:20] [V] [TRT] Convolution output dimensions: (1, 128, 180, 180)
[08/10/2023-11:16:20] [V] [TRT] Registering tensor: input.32 for ONNX tensor: input.32
[08/10/2023-11:16:20] [V] [TRT] Conv_19 [Conv] outputs: [input.32 -> (1, 128, 180, 180)[FLOAT]], 
[08/10/2023-11:16:20] [V] [TRT] Parsing node: Relu_20 [Relu]
[08/10/2023-11:16:20] [V] [TRT] Searching for input: input.32
[08/10/2023-11:16:20] [V] [TRT] Relu_20 [Relu] inputs: [input.32 -> (1, 128, 180, 180)[FLOAT]], 
[08/10/2023-11:16:20] [V] [TRT] Registering layer: Relu_20 for ONNX node: Relu_20
[08/10/2023-11:16:20] [V] [TRT] Registering tensor: input.36 for ONNX tensor: input.36
[08/10/2023-11:16:20] [V] [TRT] Relu_20 [Relu] outputs: [input.36 -> (1, 128, 180, 180)[FLOAT]], 
[08/10/2023-11:16:20] [V] [TRT] Parsing node: Conv_21 [Conv]
[08/10/2023-11:16:20] [V] [TRT] Searching for input: input.36
[08/10/2023-11:16:20] [V] [TRT] Searching for input: onnx::Conv_806
[08/10/2023-11:16:20] [V] [TRT] Searching for input: onnx::Conv_807
[08/10/2023-11:16:20] [V] [TRT] Conv_21 [Conv] inputs: [input.36 -> (1, 128, 180, 180)[FLOAT]], [onnx::Conv_806 -> (128, 128, 3, 3)[FLOAT]], [onnx::Conv_807 -> (128)[FLOAT]], 
[08/10/2023-11:16:20] [V] [TRT] Convolution input dimensions: (1, 128, 180, 180)
[08/10/2023-11:16:20] [V] [TRT] Registering layer: Conv_21 for ONNX node: Conv_21
[08/10/2023-11:16:20] [V] [TRT] Using kernel: (3, 3), strides: (1, 1), prepadding: (1, 1), postpadding: (1, 1), dilations: (1, 1), numOutputs: 128
[08/10/2023-11:16:20] [V] [TRT] Convolution output dimensions: (1, 128, 180, 180)
[08/10/2023-11:16:20] [V] [TRT] Registering tensor: input.44 for ONNX tensor: input.44
[08/10/2023-11:16:20] [V] [TRT] Conv_21 [Conv] outputs: [input.44 -> (1, 128, 180, 180)[FLOAT]], 
[08/10/2023-11:16:20] [V] [TRT] Parsing node: Relu_22 [Relu]
[08/10/2023-11:16:20] [V] [TRT] Searching for input: input.44
[08/10/2023-11:16:20] [V] [TRT] Relu_22 [Relu] inputs: [input.44 -> (1, 128, 180, 180)[FLOAT]], 
[08/10/2023-11:16:20] [V] [TRT] Registering layer: Relu_22 for ONNX node: Relu_22
[08/10/2023-11:16:20] [V] [TRT] Registering tensor: input.48 for ONNX tensor: input.48
[08/10/2023-11:16:20] [V] [TRT] Relu_22 [Relu] outputs: [input.48 -> (1, 128, 180, 180)[FLOAT]], 
[08/10/2023-11:16:20] [V] [TRT] Parsing node: Conv_23 [Conv]
[08/10/2023-11:16:20] [V] [TRT] Searching for input: input.48
[08/10/2023-11:16:20] [V] [TRT] Searching for input: onnx::Conv_809
[08/10/2023-11:16:20] [V] [TRT] Searching for input: onnx::Conv_810
[08/10/2023-11:16:20] [V] [TRT] Conv_23 [Conv] inputs: [input.48 -> (1, 128, 180, 180)[FLOAT]], [onnx::Conv_809 -> (128, 128, 3, 3)[FLOAT]], [onnx::Conv_810 -> (128)[FLOAT]], 
[08/10/2023-11:16:20] [V] [TRT] Convolution input dimensions: (1, 128, 180, 180)
[08/10/2023-11:16:20] [V] [TRT] Registering layer: Conv_23 for ONNX node: Conv_23
[08/10/2023-11:16:20] [V] [TRT] Using kernel: (3, 3), strides: (1, 1), prepadding: (1, 1), postpadding: (1, 1), dilations: (1, 1), numOutputs: 128
[08/10/2023-11:16:20] [V] [TRT] Convolution output dimensions: (1, 128, 180, 180)
[08/10/2023-11:16:20] [V] [TRT] Registering tensor: input.56 for ONNX tensor: input.56
[08/10/2023-11:16:20] [V] [TRT] Conv_23 [Conv] outputs: [input.56 -> (1, 128, 180, 180)[FLOAT]], 
[08/10/2023-11:16:20] [V] [TRT] Parsing node: Relu_24 [Relu]
[08/10/2023-11:16:20] [V] [TRT] Searching for input: input.56
[08/10/2023-11:16:20] [V] [TRT] Relu_24 [Relu] inputs: [input.56 -> (1, 128, 180, 180)[FLOAT]], 
[08/10/2023-11:16:20] [V] [TRT] Registering layer: Relu_24 for ONNX node: Relu_24
[08/10/2023-11:16:20] [V] [TRT] Registering tensor: input.60 for ONNX tensor: input.60
[08/10/2023-11:16:20] [V] [TRT] Relu_24 [Relu] outputs: [input.60 -> (1, 128, 180, 180)[FLOAT]], 
[08/10/2023-11:16:20] [V] [TRT] Parsing node: Conv_25 [Conv]
[08/10/2023-11:16:20] [V] [TRT] Searching for input: input.60
[08/10/2023-11:16:20] [V] [TRT] Searching for input: onnx::Conv_812
[08/10/2023-11:16:20] [V] [TRT] Searching for input: onnx::Conv_813
[08/10/2023-11:16:20] [V] [TRT] Conv_25 [Conv] inputs: [input.60 -> (1, 128, 180, 180)[FLOAT]], [onnx::Conv_812 -> (128, 128, 3, 3)[FLOAT]], [onnx::Conv_813 -> (128)[FLOAT]], 
[08/10/2023-11:16:20] [V] [TRT] Convolution input dimensions: (1, 128, 180, 180)
[08/10/2023-11:16:20] [V] [TRT] Registering layer: Conv_25 for ONNX node: Conv_25
[08/10/2023-11:16:20] [V] [TRT] Using kernel: (3, 3), strides: (1, 1), prepadding: (1, 1), postpadding: (1, 1), dilations: (1, 1), numOutputs: 128
[08/10/2023-11:16:20] [V] [TRT] Convolution output dimensions: (1, 128, 180, 180)
[08/10/2023-11:16:20] [V] [TRT] Registering tensor: input.68 for ONNX tensor: input.68
[08/10/2023-11:16:20] [V] [TRT] Conv_25 [Conv] outputs: [input.68 -> (1, 128, 180, 180)[FLOAT]], 
[08/10/2023-11:16:20] [V] [TRT] Parsing node: Relu_26 [Relu]
[08/10/2023-11:16:20] [V] [TRT] Searching for input: input.68
[08/10/2023-11:16:20] [V] [TRT] Relu_26 [Relu] inputs: [input.68 -> (1, 128, 180, 180)[FLOAT]], 
[08/10/2023-11:16:20] [V] [TRT] Registering layer: Relu_26 for ONNX node: Relu_26
[08/10/2023-11:16:20] [V] [TRT] Registering tensor: input.72 for ONNX tensor: input.72
[08/10/2023-11:16:20] [V] [TRT] Relu_26 [Relu] outputs: [input.72 -> (1, 128, 180, 180)[FLOAT]], 
[08/10/2023-11:16:20] [V] [TRT] Parsing node: Conv_27 [Conv]
[08/10/2023-11:16:20] [V] [TRT] Searching for input: input.72
[08/10/2023-11:16:20] [V] [TRT] Searching for input: onnx::Conv_815
[08/10/2023-11:16:20] [V] [TRT] Searching for input: onnx::Conv_816
[08/10/2023-11:16:20] [V] [TRT] Conv_27 [Conv] inputs: [input.72 -> (1, 128, 180, 180)[FLOAT]], [onnx::Conv_815 -> (256, 128, 1, 1)[FLOAT]], [onnx::Conv_816 -> (256)[FLOAT]], 
[08/10/2023-11:16:20] [V] [TRT] Convolution input dimensions: (1, 128, 180, 180)
[08/10/2023-11:16:20] [V] [TRT] Registering layer: Conv_27 for ONNX node: Conv_27
[08/10/2023-11:16:20] [V] [TRT] Using kernel: (1, 1), strides: (1, 1), prepadding: (0, 0), postpadding: (0, 0), dilations: (1, 1), numOutputs: 256
[08/10/2023-11:16:20] [V] [TRT] Convolution output dimensions: (1, 256, 180, 180)
[08/10/2023-11:16:20] [V] [TRT] Registering tensor: input.80 for ONNX tensor: input.80
[08/10/2023-11:16:20] [V] [TRT] Conv_27 [Conv] outputs: [input.80 -> (1, 256, 180, 180)[FLOAT]], 
[08/10/2023-11:16:20] [V] [TRT] Parsing node: Relu_28 [Relu]
[08/10/2023-11:16:20] [V] [TRT] Searching for input: input.80
[08/10/2023-11:16:20] [V] [TRT] Relu_28 [Relu] inputs: [input.80 -> (1, 256, 180, 180)[FLOAT]], 
[08/10/2023-11:16:20] [V] [TRT] Registering layer: Relu_28 for ONNX node: Relu_28
[08/10/2023-11:16:20] [V] [TRT] Registering tensor: onnx::Concat_602 for ONNX tensor: onnx::Concat_602
[08/10/2023-11:16:20] [V] [TRT] Relu_28 [Relu] outputs: [onnx::Concat_602 -> (1, 256, 180, 180)[FLOAT]], 
[08/10/2023-11:16:20] [V] [TRT] Parsing node: Conv_44 [Conv]
[08/10/2023-11:16:20] [V] [TRT] Searching for input: input.72
[08/10/2023-11:16:20] [V] [TRT] Searching for input: onnx::Conv_818
[08/10/2023-11:16:20] [V] [TRT] Searching for input: onnx::Conv_819
[08/10/2023-11:16:20] [V] [TRT] Conv_44 [Conv] inputs: [input.72 -> (1, 128, 180, 180)[FLOAT]], [onnx::Conv_818 -> (256, 128, 3, 3)[FLOAT]], [onnx::Conv_819 -> (256)[FLOAT]], 
[08/10/2023-11:16:20] [V] [TRT] Convolution input dimensions: (1, 128, 180, 180)
[08/10/2023-11:16:20] [V] [TRT] Registering layer: Conv_44 for ONNX node: Conv_44
[08/10/2023-11:16:20] [V] [TRT] Using kernel: (3, 3), strides: (2, 2), prepadding: (1, 1), postpadding: (1, 1), dilations: (1, 1), numOutputs: 256
[08/10/2023-11:16:20] [V] [TRT] Convolution output dimensions: (1, 256, 90, 90)
[08/10/2023-11:16:20] [V] [TRT] Registering tensor: input.92 for ONNX tensor: input.92
[08/10/2023-11:16:20] [V] [TRT] Conv_44 [Conv] outputs: [input.92 -> (1, 256, 90, 90)[FLOAT]], 
[08/10/2023-11:16:20] [V] [TRT] Parsing node: Relu_45 [Relu]
[08/10/2023-11:16:20] [V] [TRT] Searching for input: input.92
[08/10/2023-11:16:20] [V] [TRT] Relu_45 [Relu] inputs: [input.92 -> (1, 256, 90, 90)[FLOAT]], 
[08/10/2023-11:16:20] [V] [TRT] Registering layer: Relu_45 for ONNX node: Relu_45
[08/10/2023-11:16:20] [V] [TRT] Registering tensor: input.96 for ONNX tensor: input.96
[08/10/2023-11:16:20] [V] [TRT] Relu_45 [Relu] outputs: [input.96 -> (1, 256, 90, 90)[FLOAT]], 
[08/10/2023-11:16:20] [V] [TRT] Parsing node: Conv_46 [Conv]
[08/10/2023-11:16:20] [V] [TRT] Searching for input: input.96
[08/10/2023-11:16:20] [V] [TRT] Searching for input: onnx::Conv_821
[08/10/2023-11:16:20] [V] [TRT] Searching for input: onnx::Conv_822
[08/10/2023-11:16:20] [V] [TRT] Conv_46 [Conv] inputs: [input.96 -> (1, 256, 90, 90)[FLOAT]], [onnx::Conv_821 -> (256, 256, 3, 3)[FLOAT]], [onnx::Conv_822 -> (256)[FLOAT]], 
[08/10/2023-11:16:20] [V] [TRT] Convolution input dimensions: (1, 256, 90, 90)
[08/10/2023-11:16:20] [V] [TRT] Registering layer: Conv_46 for ONNX node: Conv_46
[08/10/2023-11:16:20] [V] [TRT] Using kernel: (3, 3), strides: (1, 1), prepadding: (1, 1), postpadding: (1, 1), dilations: (1, 1), numOutputs: 256
[08/10/2023-11:16:20] [V] [TRT] Convolution output dimensions: (1, 256, 90, 90)
[08/10/2023-11:16:20] [V] [TRT] Registering tensor: input.104 for ONNX tensor: input.104
[08/10/2023-11:16:20] [V] [TRT] Conv_46 [Conv] outputs: [input.104 -> (1, 256, 90, 90)[FLOAT]], 
[08/10/2023-11:16:20] [V] [TRT] Parsing node: Relu_47 [Relu]
[08/10/2023-11:16:20] [V] [TRT] Searching for input: input.104
[08/10/2023-11:16:20] [V] [TRT] Relu_47 [Relu] inputs: [input.104 -> (1, 256, 90, 90)[FLOAT]], 
[08/10/2023-11:16:20] [V] [TRT] Registering layer: Relu_47 for ONNX node: Relu_47
[08/10/2023-11:16:20] [V] [TRT] Registering tensor: input.108 for ONNX tensor: input.108
[08/10/2023-11:16:20] [V] [TRT] Relu_47 [Relu] outputs: [input.108 -> (1, 256, 90, 90)[FLOAT]], 
[08/10/2023-11:16:20] [V] [TRT] Parsing node: Conv_48 [Conv]
[08/10/2023-11:16:20] [V] [TRT] Searching for input: input.108
[08/10/2023-11:16:20] [V] [TRT] Searching for input: onnx::Conv_824
[08/10/2023-11:16:20] [V] [TRT] Searching for input: onnx::Conv_825
[08/10/2023-11:16:20] [V] [TRT] Conv_48 [Conv] inputs: [input.108 -> (1, 256, 90, 90)[FLOAT]], [onnx::Conv_824 -> (256, 256, 3, 3)[FLOAT]], [onnx::Conv_825 -> (256)[FLOAT]], 
[08/10/2023-11:16:20] [V] [TRT] Convolution input dimensions: (1, 256, 90, 90)
[08/10/2023-11:16:20] [V] [TRT] Registering layer: Conv_48 for ONNX node: Conv_48
[08/10/2023-11:16:20] [V] [TRT] Using kernel: (3, 3), strides: (1, 1), prepadding: (1, 1), postpadding: (1, 1), dilations: (1, 1), numOutputs: 256
[08/10/2023-11:16:20] [V] [TRT] Convolution output dimensions: (1, 256, 90, 90)
[08/10/2023-11:16:20] [V] [TRT] Registering tensor: input.116 for ONNX tensor: input.116
[08/10/2023-11:16:20] [V] [TRT] Conv_48 [Conv] outputs: [input.116 -> (1, 256, 90, 90)[FLOAT]], 
[08/10/2023-11:16:20] [V] [TRT] Parsing node: Relu_49 [Relu]
[08/10/2023-11:16:20] [V] [TRT] Searching for input: input.116
[08/10/2023-11:16:20] [V] [TRT] Relu_49 [Relu] inputs: [input.116 -> (1, 256, 90, 90)[FLOAT]], 
[08/10/2023-11:16:20] [V] [TRT] Registering layer: Relu_49 for ONNX node: Relu_49
[08/10/2023-11:16:20] [V] [TRT] Registering tensor: input.120 for ONNX tensor: input.120
[08/10/2023-11:16:20] [V] [TRT] Relu_49 [Relu] outputs: [input.120 -> (1, 256, 90, 90)[FLOAT]], 
[08/10/2023-11:16:20] [V] [TRT] Parsing node: Conv_50 [Conv]
[08/10/2023-11:16:20] [V] [TRT] Searching for input: input.120
[08/10/2023-11:16:20] [V] [TRT] Searching for input: onnx::Conv_827
[08/10/2023-11:16:20] [V] [TRT] Searching for input: onnx::Conv_828
[08/10/2023-11:16:20] [V] [TRT] Conv_50 [Conv] inputs: [input.120 -> (1, 256, 90, 90)[FLOAT]], [onnx::Conv_827 -> (256, 256, 3, 3)[FLOAT]], [onnx::Conv_828 -> (256)[FLOAT]], 
[08/10/2023-11:16:20] [V] [TRT] Convolution input dimensions: (1, 256, 90, 90)
[08/10/2023-11:16:20] [V] [TRT] Registering layer: Conv_50 for ONNX node: Conv_50
[08/10/2023-11:16:20] [V] [TRT] Using kernel: (3, 3), strides: (1, 1), prepadding: (1, 1), postpadding: (1, 1), dilations: (1, 1), numOutputs: 256
[08/10/2023-11:16:20] [V] [TRT] Convolution output dimensions: (1, 256, 90, 90)
[08/10/2023-11:16:20] [V] [TRT] Registering tensor: input.128 for ONNX tensor: input.128
[08/10/2023-11:16:20] [V] [TRT] Conv_50 [Conv] outputs: [input.128 -> (1, 256, 90, 90)[FLOAT]], 
[08/10/2023-11:16:20] [V] [TRT] Parsing node: Relu_51 [Relu]
[08/10/2023-11:16:20] [V] [TRT] Searching for input: input.128
[08/10/2023-11:16:20] [V] [TRT] Relu_51 [Relu] inputs: [input.128 -> (1, 256, 90, 90)[FLOAT]], 
[08/10/2023-11:16:20] [V] [TRT] Registering layer: Relu_51 for ONNX node: Relu_51
[08/10/2023-11:16:20] [V] [TRT] Registering tensor: input.132 for ONNX tensor: input.132
[08/10/2023-11:16:20] [V] [TRT] Relu_51 [Relu] outputs: [input.132 -> (1, 256, 90, 90)[FLOAT]], 
[08/10/2023-11:16:20] [V] [TRT] Parsing node: Conv_52 [Conv]
[08/10/2023-11:16:20] [V] [TRT] Searching for input: input.132
[08/10/2023-11:16:20] [V] [TRT] Searching for input: onnx::Conv_830
[08/10/2023-11:16:20] [V] [TRT] Searching for input: onnx::Conv_831
[08/10/2023-11:16:20] [V] [TRT] Conv_52 [Conv] inputs: [input.132 -> (1, 256, 90, 90)[FLOAT]], [onnx::Conv_830 -> (256, 256, 3, 3)[FLOAT]], [onnx::Conv_831 -> (256)[FLOAT]], 
[08/10/2023-11:16:20] [V] [TRT] Convolution input dimensions: (1, 256, 90, 90)
[08/10/2023-11:16:20] [V] [TRT] Registering layer: Conv_52 for ONNX node: Conv_52
[08/10/2023-11:16:20] [V] [TRT] Using kernel: (3, 3), strides: (1, 1), prepadding: (1, 1), postpadding: (1, 1), dilations: (1, 1), numOutputs: 256
[08/10/2023-11:16:20] [V] [TRT] Convolution output dimensions: (1, 256, 90, 90)
[08/10/2023-11:16:20] [V] [TRT] Registering tensor: input.140 for ONNX tensor: input.140
[08/10/2023-11:16:20] [V] [TRT] Conv_52 [Conv] outputs: [input.140 -> (1, 256, 90, 90)[FLOAT]], 
[08/10/2023-11:16:20] [V] [TRT] Parsing node: Relu_53 [Relu]
[08/10/2023-11:16:20] [V] [TRT] Searching for input: input.140
[08/10/2023-11:16:20] [V] [TRT] Relu_53 [Relu] inputs: [input.140 -> (1, 256, 90, 90)[FLOAT]], 
[08/10/2023-11:16:20] [V] [TRT] Registering layer: Relu_53 for ONNX node: Relu_53
[08/10/2023-11:16:20] [V] [TRT] Registering tensor: input.144 for ONNX tensor: input.144
[08/10/2023-11:16:20] [V] [TRT] Relu_53 [Relu] outputs: [input.144 -> (1, 256, 90, 90)[FLOAT]], 
[08/10/2023-11:16:20] [V] [TRT] Parsing node: Conv_54 [Conv]
[08/10/2023-11:16:20] [V] [TRT] Searching for input: input.144
[08/10/2023-11:16:20] [V] [TRT] Searching for input: onnx::Conv_833
[08/10/2023-11:16:20] [V] [TRT] Searching for input: onnx::Conv_834
[08/10/2023-11:16:20] [V] [TRT] Conv_54 [Conv] inputs: [input.144 -> (1, 256, 90, 90)[FLOAT]], [onnx::Conv_833 -> (256, 256, 3, 3)[FLOAT]], [onnx::Conv_834 -> (256)[FLOAT]], 
[08/10/2023-11:16:20] [V] [TRT] Convolution input dimensions: (1, 256, 90, 90)
[08/10/2023-11:16:20] [V] [TRT] Registering layer: Conv_54 for ONNX node: Conv_54
[08/10/2023-11:16:20] [V] [TRT] Using kernel: (3, 3), strides: (1, 1), prepadding: (1, 1), postpadding: (1, 1), dilations: (1, 1), numOutputs: 256
[08/10/2023-11:16:20] [V] [TRT] Convolution output dimensions: (1, 256, 90, 90)
[08/10/2023-11:16:20] [V] [TRT] Registering tensor: input.152 for ONNX tensor: input.152
[08/10/2023-11:16:20] [V] [TRT] Conv_54 [Conv] outputs: [input.152 -> (1, 256, 90, 90)[FLOAT]], 
[08/10/2023-11:16:20] [V] [TRT] Parsing node: Relu_55 [Relu]
[08/10/2023-11:16:20] [V] [TRT] Searching for input: input.152
[08/10/2023-11:16:20] [V] [TRT] Relu_55 [Relu] inputs: [input.152 -> (1, 256, 90, 90)[FLOAT]], 
[08/10/2023-11:16:20] [V] [TRT] Registering layer: Relu_55 for ONNX node: Relu_55
[08/10/2023-11:16:20] [V] [TRT] Registering tensor: onnx::ConvTranspose_644 for ONNX tensor: onnx::ConvTranspose_644
[08/10/2023-11:16:20] [V] [TRT] Relu_55 [Relu] outputs: [onnx::ConvTranspose_644 -> (1, 256, 90, 90)[FLOAT]], 
[08/10/2023-11:16:20] [V] [TRT] Parsing node: ConvTranspose_56 [ConvTranspose]
[08/10/2023-11:16:20] [V] [TRT] Searching for input: onnx::ConvTranspose_644
[08/10/2023-11:16:20] [V] [TRT] Searching for input: model.neck.deblocks.1.0.weight
[08/10/2023-11:16:20] [V] [TRT] ConvTranspose_56 [ConvTranspose] inputs: [onnx::ConvTranspose_644 -> (1, 256, 90, 90)[FLOAT]], [model.neck.deblocks.1.0.weight -> (256, 256, 2, 2)[FLOAT]], 
[08/10/2023-11:16:20] [V] [TRT] Running deconvolution with: 
Padding mode: NOTSET
Pre-padding: (0, 0)
Post-padding: (0, 0)
[08/10/2023-11:16:20] [V] [TRT] Registering layer: ConvTranspose_56 for ONNX node: ConvTranspose_56
[08/10/2023-11:16:20] [V] [TRT] Registering tensor: input.156 for ONNX tensor: input.156
[08/10/2023-11:16:20] [V] [TRT] ConvTranspose_56 [ConvTranspose] outputs: [input.156 -> (1, 256, 180, 180)[FLOAT]], 
[08/10/2023-11:16:20] [V] [TRT] Parsing node: BatchNormalization_57 [BatchNormalization]
[08/10/2023-11:16:20] [V] [TRT] Searching for input: input.156
[08/10/2023-11:16:20] [V] [TRT] Searching for input: model.neck.deblocks.1.1.weight
[08/10/2023-11:16:20] [V] [TRT] Searching for input: model.neck.deblocks.1.1.bias
[08/10/2023-11:16:20] [V] [TRT] Searching for input: model.neck.deblocks.1.1.running_mean
[08/10/2023-11:16:20] [V] [TRT] Searching for input: model.neck.deblocks.1.1.running_var
[08/10/2023-11:16:20] [V] [TRT] BatchNormalization_57 [BatchNormalization] inputs: [input.156 -> (1, 256, 180, 180)[FLOAT]], [model.neck.deblocks.1.1.weight -> (256)[FLOAT]], [model.neck.deblocks.1.1.bias -> (256)[FLOAT]], [model.neck.deblocks.1.1.running_mean -> (256)[FLOAT]], [model.neck.deblocks.1.1.running_var -> (256)[FLOAT]], 
[08/10/2023-11:16:20] [V] [TRT] Registering layer: BatchNormalization_57 for ONNX node: BatchNormalization_57
[08/10/2023-11:16:20] [V] [TRT] Registering tensor: input.160 for ONNX tensor: input.160
[08/10/2023-11:16:20] [V] [TRT] BatchNormalization_57 [BatchNormalization] outputs: [input.160 -> (1, 256, 180, 180)[FLOAT]], 
[08/10/2023-11:16:20] [V] [TRT] Parsing node: Relu_58 [Relu]
[08/10/2023-11:16:20] [V] [TRT] Searching for input: input.160
[08/10/2023-11:16:20] [V] [TRT] Relu_58 [Relu] inputs: [input.160 -> (1, 256, 180, 180)[FLOAT]], 
[08/10/2023-11:16:20] [V] [TRT] Registering layer: Relu_58 for ONNX node: Relu_58
[08/10/2023-11:16:20] [V] [TRT] Registering tensor: onnx::Concat_647 for ONNX tensor: onnx::Concat_647
[08/10/2023-11:16:20] [V] [TRT] Relu_58 [Relu] outputs: [onnx::Concat_647 -> (1, 256, 180, 180)[FLOAT]], 
[08/10/2023-11:16:20] [V] [TRT] Parsing node: Concat_59 [Concat]
[08/10/2023-11:16:20] [V] [TRT] Searching for input: onnx::Concat_602
[08/10/2023-11:16:20] [V] [TRT] Searching for input: onnx::Concat_647
[08/10/2023-11:16:20] [V] [TRT] Concat_59 [Concat] inputs: [onnx::Concat_602 -> (1, 256, 180, 180)[FLOAT]], [onnx::Concat_647 -> (1, 256, 180, 180)[FLOAT]], 
[08/10/2023-11:16:20] [V] [TRT] Registering layer: Concat_59 for ONNX node: Concat_59
[08/10/2023-11:16:20] [V] [TRT] Registering tensor: input.164 for ONNX tensor: input.164
[08/10/2023-11:16:20] [V] [TRT] Concat_59 [Concat] outputs: [input.164 -> (1, 512, 180, 180)[FLOAT]], 
[08/10/2023-11:16:20] [V] [TRT] Parsing node: Conv_60 [Conv]
[08/10/2023-11:16:20] [V] [TRT] Searching for input: input.164
[08/10/2023-11:16:20] [V] [TRT] Searching for input: onnx::Conv_836
[08/10/2023-11:16:20] [V] [TRT] Searching for input: onnx::Conv_837
[08/10/2023-11:16:20] [V] [TRT] Conv_60 [Conv] inputs: [input.164 -> (1, 512, 180, 180)[FLOAT]], [onnx::Conv_836 -> (64, 512, 3, 3)[FLOAT]], [onnx::Conv_837 -> (64)[FLOAT]], 
[08/10/2023-11:16:20] [V] [TRT] Convolution input dimensions: (1, 512, 180, 180)
[08/10/2023-11:16:20] [V] [TRT] Registering layer: Conv_60 for ONNX node: Conv_60
[08/10/2023-11:16:20] [V] [TRT] Using kernel: (3, 3), strides: (1, 1), prepadding: (1, 1), postpadding: (1, 1), dilations: (1, 1), numOutputs: 64
[08/10/2023-11:16:20] [V] [TRT] Convolution output dimensions: (1, 64, 180, 180)
[08/10/2023-11:16:20] [V] [TRT] Registering tensor: input.172 for ONNX tensor: input.172
[08/10/2023-11:16:20] [V] [TRT] Conv_60 [Conv] outputs: [input.172 -> (1, 64, 180, 180)[FLOAT]], 
[08/10/2023-11:16:20] [V] [TRT] Parsing node: Relu_61 [Relu]
[08/10/2023-11:16:20] [V] [TRT] Searching for input: input.172
[08/10/2023-11:16:20] [V] [TRT] Relu_61 [Relu] inputs: [input.172 -> (1, 64, 180, 180)[FLOAT]], 
[08/10/2023-11:16:20] [V] [TRT] Registering layer: Relu_61 for ONNX node: Relu_61
[08/10/2023-11:16:20] [V] [TRT] Registering tensor: onnx::Conv_651 for ONNX tensor: onnx::Conv_651
[08/10/2023-11:16:20] [V] [TRT] Relu_61 [Relu] outputs: [onnx::Conv_651 -> (1, 64, 180, 180)[FLOAT]], 
[08/10/2023-11:16:20] [V] [TRT] Parsing node: Conv_62 [Conv]
[08/10/2023-11:16:20] [V] [TRT] Searching for input: onnx::Conv_651
[08/10/2023-11:16:20] [V] [TRT] Searching for input: onnx::Conv_839
[08/10/2023-11:16:20] [V] [TRT] Searching for input: onnx::Conv_840
[08/10/2023-11:16:20] [V] [TRT] Conv_62 [Conv] inputs: [onnx::Conv_651 -> (1, 64, 180, 180)[FLOAT]], [onnx::Conv_839 -> (64, 64, 3, 3)[FLOAT]], [onnx::Conv_840 -> (64)[FLOAT]], 
[08/10/2023-11:16:20] [V] [TRT] Convolution input dimensions: (1, 64, 180, 180)
[08/10/2023-11:16:20] [V] [TRT] Registering layer: Conv_62 for ONNX node: Conv_62
[08/10/2023-11:16:20] [V] [TRT] Using kernel: (3, 3), strides: (1, 1), prepadding: (1, 1), postpadding: (1, 1), dilations: (1, 1), numOutputs: 64
[08/10/2023-11:16:20] [V] [TRT] Convolution output dimensions: (1, 64, 180, 180)
[08/10/2023-11:16:20] [V] [TRT] Registering tensor: input.180 for ONNX tensor: input.180
[08/10/2023-11:16:20] [V] [TRT] Conv_62 [Conv] outputs: [input.180 -> (1, 64, 180, 180)[FLOAT]], 
[08/10/2023-11:16:20] [V] [TRT] Parsing node: Relu_63 [Relu]
[08/10/2023-11:16:20] [V] [TRT] Searching for input: input.180
[08/10/2023-11:16:20] [V] [TRT] Relu_63 [Relu] inputs: [input.180 -> (1, 64, 180, 180)[FLOAT]], 
[08/10/2023-11:16:20] [V] [TRT] Registering layer: Relu_63 for ONNX node: Relu_63
[08/10/2023-11:16:20] [V] [TRT] Registering tensor: input.184 for ONNX tensor: input.184
[08/10/2023-11:16:20] [V] [TRT] Relu_63 [Relu] outputs: [input.184 -> (1, 64, 180, 180)[FLOAT]], 
[08/10/2023-11:16:20] [V] [TRT] Parsing node: Conv_64 [Conv]
[08/10/2023-11:16:20] [V] [TRT] Searching for input: input.184
[08/10/2023-11:16:20] [V] [TRT] Searching for input: model.bbox_head.tasks.0.reg.3.weight
[08/10/2023-11:16:20] [V] [TRT] Searching for input: model.bbox_head.tasks.0.reg.3.bias
[08/10/2023-11:16:20] [V] [TRT] Conv_64 [Conv] inputs: [input.184 -> (1, 64, 180, 180)[FLOAT]], [model.bbox_head.tasks.0.reg.3.weight -> (2, 64, 3, 3)[FLOAT]], [model.bbox_head.tasks.0.reg.3.bias -> (2)[FLOAT]], 
[08/10/2023-11:16:20] [V] [TRT] Convolution input dimensions: (1, 64, 180, 180)
[08/10/2023-11:16:20] [V] [TRT] Registering layer: Conv_64 for ONNX node: Conv_64
[08/10/2023-11:16:20] [V] [TRT] Using kernel: (3, 3), strides: (1, 1), prepadding: (1, 1), postpadding: (1, 1), dilations: (1, 1), numOutputs: 2
[08/10/2023-11:16:20] [V] [TRT] Convolution output dimensions: (1, 2, 180, 180)
[08/10/2023-11:16:20] [V] [TRT] Registering tensor: reg_0_1 for ONNX tensor: reg_0
[08/10/2023-11:16:20] [V] [TRT] Conv_64 [Conv] outputs: [reg_0 -> (1, 2, 180, 180)[FLOAT]], 
[08/10/2023-11:16:20] [V] [TRT] Parsing node: Conv_65 [Conv]
[08/10/2023-11:16:20] [V] [TRT] Searching for input: onnx::Conv_651
[08/10/2023-11:16:20] [V] [TRT] Searching for input: onnx::Conv_842
[08/10/2023-11:16:20] [V] [TRT] Searching for input: onnx::Conv_843
[08/10/2023-11:16:20] [V] [TRT] Conv_65 [Conv] inputs: [onnx::Conv_651 -> (1, 64, 180, 180)[FLOAT]], [onnx::Conv_842 -> (64, 64, 3, 3)[FLOAT]], [onnx::Conv_843 -> (64)[FLOAT]], 
[08/10/2023-11:16:20] [V] [TRT] Convolution input dimensions: (1, 64, 180, 180)
[08/10/2023-11:16:20] [V] [TRT] Registering layer: Conv_65 for ONNX node: Conv_65
[08/10/2023-11:16:20] [V] [TRT] Using kernel: (3, 3), strides: (1, 1), prepadding: (1, 1), postpadding: (1, 1), dilations: (1, 1), numOutputs: 64
[08/10/2023-11:16:20] [V] [TRT] Convolution output dimensions: (1, 64, 180, 180)
[08/10/2023-11:16:20] [V] [TRT] Registering tensor: input.192 for ONNX tensor: input.192
[08/10/2023-11:16:20] [V] [TRT] Conv_65 [Conv] outputs: [input.192 -> (1, 64, 180, 180)[FLOAT]], 
[08/10/2023-11:16:20] [V] [TRT] Parsing node: Relu_66 [Relu]
[08/10/2023-11:16:20] [V] [TRT] Searching for input: input.192
[08/10/2023-11:16:20] [V] [TRT] Relu_66 [Relu] inputs: [input.192 -> (1, 64, 180, 180)[FLOAT]], 
[08/10/2023-11:16:20] [V] [TRT] Registering layer: Relu_66 for ONNX node: Relu_66
[08/10/2023-11:16:20] [V] [TRT] Registering tensor: input.196 for ONNX tensor: input.196
[08/10/2023-11:16:20] [V] [TRT] Relu_66 [Relu] outputs: [input.196 -> (1, 64, 180, 180)[FLOAT]], 
[08/10/2023-11:16:20] [V] [TRT] Parsing node: Conv_67 [Conv]
[08/10/2023-11:16:20] [V] [TRT] Searching for input: input.196
[08/10/2023-11:16:20] [V] [TRT] Searching for input: model.bbox_head.tasks.0.height.3.weight
[08/10/2023-11:16:20] [V] [TRT] Searching for input: model.bbox_head.tasks.0.height.3.bias
[08/10/2023-11:16:20] [V] [TRT] Conv_67 [Conv] inputs: [input.196 -> (1, 64, 180, 180)[FLOAT]], [model.bbox_head.tasks.0.height.3.weight -> (1, 64, 3, 3)[FLOAT]], [model.bbox_head.tasks.0.height.3.bias -> (1)[FLOAT]], 
[08/10/2023-11:16:20] [V] [TRT] Convolution input dimensions: (1, 64, 180, 180)
[08/10/2023-11:16:20] [V] [TRT] Registering layer: Conv_67 for ONNX node: Conv_67
[08/10/2023-11:16:20] [V] [TRT] Using kernel: (3, 3), strides: (1, 1), prepadding: (1, 1), postpadding: (1, 1), dilations: (1, 1), numOutputs: 1
[08/10/2023-11:16:20] [V] [TRT] Convolution output dimensions: (1, 1, 180, 180)
[08/10/2023-11:16:20] [V] [TRT] Registering tensor: height_0_2 for ONNX tensor: height_0
[08/10/2023-11:16:20] [V] [TRT] Conv_67 [Conv] outputs: [height_0 -> (1, 1, 180, 180)[FLOAT]], 
[08/10/2023-11:16:20] [V] [TRT] Parsing node: Conv_68 [Conv]
[08/10/2023-11:16:20] [V] [TRT] Searching for input: onnx::Conv_651
[08/10/2023-11:16:20] [V] [TRT] Searching for input: onnx::Conv_845
[08/10/2023-11:16:20] [V] [TRT] Searching for input: onnx::Conv_846
[08/10/2023-11:16:20] [V] [TRT] Conv_68 [Conv] inputs: [onnx::Conv_651 -> (1, 64, 180, 180)[FLOAT]], [onnx::Conv_845 -> (64, 64, 3, 3)[FLOAT]], [onnx::Conv_846 -> (64)[FLOAT]], 
[08/10/2023-11:16:20] [V] [TRT] Convolution input dimensions: (1, 64, 180, 180)
[08/10/2023-11:16:20] [V] [TRT] Registering layer: Conv_68 for ONNX node: Conv_68
[08/10/2023-11:16:20] [V] [TRT] Using kernel: (3, 3), strides: (1, 1), prepadding: (1, 1), postpadding: (1, 1), dilations: (1, 1), numOutputs: 64
[08/10/2023-11:16:20] [V] [TRT] Convolution output dimensions: (1, 64, 180, 180)
[08/10/2023-11:16:20] [V] [TRT] Registering tensor: input.204 for ONNX tensor: input.204
[08/10/2023-11:16:20] [V] [TRT] Conv_68 [Conv] outputs: [input.204 -> (1, 64, 180, 180)[FLOAT]], 
[08/10/2023-11:16:20] [V] [TRT] Parsing node: Relu_69 [Relu]
[08/10/2023-11:16:20] [V] [TRT] Searching for input: input.204
[08/10/2023-11:16:20] [V] [TRT] Relu_69 [Relu] inputs: [input.204 -> (1, 64, 180, 180)[FLOAT]], 
[08/10/2023-11:16:20] [V] [TRT] Registering layer: Relu_69 for ONNX node: Relu_69
[08/10/2023-11:16:20] [V] [TRT] Registering tensor: input.208 for ONNX tensor: input.208
[08/10/2023-11:16:20] [V] [TRT] Relu_69 [Relu] outputs: [input.208 -> (1, 64, 180, 180)[FLOAT]], 
[08/10/2023-11:16:20] [V] [TRT] Parsing node: Conv_70 [Conv]
[08/10/2023-11:16:20] [V] [TRT] Searching for input: input.208
[08/10/2023-11:16:20] [V] [TRT] Searching for input: model.bbox_head.tasks.0.dim.3.weight
[08/10/2023-11:16:20] [V] [TRT] Searching for input: model.bbox_head.tasks.0.dim.3.bias
[08/10/2023-11:16:20] [V] [TRT] Conv_70 [Conv] inputs: [input.208 -> (1, 64, 180, 180)[FLOAT]], [model.bbox_head.tasks.0.dim.3.weight -> (3, 64, 3, 3)[FLOAT]], [model.bbox_head.tasks.0.dim.3.bias -> (3)[FLOAT]], 
[08/10/2023-11:16:20] [V] [TRT] Convolution input dimensions: (1, 64, 180, 180)
[08/10/2023-11:16:20] [V] [TRT] Registering layer: Conv_70 for ONNX node: Conv_70
[08/10/2023-11:16:20] [V] [TRT] Using kernel: (3, 3), strides: (1, 1), prepadding: (1, 1), postpadding: (1, 1), dilations: (1, 1), numOutputs: 3
[08/10/2023-11:16:20] [V] [TRT] Convolution output dimensions: (1, 3, 180, 180)
[08/10/2023-11:16:20] [V] [TRT] Registering tensor: dim_0_3 for ONNX tensor: dim_0
[08/10/2023-11:16:20] [V] [TRT] Conv_70 [Conv] outputs: [dim_0 -> (1, 3, 180, 180)[FLOAT]], 
[08/10/2023-11:16:20] [V] [TRT] Parsing node: Conv_71 [Conv]
[08/10/2023-11:16:20] [V] [TRT] Searching for input: onnx::Conv_651
[08/10/2023-11:16:20] [V] [TRT] Searching for input: onnx::Conv_848
[08/10/2023-11:16:20] [V] [TRT] Searching for input: onnx::Conv_849
[08/10/2023-11:16:20] [V] [TRT] Conv_71 [Conv] inputs: [onnx::Conv_651 -> (1, 64, 180, 180)[FLOAT]], [onnx::Conv_848 -> (64, 64, 3, 3)[FLOAT]], [onnx::Conv_849 -> (64)[FLOAT]], 
[08/10/2023-11:16:20] [V] [TRT] Convolution input dimensions: (1, 64, 180, 180)
[08/10/2023-11:16:20] [V] [TRT] Registering layer: Conv_71 for ONNX node: Conv_71
[08/10/2023-11:16:20] [V] [TRT] Using kernel: (3, 3), strides: (1, 1), prepadding: (1, 1), postpadding: (1, 1), dilations: (1, 1), numOutputs: 64
[08/10/2023-11:16:20] [V] [TRT] Convolution output dimensions: (1, 64, 180, 180)
[08/10/2023-11:16:20] [V] [TRT] Registering tensor: input.216 for ONNX tensor: input.216
[08/10/2023-11:16:20] [V] [TRT] Conv_71 [Conv] outputs: [input.216 -> (1, 64, 180, 180)[FLOAT]], 
[08/10/2023-11:16:20] [V] [TRT] Parsing node: Relu_72 [Relu]
[08/10/2023-11:16:20] [V] [TRT] Searching for input: input.216
[08/10/2023-11:16:20] [V] [TRT] Relu_72 [Relu] inputs: [input.216 -> (1, 64, 180, 180)[FLOAT]], 
[08/10/2023-11:16:20] [V] [TRT] Registering layer: Relu_72 for ONNX node: Relu_72
[08/10/2023-11:16:20] [V] [TRT] Registering tensor: input.220 for ONNX tensor: input.220
[08/10/2023-11:16:20] [V] [TRT] Relu_72 [Relu] outputs: [input.220 -> (1, 64, 180, 180)[FLOAT]], 
[08/10/2023-11:16:20] [V] [TRT] Parsing node: Conv_73 [Conv]
[08/10/2023-11:16:20] [V] [TRT] Searching for input: input.220
[08/10/2023-11:16:20] [V] [TRT] Searching for input: model.bbox_head.tasks.0.rot.3.weight
[08/10/2023-11:16:20] [V] [TRT] Searching for input: model.bbox_head.tasks.0.rot.3.bias
[08/10/2023-11:16:20] [V] [TRT] Conv_73 [Conv] inputs: [input.220 -> (1, 64, 180, 180)[FLOAT]], [model.bbox_head.tasks.0.rot.3.weight -> (2, 64, 3, 3)[FLOAT]], [model.bbox_head.tasks.0.rot.3.bias -> (2)[FLOAT]], 
[08/10/2023-11:16:20] [V] [TRT] Convolution input dimensions: (1, 64, 180, 180)
[08/10/2023-11:16:20] [V] [TRT] Registering layer: Conv_73 for ONNX node: Conv_73
[08/10/2023-11:16:20] [V] [TRT] Using kernel: (3, 3), strides: (1, 1), prepadding: (1, 1), postpadding: (1, 1), dilations: (1, 1), numOutputs: 2
[08/10/2023-11:16:20] [V] [TRT] Convolution output dimensions: (1, 2, 180, 180)
[08/10/2023-11:16:20] [V] [TRT] Registering tensor: rot_0_4 for ONNX tensor: rot_0
[08/10/2023-11:16:20] [V] [TRT] Conv_73 [Conv] outputs: [rot_0 -> (1, 2, 180, 180)[FLOAT]], 
[08/10/2023-11:16:20] [V] [TRT] Parsing node: Conv_74 [Conv]
[08/10/2023-11:16:20] [V] [TRT] Searching for input: onnx::Conv_651
[08/10/2023-11:16:20] [V] [TRT] Searching for input: onnx::Conv_851
[08/10/2023-11:16:20] [V] [TRT] Searching for input: onnx::Conv_852
[08/10/2023-11:16:20] [V] [TRT] Conv_74 [Conv] inputs: [onnx::Conv_651 -> (1, 64, 180, 180)[FLOAT]], [onnx::Conv_851 -> (64, 64, 3, 3)[FLOAT]], [onnx::Conv_852 -> (64)[FLOAT]], 
[08/10/2023-11:16:20] [V] [TRT] Convolution input dimensions: (1, 64, 180, 180)
[08/10/2023-11:16:20] [V] [TRT] Registering layer: Conv_74 for ONNX node: Conv_74
[08/10/2023-11:16:20] [V] [TRT] Using kernel: (3, 3), strides: (1, 1), prepadding: (1, 1), postpadding: (1, 1), dilations: (1, 1), numOutputs: 64
[08/10/2023-11:16:20] [V] [TRT] Convolution output dimensions: (1, 64, 180, 180)
[08/10/2023-11:16:20] [V] [TRT] Registering tensor: input.228 for ONNX tensor: input.228
[08/10/2023-11:16:20] [V] [TRT] Conv_74 [Conv] outputs: [input.228 -> (1, 64, 180, 180)[FLOAT]], 
[08/10/2023-11:16:20] [V] [TRT] Parsing node: Relu_75 [Relu]
[08/10/2023-11:16:20] [V] [TRT] Searching for input: input.228
[08/10/2023-11:16:20] [V] [TRT] Relu_75 [Relu] inputs: [input.228 -> (1, 64, 180, 180)[FLOAT]], 
[08/10/2023-11:16:20] [V] [TRT] Registering layer: Relu_75 for ONNX node: Relu_75
[08/10/2023-11:16:20] [V] [TRT] Registering tensor: input.232 for ONNX tensor: input.232
[08/10/2023-11:16:20] [V] [TRT] Relu_75 [Relu] outputs: [input.232 -> (1, 64, 180, 180)[FLOAT]], 
[08/10/2023-11:16:20] [V] [TRT] Parsing node: Conv_76 [Conv]
[08/10/2023-11:16:20] [V] [TRT] Searching for input: input.232
[08/10/2023-11:16:20] [V] [TRT] Searching for input: model.bbox_head.tasks.0.vel.3.weight
[08/10/2023-11:16:20] [V] [TRT] Searching for input: model.bbox_head.tasks.0.vel.3.bias
[08/10/2023-11:16:20] [V] [TRT] Conv_76 [Conv] inputs: [input.232 -> (1, 64, 180, 180)[FLOAT]], [model.bbox_head.tasks.0.vel.3.weight -> (2, 64, 3, 3)[FLOAT]], [model.bbox_head.tasks.0.vel.3.bias -> (2)[FLOAT]], 
[08/10/2023-11:16:20] [V] [TRT] Convolution input dimensions: (1, 64, 180, 180)
[08/10/2023-11:16:20] [V] [TRT] Registering layer: Conv_76 for ONNX node: Conv_76
[08/10/2023-11:16:20] [V] [TRT] Using kernel: (3, 3), strides: (1, 1), prepadding: (1, 1), postpadding: (1, 1), dilations: (1, 1), numOutputs: 2
[08/10/2023-11:16:20] [V] [TRT] Convolution output dimensions: (1, 2, 180, 180)
[08/10/2023-11:16:20] [V] [TRT] Registering tensor: vel_0_5 for ONNX tensor: vel_0
[08/10/2023-11:16:20] [V] [TRT] Conv_76 [Conv] outputs: [vel_0 -> (1, 2, 180, 180)[FLOAT]], 
[08/10/2023-11:16:20] [V] [TRT] Parsing node: Conv_77 [Conv]
[08/10/2023-11:16:20] [V] [TRT] Searching for input: onnx::Conv_651
[08/10/2023-11:16:20] [V] [TRT] Searching for input: onnx::Conv_854
[08/10/2023-11:16:20] [V] [TRT] Searching for input: onnx::Conv_855
[08/10/2023-11:16:20] [V] [TRT] Conv_77 [Conv] inputs: [onnx::Conv_651 -> (1, 64, 180, 180)[FLOAT]], [onnx::Conv_854 -> (64, 64, 3, 3)[FLOAT]], [onnx::Conv_855 -> (64)[FLOAT]], 
[08/10/2023-11:16:20] [V] [TRT] Convolution input dimensions: (1, 64, 180, 180)
[08/10/2023-11:16:20] [V] [TRT] Registering layer: Conv_77 for ONNX node: Conv_77
[08/10/2023-11:16:20] [V] [TRT] Using kernel: (3, 3), strides: (1, 1), prepadding: (1, 1), postpadding: (1, 1), dilations: (1, 1), numOutputs: 64
[08/10/2023-11:16:20] [V] [TRT] Convolution output dimensions: (1, 64, 180, 180)
[08/10/2023-11:16:20] [V] [TRT] Registering tensor: input.240 for ONNX tensor: input.240
[08/10/2023-11:16:20] [V] [TRT] Conv_77 [Conv] outputs: [input.240 -> (1, 64, 180, 180)[FLOAT]], 
[08/10/2023-11:16:20] [V] [TRT] Parsing node: Relu_78 [Relu]
[08/10/2023-11:16:20] [V] [TRT] Searching for input: input.240
[08/10/2023-11:16:20] [V] [TRT] Relu_78 [Relu] inputs: [input.240 -> (1, 64, 180, 180)[FLOAT]], 
[08/10/2023-11:16:20] [V] [TRT] Registering layer: Relu_78 for ONNX node: Relu_78
[08/10/2023-11:16:20] [V] [TRT] Registering tensor: input.244 for ONNX tensor: input.244
[08/10/2023-11:16:20] [V] [TRT] Relu_78 [Relu] outputs: [input.244 -> (1, 64, 180, 180)[FLOAT]], 
[08/10/2023-11:16:20] [V] [TRT] Parsing node: Conv_79 [Conv]
[08/10/2023-11:16:20] [V] [TRT] Searching for input: input.244
[08/10/2023-11:16:20] [V] [TRT] Searching for input: model.bbox_head.tasks.0.hm.3.weight
[08/10/2023-11:16:20] [V] [TRT] Searching for input: model.bbox_head.tasks.0.hm.3.bias
[08/10/2023-11:16:20] [V] [TRT] Conv_79 [Conv] inputs: [input.244 -> (1, 64, 180, 180)[FLOAT]], [model.bbox_head.tasks.0.hm.3.weight -> (1, 64, 3, 3)[FLOAT]], [model.bbox_head.tasks.0.hm.3.bias -> (1)[FLOAT]], 
[08/10/2023-11:16:20] [V] [TRT] Convolution input dimensions: (1, 64, 180, 180)
[08/10/2023-11:16:20] [V] [TRT] Registering layer: Conv_79 for ONNX node: Conv_79
[08/10/2023-11:16:20] [V] [TRT] Using kernel: (3, 3), strides: (1, 1), prepadding: (1, 1), postpadding: (1, 1), dilations: (1, 1), numOutputs: 1
[08/10/2023-11:16:20] [V] [TRT] Convolution output dimensions: (1, 1, 180, 180)
[08/10/2023-11:16:20] [V] [TRT] Registering tensor: hm_0_6 for ONNX tensor: hm_0
[08/10/2023-11:16:20] [V] [TRT] Conv_79 [Conv] outputs: [hm_0 -> (1, 1, 180, 180)[FLOAT]], 
[08/10/2023-11:16:20] [V] [TRT] Parsing node: Conv_80 [Conv]
[08/10/2023-11:16:20] [V] [TRT] Searching for input: onnx::Conv_651
[08/10/2023-11:16:20] [V] [TRT] Searching for input: onnx::Conv_857
[08/10/2023-11:16:20] [V] [TRT] Searching for input: onnx::Conv_858
[08/10/2023-11:16:20] [V] [TRT] Conv_80 [Conv] inputs: [onnx::Conv_651 -> (1, 64, 180, 180)[FLOAT]], [onnx::Conv_857 -> (64, 64, 3, 3)[FLOAT]], [onnx::Conv_858 -> (64)[FLOAT]], 
[08/10/2023-11:16:20] [V] [TRT] Convolution input dimensions: (1, 64, 180, 180)
[08/10/2023-11:16:20] [V] [TRT] Registering layer: Conv_80 for ONNX node: Conv_80
[08/10/2023-11:16:20] [V] [TRT] Using kernel: (3, 3), strides: (1, 1), prepadding: (1, 1), postpadding: (1, 1), dilations: (1, 1), numOutputs: 64
[08/10/2023-11:16:20] [V] [TRT] Convolution output dimensions: (1, 64, 180, 180)
[08/10/2023-11:16:20] [V] [TRT] Registering tensor: input.252 for ONNX tensor: input.252
[08/10/2023-11:16:20] [V] [TRT] Conv_80 [Conv] outputs: [input.252 -> (1, 64, 180, 180)[FLOAT]], 
[08/10/2023-11:16:20] [V] [TRT] Parsing node: Relu_81 [Relu]
[08/10/2023-11:16:20] [V] [TRT] Searching for input: input.252
[08/10/2023-11:16:20] [V] [TRT] Relu_81 [Relu] inputs: [input.252 -> (1, 64, 180, 180)[FLOAT]], 
[08/10/2023-11:16:20] [V] [TRT] Registering layer: Relu_81 for ONNX node: Relu_81
[08/10/2023-11:16:20] [V] [TRT] Registering tensor: input.256 for ONNX tensor: input.256
[08/10/2023-11:16:20] [V] [TRT] Relu_81 [Relu] outputs: [input.256 -> (1, 64, 180, 180)[FLOAT]], 
[08/10/2023-11:16:20] [V] [TRT] Parsing node: Conv_82 [Conv]
[08/10/2023-11:16:20] [V] [TRT] Searching for input: input.256
[08/10/2023-11:16:20] [V] [TRT] Searching for input: model.bbox_head.tasks.1.reg.3.weight
[08/10/2023-11:16:20] [V] [TRT] Searching for input: model.bbox_head.tasks.1.reg.3.bias
[08/10/2023-11:16:20] [V] [TRT] Conv_82 [Conv] inputs: [input.256 -> (1, 64, 180, 180)[FLOAT]], [model.bbox_head.tasks.1.reg.3.weight -> (2, 64, 3, 3)[FLOAT]], [model.bbox_head.tasks.1.reg.3.bias -> (2)[FLOAT]], 
[08/10/2023-11:16:20] [V] [TRT] Convolution input dimensions: (1, 64, 180, 180)
[08/10/2023-11:16:20] [V] [TRT] Registering layer: Conv_82 for ONNX node: Conv_82
[08/10/2023-11:16:20] [V] [TRT] Using kernel: (3, 3), strides: (1, 1), prepadding: (1, 1), postpadding: (1, 1), dilations: (1, 1), numOutputs: 2
[08/10/2023-11:16:20] [V] [TRT] Convolution output dimensions: (1, 2, 180, 180)
[08/10/2023-11:16:20] [V] [TRT] Registering tensor: reg_1_7 for ONNX tensor: reg_1
[08/10/2023-11:16:20] [V] [TRT] Conv_82 [Conv] outputs: [reg_1 -> (1, 2, 180, 180)[FLOAT]], 
[08/10/2023-11:16:20] [V] [TRT] Parsing node: Conv_83 [Conv]
[08/10/2023-11:16:20] [V] [TRT] Searching for input: onnx::Conv_651
[08/10/2023-11:16:20] [V] [TRT] Searching for input: onnx::Conv_860
[08/10/2023-11:16:20] [V] [TRT] Searching for input: onnx::Conv_861
[08/10/2023-11:16:20] [V] [TRT] Conv_83 [Conv] inputs: [onnx::Conv_651 -> (1, 64, 180, 180)[FLOAT]], [onnx::Conv_860 -> (64, 64, 3, 3)[FLOAT]], [onnx::Conv_861 -> (64)[FLOAT]], 
[08/10/2023-11:16:20] [V] [TRT] Convolution input dimensions: (1, 64, 180, 180)
[08/10/2023-11:16:20] [V] [TRT] Registering layer: Conv_83 for ONNX node: Conv_83
[08/10/2023-11:16:20] [V] [TRT] Using kernel: (3, 3), strides: (1, 1), prepadding: (1, 1), postpadding: (1, 1), dilations: (1, 1), numOutputs: 64
[08/10/2023-11:16:20] [V] [TRT] Convolution output dimensions: (1, 64, 180, 180)
[08/10/2023-11:16:20] [V] [TRT] Registering tensor: input.264 for ONNX tensor: input.264
[08/10/2023-11:16:20] [V] [TRT] Conv_83 [Conv] outputs: [input.264 -> (1, 64, 180, 180)[FLOAT]], 
[08/10/2023-11:16:20] [V] [TRT] Parsing node: Relu_84 [Relu]
[08/10/2023-11:16:20] [V] [TRT] Searching for input: input.264
[08/10/2023-11:16:20] [V] [TRT] Relu_84 [Relu] inputs: [input.264 -> (1, 64, 180, 180)[FLOAT]], 
[08/10/2023-11:16:20] [V] [TRT] Registering layer: Relu_84 for ONNX node: Relu_84
[08/10/2023-11:16:20] [V] [TRT] Registering tensor: input.268 for ONNX tensor: input.268
[08/10/2023-11:16:20] [V] [TRT] Relu_84 [Relu] outputs: [input.268 -> (1, 64, 180, 180)[FLOAT]], 
[08/10/2023-11:16:20] [V] [TRT] Parsing node: Conv_85 [Conv]
[08/10/2023-11:16:20] [V] [TRT] Searching for input: input.268
[08/10/2023-11:16:20] [V] [TRT] Searching for input: model.bbox_head.tasks.1.height.3.weight
[08/10/2023-11:16:20] [V] [TRT] Searching for input: model.bbox_head.tasks.1.height.3.bias
[08/10/2023-11:16:20] [V] [TRT] Conv_85 [Conv] inputs: [input.268 -> (1, 64, 180, 180)[FLOAT]], [model.bbox_head.tasks.1.height.3.weight -> (1, 64, 3, 3)[FLOAT]], [model.bbox_head.tasks.1.height.3.bias -> (1)[FLOAT]], 
[08/10/2023-11:16:20] [V] [TRT] Convolution input dimensions: (1, 64, 180, 180)
[08/10/2023-11:16:20] [V] [TRT] Registering layer: Conv_85 for ONNX node: Conv_85
[08/10/2023-11:16:20] [V] [TRT] Using kernel: (3, 3), strides: (1, 1), prepadding: (1, 1), postpadding: (1, 1), dilations: (1, 1), numOutputs: 1
[08/10/2023-11:16:20] [V] [TRT] Convolution output dimensions: (1, 1, 180, 180)
[08/10/2023-11:16:20] [V] [TRT] Registering tensor: height_1_8 for ONNX tensor: height_1
[08/10/2023-11:16:20] [V] [TRT] Conv_85 [Conv] outputs: [height_1 -> (1, 1, 180, 180)[FLOAT]], 
[08/10/2023-11:16:20] [V] [TRT] Parsing node: Conv_86 [Conv]
[08/10/2023-11:16:20] [V] [TRT] Searching for input: onnx::Conv_651
[08/10/2023-11:16:20] [V] [TRT] Searching for input: onnx::Conv_863
[08/10/2023-11:16:20] [V] [TRT] Searching for input: onnx::Conv_864
[08/10/2023-11:16:20] [V] [TRT] Conv_86 [Conv] inputs: [onnx::Conv_651 -> (1, 64, 180, 180)[FLOAT]], [onnx::Conv_863 -> (64, 64, 3, 3)[FLOAT]], [onnx::Conv_864 -> (64)[FLOAT]], 
[08/10/2023-11:16:20] [V] [TRT] Convolution input dimensions: (1, 64, 180, 180)
[08/10/2023-11:16:20] [V] [TRT] Registering layer: Conv_86 for ONNX node: Conv_86
[08/10/2023-11:16:20] [V] [TRT] Using kernel: (3, 3), strides: (1, 1), prepadding: (1, 1), postpadding: (1, 1), dilations: (1, 1), numOutputs: 64
[08/10/2023-11:16:20] [V] [TRT] Convolution output dimensions: (1, 64, 180, 180)
[08/10/2023-11:16:20] [V] [TRT] Registering tensor: input.276 for ONNX tensor: input.276
[08/10/2023-11:16:20] [V] [TRT] Conv_86 [Conv] outputs: [input.276 -> (1, 64, 180, 180)[FLOAT]], 
[08/10/2023-11:16:20] [V] [TRT] Parsing node: Relu_87 [Relu]
[08/10/2023-11:16:20] [V] [TRT] Searching for input: input.276
[08/10/2023-11:16:20] [V] [TRT] Relu_87 [Relu] inputs: [input.276 -> (1, 64, 180, 180)[FLOAT]], 
[08/10/2023-11:16:20] [V] [TRT] Registering layer: Relu_87 for ONNX node: Relu_87
[08/10/2023-11:16:20] [V] [TRT] Registering tensor: input.280 for ONNX tensor: input.280
[08/10/2023-11:16:20] [V] [TRT] Relu_87 [Relu] outputs: [input.280 -> (1, 64, 180, 180)[FLOAT]], 
[08/10/2023-11:16:20] [V] [TRT] Parsing node: Conv_88 [Conv]
[08/10/2023-11:16:20] [V] [TRT] Searching for input: input.280
[08/10/2023-11:16:20] [V] [TRT] Searching for input: model.bbox_head.tasks.1.dim.3.weight
[08/10/2023-11:16:20] [V] [TRT] Searching for input: model.bbox_head.tasks.1.dim.3.bias
[08/10/2023-11:16:20] [V] [TRT] Conv_88 [Conv] inputs: [input.280 -> (1, 64, 180, 180)[FLOAT]], [model.bbox_head.tasks.1.dim.3.weight -> (3, 64, 3, 3)[FLOAT]], [model.bbox_head.tasks.1.dim.3.bias -> (3)[FLOAT]], 
[08/10/2023-11:16:20] [V] [TRT] Convolution input dimensions: (1, 64, 180, 180)
[08/10/2023-11:16:20] [V] [TRT] Registering layer: Conv_88 for ONNX node: Conv_88
[08/10/2023-11:16:20] [V] [TRT] Using kernel: (3, 3), strides: (1, 1), prepadding: (1, 1), postpadding: (1, 1), dilations: (1, 1), numOutputs: 3
[08/10/2023-11:16:20] [V] [TRT] Convolution output dimensions: (1, 3, 180, 180)
[08/10/2023-11:16:20] [V] [TRT] Registering tensor: dim_1_9 for ONNX tensor: dim_1
[08/10/2023-11:16:20] [V] [TRT] Conv_88 [Conv] outputs: [dim_1 -> (1, 3, 180, 180)[FLOAT]], 
[08/10/2023-11:16:20] [V] [TRT] Parsing node: Conv_89 [Conv]
[08/10/2023-11:16:20] [V] [TRT] Searching for input: onnx::Conv_651
[08/10/2023-11:16:20] [V] [TRT] Searching for input: onnx::Conv_866
[08/10/2023-11:16:20] [V] [TRT] Searching for input: onnx::Conv_867
[08/10/2023-11:16:20] [V] [TRT] Conv_89 [Conv] inputs: [onnx::Conv_651 -> (1, 64, 180, 180)[FLOAT]], [onnx::Conv_866 -> (64, 64, 3, 3)[FLOAT]], [onnx::Conv_867 -> (64)[FLOAT]], 
[08/10/2023-11:16:20] [V] [TRT] Convolution input dimensions: (1, 64, 180, 180)
[08/10/2023-11:16:20] [V] [TRT] Registering layer: Conv_89 for ONNX node: Conv_89
[08/10/2023-11:16:20] [V] [TRT] Using kernel: (3, 3), strides: (1, 1), prepadding: (1, 1), postpadding: (1, 1), dilations: (1, 1), numOutputs: 64
[08/10/2023-11:16:20] [V] [TRT] Convolution output dimensions: (1, 64, 180, 180)
[08/10/2023-11:16:20] [V] [TRT] Registering tensor: input.288 for ONNX tensor: input.288
[08/10/2023-11:16:20] [V] [TRT] Conv_89 [Conv] outputs: [input.288 -> (1, 64, 180, 180)[FLOAT]], 
[08/10/2023-11:16:20] [V] [TRT] Parsing node: Relu_90 [Relu]
[08/10/2023-11:16:20] [V] [TRT] Searching for input: input.288
[08/10/2023-11:16:20] [V] [TRT] Relu_90 [Relu] inputs: [input.288 -> (1, 64, 180, 180)[FLOAT]], 
[08/10/2023-11:16:20] [V] [TRT] Registering layer: Relu_90 for ONNX node: Relu_90
[08/10/2023-11:16:20] [V] [TRT] Registering tensor: input.292 for ONNX tensor: input.292
[08/10/2023-11:16:20] [V] [TRT] Relu_90 [Relu] outputs: [input.292 -> (1, 64, 180, 180)[FLOAT]], 
[08/10/2023-11:16:20] [V] [TRT] Parsing node: Conv_91 [Conv]
[08/10/2023-11:16:20] [V] [TRT] Searching for input: input.292
[08/10/2023-11:16:20] [V] [TRT] Searching for input: model.bbox_head.tasks.1.rot.3.weight
[08/10/2023-11:16:20] [V] [TRT] Searching for input: model.bbox_head.tasks.1.rot.3.bias
[08/10/2023-11:16:20] [V] [TRT] Conv_91 [Conv] inputs: [input.292 -> (1, 64, 180, 180)[FLOAT]], [model.bbox_head.tasks.1.rot.3.weight -> (2, 64, 3, 3)[FLOAT]], [model.bbox_head.tasks.1.rot.3.bias -> (2)[FLOAT]], 
[08/10/2023-11:16:20] [V] [TRT] Convolution input dimensions: (1, 64, 180, 180)
[08/10/2023-11:16:20] [V] [TRT] Registering layer: Conv_91 for ONNX node: Conv_91
[08/10/2023-11:16:20] [V] [TRT] Using kernel: (3, 3), strides: (1, 1), prepadding: (1, 1), postpadding: (1, 1), dilations: (1, 1), numOutputs: 2
[08/10/2023-11:16:20] [V] [TRT] Convolution output dimensions: (1, 2, 180, 180)
[08/10/2023-11:16:20] [V] [TRT] Registering tensor: rot_1_10 for ONNX tensor: rot_1
[08/10/2023-11:16:20] [V] [TRT] Conv_91 [Conv] outputs: [rot_1 -> (1, 2, 180, 180)[FLOAT]], 
[08/10/2023-11:16:20] [V] [TRT] Parsing node: Conv_92 [Conv]
[08/10/2023-11:16:20] [V] [TRT] Searching for input: onnx::Conv_651
[08/10/2023-11:16:20] [V] [TRT] Searching for input: onnx::Conv_869
[08/10/2023-11:16:20] [V] [TRT] Searching for input: onnx::Conv_870
[08/10/2023-11:16:20] [V] [TRT] Conv_92 [Conv] inputs: [onnx::Conv_651 -> (1, 64, 180, 180)[FLOAT]], [onnx::Conv_869 -> (64, 64, 3, 3)[FLOAT]], [onnx::Conv_870 -> (64)[FLOAT]], 
[08/10/2023-11:16:20] [V] [TRT] Convolution input dimensions: (1, 64, 180, 180)
[08/10/2023-11:16:20] [V] [TRT] Registering layer: Conv_92 for ONNX node: Conv_92
[08/10/2023-11:16:20] [V] [TRT] Using kernel: (3, 3), strides: (1, 1), prepadding: (1, 1), postpadding: (1, 1), dilations: (1, 1), numOutputs: 64
[08/10/2023-11:16:20] [V] [TRT] Convolution output dimensions: (1, 64, 180, 180)
[08/10/2023-11:16:20] [V] [TRT] Registering tensor: input.300 for ONNX tensor: input.300
[08/10/2023-11:16:20] [V] [TRT] Conv_92 [Conv] outputs: [input.300 -> (1, 64, 180, 180)[FLOAT]], 
[08/10/2023-11:16:20] [V] [TRT] Parsing node: Relu_93 [Relu]
[08/10/2023-11:16:20] [V] [TRT] Searching for input: input.300
[08/10/2023-11:16:20] [V] [TRT] Relu_93 [Relu] inputs: [input.300 -> (1, 64, 180, 180)[FLOAT]], 
[08/10/2023-11:16:20] [V] [TRT] Registering layer: Relu_93 for ONNX node: Relu_93
[08/10/2023-11:16:20] [V] [TRT] Registering tensor: input.304 for ONNX tensor: input.304
[08/10/2023-11:16:20] [V] [TRT] Relu_93 [Relu] outputs: [input.304 -> (1, 64, 180, 180)[FLOAT]], 
[08/10/2023-11:16:20] [V] [TRT] Parsing node: Conv_94 [Conv]
[08/10/2023-11:16:20] [V] [TRT] Searching for input: input.304
[08/10/2023-11:16:20] [V] [TRT] Searching for input: model.bbox_head.tasks.1.vel.3.weight
[08/10/2023-11:16:20] [V] [TRT] Searching for input: model.bbox_head.tasks.1.vel.3.bias
[08/10/2023-11:16:20] [V] [TRT] Conv_94 [Conv] inputs: [input.304 -> (1, 64, 180, 180)[FLOAT]], [model.bbox_head.tasks.1.vel.3.weight -> (2, 64, 3, 3)[FLOAT]], [model.bbox_head.tasks.1.vel.3.bias -> (2)[FLOAT]], 
[08/10/2023-11:16:20] [V] [TRT] Convolution input dimensions: (1, 64, 180, 180)
[08/10/2023-11:16:20] [V] [TRT] Registering layer: Conv_94 for ONNX node: Conv_94
[08/10/2023-11:16:20] [V] [TRT] Using kernel: (3, 3), strides: (1, 1), prepadding: (1, 1), postpadding: (1, 1), dilations: (1, 1), numOutputs: 2
[08/10/2023-11:16:20] [V] [TRT] Convolution output dimensions: (1, 2, 180, 180)
[08/10/2023-11:16:20] [V] [TRT] Registering tensor: vel_1_11 for ONNX tensor: vel_1
[08/10/2023-11:16:20] [V] [TRT] Conv_94 [Conv] outputs: [vel_1 -> (1, 2, 180, 180)[FLOAT]], 
[08/10/2023-11:16:20] [V] [TRT] Parsing node: Conv_95 [Conv]
[08/10/2023-11:16:20] [V] [TRT] Searching for input: onnx::Conv_651
[08/10/2023-11:16:20] [V] [TRT] Searching for input: onnx::Conv_872
[08/10/2023-11:16:20] [V] [TRT] Searching for input: onnx::Conv_873
[08/10/2023-11:16:20] [V] [TRT] Conv_95 [Conv] inputs: [onnx::Conv_651 -> (1, 64, 180, 180)[FLOAT]], [onnx::Conv_872 -> (64, 64, 3, 3)[FLOAT]], [onnx::Conv_873 -> (64)[FLOAT]], 
[08/10/2023-11:16:20] [V] [TRT] Convolution input dimensions: (1, 64, 180, 180)
[08/10/2023-11:16:20] [V] [TRT] Registering layer: Conv_95 for ONNX node: Conv_95
[08/10/2023-11:16:20] [V] [TRT] Using kernel: (3, 3), strides: (1, 1), prepadding: (1, 1), postpadding: (1, 1), dilations: (1, 1), numOutputs: 64
[08/10/2023-11:16:20] [V] [TRT] Convolution output dimensions: (1, 64, 180, 180)
[08/10/2023-11:16:20] [V] [TRT] Registering tensor: input.312 for ONNX tensor: input.312
[08/10/2023-11:16:20] [V] [TRT] Conv_95 [Conv] outputs: [input.312 -> (1, 64, 180, 180)[FLOAT]], 
[08/10/2023-11:16:20] [V] [TRT] Parsing node: Relu_96 [Relu]
[08/10/2023-11:16:20] [V] [TRT] Searching for input: input.312
[08/10/2023-11:16:20] [V] [TRT] Relu_96 [Relu] inputs: [input.312 -> (1, 64, 180, 180)[FLOAT]], 
[08/10/2023-11:16:20] [V] [TRT] Registering layer: Relu_96 for ONNX node: Relu_96
[08/10/2023-11:16:20] [V] [TRT] Registering tensor: input.316 for ONNX tensor: input.316
[08/10/2023-11:16:20] [V] [TRT] Relu_96 [Relu] outputs: [input.316 -> (1, 64, 180, 180)[FLOAT]], 
[08/10/2023-11:16:20] [V] [TRT] Parsing node: Conv_97 [Conv]
[08/10/2023-11:16:20] [V] [TRT] Searching for input: input.316
[08/10/2023-11:16:20] [V] [TRT] Searching for input: model.bbox_head.tasks.1.hm.3.weight
[08/10/2023-11:16:20] [V] [TRT] Searching for input: model.bbox_head.tasks.1.hm.3.bias
[08/10/2023-11:16:20] [V] [TRT] Conv_97 [Conv] inputs: [input.316 -> (1, 64, 180, 180)[FLOAT]], [model.bbox_head.tasks.1.hm.3.weight -> (2, 64, 3, 3)[FLOAT]], [model.bbox_head.tasks.1.hm.3.bias -> (2)[FLOAT]], 
[08/10/2023-11:16:20] [V] [TRT] Convolution input dimensions: (1, 64, 180, 180)
[08/10/2023-11:16:20] [V] [TRT] Registering layer: Conv_97 for ONNX node: Conv_97
[08/10/2023-11:16:20] [V] [TRT] Using kernel: (3, 3), strides: (1, 1), prepadding: (1, 1), postpadding: (1, 1), dilations: (1, 1), numOutputs: 2
[08/10/2023-11:16:20] [V] [TRT] Convolution output dimensions: (1, 2, 180, 180)
[08/10/2023-11:16:20] [V] [TRT] Registering tensor: hm_1_12 for ONNX tensor: hm_1
[08/10/2023-11:16:20] [V] [TRT] Conv_97 [Conv] outputs: [hm_1 -> (1, 2, 180, 180)[FLOAT]], 
[08/10/2023-11:16:20] [V] [TRT] Parsing node: Conv_98 [Conv]
[08/10/2023-11:16:20] [V] [TRT] Searching for input: onnx::Conv_651
[08/10/2023-11:16:20] [V] [TRT] Searching for input: onnx::Conv_875
[08/10/2023-11:16:20] [V] [TRT] Searching for input: onnx::Conv_876
[08/10/2023-11:16:20] [V] [TRT] Conv_98 [Conv] inputs: [onnx::Conv_651 -> (1, 64, 180, 180)[FLOAT]], [onnx::Conv_875 -> (64, 64, 3, 3)[FLOAT]], [onnx::Conv_876 -> (64)[FLOAT]], 
[08/10/2023-11:16:20] [V] [TRT] Convolution input dimensions: (1, 64, 180, 180)
[08/10/2023-11:16:20] [V] [TRT] Registering layer: Conv_98 for ONNX node: Conv_98
[08/10/2023-11:16:20] [V] [TRT] Using kernel: (3, 3), strides: (1, 1), prepadding: (1, 1), postpadding: (1, 1), dilations: (1, 1), numOutputs: 64
[08/10/2023-11:16:20] [V] [TRT] Convolution output dimensions: (1, 64, 180, 180)
[08/10/2023-11:16:20] [V] [TRT] Registering tensor: input.324 for ONNX tensor: input.324
[08/10/2023-11:16:20] [V] [TRT] Conv_98 [Conv] outputs: [input.324 -> (1, 64, 180, 180)[FLOAT]], 
[08/10/2023-11:16:20] [V] [TRT] Parsing node: Relu_99 [Relu]
[08/10/2023-11:16:20] [V] [TRT] Searching for input: input.324
[08/10/2023-11:16:20] [V] [TRT] Relu_99 [Relu] inputs: [input.324 -> (1, 64, 180, 180)[FLOAT]], 
[08/10/2023-11:16:20] [V] [TRT] Registering layer: Relu_99 for ONNX node: Relu_99
[08/10/2023-11:16:20] [V] [TRT] Registering tensor: input.328 for ONNX tensor: input.328
[08/10/2023-11:16:20] [V] [TRT] Relu_99 [Relu] outputs: [input.328 -> (1, 64, 180, 180)[FLOAT]], 
[08/10/2023-11:16:20] [V] [TRT] Parsing node: Conv_100 [Conv]
[08/10/2023-11:16:20] [V] [TRT] Searching for input: input.328
[08/10/2023-11:16:20] [V] [TRT] Searching for input: model.bbox_head.tasks.2.reg.3.weight
[08/10/2023-11:16:20] [V] [TRT] Searching for input: model.bbox_head.tasks.2.reg.3.bias
[08/10/2023-11:16:20] [V] [TRT] Conv_100 [Conv] inputs: [input.328 -> (1, 64, 180, 180)[FLOAT]], [model.bbox_head.tasks.2.reg.3.weight -> (2, 64, 3, 3)[FLOAT]], [model.bbox_head.tasks.2.reg.3.bias -> (2)[FLOAT]], 
[08/10/2023-11:16:20] [V] [TRT] Convolution input dimensions: (1, 64, 180, 180)
[08/10/2023-11:16:20] [V] [TRT] Registering layer: Conv_100 for ONNX node: Conv_100
[08/10/2023-11:16:20] [V] [TRT] Using kernel: (3, 3), strides: (1, 1), prepadding: (1, 1), postpadding: (1, 1), dilations: (1, 1), numOutputs: 2
[08/10/2023-11:16:20] [V] [TRT] Convolution output dimensions: (1, 2, 180, 180)
[08/10/2023-11:16:20] [V] [TRT] Registering tensor: reg_2_13 for ONNX tensor: reg_2
[08/10/2023-11:16:20] [V] [TRT] Conv_100 [Conv] outputs: [reg_2 -> (1, 2, 180, 180)[FLOAT]], 
[08/10/2023-11:16:20] [V] [TRT] Parsing node: Conv_101 [Conv]
[08/10/2023-11:16:20] [V] [TRT] Searching for input: onnx::Conv_651
[08/10/2023-11:16:20] [V] [TRT] Searching for input: onnx::Conv_878
[08/10/2023-11:16:20] [V] [TRT] Searching for input: onnx::Conv_879
[08/10/2023-11:16:20] [V] [TRT] Conv_101 [Conv] inputs: [onnx::Conv_651 -> (1, 64, 180, 180)[FLOAT]], [onnx::Conv_878 -> (64, 64, 3, 3)[FLOAT]], [onnx::Conv_879 -> (64)[FLOAT]], 
[08/10/2023-11:16:20] [V] [TRT] Convolution input dimensions: (1, 64, 180, 180)
[08/10/2023-11:16:20] [V] [TRT] Registering layer: Conv_101 for ONNX node: Conv_101
[08/10/2023-11:16:20] [V] [TRT] Using kernel: (3, 3), strides: (1, 1), prepadding: (1, 1), postpadding: (1, 1), dilations: (1, 1), numOutputs: 64
[08/10/2023-11:16:20] [V] [TRT] Convolution output dimensions: (1, 64, 180, 180)
[08/10/2023-11:16:20] [V] [TRT] Registering tensor: input.336 for ONNX tensor: input.336
[08/10/2023-11:16:20] [V] [TRT] Conv_101 [Conv] outputs: [input.336 -> (1, 64, 180, 180)[FLOAT]], 
[08/10/2023-11:16:20] [V] [TRT] Parsing node: Relu_102 [Relu]
[08/10/2023-11:16:20] [V] [TRT] Searching for input: input.336
[08/10/2023-11:16:20] [V] [TRT] Relu_102 [Relu] inputs: [input.336 -> (1, 64, 180, 180)[FLOAT]], 
[08/10/2023-11:16:20] [V] [TRT] Registering layer: Relu_102 for ONNX node: Relu_102
[08/10/2023-11:16:20] [V] [TRT] Registering tensor: input.340 for ONNX tensor: input.340
[08/10/2023-11:16:20] [V] [TRT] Relu_102 [Relu] outputs: [input.340 -> (1, 64, 180, 180)[FLOAT]], 
[08/10/2023-11:16:20] [V] [TRT] Parsing node: Conv_103 [Conv]
[08/10/2023-11:16:20] [V] [TRT] Searching for input: input.340
[08/10/2023-11:16:20] [V] [TRT] Searching for input: model.bbox_head.tasks.2.height.3.weight
[08/10/2023-11:16:20] [V] [TRT] Searching for input: model.bbox_head.tasks.2.height.3.bias
[08/10/2023-11:16:20] [V] [TRT] Conv_103 [Conv] inputs: [input.340 -> (1, 64, 180, 180)[FLOAT]], [model.bbox_head.tasks.2.height.3.weight -> (1, 64, 3, 3)[FLOAT]], [model.bbox_head.tasks.2.height.3.bias -> (1)[FLOAT]], 
[08/10/2023-11:16:20] [V] [TRT] Convolution input dimensions: (1, 64, 180, 180)
[08/10/2023-11:16:20] [V] [TRT] Registering layer: Conv_103 for ONNX node: Conv_103
[08/10/2023-11:16:20] [V] [TRT] Using kernel: (3, 3), strides: (1, 1), prepadding: (1, 1), postpadding: (1, 1), dilations: (1, 1), numOutputs: 1
[08/10/2023-11:16:20] [V] [TRT] Convolution output dimensions: (1, 1, 180, 180)
[08/10/2023-11:16:20] [V] [TRT] Registering tensor: height_2_14 for ONNX tensor: height_2
[08/10/2023-11:16:20] [V] [TRT] Conv_103 [Conv] outputs: [height_2 -> (1, 1, 180, 180)[FLOAT]], 
[08/10/2023-11:16:20] [V] [TRT] Parsing node: Conv_104 [Conv]
[08/10/2023-11:16:20] [V] [TRT] Searching for input: onnx::Conv_651
[08/10/2023-11:16:20] [V] [TRT] Searching for input: onnx::Conv_881
[08/10/2023-11:16:20] [V] [TRT] Searching for input: onnx::Conv_882
[08/10/2023-11:16:20] [V] [TRT] Conv_104 [Conv] inputs: [onnx::Conv_651 -> (1, 64, 180, 180)[FLOAT]], [onnx::Conv_881 -> (64, 64, 3, 3)[FLOAT]], [onnx::Conv_882 -> (64)[FLOAT]], 
[08/10/2023-11:16:20] [V] [TRT] Convolution input dimensions: (1, 64, 180, 180)
[08/10/2023-11:16:20] [V] [TRT] Registering layer: Conv_104 for ONNX node: Conv_104
[08/10/2023-11:16:20] [V] [TRT] Using kernel: (3, 3), strides: (1, 1), prepadding: (1, 1), postpadding: (1, 1), dilations: (1, 1), numOutputs: 64
[08/10/2023-11:16:20] [V] [TRT] Convolution output dimensions: (1, 64, 180, 180)
[08/10/2023-11:16:20] [V] [TRT] Registering tensor: input.348 for ONNX tensor: input.348
[08/10/2023-11:16:20] [V] [TRT] Conv_104 [Conv] outputs: [input.348 -> (1, 64, 180, 180)[FLOAT]], 
[08/10/2023-11:16:20] [V] [TRT] Parsing node: Relu_105 [Relu]
[08/10/2023-11:16:20] [V] [TRT] Searching for input: input.348
[08/10/2023-11:16:20] [V] [TRT] Relu_105 [Relu] inputs: [input.348 -> (1, 64, 180, 180)[FLOAT]], 
[08/10/2023-11:16:20] [V] [TRT] Registering layer: Relu_105 for ONNX node: Relu_105
[08/10/2023-11:16:20] [V] [TRT] Registering tensor: input.352 for ONNX tensor: input.352
[08/10/2023-11:16:20] [V] [TRT] Relu_105 [Relu] outputs: [input.352 -> (1, 64, 180, 180)[FLOAT]], 
[08/10/2023-11:16:20] [V] [TRT] Parsing node: Conv_106 [Conv]
[08/10/2023-11:16:20] [V] [TRT] Searching for input: input.352
[08/10/2023-11:16:20] [V] [TRT] Searching for input: model.bbox_head.tasks.2.dim.3.weight
[08/10/2023-11:16:20] [V] [TRT] Searching for input: model.bbox_head.tasks.2.dim.3.bias
[08/10/2023-11:16:20] [V] [TRT] Conv_106 [Conv] inputs: [input.352 -> (1, 64, 180, 180)[FLOAT]], [model.bbox_head.tasks.2.dim.3.weight -> (3, 64, 3, 3)[FLOAT]], [model.bbox_head.tasks.2.dim.3.bias -> (3)[FLOAT]], 
[08/10/2023-11:16:20] [V] [TRT] Convolution input dimensions: (1, 64, 180, 180)
[08/10/2023-11:16:20] [V] [TRT] Registering layer: Conv_106 for ONNX node: Conv_106
[08/10/2023-11:16:20] [V] [TRT] Using kernel: (3, 3), strides: (1, 1), prepadding: (1, 1), postpadding: (1, 1), dilations: (1, 1), numOutputs: 3
[08/10/2023-11:16:20] [V] [TRT] Convolution output dimensions: (1, 3, 180, 180)
[08/10/2023-11:16:20] [V] [TRT] Registering tensor: dim_2_15 for ONNX tensor: dim_2
[08/10/2023-11:16:20] [V] [TRT] Conv_106 [Conv] outputs: [dim_2 -> (1, 3, 180, 180)[FLOAT]], 
[08/10/2023-11:16:20] [V] [TRT] Parsing node: Conv_107 [Conv]
[08/10/2023-11:16:20] [V] [TRT] Searching for input: onnx::Conv_651
[08/10/2023-11:16:20] [V] [TRT] Searching for input: onnx::Conv_884
[08/10/2023-11:16:20] [V] [TRT] Searching for input: onnx::Conv_885
[08/10/2023-11:16:20] [V] [TRT] Conv_107 [Conv] inputs: [onnx::Conv_651 -> (1, 64, 180, 180)[FLOAT]], [onnx::Conv_884 -> (64, 64, 3, 3)[FLOAT]], [onnx::Conv_885 -> (64)[FLOAT]], 
[08/10/2023-11:16:20] [V] [TRT] Convolution input dimensions: (1, 64, 180, 180)
[08/10/2023-11:16:20] [V] [TRT] Registering layer: Conv_107 for ONNX node: Conv_107
[08/10/2023-11:16:20] [V] [TRT] Using kernel: (3, 3), strides: (1, 1), prepadding: (1, 1), postpadding: (1, 1), dilations: (1, 1), numOutputs: 64
[08/10/2023-11:16:20] [V] [TRT] Convolution output dimensions: (1, 64, 180, 180)
[08/10/2023-11:16:20] [V] [TRT] Registering tensor: input.360 for ONNX tensor: input.360
[08/10/2023-11:16:20] [V] [TRT] Conv_107 [Conv] outputs: [input.360 -> (1, 64, 180, 180)[FLOAT]], 
[08/10/2023-11:16:20] [V] [TRT] Parsing node: Relu_108 [Relu]
[08/10/2023-11:16:20] [V] [TRT] Searching for input: input.360
[08/10/2023-11:16:20] [V] [TRT] Relu_108 [Relu] inputs: [input.360 -> (1, 64, 180, 180)[FLOAT]], 
[08/10/2023-11:16:20] [V] [TRT] Registering layer: Relu_108 for ONNX node: Relu_108
[08/10/2023-11:16:20] [V] [TRT] Registering tensor: input.364 for ONNX tensor: input.364
[08/10/2023-11:16:20] [V] [TRT] Relu_108 [Relu] outputs: [input.364 -> (1, 64, 180, 180)[FLOAT]], 
[08/10/2023-11:16:20] [V] [TRT] Parsing node: Conv_109 [Conv]
[08/10/2023-11:16:20] [V] [TRT] Searching for input: input.364
[08/10/2023-11:16:20] [V] [TRT] Searching for input: model.bbox_head.tasks.2.rot.3.weight
[08/10/2023-11:16:20] [V] [TRT] Searching for input: model.bbox_head.tasks.2.rot.3.bias
[08/10/2023-11:16:20] [V] [TRT] Conv_109 [Conv] inputs: [input.364 -> (1, 64, 180, 180)[FLOAT]], [model.bbox_head.tasks.2.rot.3.weight -> (2, 64, 3, 3)[FLOAT]], [model.bbox_head.tasks.2.rot.3.bias -> (2)[FLOAT]], 
[08/10/2023-11:16:20] [V] [TRT] Convolution input dimensions: (1, 64, 180, 180)
[08/10/2023-11:16:20] [V] [TRT] Registering layer: Conv_109 for ONNX node: Conv_109
[08/10/2023-11:16:20] [V] [TRT] Using kernel: (3, 3), strides: (1, 1), prepadding: (1, 1), postpadding: (1, 1), dilations: (1, 1), numOutputs: 2
[08/10/2023-11:16:20] [V] [TRT] Convolution output dimensions: (1, 2, 180, 180)
[08/10/2023-11:16:20] [V] [TRT] Registering tensor: rot_2_16 for ONNX tensor: rot_2
[08/10/2023-11:16:20] [V] [TRT] Conv_109 [Conv] outputs: [rot_2 -> (1, 2, 180, 180)[FLOAT]], 
[08/10/2023-11:16:20] [V] [TRT] Parsing node: Conv_110 [Conv]
[08/10/2023-11:16:20] [V] [TRT] Searching for input: onnx::Conv_651
[08/10/2023-11:16:20] [V] [TRT] Searching for input: onnx::Conv_887
[08/10/2023-11:16:20] [V] [TRT] Searching for input: onnx::Conv_888
[08/10/2023-11:16:20] [V] [TRT] Conv_110 [Conv] inputs: [onnx::Conv_651 -> (1, 64, 180, 180)[FLOAT]], [onnx::Conv_887 -> (64, 64, 3, 3)[FLOAT]], [onnx::Conv_888 -> (64)[FLOAT]], 
[08/10/2023-11:16:20] [V] [TRT] Convolution input dimensions: (1, 64, 180, 180)
[08/10/2023-11:16:20] [V] [TRT] Registering layer: Conv_110 for ONNX node: Conv_110
[08/10/2023-11:16:20] [V] [TRT] Using kernel: (3, 3), strides: (1, 1), prepadding: (1, 1), postpadding: (1, 1), dilations: (1, 1), numOutputs: 64
[08/10/2023-11:16:20] [V] [TRT] Convolution output dimensions: (1, 64, 180, 180)
[08/10/2023-11:16:20] [V] [TRT] Registering tensor: input.372 for ONNX tensor: input.372
[08/10/2023-11:16:20] [V] [TRT] Conv_110 [Conv] outputs: [input.372 -> (1, 64, 180, 180)[FLOAT]], 
[08/10/2023-11:16:20] [V] [TRT] Parsing node: Relu_111 [Relu]
[08/10/2023-11:16:20] [V] [TRT] Searching for input: input.372
[08/10/2023-11:16:20] [V] [TRT] Relu_111 [Relu] inputs: [input.372 -> (1, 64, 180, 180)[FLOAT]], 
[08/10/2023-11:16:20] [V] [TRT] Registering layer: Relu_111 for ONNX node: Relu_111
[08/10/2023-11:16:20] [V] [TRT] Registering tensor: input.376 for ONNX tensor: input.376
[08/10/2023-11:16:20] [V] [TRT] Relu_111 [Relu] outputs: [input.376 -> (1, 64, 180, 180)[FLOAT]], 
[08/10/2023-11:16:20] [V] [TRT] Parsing node: Conv_112 [Conv]
[08/10/2023-11:16:20] [V] [TRT] Searching for input: input.376
[08/10/2023-11:16:20] [V] [TRT] Searching for input: model.bbox_head.tasks.2.vel.3.weight
[08/10/2023-11:16:20] [V] [TRT] Searching for input: model.bbox_head.tasks.2.vel.3.bias
[08/10/2023-11:16:20] [V] [TRT] Conv_112 [Conv] inputs: [input.376 -> (1, 64, 180, 180)[FLOAT]], [model.bbox_head.tasks.2.vel.3.weight -> (2, 64, 3, 3)[FLOAT]], [model.bbox_head.tasks.2.vel.3.bias -> (2)[FLOAT]], 
[08/10/2023-11:16:20] [V] [TRT] Convolution input dimensions: (1, 64, 180, 180)
[08/10/2023-11:16:20] [V] [TRT] Registering layer: Conv_112 for ONNX node: Conv_112
[08/10/2023-11:16:20] [V] [TRT] Using kernel: (3, 3), strides: (1, 1), prepadding: (1, 1), postpadding: (1, 1), dilations: (1, 1), numOutputs: 2
[08/10/2023-11:16:20] [V] [TRT] Convolution output dimensions: (1, 2, 180, 180)
[08/10/2023-11:16:20] [V] [TRT] Registering tensor: vel_2_17 for ONNX tensor: vel_2
[08/10/2023-11:16:20] [V] [TRT] Conv_112 [Conv] outputs: [vel_2 -> (1, 2, 180, 180)[FLOAT]], 
[08/10/2023-11:16:20] [V] [TRT] Parsing node: Conv_113 [Conv]
[08/10/2023-11:16:20] [V] [TRT] Searching for input: onnx::Conv_651
[08/10/2023-11:16:20] [V] [TRT] Searching for input: onnx::Conv_890
[08/10/2023-11:16:20] [V] [TRT] Searching for input: onnx::Conv_891
[08/10/2023-11:16:20] [V] [TRT] Conv_113 [Conv] inputs: [onnx::Conv_651 -> (1, 64, 180, 180)[FLOAT]], [onnx::Conv_890 -> (64, 64, 3, 3)[FLOAT]], [onnx::Conv_891 -> (64)[FLOAT]], 
[08/10/2023-11:16:20] [V] [TRT] Convolution input dimensions: (1, 64, 180, 180)
[08/10/2023-11:16:20] [V] [TRT] Registering layer: Conv_113 for ONNX node: Conv_113
[08/10/2023-11:16:20] [V] [TRT] Using kernel: (3, 3), strides: (1, 1), prepadding: (1, 1), postpadding: (1, 1), dilations: (1, 1), numOutputs: 64
[08/10/2023-11:16:20] [V] [TRT] Convolution output dimensions: (1, 64, 180, 180)
[08/10/2023-11:16:20] [V] [TRT] Registering tensor: input.384 for ONNX tensor: input.384
[08/10/2023-11:16:20] [V] [TRT] Conv_113 [Conv] outputs: [input.384 -> (1, 64, 180, 180)[FLOAT]], 
[08/10/2023-11:16:20] [V] [TRT] Parsing node: Relu_114 [Relu]
[08/10/2023-11:16:20] [V] [TRT] Searching for input: input.384
[08/10/2023-11:16:20] [V] [TRT] Relu_114 [Relu] inputs: [input.384 -> (1, 64, 180, 180)[FLOAT]], 
[08/10/2023-11:16:20] [V] [TRT] Registering layer: Relu_114 for ONNX node: Relu_114
[08/10/2023-11:16:20] [V] [TRT] Registering tensor: input.388 for ONNX tensor: input.388
[08/10/2023-11:16:20] [V] [TRT] Relu_114 [Relu] outputs: [input.388 -> (1, 64, 180, 180)[FLOAT]], 
[08/10/2023-11:16:20] [V] [TRT] Parsing node: Conv_115 [Conv]
[08/10/2023-11:16:20] [V] [TRT] Searching for input: input.388
[08/10/2023-11:16:20] [V] [TRT] Searching for input: model.bbox_head.tasks.2.hm.3.weight
[08/10/2023-11:16:20] [V] [TRT] Searching for input: model.bbox_head.tasks.2.hm.3.bias
[08/10/2023-11:16:20] [V] [TRT] Conv_115 [Conv] inputs: [input.388 -> (1, 64, 180, 180)[FLOAT]], [model.bbox_head.tasks.2.hm.3.weight -> (2, 64, 3, 3)[FLOAT]], [model.bbox_head.tasks.2.hm.3.bias -> (2)[FLOAT]], 
[08/10/2023-11:16:20] [V] [TRT] Convolution input dimensions: (1, 64, 180, 180)
[08/10/2023-11:16:20] [V] [TRT] Registering layer: Conv_115 for ONNX node: Conv_115
[08/10/2023-11:16:20] [V] [TRT] Using kernel: (3, 3), strides: (1, 1), prepadding: (1, 1), postpadding: (1, 1), dilations: (1, 1), numOutputs: 2
[08/10/2023-11:16:20] [V] [TRT] Convolution output dimensions: (1, 2, 180, 180)
[08/10/2023-11:16:20] [V] [TRT] Registering tensor: hm_2_18 for ONNX tensor: hm_2
[08/10/2023-11:16:20] [V] [TRT] Conv_115 [Conv] outputs: [hm_2 -> (1, 2, 180, 180)[FLOAT]], 
[08/10/2023-11:16:20] [V] [TRT] Parsing node: Conv_116 [Conv]
[08/10/2023-11:16:20] [V] [TRT] Searching for input: onnx::Conv_651
[08/10/2023-11:16:20] [V] [TRT] Searching for input: onnx::Conv_893
[08/10/2023-11:16:20] [V] [TRT] Searching for input: onnx::Conv_894
[08/10/2023-11:16:20] [V] [TRT] Conv_116 [Conv] inputs: [onnx::Conv_651 -> (1, 64, 180, 180)[FLOAT]], [onnx::Conv_893 -> (64, 64, 3, 3)[FLOAT]], [onnx::Conv_894 -> (64)[FLOAT]], 
[08/10/2023-11:16:20] [V] [TRT] Convolution input dimensions: (1, 64, 180, 180)
[08/10/2023-11:16:20] [V] [TRT] Registering layer: Conv_116 for ONNX node: Conv_116
[08/10/2023-11:16:20] [V] [TRT] Using kernel: (3, 3), strides: (1, 1), prepadding: (1, 1), postpadding: (1, 1), dilations: (1, 1), numOutputs: 64
[08/10/2023-11:16:20] [V] [TRT] Convolution output dimensions: (1, 64, 180, 180)
[08/10/2023-11:16:20] [V] [TRT] Registering tensor: input.396 for ONNX tensor: input.396
[08/10/2023-11:16:20] [V] [TRT] Conv_116 [Conv] outputs: [input.396 -> (1, 64, 180, 180)[FLOAT]], 
[08/10/2023-11:16:20] [V] [TRT] Parsing node: Relu_117 [Relu]
[08/10/2023-11:16:20] [V] [TRT] Searching for input: input.396
[08/10/2023-11:16:20] [V] [TRT] Relu_117 [Relu] inputs: [input.396 -> (1, 64, 180, 180)[FLOAT]], 
[08/10/2023-11:16:20] [V] [TRT] Registering layer: Relu_117 for ONNX node: Relu_117
[08/10/2023-11:16:20] [V] [TRT] Registering tensor: input.400 for ONNX tensor: input.400
[08/10/2023-11:16:20] [V] [TRT] Relu_117 [Relu] outputs: [input.400 -> (1, 64, 180, 180)[FLOAT]], 
[08/10/2023-11:16:20] [V] [TRT] Parsing node: Conv_118 [Conv]
[08/10/2023-11:16:20] [V] [TRT] Searching for input: input.400
[08/10/2023-11:16:20] [V] [TRT] Searching for input: model.bbox_head.tasks.3.reg.3.weight
[08/10/2023-11:16:20] [V] [TRT] Searching for input: model.bbox_head.tasks.3.reg.3.bias
[08/10/2023-11:16:20] [V] [TRT] Conv_118 [Conv] inputs: [input.400 -> (1, 64, 180, 180)[FLOAT]], [model.bbox_head.tasks.3.reg.3.weight -> (2, 64, 3, 3)[FLOAT]], [model.bbox_head.tasks.3.reg.3.bias -> (2)[FLOAT]], 
[08/10/2023-11:16:20] [V] [TRT] Convolution input dimensions: (1, 64, 180, 180)
[08/10/2023-11:16:20] [V] [TRT] Registering layer: Conv_118 for ONNX node: Conv_118
[08/10/2023-11:16:20] [V] [TRT] Using kernel: (3, 3), strides: (1, 1), prepadding: (1, 1), postpadding: (1, 1), dilations: (1, 1), numOutputs: 2
[08/10/2023-11:16:20] [V] [TRT] Convolution output dimensions: (1, 2, 180, 180)
[08/10/2023-11:16:20] [V] [TRT] Registering tensor: reg_3_19 for ONNX tensor: reg_3
[08/10/2023-11:16:20] [V] [TRT] Conv_118 [Conv] outputs: [reg_3 -> (1, 2, 180, 180)[FLOAT]], 
[08/10/2023-11:16:20] [V] [TRT] Parsing node: Conv_119 [Conv]
[08/10/2023-11:16:20] [V] [TRT] Searching for input: onnx::Conv_651
[08/10/2023-11:16:20] [V] [TRT] Searching for input: onnx::Conv_896
[08/10/2023-11:16:20] [V] [TRT] Searching for input: onnx::Conv_897
[08/10/2023-11:16:20] [V] [TRT] Conv_119 [Conv] inputs: [onnx::Conv_651 -> (1, 64, 180, 180)[FLOAT]], [onnx::Conv_896 -> (64, 64, 3, 3)[FLOAT]], [onnx::Conv_897 -> (64)[FLOAT]], 
[08/10/2023-11:16:20] [V] [TRT] Convolution input dimensions: (1, 64, 180, 180)
[08/10/2023-11:16:20] [V] [TRT] Registering layer: Conv_119 for ONNX node: Conv_119
[08/10/2023-11:16:20] [V] [TRT] Using kernel: (3, 3), strides: (1, 1), prepadding: (1, 1), postpadding: (1, 1), dilations: (1, 1), numOutputs: 64
[08/10/2023-11:16:20] [V] [TRT] Convolution output dimensions: (1, 64, 180, 180)
[08/10/2023-11:16:20] [V] [TRT] Registering tensor: input.408 for ONNX tensor: input.408
[08/10/2023-11:16:20] [V] [TRT] Conv_119 [Conv] outputs: [input.408 -> (1, 64, 180, 180)[FLOAT]], 
[08/10/2023-11:16:20] [V] [TRT] Parsing node: Relu_120 [Relu]
[08/10/2023-11:16:20] [V] [TRT] Searching for input: input.408
[08/10/2023-11:16:20] [V] [TRT] Relu_120 [Relu] inputs: [input.408 -> (1, 64, 180, 180)[FLOAT]], 
[08/10/2023-11:16:20] [V] [TRT] Registering layer: Relu_120 for ONNX node: Relu_120
[08/10/2023-11:16:20] [V] [TRT] Registering tensor: input.412 for ONNX tensor: input.412
[08/10/2023-11:16:20] [V] [TRT] Relu_120 [Relu] outputs: [input.412 -> (1, 64, 180, 180)[FLOAT]], 
[08/10/2023-11:16:20] [V] [TRT] Parsing node: Conv_121 [Conv]
[08/10/2023-11:16:20] [V] [TRT] Searching for input: input.412
[08/10/2023-11:16:20] [V] [TRT] Searching for input: model.bbox_head.tasks.3.height.3.weight
[08/10/2023-11:16:20] [V] [TRT] Searching for input: model.bbox_head.tasks.3.height.3.bias
[08/10/2023-11:16:20] [V] [TRT] Conv_121 [Conv] inputs: [input.412 -> (1, 64, 180, 180)[FLOAT]], [model.bbox_head.tasks.3.height.3.weight -> (1, 64, 3, 3)[FLOAT]], [model.bbox_head.tasks.3.height.3.bias -> (1)[FLOAT]], 
[08/10/2023-11:16:20] [V] [TRT] Convolution input dimensions: (1, 64, 180, 180)
[08/10/2023-11:16:20] [V] [TRT] Registering layer: Conv_121 for ONNX node: Conv_121
[08/10/2023-11:16:20] [V] [TRT] Using kernel: (3, 3), strides: (1, 1), prepadding: (1, 1), postpadding: (1, 1), dilations: (1, 1), numOutputs: 1
[08/10/2023-11:16:20] [V] [TRT] Convolution output dimensions: (1, 1, 180, 180)
[08/10/2023-11:16:20] [V] [TRT] Registering tensor: height_3_20 for ONNX tensor: height_3
[08/10/2023-11:16:20] [V] [TRT] Conv_121 [Conv] outputs: [height_3 -> (1, 1, 180, 180)[FLOAT]], 
[08/10/2023-11:16:20] [V] [TRT] Parsing node: Conv_122 [Conv]
[08/10/2023-11:16:20] [V] [TRT] Searching for input: onnx::Conv_651
[08/10/2023-11:16:20] [V] [TRT] Searching for input: onnx::Conv_899
[08/10/2023-11:16:20] [V] [TRT] Searching for input: onnx::Conv_900
[08/10/2023-11:16:20] [V] [TRT] Conv_122 [Conv] inputs: [onnx::Conv_651 -> (1, 64, 180, 180)[FLOAT]], [onnx::Conv_899 -> (64, 64, 3, 3)[FLOAT]], [onnx::Conv_900 -> (64)[FLOAT]], 
[08/10/2023-11:16:20] [V] [TRT] Convolution input dimensions: (1, 64, 180, 180)
[08/10/2023-11:16:20] [V] [TRT] Registering layer: Conv_122 for ONNX node: Conv_122
[08/10/2023-11:16:20] [V] [TRT] Using kernel: (3, 3), strides: (1, 1), prepadding: (1, 1), postpadding: (1, 1), dilations: (1, 1), numOutputs: 64
[08/10/2023-11:16:20] [V] [TRT] Convolution output dimensions: (1, 64, 180, 180)
[08/10/2023-11:16:20] [V] [TRT] Registering tensor: input.420 for ONNX tensor: input.420
[08/10/2023-11:16:20] [V] [TRT] Conv_122 [Conv] outputs: [input.420 -> (1, 64, 180, 180)[FLOAT]], 
[08/10/2023-11:16:20] [V] [TRT] Parsing node: Relu_123 [Relu]
[08/10/2023-11:16:20] [V] [TRT] Searching for input: input.420
[08/10/2023-11:16:20] [V] [TRT] Relu_123 [Relu] inputs: [input.420 -> (1, 64, 180, 180)[FLOAT]], 
[08/10/2023-11:16:20] [V] [TRT] Registering layer: Relu_123 for ONNX node: Relu_123
[08/10/2023-11:16:20] [V] [TRT] Registering tensor: input.424 for ONNX tensor: input.424
[08/10/2023-11:16:20] [V] [TRT] Relu_123 [Relu] outputs: [input.424 -> (1, 64, 180, 180)[FLOAT]], 
[08/10/2023-11:16:20] [V] [TRT] Parsing node: Conv_124 [Conv]
[08/10/2023-11:16:20] [V] [TRT] Searching for input: input.424
[08/10/2023-11:16:20] [V] [TRT] Searching for input: model.bbox_head.tasks.3.dim.3.weight
[08/10/2023-11:16:20] [V] [TRT] Searching for input: model.bbox_head.tasks.3.dim.3.bias
[08/10/2023-11:16:20] [V] [TRT] Conv_124 [Conv] inputs: [input.424 -> (1, 64, 180, 180)[FLOAT]], [model.bbox_head.tasks.3.dim.3.weight -> (3, 64, 3, 3)[FLOAT]], [model.bbox_head.tasks.3.dim.3.bias -> (3)[FLOAT]], 
[08/10/2023-11:16:20] [V] [TRT] Convolution input dimensions: (1, 64, 180, 180)
[08/10/2023-11:16:20] [V] [TRT] Registering layer: Conv_124 for ONNX node: Conv_124
[08/10/2023-11:16:20] [V] [TRT] Using kernel: (3, 3), strides: (1, 1), prepadding: (1, 1), postpadding: (1, 1), dilations: (1, 1), numOutputs: 3
[08/10/2023-11:16:20] [V] [TRT] Convolution output dimensions: (1, 3, 180, 180)
[08/10/2023-11:16:20] [V] [TRT] Registering tensor: dim_3_21 for ONNX tensor: dim_3
[08/10/2023-11:16:20] [V] [TRT] Conv_124 [Conv] outputs: [dim_3 -> (1, 3, 180, 180)[FLOAT]], 
[08/10/2023-11:16:20] [V] [TRT] Parsing node: Conv_125 [Conv]
[08/10/2023-11:16:20] [V] [TRT] Searching for input: onnx::Conv_651
[08/10/2023-11:16:20] [V] [TRT] Searching for input: onnx::Conv_902
[08/10/2023-11:16:20] [V] [TRT] Searching for input: onnx::Conv_903
[08/10/2023-11:16:20] [V] [TRT] Conv_125 [Conv] inputs: [onnx::Conv_651 -> (1, 64, 180, 180)[FLOAT]], [onnx::Conv_902 -> (64, 64, 3, 3)[FLOAT]], [onnx::Conv_903 -> (64)[FLOAT]], 
[08/10/2023-11:16:20] [V] [TRT] Convolution input dimensions: (1, 64, 180, 180)
[08/10/2023-11:16:20] [V] [TRT] Registering layer: Conv_125 for ONNX node: Conv_125
[08/10/2023-11:16:20] [V] [TRT] Using kernel: (3, 3), strides: (1, 1), prepadding: (1, 1), postpadding: (1, 1), dilations: (1, 1), numOutputs: 64
[08/10/2023-11:16:20] [V] [TRT] Convolution output dimensions: (1, 64, 180, 180)
[08/10/2023-11:16:20] [V] [TRT] Registering tensor: input.432 for ONNX tensor: input.432
[08/10/2023-11:16:20] [V] [TRT] Conv_125 [Conv] outputs: [input.432 -> (1, 64, 180, 180)[FLOAT]], 
[08/10/2023-11:16:20] [V] [TRT] Parsing node: Relu_126 [Relu]
[08/10/2023-11:16:20] [V] [TRT] Searching for input: input.432
[08/10/2023-11:16:20] [V] [TRT] Relu_126 [Relu] inputs: [input.432 -> (1, 64, 180, 180)[FLOAT]], 
[08/10/2023-11:16:20] [V] [TRT] Registering layer: Relu_126 for ONNX node: Relu_126
[08/10/2023-11:16:20] [V] [TRT] Registering tensor: input.436 for ONNX tensor: input.436
[08/10/2023-11:16:20] [V] [TRT] Relu_126 [Relu] outputs: [input.436 -> (1, 64, 180, 180)[FLOAT]], 
[08/10/2023-11:16:20] [V] [TRT] Parsing node: Conv_127 [Conv]
[08/10/2023-11:16:20] [V] [TRT] Searching for input: input.436
[08/10/2023-11:16:20] [V] [TRT] Searching for input: model.bbox_head.tasks.3.rot.3.weight
[08/10/2023-11:16:20] [V] [TRT] Searching for input: model.bbox_head.tasks.3.rot.3.bias
[08/10/2023-11:16:20] [V] [TRT] Conv_127 [Conv] inputs: [input.436 -> (1, 64, 180, 180)[FLOAT]], [model.bbox_head.tasks.3.rot.3.weight -> (2, 64, 3, 3)[FLOAT]], [model.bbox_head.tasks.3.rot.3.bias -> (2)[FLOAT]], 
[08/10/2023-11:16:20] [V] [TRT] Convolution input dimensions: (1, 64, 180, 180)
[08/10/2023-11:16:20] [V] [TRT] Registering layer: Conv_127 for ONNX node: Conv_127
[08/10/2023-11:16:20] [V] [TRT] Using kernel: (3, 3), strides: (1, 1), prepadding: (1, 1), postpadding: (1, 1), dilations: (1, 1), numOutputs: 2
[08/10/2023-11:16:20] [V] [TRT] Convolution output dimensions: (1, 2, 180, 180)
[08/10/2023-11:16:20] [V] [TRT] Registering tensor: rot_3_22 for ONNX tensor: rot_3
[08/10/2023-11:16:20] [V] [TRT] Conv_127 [Conv] outputs: [rot_3 -> (1, 2, 180, 180)[FLOAT]], 
[08/10/2023-11:16:20] [V] [TRT] Parsing node: Conv_128 [Conv]
[08/10/2023-11:16:20] [V] [TRT] Searching for input: onnx::Conv_651
[08/10/2023-11:16:20] [V] [TRT] Searching for input: onnx::Conv_905
[08/10/2023-11:16:20] [V] [TRT] Searching for input: onnx::Conv_906
[08/10/2023-11:16:20] [V] [TRT] Conv_128 [Conv] inputs: [onnx::Conv_651 -> (1, 64, 180, 180)[FLOAT]], [onnx::Conv_905 -> (64, 64, 3, 3)[FLOAT]], [onnx::Conv_906 -> (64)[FLOAT]], 
[08/10/2023-11:16:20] [V] [TRT] Convolution input dimensions: (1, 64, 180, 180)
[08/10/2023-11:16:20] [V] [TRT] Registering layer: Conv_128 for ONNX node: Conv_128
[08/10/2023-11:16:20] [V] [TRT] Using kernel: (3, 3), strides: (1, 1), prepadding: (1, 1), postpadding: (1, 1), dilations: (1, 1), numOutputs: 64
[08/10/2023-11:16:20] [V] [TRT] Convolution output dimensions: (1, 64, 180, 180)
[08/10/2023-11:16:20] [V] [TRT] Registering tensor: input.444 for ONNX tensor: input.444
[08/10/2023-11:16:20] [V] [TRT] Conv_128 [Conv] outputs: [input.444 -> (1, 64, 180, 180)[FLOAT]], 
[08/10/2023-11:16:20] [V] [TRT] Parsing node: Relu_129 [Relu]
[08/10/2023-11:16:20] [V] [TRT] Searching for input: input.444
[08/10/2023-11:16:20] [V] [TRT] Relu_129 [Relu] inputs: [input.444 -> (1, 64, 180, 180)[FLOAT]], 
[08/10/2023-11:16:20] [V] [TRT] Registering layer: Relu_129 for ONNX node: Relu_129
[08/10/2023-11:16:20] [V] [TRT] Registering tensor: input.448 for ONNX tensor: input.448
[08/10/2023-11:16:20] [V] [TRT] Relu_129 [Relu] outputs: [input.448 -> (1, 64, 180, 180)[FLOAT]], 
[08/10/2023-11:16:20] [V] [TRT] Parsing node: Conv_130 [Conv]
[08/10/2023-11:16:20] [V] [TRT] Searching for input: input.448
[08/10/2023-11:16:20] [V] [TRT] Searching for input: model.bbox_head.tasks.3.vel.3.weight
[08/10/2023-11:16:20] [V] [TRT] Searching for input: model.bbox_head.tasks.3.vel.3.bias
[08/10/2023-11:16:20] [V] [TRT] Conv_130 [Conv] inputs: [input.448 -> (1, 64, 180, 180)[FLOAT]], [model.bbox_head.tasks.3.vel.3.weight -> (2, 64, 3, 3)[FLOAT]], [model.bbox_head.tasks.3.vel.3.bias -> (2)[FLOAT]], 
[08/10/2023-11:16:20] [V] [TRT] Convolution input dimensions: (1, 64, 180, 180)
[08/10/2023-11:16:20] [V] [TRT] Registering layer: Conv_130 for ONNX node: Conv_130
[08/10/2023-11:16:20] [V] [TRT] Using kernel: (3, 3), strides: (1, 1), prepadding: (1, 1), postpadding: (1, 1), dilations: (1, 1), numOutputs: 2
[08/10/2023-11:16:20] [V] [TRT] Convolution output dimensions: (1, 2, 180, 180)
[08/10/2023-11:16:20] [V] [TRT] Registering tensor: vel_3_23 for ONNX tensor: vel_3
[08/10/2023-11:16:20] [V] [TRT] Conv_130 [Conv] outputs: [vel_3 -> (1, 2, 180, 180)[FLOAT]], 
[08/10/2023-11:16:20] [V] [TRT] Parsing node: Conv_131 [Conv]
[08/10/2023-11:16:20] [V] [TRT] Searching for input: onnx::Conv_651
[08/10/2023-11:16:20] [V] [TRT] Searching for input: onnx::Conv_908
[08/10/2023-11:16:20] [V] [TRT] Searching for input: onnx::Conv_909
[08/10/2023-11:16:20] [V] [TRT] Conv_131 [Conv] inputs: [onnx::Conv_651 -> (1, 64, 180, 180)[FLOAT]], [onnx::Conv_908 -> (64, 64, 3, 3)[FLOAT]], [onnx::Conv_909 -> (64)[FLOAT]], 
[08/10/2023-11:16:20] [V] [TRT] Convolution input dimensions: (1, 64, 180, 180)
[08/10/2023-11:16:20] [V] [TRT] Registering layer: Conv_131 for ONNX node: Conv_131
[08/10/2023-11:16:20] [V] [TRT] Using kernel: (3, 3), strides: (1, 1), prepadding: (1, 1), postpadding: (1, 1), dilations: (1, 1), numOutputs: 64
[08/10/2023-11:16:20] [V] [TRT] Convolution output dimensions: (1, 64, 180, 180)
[08/10/2023-11:16:20] [V] [TRT] Registering tensor: input.456 for ONNX tensor: input.456
[08/10/2023-11:16:20] [V] [TRT] Conv_131 [Conv] outputs: [input.456 -> (1, 64, 180, 180)[FLOAT]], 
[08/10/2023-11:16:20] [V] [TRT] Parsing node: Relu_132 [Relu]
[08/10/2023-11:16:20] [V] [TRT] Searching for input: input.456
[08/10/2023-11:16:20] [V] [TRT] Relu_132 [Relu] inputs: [input.456 -> (1, 64, 180, 180)[FLOAT]], 
[08/10/2023-11:16:20] [V] [TRT] Registering layer: Relu_132 for ONNX node: Relu_132
[08/10/2023-11:16:20] [V] [TRT] Registering tensor: input.460 for ONNX tensor: input.460
[08/10/2023-11:16:20] [V] [TRT] Relu_132 [Relu] outputs: [input.460 -> (1, 64, 180, 180)[FLOAT]], 
[08/10/2023-11:16:20] [V] [TRT] Parsing node: Conv_133 [Conv]
[08/10/2023-11:16:20] [V] [TRT] Searching for input: input.460
[08/10/2023-11:16:20] [V] [TRT] Searching for input: model.bbox_head.tasks.3.hm.3.weight
[08/10/2023-11:16:20] [V] [TRT] Searching for input: model.bbox_head.tasks.3.hm.3.bias
[08/10/2023-11:16:20] [V] [TRT] Conv_133 [Conv] inputs: [input.460 -> (1, 64, 180, 180)[FLOAT]], [model.bbox_head.tasks.3.hm.3.weight -> (1, 64, 3, 3)[FLOAT]], [model.bbox_head.tasks.3.hm.3.bias -> (1)[FLOAT]], 
[08/10/2023-11:16:20] [V] [TRT] Convolution input dimensions: (1, 64, 180, 180)
[08/10/2023-11:16:20] [V] [TRT] Registering layer: Conv_133 for ONNX node: Conv_133
[08/10/2023-11:16:20] [V] [TRT] Using kernel: (3, 3), strides: (1, 1), prepadding: (1, 1), postpadding: (1, 1), dilations: (1, 1), numOutputs: 1
[08/10/2023-11:16:20] [V] [TRT] Convolution output dimensions: (1, 1, 180, 180)
[08/10/2023-11:16:20] [V] [TRT] Registering tensor: hm_3_24 for ONNX tensor: hm_3
[08/10/2023-11:16:20] [V] [TRT] Conv_133 [Conv] outputs: [hm_3 -> (1, 1, 180, 180)[FLOAT]], 
[08/10/2023-11:16:20] [V] [TRT] Parsing node: Conv_134 [Conv]
[08/10/2023-11:16:20] [V] [TRT] Searching for input: onnx::Conv_651
[08/10/2023-11:16:20] [V] [TRT] Searching for input: onnx::Conv_911
[08/10/2023-11:16:20] [V] [TRT] Searching for input: onnx::Conv_912
[08/10/2023-11:16:20] [V] [TRT] Conv_134 [Conv] inputs: [onnx::Conv_651 -> (1, 64, 180, 180)[FLOAT]], [onnx::Conv_911 -> (64, 64, 3, 3)[FLOAT]], [onnx::Conv_912 -> (64)[FLOAT]], 
[08/10/2023-11:16:20] [V] [TRT] Convolution input dimensions: (1, 64, 180, 180)
[08/10/2023-11:16:20] [V] [TRT] Registering layer: Conv_134 for ONNX node: Conv_134
[08/10/2023-11:16:20] [V] [TRT] Using kernel: (3, 3), strides: (1, 1), prepadding: (1, 1), postpadding: (1, 1), dilations: (1, 1), numOutputs: 64
[08/10/2023-11:16:20] [V] [TRT] Convolution output dimensions: (1, 64, 180, 180)
[08/10/2023-11:16:20] [V] [TRT] Registering tensor: input.468 for ONNX tensor: input.468
[08/10/2023-11:16:20] [V] [TRT] Conv_134 [Conv] outputs: [input.468 -> (1, 64, 180, 180)[FLOAT]], 
[08/10/2023-11:16:20] [V] [TRT] Parsing node: Relu_135 [Relu]
[08/10/2023-11:16:20] [V] [TRT] Searching for input: input.468
[08/10/2023-11:16:20] [V] [TRT] Relu_135 [Relu] inputs: [input.468 -> (1, 64, 180, 180)[FLOAT]], 
[08/10/2023-11:16:20] [V] [TRT] Registering layer: Relu_135 for ONNX node: Relu_135
[08/10/2023-11:16:20] [V] [TRT] Registering tensor: input.472 for ONNX tensor: input.472
[08/10/2023-11:16:20] [V] [TRT] Relu_135 [Relu] outputs: [input.472 -> (1, 64, 180, 180)[FLOAT]], 
[08/10/2023-11:16:20] [V] [TRT] Parsing node: Conv_136 [Conv]
[08/10/2023-11:16:20] [V] [TRT] Searching for input: input.472
[08/10/2023-11:16:20] [V] [TRT] Searching for input: model.bbox_head.tasks.4.reg.3.weight
[08/10/2023-11:16:20] [V] [TRT] Searching for input: model.bbox_head.tasks.4.reg.3.bias
[08/10/2023-11:16:20] [V] [TRT] Conv_136 [Conv] inputs: [input.472 -> (1, 64, 180, 180)[FLOAT]], [model.bbox_head.tasks.4.reg.3.weight -> (2, 64, 3, 3)[FLOAT]], [model.bbox_head.tasks.4.reg.3.bias -> (2)[FLOAT]], 
[08/10/2023-11:16:20] [V] [TRT] Convolution input dimensions: (1, 64, 180, 180)
[08/10/2023-11:16:20] [V] [TRT] Registering layer: Conv_136 for ONNX node: Conv_136
[08/10/2023-11:16:20] [V] [TRT] Using kernel: (3, 3), strides: (1, 1), prepadding: (1, 1), postpadding: (1, 1), dilations: (1, 1), numOutputs: 2
[08/10/2023-11:16:20] [V] [TRT] Convolution output dimensions: (1, 2, 180, 180)
[08/10/2023-11:16:20] [V] [TRT] Registering tensor: reg_4_25 for ONNX tensor: reg_4
[08/10/2023-11:16:20] [V] [TRT] Conv_136 [Conv] outputs: [reg_4 -> (1, 2, 180, 180)[FLOAT]], 
[08/10/2023-11:16:20] [V] [TRT] Parsing node: Conv_137 [Conv]
[08/10/2023-11:16:20] [V] [TRT] Searching for input: onnx::Conv_651
[08/10/2023-11:16:20] [V] [TRT] Searching for input: onnx::Conv_914
[08/10/2023-11:16:20] [V] [TRT] Searching for input: onnx::Conv_915
[08/10/2023-11:16:20] [V] [TRT] Conv_137 [Conv] inputs: [onnx::Conv_651 -> (1, 64, 180, 180)[FLOAT]], [onnx::Conv_914 -> (64, 64, 3, 3)[FLOAT]], [onnx::Conv_915 -> (64)[FLOAT]], 
[08/10/2023-11:16:20] [V] [TRT] Convolution input dimensions: (1, 64, 180, 180)
[08/10/2023-11:16:20] [V] [TRT] Registering layer: Conv_137 for ONNX node: Conv_137
[08/10/2023-11:16:20] [V] [TRT] Using kernel: (3, 3), strides: (1, 1), prepadding: (1, 1), postpadding: (1, 1), dilations: (1, 1), numOutputs: 64
[08/10/2023-11:16:20] [V] [TRT] Convolution output dimensions: (1, 64, 180, 180)
[08/10/2023-11:16:20] [V] [TRT] Registering tensor: input.480 for ONNX tensor: input.480
[08/10/2023-11:16:20] [V] [TRT] Conv_137 [Conv] outputs: [input.480 -> (1, 64, 180, 180)[FLOAT]], 
[08/10/2023-11:16:20] [V] [TRT] Parsing node: Relu_138 [Relu]
[08/10/2023-11:16:20] [V] [TRT] Searching for input: input.480
[08/10/2023-11:16:20] [V] [TRT] Relu_138 [Relu] inputs: [input.480 -> (1, 64, 180, 180)[FLOAT]], 
[08/10/2023-11:16:20] [V] [TRT] Registering layer: Relu_138 for ONNX node: Relu_138
[08/10/2023-11:16:20] [V] [TRT] Registering tensor: input.484 for ONNX tensor: input.484
[08/10/2023-11:16:20] [V] [TRT] Relu_138 [Relu] outputs: [input.484 -> (1, 64, 180, 180)[FLOAT]], 
[08/10/2023-11:16:20] [V] [TRT] Parsing node: Conv_139 [Conv]
[08/10/2023-11:16:20] [V] [TRT] Searching for input: input.484
[08/10/2023-11:16:20] [V] [TRT] Searching for input: model.bbox_head.tasks.4.height.3.weight
[08/10/2023-11:16:20] [V] [TRT] Searching for input: model.bbox_head.tasks.4.height.3.bias
[08/10/2023-11:16:20] [V] [TRT] Conv_139 [Conv] inputs: [input.484 -> (1, 64, 180, 180)[FLOAT]], [model.bbox_head.tasks.4.height.3.weight -> (1, 64, 3, 3)[FLOAT]], [model.bbox_head.tasks.4.height.3.bias -> (1)[FLOAT]], 
[08/10/2023-11:16:20] [V] [TRT] Convolution input dimensions: (1, 64, 180, 180)
[08/10/2023-11:16:20] [V] [TRT] Registering layer: Conv_139 for ONNX node: Conv_139
[08/10/2023-11:16:20] [V] [TRT] Using kernel: (3, 3), strides: (1, 1), prepadding: (1, 1), postpadding: (1, 1), dilations: (1, 1), numOutputs: 1
[08/10/2023-11:16:20] [V] [TRT] Convolution output dimensions: (1, 1, 180, 180)
[08/10/2023-11:16:20] [V] [TRT] Registering tensor: height_4_26 for ONNX tensor: height_4
[08/10/2023-11:16:20] [V] [TRT] Conv_139 [Conv] outputs: [height_4 -> (1, 1, 180, 180)[FLOAT]], 
[08/10/2023-11:16:20] [V] [TRT] Parsing node: Conv_140 [Conv]
[08/10/2023-11:16:20] [V] [TRT] Searching for input: onnx::Conv_651
[08/10/2023-11:16:20] [V] [TRT] Searching for input: onnx::Conv_917
[08/10/2023-11:16:20] [V] [TRT] Searching for input: onnx::Conv_918
[08/10/2023-11:16:20] [V] [TRT] Conv_140 [Conv] inputs: [onnx::Conv_651 -> (1, 64, 180, 180)[FLOAT]], [onnx::Conv_917 -> (64, 64, 3, 3)[FLOAT]], [onnx::Conv_918 -> (64)[FLOAT]], 
[08/10/2023-11:16:20] [V] [TRT] Convolution input dimensions: (1, 64, 180, 180)
[08/10/2023-11:16:20] [V] [TRT] Registering layer: Conv_140 for ONNX node: Conv_140
[08/10/2023-11:16:20] [V] [TRT] Using kernel: (3, 3), strides: (1, 1), prepadding: (1, 1), postpadding: (1, 1), dilations: (1, 1), numOutputs: 64
[08/10/2023-11:16:20] [V] [TRT] Convolution output dimensions: (1, 64, 180, 180)
[08/10/2023-11:16:20] [V] [TRT] Registering tensor: input.492 for ONNX tensor: input.492
[08/10/2023-11:16:20] [V] [TRT] Conv_140 [Conv] outputs: [input.492 -> (1, 64, 180, 180)[FLOAT]], 
[08/10/2023-11:16:20] [V] [TRT] Parsing node: Relu_141 [Relu]
[08/10/2023-11:16:20] [V] [TRT] Searching for input: input.492
[08/10/2023-11:16:20] [V] [TRT] Relu_141 [Relu] inputs: [input.492 -> (1, 64, 180, 180)[FLOAT]], 
[08/10/2023-11:16:20] [V] [TRT] Registering layer: Relu_141 for ONNX node: Relu_141
[08/10/2023-11:16:20] [V] [TRT] Registering tensor: input.496 for ONNX tensor: input.496
[08/10/2023-11:16:20] [V] [TRT] Relu_141 [Relu] outputs: [input.496 -> (1, 64, 180, 180)[FLOAT]], 
[08/10/2023-11:16:20] [V] [TRT] Parsing node: Conv_142 [Conv]
[08/10/2023-11:16:20] [V] [TRT] Searching for input: input.496
[08/10/2023-11:16:20] [V] [TRT] Searching for input: model.bbox_head.tasks.4.dim.3.weight
[08/10/2023-11:16:20] [V] [TRT] Searching for input: model.bbox_head.tasks.4.dim.3.bias
[08/10/2023-11:16:20] [V] [TRT] Conv_142 [Conv] inputs: [input.496 -> (1, 64, 180, 180)[FLOAT]], [model.bbox_head.tasks.4.dim.3.weight -> (3, 64, 3, 3)[FLOAT]], [model.bbox_head.tasks.4.dim.3.bias -> (3)[FLOAT]], 
[08/10/2023-11:16:20] [V] [TRT] Convolution input dimensions: (1, 64, 180, 180)
[08/10/2023-11:16:20] [V] [TRT] Registering layer: Conv_142 for ONNX node: Conv_142
[08/10/2023-11:16:20] [V] [TRT] Using kernel: (3, 3), strides: (1, 1), prepadding: (1, 1), postpadding: (1, 1), dilations: (1, 1), numOutputs: 3
[08/10/2023-11:16:20] [V] [TRT] Convolution output dimensions: (1, 3, 180, 180)
[08/10/2023-11:16:20] [V] [TRT] Registering tensor: dim_4_27 for ONNX tensor: dim_4
[08/10/2023-11:16:20] [V] [TRT] Conv_142 [Conv] outputs: [dim_4 -> (1, 3, 180, 180)[FLOAT]], 
[08/10/2023-11:16:20] [V] [TRT] Parsing node: Conv_143 [Conv]
[08/10/2023-11:16:20] [V] [TRT] Searching for input: onnx::Conv_651
[08/10/2023-11:16:20] [V] [TRT] Searching for input: onnx::Conv_920
[08/10/2023-11:16:20] [V] [TRT] Searching for input: onnx::Conv_921
[08/10/2023-11:16:20] [V] [TRT] Conv_143 [Conv] inputs: [onnx::Conv_651 -> (1, 64, 180, 180)[FLOAT]], [onnx::Conv_920 -> (64, 64, 3, 3)[FLOAT]], [onnx::Conv_921 -> (64)[FLOAT]], 
[08/10/2023-11:16:20] [V] [TRT] Convolution input dimensions: (1, 64, 180, 180)
[08/10/2023-11:16:20] [V] [TRT] Registering layer: Conv_143 for ONNX node: Conv_143
[08/10/2023-11:16:20] [V] [TRT] Using kernel: (3, 3), strides: (1, 1), prepadding: (1, 1), postpadding: (1, 1), dilations: (1, 1), numOutputs: 64
[08/10/2023-11:16:20] [V] [TRT] Convolution output dimensions: (1, 64, 180, 180)
[08/10/2023-11:16:20] [V] [TRT] Registering tensor: input.504 for ONNX tensor: input.504
[08/10/2023-11:16:20] [V] [TRT] Conv_143 [Conv] outputs: [input.504 -> (1, 64, 180, 180)[FLOAT]], 
[08/10/2023-11:16:20] [V] [TRT] Parsing node: Relu_144 [Relu]
[08/10/2023-11:16:20] [V] [TRT] Searching for input: input.504
[08/10/2023-11:16:20] [V] [TRT] Relu_144 [Relu] inputs: [input.504 -> (1, 64, 180, 180)[FLOAT]], 
[08/10/2023-11:16:20] [V] [TRT] Registering layer: Relu_144 for ONNX node: Relu_144
[08/10/2023-11:16:20] [V] [TRT] Registering tensor: input.508 for ONNX tensor: input.508
[08/10/2023-11:16:20] [V] [TRT] Relu_144 [Relu] outputs: [input.508 -> (1, 64, 180, 180)[FLOAT]], 
[08/10/2023-11:16:20] [V] [TRT] Parsing node: Conv_145 [Conv]
[08/10/2023-11:16:20] [V] [TRT] Searching for input: input.508
[08/10/2023-11:16:20] [V] [TRT] Searching for input: model.bbox_head.tasks.4.rot.3.weight
[08/10/2023-11:16:20] [V] [TRT] Searching for input: model.bbox_head.tasks.4.rot.3.bias
[08/10/2023-11:16:20] [V] [TRT] Conv_145 [Conv] inputs: [input.508 -> (1, 64, 180, 180)[FLOAT]], [model.bbox_head.tasks.4.rot.3.weight -> (2, 64, 3, 3)[FLOAT]], [model.bbox_head.tasks.4.rot.3.bias -> (2)[FLOAT]], 
[08/10/2023-11:16:20] [V] [TRT] Convolution input dimensions: (1, 64, 180, 180)
[08/10/2023-11:16:20] [V] [TRT] Registering layer: Conv_145 for ONNX node: Conv_145
[08/10/2023-11:16:20] [V] [TRT] Using kernel: (3, 3), strides: (1, 1), prepadding: (1, 1), postpadding: (1, 1), dilations: (1, 1), numOutputs: 2
[08/10/2023-11:16:20] [V] [TRT] Convolution output dimensions: (1, 2, 180, 180)
[08/10/2023-11:16:20] [V] [TRT] Registering tensor: rot_4_28 for ONNX tensor: rot_4
[08/10/2023-11:16:20] [V] [TRT] Conv_145 [Conv] outputs: [rot_4 -> (1, 2, 180, 180)[FLOAT]], 
[08/10/2023-11:16:20] [V] [TRT] Parsing node: Conv_146 [Conv]
[08/10/2023-11:16:20] [V] [TRT] Searching for input: onnx::Conv_651
[08/10/2023-11:16:20] [V] [TRT] Searching for input: onnx::Conv_923
[08/10/2023-11:16:20] [V] [TRT] Searching for input: onnx::Conv_924
[08/10/2023-11:16:20] [V] [TRT] Conv_146 [Conv] inputs: [onnx::Conv_651 -> (1, 64, 180, 180)[FLOAT]], [onnx::Conv_923 -> (64, 64, 3, 3)[FLOAT]], [onnx::Conv_924 -> (64)[FLOAT]], 
[08/10/2023-11:16:20] [V] [TRT] Convolution input dimensions: (1, 64, 180, 180)
[08/10/2023-11:16:20] [V] [TRT] Registering layer: Conv_146 for ONNX node: Conv_146
[08/10/2023-11:16:20] [V] [TRT] Using kernel: (3, 3), strides: (1, 1), prepadding: (1, 1), postpadding: (1, 1), dilations: (1, 1), numOutputs: 64
[08/10/2023-11:16:20] [V] [TRT] Convolution output dimensions: (1, 64, 180, 180)
[08/10/2023-11:16:20] [V] [TRT] Registering tensor: input.516 for ONNX tensor: input.516
[08/10/2023-11:16:20] [V] [TRT] Conv_146 [Conv] outputs: [input.516 -> (1, 64, 180, 180)[FLOAT]], 
[08/10/2023-11:16:20] [V] [TRT] Parsing node: Relu_147 [Relu]
[08/10/2023-11:16:20] [V] [TRT] Searching for input: input.516
[08/10/2023-11:16:20] [V] [TRT] Relu_147 [Relu] inputs: [input.516 -> (1, 64, 180, 180)[FLOAT]], 
[08/10/2023-11:16:20] [V] [TRT] Registering layer: Relu_147 for ONNX node: Relu_147
[08/10/2023-11:16:20] [V] [TRT] Registering tensor: input.520 for ONNX tensor: input.520
[08/10/2023-11:16:20] [V] [TRT] Relu_147 [Relu] outputs: [input.520 -> (1, 64, 180, 180)[FLOAT]], 
[08/10/2023-11:16:20] [V] [TRT] Parsing node: Conv_148 [Conv]
[08/10/2023-11:16:20] [V] [TRT] Searching for input: input.520
[08/10/2023-11:16:20] [V] [TRT] Searching for input: model.bbox_head.tasks.4.vel.3.weight
[08/10/2023-11:16:20] [V] [TRT] Searching for input: model.bbox_head.tasks.4.vel.3.bias
[08/10/2023-11:16:20] [V] [TRT] Conv_148 [Conv] inputs: [input.520 -> (1, 64, 180, 180)[FLOAT]], [model.bbox_head.tasks.4.vel.3.weight -> (2, 64, 3, 3)[FLOAT]], [model.bbox_head.tasks.4.vel.3.bias -> (2)[FLOAT]], 
[08/10/2023-11:16:20] [V] [TRT] Convolution input dimensions: (1, 64, 180, 180)
[08/10/2023-11:16:20] [V] [TRT] Registering layer: Conv_148 for ONNX node: Conv_148
[08/10/2023-11:16:20] [V] [TRT] Using kernel: (3, 3), strides: (1, 1), prepadding: (1, 1), postpadding: (1, 1), dilations: (1, 1), numOutputs: 2
[08/10/2023-11:16:20] [V] [TRT] Convolution output dimensions: (1, 2, 180, 180)
[08/10/2023-11:16:20] [V] [TRT] Registering tensor: vel_4_29 for ONNX tensor: vel_4
[08/10/2023-11:16:20] [V] [TRT] Conv_148 [Conv] outputs: [vel_4 -> (1, 2, 180, 180)[FLOAT]], 
[08/10/2023-11:16:20] [V] [TRT] Parsing node: Conv_149 [Conv]
[08/10/2023-11:16:20] [V] [TRT] Searching for input: onnx::Conv_651
[08/10/2023-11:16:20] [V] [TRT] Searching for input: onnx::Conv_926
[08/10/2023-11:16:20] [V] [TRT] Searching for input: onnx::Conv_927
[08/10/2023-11:16:20] [V] [TRT] Conv_149 [Conv] inputs: [onnx::Conv_651 -> (1, 64, 180, 180)[FLOAT]], [onnx::Conv_926 -> (64, 64, 3, 3)[FLOAT]], [onnx::Conv_927 -> (64)[FLOAT]], 
[08/10/2023-11:16:20] [V] [TRT] Convolution input dimensions: (1, 64, 180, 180)
[08/10/2023-11:16:20] [V] [TRT] Registering layer: Conv_149 for ONNX node: Conv_149
[08/10/2023-11:16:20] [V] [TRT] Using kernel: (3, 3), strides: (1, 1), prepadding: (1, 1), postpadding: (1, 1), dilations: (1, 1), numOutputs: 64
[08/10/2023-11:16:20] [V] [TRT] Convolution output dimensions: (1, 64, 180, 180)
[08/10/2023-11:16:20] [V] [TRT] Registering tensor: input.528 for ONNX tensor: input.528
[08/10/2023-11:16:20] [V] [TRT] Conv_149 [Conv] outputs: [input.528 -> (1, 64, 180, 180)[FLOAT]], 
[08/10/2023-11:16:20] [V] [TRT] Parsing node: Relu_150 [Relu]
[08/10/2023-11:16:20] [V] [TRT] Searching for input: input.528
[08/10/2023-11:16:20] [V] [TRT] Relu_150 [Relu] inputs: [input.528 -> (1, 64, 180, 180)[FLOAT]], 
[08/10/2023-11:16:20] [V] [TRT] Registering layer: Relu_150 for ONNX node: Relu_150
[08/10/2023-11:16:20] [V] [TRT] Registering tensor: input.532 for ONNX tensor: input.532
[08/10/2023-11:16:20] [V] [TRT] Relu_150 [Relu] outputs: [input.532 -> (1, 64, 180, 180)[FLOAT]], 
[08/10/2023-11:16:20] [V] [TRT] Parsing node: Conv_151 [Conv]
[08/10/2023-11:16:20] [V] [TRT] Searching for input: input.532
[08/10/2023-11:16:20] [V] [TRT] Searching for input: model.bbox_head.tasks.4.hm.3.weight
[08/10/2023-11:16:20] [V] [TRT] Searching for input: model.bbox_head.tasks.4.hm.3.bias
[08/10/2023-11:16:20] [V] [TRT] Conv_151 [Conv] inputs: [input.532 -> (1, 64, 180, 180)[FLOAT]], [model.bbox_head.tasks.4.hm.3.weight -> (2, 64, 3, 3)[FLOAT]], [model.bbox_head.tasks.4.hm.3.bias -> (2)[FLOAT]], 
[08/10/2023-11:16:20] [V] [TRT] Convolution input dimensions: (1, 64, 180, 180)
[08/10/2023-11:16:20] [V] [TRT] Registering layer: Conv_151 for ONNX node: Conv_151
[08/10/2023-11:16:20] [V] [TRT] Using kernel: (3, 3), strides: (1, 1), prepadding: (1, 1), postpadding: (1, 1), dilations: (1, 1), numOutputs: 2
[08/10/2023-11:16:20] [V] [TRT] Convolution output dimensions: (1, 2, 180, 180)
[08/10/2023-11:16:20] [V] [TRT] Registering tensor: hm_4_30 for ONNX tensor: hm_4
[08/10/2023-11:16:20] [V] [TRT] Conv_151 [Conv] outputs: [hm_4 -> (1, 2, 180, 180)[FLOAT]], 
[08/10/2023-11:16:20] [V] [TRT] Parsing node: Conv_152 [Conv]
[08/10/2023-11:16:20] [V] [TRT] Searching for input: onnx::Conv_651
[08/10/2023-11:16:20] [V] [TRT] Searching for input: onnx::Conv_929
[08/10/2023-11:16:20] [V] [TRT] Searching for input: onnx::Conv_930
[08/10/2023-11:16:20] [V] [TRT] Conv_152 [Conv] inputs: [onnx::Conv_651 -> (1, 64, 180, 180)[FLOAT]], [onnx::Conv_929 -> (64, 64, 3, 3)[FLOAT]], [onnx::Conv_930 -> (64)[FLOAT]], 
[08/10/2023-11:16:20] [V] [TRT] Convolution input dimensions: (1, 64, 180, 180)
[08/10/2023-11:16:20] [V] [TRT] Registering layer: Conv_152 for ONNX node: Conv_152
[08/10/2023-11:16:20] [V] [TRT] Using kernel: (3, 3), strides: (1, 1), prepadding: (1, 1), postpadding: (1, 1), dilations: (1, 1), numOutputs: 64
[08/10/2023-11:16:20] [V] [TRT] Convolution output dimensions: (1, 64, 180, 180)
[08/10/2023-11:16:20] [V] [TRT] Registering tensor: input.540 for ONNX tensor: input.540
[08/10/2023-11:16:20] [V] [TRT] Conv_152 [Conv] outputs: [input.540 -> (1, 64, 180, 180)[FLOAT]], 
[08/10/2023-11:16:20] [V] [TRT] Parsing node: Relu_153 [Relu]
[08/10/2023-11:16:20] [V] [TRT] Searching for input: input.540
[08/10/2023-11:16:20] [V] [TRT] Relu_153 [Relu] inputs: [input.540 -> (1, 64, 180, 180)[FLOAT]], 
[08/10/2023-11:16:20] [V] [TRT] Registering layer: Relu_153 for ONNX node: Relu_153
[08/10/2023-11:16:20] [V] [TRT] Registering tensor: input.544 for ONNX tensor: input.544
[08/10/2023-11:16:20] [V] [TRT] Relu_153 [Relu] outputs: [input.544 -> (1, 64, 180, 180)[FLOAT]], 
[08/10/2023-11:16:20] [V] [TRT] Parsing node: Conv_154 [Conv]
[08/10/2023-11:16:20] [V] [TRT] Searching for input: input.544
[08/10/2023-11:16:20] [V] [TRT] Searching for input: model.bbox_head.tasks.5.reg.3.weight
[08/10/2023-11:16:20] [V] [TRT] Searching for input: model.bbox_head.tasks.5.reg.3.bias
[08/10/2023-11:16:20] [V] [TRT] Conv_154 [Conv] inputs: [input.544 -> (1, 64, 180, 180)[FLOAT]], [model.bbox_head.tasks.5.reg.3.weight -> (2, 64, 3, 3)[FLOAT]], [model.bbox_head.tasks.5.reg.3.bias -> (2)[FLOAT]], 
[08/10/2023-11:16:20] [V] [TRT] Convolution input dimensions: (1, 64, 180, 180)
[08/10/2023-11:16:20] [V] [TRT] Registering layer: Conv_154 for ONNX node: Conv_154
[08/10/2023-11:16:20] [V] [TRT] Using kernel: (3, 3), strides: (1, 1), prepadding: (1, 1), postpadding: (1, 1), dilations: (1, 1), numOutputs: 2
[08/10/2023-11:16:20] [V] [TRT] Convolution output dimensions: (1, 2, 180, 180)
[08/10/2023-11:16:20] [V] [TRT] Registering tensor: reg_5_31 for ONNX tensor: reg_5
[08/10/2023-11:16:20] [V] [TRT] Conv_154 [Conv] outputs: [reg_5 -> (1, 2, 180, 180)[FLOAT]], 
[08/10/2023-11:16:20] [V] [TRT] Parsing node: Conv_155 [Conv]
[08/10/2023-11:16:20] [V] [TRT] Searching for input: onnx::Conv_651
[08/10/2023-11:16:20] [V] [TRT] Searching for input: onnx::Conv_932
[08/10/2023-11:16:20] [V] [TRT] Searching for input: onnx::Conv_933
[08/10/2023-11:16:20] [V] [TRT] Conv_155 [Conv] inputs: [onnx::Conv_651 -> (1, 64, 180, 180)[FLOAT]], [onnx::Conv_932 -> (64, 64, 3, 3)[FLOAT]], [onnx::Conv_933 -> (64)[FLOAT]], 
[08/10/2023-11:16:20] [V] [TRT] Convolution input dimensions: (1, 64, 180, 180)
[08/10/2023-11:16:20] [V] [TRT] Registering layer: Conv_155 for ONNX node: Conv_155
[08/10/2023-11:16:20] [V] [TRT] Using kernel: (3, 3), strides: (1, 1), prepadding: (1, 1), postpadding: (1, 1), dilations: (1, 1), numOutputs: 64
[08/10/2023-11:16:20] [V] [TRT] Convolution output dimensions: (1, 64, 180, 180)
[08/10/2023-11:16:20] [V] [TRT] Registering tensor: input.552 for ONNX tensor: input.552
[08/10/2023-11:16:20] [V] [TRT] Conv_155 [Conv] outputs: [input.552 -> (1, 64, 180, 180)[FLOAT]], 
[08/10/2023-11:16:20] [V] [TRT] Parsing node: Relu_156 [Relu]
[08/10/2023-11:16:20] [V] [TRT] Searching for input: input.552
[08/10/2023-11:16:20] [V] [TRT] Relu_156 [Relu] inputs: [input.552 -> (1, 64, 180, 180)[FLOAT]], 
[08/10/2023-11:16:20] [V] [TRT] Registering layer: Relu_156 for ONNX node: Relu_156
[08/10/2023-11:16:20] [V] [TRT] Registering tensor: input.556 for ONNX tensor: input.556
[08/10/2023-11:16:20] [V] [TRT] Relu_156 [Relu] outputs: [input.556 -> (1, 64, 180, 180)[FLOAT]], 
[08/10/2023-11:16:20] [V] [TRT] Parsing node: Conv_157 [Conv]
[08/10/2023-11:16:20] [V] [TRT] Searching for input: input.556
[08/10/2023-11:16:20] [V] [TRT] Searching for input: model.bbox_head.tasks.5.height.3.weight
[08/10/2023-11:16:20] [V] [TRT] Searching for input: model.bbox_head.tasks.5.height.3.bias
[08/10/2023-11:16:20] [V] [TRT] Conv_157 [Conv] inputs: [input.556 -> (1, 64, 180, 180)[FLOAT]], [model.bbox_head.tasks.5.height.3.weight -> (1, 64, 3, 3)[FLOAT]], [model.bbox_head.tasks.5.height.3.bias -> (1)[FLOAT]], 
[08/10/2023-11:16:20] [V] [TRT] Convolution input dimensions: (1, 64, 180, 180)
[08/10/2023-11:16:20] [V] [TRT] Registering layer: Conv_157 for ONNX node: Conv_157
[08/10/2023-11:16:20] [V] [TRT] Using kernel: (3, 3), strides: (1, 1), prepadding: (1, 1), postpadding: (1, 1), dilations: (1, 1), numOutputs: 1
[08/10/2023-11:16:20] [V] [TRT] Convolution output dimensions: (1, 1, 180, 180)
[08/10/2023-11:16:20] [V] [TRT] Registering tensor: height_5_32 for ONNX tensor: height_5
[08/10/2023-11:16:20] [V] [TRT] Conv_157 [Conv] outputs: [height_5 -> (1, 1, 180, 180)[FLOAT]], 
[08/10/2023-11:16:20] [V] [TRT] Parsing node: Conv_158 [Conv]
[08/10/2023-11:16:20] [V] [TRT] Searching for input: onnx::Conv_651
[08/10/2023-11:16:20] [V] [TRT] Searching for input: onnx::Conv_935
[08/10/2023-11:16:20] [V] [TRT] Searching for input: onnx::Conv_936
[08/10/2023-11:16:20] [V] [TRT] Conv_158 [Conv] inputs: [onnx::Conv_651 -> (1, 64, 180, 180)[FLOAT]], [onnx::Conv_935 -> (64, 64, 3, 3)[FLOAT]], [onnx::Conv_936 -> (64)[FLOAT]], 
[08/10/2023-11:16:20] [V] [TRT] Convolution input dimensions: (1, 64, 180, 180)
[08/10/2023-11:16:20] [V] [TRT] Registering layer: Conv_158 for ONNX node: Conv_158
[08/10/2023-11:16:20] [V] [TRT] Using kernel: (3, 3), strides: (1, 1), prepadding: (1, 1), postpadding: (1, 1), dilations: (1, 1), numOutputs: 64
[08/10/2023-11:16:20] [V] [TRT] Convolution output dimensions: (1, 64, 180, 180)
[08/10/2023-11:16:20] [V] [TRT] Registering tensor: input.564 for ONNX tensor: input.564
[08/10/2023-11:16:20] [V] [TRT] Conv_158 [Conv] outputs: [input.564 -> (1, 64, 180, 180)[FLOAT]], 
[08/10/2023-11:16:20] [V] [TRT] Parsing node: Relu_159 [Relu]
[08/10/2023-11:16:20] [V] [TRT] Searching for input: input.564
[08/10/2023-11:16:20] [V] [TRT] Relu_159 [Relu] inputs: [input.564 -> (1, 64, 180, 180)[FLOAT]], 
[08/10/2023-11:16:20] [V] [TRT] Registering layer: Relu_159 for ONNX node: Relu_159
[08/10/2023-11:16:20] [V] [TRT] Registering tensor: input.568 for ONNX tensor: input.568
[08/10/2023-11:16:20] [V] [TRT] Relu_159 [Relu] outputs: [input.568 -> (1, 64, 180, 180)[FLOAT]], 
[08/10/2023-11:16:20] [V] [TRT] Parsing node: Conv_160 [Conv]
[08/10/2023-11:16:20] [V] [TRT] Searching for input: input.568
[08/10/2023-11:16:20] [V] [TRT] Searching for input: model.bbox_head.tasks.5.dim.3.weight
[08/10/2023-11:16:20] [V] [TRT] Searching for input: model.bbox_head.tasks.5.dim.3.bias
[08/10/2023-11:16:20] [V] [TRT] Conv_160 [Conv] inputs: [input.568 -> (1, 64, 180, 180)[FLOAT]], [model.bbox_head.tasks.5.dim.3.weight -> (3, 64, 3, 3)[FLOAT]], [model.bbox_head.tasks.5.dim.3.bias -> (3)[FLOAT]], 
[08/10/2023-11:16:20] [V] [TRT] Convolution input dimensions: (1, 64, 180, 180)
[08/10/2023-11:16:20] [V] [TRT] Registering layer: Conv_160 for ONNX node: Conv_160
[08/10/2023-11:16:20] [V] [TRT] Using kernel: (3, 3), strides: (1, 1), prepadding: (1, 1), postpadding: (1, 1), dilations: (1, 1), numOutputs: 3
[08/10/2023-11:16:20] [V] [TRT] Convolution output dimensions: (1, 3, 180, 180)
[08/10/2023-11:16:20] [V] [TRT] Registering tensor: dim_5_33 for ONNX tensor: dim_5
[08/10/2023-11:16:20] [V] [TRT] Conv_160 [Conv] outputs: [dim_5 -> (1, 3, 180, 180)[FLOAT]], 
[08/10/2023-11:16:20] [V] [TRT] Parsing node: Conv_161 [Conv]
[08/10/2023-11:16:20] [V] [TRT] Searching for input: onnx::Conv_651
[08/10/2023-11:16:20] [V] [TRT] Searching for input: onnx::Conv_938
[08/10/2023-11:16:20] [V] [TRT] Searching for input: onnx::Conv_939
[08/10/2023-11:16:20] [V] [TRT] Conv_161 [Conv] inputs: [onnx::Conv_651 -> (1, 64, 180, 180)[FLOAT]], [onnx::Conv_938 -> (64, 64, 3, 3)[FLOAT]], [onnx::Conv_939 -> (64)[FLOAT]], 
[08/10/2023-11:16:20] [V] [TRT] Convolution input dimensions: (1, 64, 180, 180)
[08/10/2023-11:16:20] [V] [TRT] Registering layer: Conv_161 for ONNX node: Conv_161
[08/10/2023-11:16:20] [V] [TRT] Using kernel: (3, 3), strides: (1, 1), prepadding: (1, 1), postpadding: (1, 1), dilations: (1, 1), numOutputs: 64
[08/10/2023-11:16:20] [V] [TRT] Convolution output dimensions: (1, 64, 180, 180)
[08/10/2023-11:16:20] [V] [TRT] Registering tensor: input.576 for ONNX tensor: input.576
[08/10/2023-11:16:20] [V] [TRT] Conv_161 [Conv] outputs: [input.576 -> (1, 64, 180, 180)[FLOAT]], 
[08/10/2023-11:16:20] [V] [TRT] Parsing node: Relu_162 [Relu]
[08/10/2023-11:16:20] [V] [TRT] Searching for input: input.576
[08/10/2023-11:16:20] [V] [TRT] Relu_162 [Relu] inputs: [input.576 -> (1, 64, 180, 180)[FLOAT]], 
[08/10/2023-11:16:20] [V] [TRT] Registering layer: Relu_162 for ONNX node: Relu_162
[08/10/2023-11:16:20] [V] [TRT] Registering tensor: input.580 for ONNX tensor: input.580
[08/10/2023-11:16:20] [V] [TRT] Relu_162 [Relu] outputs: [input.580 -> (1, 64, 180, 180)[FLOAT]], 
[08/10/2023-11:16:20] [V] [TRT] Parsing node: Conv_163 [Conv]
[08/10/2023-11:16:20] [V] [TRT] Searching for input: input.580
[08/10/2023-11:16:20] [V] [TRT] Searching for input: model.bbox_head.tasks.5.rot.3.weight
[08/10/2023-11:16:20] [V] [TRT] Searching for input: model.bbox_head.tasks.5.rot.3.bias
[08/10/2023-11:16:20] [V] [TRT] Conv_163 [Conv] inputs: [input.580 -> (1, 64, 180, 180)[FLOAT]], [model.bbox_head.tasks.5.rot.3.weight -> (2, 64, 3, 3)[FLOAT]], [model.bbox_head.tasks.5.rot.3.bias -> (2)[FLOAT]], 
[08/10/2023-11:16:20] [V] [TRT] Convolution input dimensions: (1, 64, 180, 180)
[08/10/2023-11:16:20] [V] [TRT] Registering layer: Conv_163 for ONNX node: Conv_163
[08/10/2023-11:16:20] [V] [TRT] Using kernel: (3, 3), strides: (1, 1), prepadding: (1, 1), postpadding: (1, 1), dilations: (1, 1), numOutputs: 2
[08/10/2023-11:16:20] [V] [TRT] Convolution output dimensions: (1, 2, 180, 180)
[08/10/2023-11:16:20] [V] [TRT] Registering tensor: rot_5_34 for ONNX tensor: rot_5
[08/10/2023-11:16:20] [V] [TRT] Conv_163 [Conv] outputs: [rot_5 -> (1, 2, 180, 180)[FLOAT]], 
[08/10/2023-11:16:20] [V] [TRT] Parsing node: Conv_164 [Conv]
[08/10/2023-11:16:20] [V] [TRT] Searching for input: onnx::Conv_651
[08/10/2023-11:16:20] [V] [TRT] Searching for input: onnx::Conv_941
[08/10/2023-11:16:20] [V] [TRT] Searching for input: onnx::Conv_942
[08/10/2023-11:16:20] [V] [TRT] Conv_164 [Conv] inputs: [onnx::Conv_651 -> (1, 64, 180, 180)[FLOAT]], [onnx::Conv_941 -> (64, 64, 3, 3)[FLOAT]], [onnx::Conv_942 -> (64)[FLOAT]], 
[08/10/2023-11:16:20] [V] [TRT] Convolution input dimensions: (1, 64, 180, 180)
[08/10/2023-11:16:20] [V] [TRT] Registering layer: Conv_164 for ONNX node: Conv_164
[08/10/2023-11:16:20] [V] [TRT] Using kernel: (3, 3), strides: (1, 1), prepadding: (1, 1), postpadding: (1, 1), dilations: (1, 1), numOutputs: 64
[08/10/2023-11:16:20] [V] [TRT] Convolution output dimensions: (1, 64, 180, 180)
[08/10/2023-11:16:20] [V] [TRT] Registering tensor: input.588 for ONNX tensor: input.588
[08/10/2023-11:16:20] [V] [TRT] Conv_164 [Conv] outputs: [input.588 -> (1, 64, 180, 180)[FLOAT]], 
[08/10/2023-11:16:20] [V] [TRT] Parsing node: Relu_165 [Relu]
[08/10/2023-11:16:20] [V] [TRT] Searching for input: input.588
[08/10/2023-11:16:20] [V] [TRT] Relu_165 [Relu] inputs: [input.588 -> (1, 64, 180, 180)[FLOAT]], 
[08/10/2023-11:16:20] [V] [TRT] Registering layer: Relu_165 for ONNX node: Relu_165
[08/10/2023-11:16:20] [V] [TRT] Registering tensor: input.592 for ONNX tensor: input.592
[08/10/2023-11:16:20] [V] [TRT] Relu_165 [Relu] outputs: [input.592 -> (1, 64, 180, 180)[FLOAT]], 
[08/10/2023-11:16:20] [V] [TRT] Parsing node: Conv_166 [Conv]
[08/10/2023-11:16:20] [V] [TRT] Searching for input: input.592
[08/10/2023-11:16:20] [V] [TRT] Searching for input: model.bbox_head.tasks.5.vel.3.weight
[08/10/2023-11:16:20] [V] [TRT] Searching for input: model.bbox_head.tasks.5.vel.3.bias
[08/10/2023-11:16:20] [V] [TRT] Conv_166 [Conv] inputs: [input.592 -> (1, 64, 180, 180)[FLOAT]], [model.bbox_head.tasks.5.vel.3.weight -> (2, 64, 3, 3)[FLOAT]], [model.bbox_head.tasks.5.vel.3.bias -> (2)[FLOAT]], 
[08/10/2023-11:16:20] [V] [TRT] Convolution input dimensions: (1, 64, 180, 180)
[08/10/2023-11:16:20] [V] [TRT] Registering layer: Conv_166 for ONNX node: Conv_166
[08/10/2023-11:16:20] [V] [TRT] Using kernel: (3, 3), strides: (1, 1), prepadding: (1, 1), postpadding: (1, 1), dilations: (1, 1), numOutputs: 2
[08/10/2023-11:16:20] [V] [TRT] Convolution output dimensions: (1, 2, 180, 180)
[08/10/2023-11:16:20] [V] [TRT] Registering tensor: vel_5_35 for ONNX tensor: vel_5
[08/10/2023-11:16:20] [V] [TRT] Conv_166 [Conv] outputs: [vel_5 -> (1, 2, 180, 180)[FLOAT]], 
[08/10/2023-11:16:20] [V] [TRT] Parsing node: Conv_167 [Conv]
[08/10/2023-11:16:20] [V] [TRT] Searching for input: onnx::Conv_651
[08/10/2023-11:16:20] [V] [TRT] Searching for input: onnx::Conv_944
[08/10/2023-11:16:20] [V] [TRT] Searching for input: onnx::Conv_945
[08/10/2023-11:16:20] [V] [TRT] Conv_167 [Conv] inputs: [onnx::Conv_651 -> (1, 64, 180, 180)[FLOAT]], [onnx::Conv_944 -> (64, 64, 3, 3)[FLOAT]], [onnx::Conv_945 -> (64)[FLOAT]], 
[08/10/2023-11:16:20] [V] [TRT] Convolution input dimensions: (1, 64, 180, 180)
[08/10/2023-11:16:20] [V] [TRT] Registering layer: Conv_167 for ONNX node: Conv_167
[08/10/2023-11:16:20] [V] [TRT] Using kernel: (3, 3), strides: (1, 1), prepadding: (1, 1), postpadding: (1, 1), dilations: (1, 1), numOutputs: 64
[08/10/2023-11:16:20] [V] [TRT] Convolution output dimensions: (1, 64, 180, 180)
[08/10/2023-11:16:20] [V] [TRT] Registering tensor: input.600 for ONNX tensor: input.600
[08/10/2023-11:16:20] [V] [TRT] Conv_167 [Conv] outputs: [input.600 -> (1, 64, 180, 180)[FLOAT]], 
[08/10/2023-11:16:20] [V] [TRT] Parsing node: Relu_168 [Relu]
[08/10/2023-11:16:20] [V] [TRT] Searching for input: input.600
[08/10/2023-11:16:20] [V] [TRT] Relu_168 [Relu] inputs: [input.600 -> (1, 64, 180, 180)[FLOAT]], 
[08/10/2023-11:16:20] [V] [TRT] Registering layer: Relu_168 for ONNX node: Relu_168
[08/10/2023-11:16:20] [V] [TRT] Registering tensor: input.604 for ONNX tensor: input.604
[08/10/2023-11:16:20] [V] [TRT] Relu_168 [Relu] outputs: [input.604 -> (1, 64, 180, 180)[FLOAT]], 
[08/10/2023-11:16:20] [V] [TRT] Parsing node: Conv_169 [Conv]
[08/10/2023-11:16:20] [V] [TRT] Searching for input: input.604
[08/10/2023-11:16:20] [V] [TRT] Searching for input: model.bbox_head.tasks.5.hm.3.weight
[08/10/2023-11:16:20] [V] [TRT] Searching for input: model.bbox_head.tasks.5.hm.3.bias
[08/10/2023-11:16:20] [V] [TRT] Conv_169 [Conv] inputs: [input.604 -> (1, 64, 180, 180)[FLOAT]], [model.bbox_head.tasks.5.hm.3.weight -> (2, 64, 3, 3)[FLOAT]], [model.bbox_head.tasks.5.hm.3.bias -> (2)[FLOAT]], 
[08/10/2023-11:16:20] [V] [TRT] Convolution input dimensions: (1, 64, 180, 180)
[08/10/2023-11:16:20] [V] [TRT] Registering layer: Conv_169 for ONNX node: Conv_169
[08/10/2023-11:16:20] [V] [TRT] Using kernel: (3, 3), strides: (1, 1), prepadding: (1, 1), postpadding: (1, 1), dilations: (1, 1), numOutputs: 2
[08/10/2023-11:16:20] [V] [TRT] Convolution output dimensions: (1, 2, 180, 180)
[08/10/2023-11:16:20] [V] [TRT] Registering tensor: hm_5_36 for ONNX tensor: hm_5
[08/10/2023-11:16:20] [V] [TRT] Conv_169 [Conv] outputs: [hm_5 -> (1, 2, 180, 180)[FLOAT]], 
[08/10/2023-11:16:20] [V] [TRT] Marking reg_0_1 as output: reg_0
[08/10/2023-11:16:20] [V] [TRT] Marking height_0_2 as output: height_0
[08/10/2023-11:16:20] [V] [TRT] Marking dim_0_3 as output: dim_0
[08/10/2023-11:16:20] [V] [TRT] Marking rot_0_4 as output: rot_0
[08/10/2023-11:16:20] [V] [TRT] Marking vel_0_5 as output: vel_0
[08/10/2023-11:16:20] [V] [TRT] Marking hm_0_6 as output: hm_0
[08/10/2023-11:16:20] [V] [TRT] Marking reg_1_7 as output: reg_1
[08/10/2023-11:16:20] [V] [TRT] Marking height_1_8 as output: height_1
[08/10/2023-11:16:20] [V] [TRT] Marking dim_1_9 as output: dim_1
[08/10/2023-11:16:20] [V] [TRT] Marking rot_1_10 as output: rot_1
[08/10/2023-11:16:20] [V] [TRT] Marking vel_1_11 as output: vel_1
[08/10/2023-11:16:20] [V] [TRT] Marking hm_1_12 as output: hm_1
[08/10/2023-11:16:20] [V] [TRT] Marking reg_2_13 as output: reg_2
[08/10/2023-11:16:20] [V] [TRT] Marking height_2_14 as output: height_2
[08/10/2023-11:16:20] [V] [TRT] Marking dim_2_15 as output: dim_2
[08/10/2023-11:16:20] [V] [TRT] Marking rot_2_16 as output: rot_2
[08/10/2023-11:16:20] [V] [TRT] Marking vel_2_17 as output: vel_2
[08/10/2023-11:16:20] [V] [TRT] Marking hm_2_18 as output: hm_2
[08/10/2023-11:16:20] [V] [TRT] Marking reg_3_19 as output: reg_3
[08/10/2023-11:16:20] [V] [TRT] Marking height_3_20 as output: height_3
[08/10/2023-11:16:20] [V] [TRT] Marking dim_3_21 as output: dim_3
[08/10/2023-11:16:20] [V] [TRT] Marking rot_3_22 as output: rot_3
[08/10/2023-11:16:20] [V] [TRT] Marking vel_3_23 as output: vel_3
[08/10/2023-11:16:20] [V] [TRT] Marking hm_3_24 as output: hm_3
[08/10/2023-11:16:20] [V] [TRT] Marking reg_4_25 as output: reg_4
[08/10/2023-11:16:20] [V] [TRT] Marking height_4_26 as output: height_4
[08/10/2023-11:16:20] [V] [TRT] Marking dim_4_27 as output: dim_4
[08/10/2023-11:16:20] [V] [TRT] Marking rot_4_28 as output: rot_4
[08/10/2023-11:16:20] [V] [TRT] Marking vel_4_29 as output: vel_4
[08/10/2023-11:16:20] [V] [TRT] Marking hm_4_30 as output: hm_4
[08/10/2023-11:16:20] [V] [TRT] Marking reg_5_31 as output: reg_5
[08/10/2023-11:16:20] [V] [TRT] Marking height_5_32 as output: height_5
[08/10/2023-11:16:20] [V] [TRT] Marking dim_5_33 as output: dim_5
[08/10/2023-11:16:20] [V] [TRT] Marking rot_5_34 as output: rot_5
[08/10/2023-11:16:20] [V] [TRT] Marking vel_5_35 as output: vel_5
[08/10/2023-11:16:20] [V] [TRT] Marking hm_5_36 as output: hm_5
[08/10/2023-11:16:20] [I] Finish parsing network model
[08/10/2023-11:16:20] [V] [TRT] Applying generic optimizations to the graph for inference.
[08/10/2023-11:16:20] [V] [TRT] Original: 140 layers
[08/10/2023-11:16:20] [V] [TRT] After dead-layer removal: 140 layers
[08/10/2023-11:16:20] [V] [TRT] After Myelin optimization: 140 layers
[08/10/2023-11:16:20] [V] [TRT] Applying ScaleNodes fusions.
[08/10/2023-11:16:20] [V] [TRT] After scale fusion: 140 layers
[08/10/2023-11:16:20] [V] [TRT] Running: ConvReluFusion on Conv_15
[08/10/2023-11:16:20] [V] [TRT] ConvReluFusion: Fusing Conv_15 with Relu_16
[08/10/2023-11:16:20] [V] [TRT] Running: ConvReluFusion on Conv_17
[08/10/2023-11:16:20] [V] [TRT] ConvReluFusion: Fusing Conv_17 with Relu_18
[08/10/2023-11:16:20] [V] [TRT] Running: ConvReluFusion on Conv_19
[08/10/2023-11:16:20] [V] [TRT] ConvReluFusion: Fusing Conv_19 with Relu_20
[08/10/2023-11:16:20] [V] [TRT] Running: ConvReluFusion on Conv_21
[08/10/2023-11:16:20] [V] [TRT] ConvReluFusion: Fusing Conv_21 with Relu_22
[08/10/2023-11:16:20] [V] [TRT] Running: ConvReluFusion on Conv_23
[08/10/2023-11:16:20] [V] [TRT] ConvReluFusion: Fusing Conv_23 with Relu_24
[08/10/2023-11:16:20] [V] [TRT] Running: ConvReluFusion on Conv_25
[08/10/2023-11:16:20] [V] [TRT] ConvReluFusion: Fusing Conv_25 with Relu_26
[08/10/2023-11:16:20] [V] [TRT] Running: ConvReluFusion on Conv_27
[08/10/2023-11:16:20] [V] [TRT] ConvReluFusion: Fusing Conv_27 with Relu_28
[08/10/2023-11:16:20] [V] [TRT] Running: ConvReluFusion on Conv_44
[08/10/2023-11:16:20] [V] [TRT] ConvReluFusion: Fusing Conv_44 with Relu_45
[08/10/2023-11:16:20] [V] [TRT] Running: ConvReluFusion on Conv_46
[08/10/2023-11:16:20] [V] [TRT] ConvReluFusion: Fusing Conv_46 with Relu_47
[08/10/2023-11:16:20] [V] [TRT] Running: ConvReluFusion on Conv_48
[08/10/2023-11:16:20] [V] [TRT] ConvReluFusion: Fusing Conv_48 with Relu_49
[08/10/2023-11:16:20] [V] [TRT] Running: ConvReluFusion on Conv_50
[08/10/2023-11:16:20] [V] [TRT] ConvReluFusion: Fusing Conv_50 with Relu_51
[08/10/2023-11:16:20] [V] [TRT] Running: ConvReluFusion on Conv_52
[08/10/2023-11:16:20] [V] [TRT] ConvReluFusion: Fusing Conv_52 with Relu_53
[08/10/2023-11:16:20] [V] [TRT] Running: ConvReluFusion on Conv_54
[08/10/2023-11:16:20] [V] [TRT] ConvReluFusion: Fusing Conv_54 with Relu_55
[08/10/2023-11:16:20] [V] [TRT] Running: DeconvScaleFusion on ConvTranspose_56
[08/10/2023-11:16:20] [V] [TRT] DeconvScaleFusion: Fusing ConvTranspose_56 with BatchNormalization_57
[08/10/2023-11:16:20] [V] [TRT] Running: DeconvReluFusion on ConvTranspose_56 + BatchNormalization_57
[08/10/2023-11:16:20] [V] [TRT] DeconvReluFusion: Fusing ConvTranspose_56 + BatchNormalization_57 with Relu_58
[08/10/2023-11:16:20] [V] [TRT] Running: ConvReluFusion on Conv_60
[08/10/2023-11:16:20] [V] [TRT] ConvReluFusion: Fusing Conv_60 with Relu_61
[08/10/2023-11:16:20] [V] [TRT] Running: ConvReluFusion on Conv_62
[08/10/2023-11:16:20] [V] [TRT] ConvReluFusion: Fusing Conv_62 with Relu_63
[08/10/2023-11:16:20] [V] [TRT] Running: ConvReluFusion on Conv_65
[08/10/2023-11:16:20] [V] [TRT] ConvReluFusion: Fusing Conv_65 with Relu_66
[08/10/2023-11:16:20] [V] [TRT] Running: ConvReluFusion on Conv_68
[08/10/2023-11:16:20] [V] [TRT] ConvReluFusion: Fusing Conv_68 with Relu_69
[08/10/2023-11:16:20] [V] [TRT] Running: ConvReluFusion on Conv_71
[08/10/2023-11:16:20] [V] [TRT] ConvReluFusion: Fusing Conv_71 with Relu_72
[08/10/2023-11:16:20] [V] [TRT] Running: ConvReluFusion on Conv_74
[08/10/2023-11:16:20] [V] [TRT] ConvReluFusion: Fusing Conv_74 with Relu_75
[08/10/2023-11:16:20] [V] [TRT] Running: ConvReluFusion on Conv_77
[08/10/2023-11:16:20] [V] [TRT] ConvReluFusion: Fusing Conv_77 with Relu_78
[08/10/2023-11:16:20] [V] [TRT] Running: ConvReluFusion on Conv_80
[08/10/2023-11:16:20] [V] [TRT] ConvReluFusion: Fusing Conv_80 with Relu_81
[08/10/2023-11:16:20] [V] [TRT] Running: ConvReluFusion on Conv_83
[08/10/2023-11:16:20] [V] [TRT] ConvReluFusion: Fusing Conv_83 with Relu_84
[08/10/2023-11:16:20] [V] [TRT] Running: ConvReluFusion on Conv_86
[08/10/2023-11:16:20] [V] [TRT] ConvReluFusion: Fusing Conv_86 with Relu_87
[08/10/2023-11:16:20] [V] [TRT] Running: ConvReluFusion on Conv_89
[08/10/2023-11:16:20] [V] [TRT] ConvReluFusion: Fusing Conv_89 with Relu_90
[08/10/2023-11:16:20] [V] [TRT] Running: ConvReluFusion on Conv_92
[08/10/2023-11:16:20] [V] [TRT] ConvReluFusion: Fusing Conv_92 with Relu_93
[08/10/2023-11:16:20] [V] [TRT] Running: ConvReluFusion on Conv_95
[08/10/2023-11:16:20] [V] [TRT] ConvReluFusion: Fusing Conv_95 with Relu_96
[08/10/2023-11:16:20] [V] [TRT] Running: ConvReluFusion on Conv_98
[08/10/2023-11:16:20] [V] [TRT] ConvReluFusion: Fusing Conv_98 with Relu_99
[08/10/2023-11:16:20] [V] [TRT] Running: ConvReluFusion on Conv_101
[08/10/2023-11:16:20] [V] [TRT] ConvReluFusion: Fusing Conv_101 with Relu_102
[08/10/2023-11:16:20] [V] [TRT] Running: ConvReluFusion on Conv_104
[08/10/2023-11:16:20] [V] [TRT] ConvReluFusion: Fusing Conv_104 with Relu_105
[08/10/2023-11:16:20] [V] [TRT] Running: ConvReluFusion on Conv_107
[08/10/2023-11:16:20] [V] [TRT] ConvReluFusion: Fusing Conv_107 with Relu_108
[08/10/2023-11:16:20] [V] [TRT] Running: ConvReluFusion on Conv_110
[08/10/2023-11:16:20] [V] [TRT] ConvReluFusion: Fusing Conv_110 with Relu_111
[08/10/2023-11:16:20] [V] [TRT] Running: ConvReluFusion on Conv_113
[08/10/2023-11:16:20] [V] [TRT] ConvReluFusion: Fusing Conv_113 with Relu_114
[08/10/2023-11:16:20] [V] [TRT] Running: ConvReluFusion on Conv_116
[08/10/2023-11:16:20] [V] [TRT] ConvReluFusion: Fusing Conv_116 with Relu_117
[08/10/2023-11:16:20] [V] [TRT] Running: ConvReluFusion on Conv_119
[08/10/2023-11:16:20] [V] [TRT] ConvReluFusion: Fusing Conv_119 with Relu_120
[08/10/2023-11:16:20] [V] [TRT] Running: ConvReluFusion on Conv_122
[08/10/2023-11:16:20] [V] [TRT] ConvReluFusion: Fusing Conv_122 with Relu_123
[08/10/2023-11:16:20] [V] [TRT] Running: ConvReluFusion on Conv_125
[08/10/2023-11:16:20] [V] [TRT] ConvReluFusion: Fusing Conv_125 with Relu_126
[08/10/2023-11:16:20] [V] [TRT] Running: ConvReluFusion on Conv_128
[08/10/2023-11:16:20] [V] [TRT] ConvReluFusion: Fusing Conv_128 with Relu_129
[08/10/2023-11:16:20] [V] [TRT] Running: ConvReluFusion on Conv_131
[08/10/2023-11:16:20] [V] [TRT] ConvReluFusion: Fusing Conv_131 with Relu_132
[08/10/2023-11:16:20] [V] [TRT] Running: ConvReluFusion on Conv_134
[08/10/2023-11:16:20] [V] [TRT] ConvReluFusion: Fusing Conv_134 with Relu_135
[08/10/2023-11:16:20] [V] [TRT] Running: ConvReluFusion on Conv_137
[08/10/2023-11:16:20] [V] [TRT] ConvReluFusion: Fusing Conv_137 with Relu_138
[08/10/2023-11:16:20] [V] [TRT] Running: ConvReluFusion on Conv_140
[08/10/2023-11:16:20] [V] [TRT] ConvReluFusion: Fusing Conv_140 with Relu_141
[08/10/2023-11:16:20] [V] [TRT] Running: ConvReluFusion on Conv_143
[08/10/2023-11:16:20] [V] [TRT] ConvReluFusion: Fusing Conv_143 with Relu_144
[08/10/2023-11:16:20] [V] [TRT] Running: ConvReluFusion on Conv_146
[08/10/2023-11:16:20] [V] [TRT] ConvReluFusion: Fusing Conv_146 with Relu_147
[08/10/2023-11:16:20] [V] [TRT] Running: ConvReluFusion on Conv_149
[08/10/2023-11:16:20] [V] [TRT] ConvReluFusion: Fusing Conv_149 with Relu_150
[08/10/2023-11:16:20] [V] [TRT] Running: ConvReluFusion on Conv_152
[08/10/2023-11:16:20] [V] [TRT] ConvReluFusion: Fusing Conv_152 with Relu_153
[08/10/2023-11:16:20] [V] [TRT] Running: ConvReluFusion on Conv_155
[08/10/2023-11:16:20] [V] [TRT] ConvReluFusion: Fusing Conv_155 with Relu_156
[08/10/2023-11:16:20] [V] [TRT] Running: ConvReluFusion on Conv_158
[08/10/2023-11:16:20] [V] [TRT] ConvReluFusion: Fusing Conv_158 with Relu_159
[08/10/2023-11:16:20] [V] [TRT] Running: ConvReluFusion on Conv_161
[08/10/2023-11:16:20] [V] [TRT] ConvReluFusion: Fusing Conv_161 with Relu_162
[08/10/2023-11:16:20] [V] [TRT] Running: ConvReluFusion on Conv_164
[08/10/2023-11:16:20] [V] [TRT] ConvReluFusion: Fusing Conv_164 with Relu_165
[08/10/2023-11:16:20] [V] [TRT] Running: ConvReluFusion on Conv_167
[08/10/2023-11:16:20] [V] [TRT] ConvReluFusion: Fusing Conv_167 with Relu_168
[08/10/2023-11:16:20] [V] [TRT] After dupe layer removal: 88 layers
[08/10/2023-11:16:20] [V] [TRT] After final dead-layer removal: 88 layers
[08/10/2023-11:16:20] [V] [TRT] After tensor merging: 88 layers
[08/10/2023-11:16:20] [V] [TRT] After vertical fusions: 88 layers
[08/10/2023-11:16:20] [V] [TRT] After dupe layer removal: 88 layers
[08/10/2023-11:16:20] [V] [TRT] After final dead-layer removal: 88 layers
[08/10/2023-11:16:20] [V] [TRT] Merging layers: Conv_62 + Relu_63 || Conv_65 + Relu_66 || Conv_68 + Relu_69 || Conv_71 + Relu_72 || Conv_74 + Relu_75 || Conv_77 + Relu_78 || Conv_80 + Relu_81 || Conv_83 + Relu_84
[08/10/2023-11:16:20] [V] [TRT] Merging layers: Conv_86 + Relu_87 || Conv_89 + Relu_90 || Conv_92 + Relu_93 || Conv_95 + Relu_96 || Conv_98 + Relu_99 || Conv_101 + Relu_102 || Conv_104 + Relu_105 || Conv_107 + Relu_108
[08/10/2023-11:16:20] [V] [TRT] Merging layers: Conv_110 + Relu_111 || Conv_113 + Relu_114 || Conv_116 + Relu_117 || Conv_119 + Relu_120 || Conv_122 + Relu_123 || Conv_125 + Relu_126 || Conv_128 + Relu_129 || Conv_131 + Relu_132
[08/10/2023-11:16:20] [V] [TRT] Merging layers: Conv_134 + Relu_135 || Conv_137 + Relu_138 || Conv_140 + Relu_141 || Conv_143 + Relu_144 || Conv_146 + Relu_147 || Conv_149 + Relu_150 || Conv_152 + Relu_153 || Conv_155 + Relu_156
[08/10/2023-11:16:20] [V] [TRT] Merging layers: Conv_158 + Relu_159 || Conv_161 + Relu_162 || Conv_164 + Relu_165 || Conv_167 + Relu_168
[08/10/2023-11:16:20] [V] [TRT] After tensor merging: 57 layers
[08/10/2023-11:16:20] [V] [TRT] After slice removal: 57 layers
[08/10/2023-11:16:20] [V] [TRT] Eliminating concatenation Concat_59
[08/10/2023-11:16:20] [V] [TRT] Retargeting onnx::Concat_602 to input.164
[08/10/2023-11:16:20] [V] [TRT] Generating copy for onnx::Concat_647 to input.164 because input does not support striding.
[08/10/2023-11:16:20] [V] [TRT] After concat removal: 57 layers
[08/10/2023-11:16:20] [V] [TRT] Trying to split Reshape and strided tensor
[08/10/2023-11:16:20] [V] [TRT] Graph construction and optimization completed in 0.0275376 seconds.
[08/10/2023-11:16:20] [I] [TRT] ---------- Layers Running on DLA ----------
[08/10/2023-11:16:20] [I] [TRT] ---------- Layers Running on GPU ----------
[08/10/2023-11:16:20] [I] [TRT] [GpuLayer] CONVOLUTION: Conv_15 + Relu_16
[08/10/2023-11:16:20] [I] [TRT] [GpuLayer] CONVOLUTION: Conv_17 + Relu_18
[08/10/2023-11:16:20] [I] [TRT] [GpuLayer] CONVOLUTION: Conv_19 + Relu_20
[08/10/2023-11:16:20] [I] [TRT] [GpuLayer] CONVOLUTION: Conv_21 + Relu_22
[08/10/2023-11:16:20] [I] [TRT] [GpuLayer] CONVOLUTION: Conv_23 + Relu_24
[08/10/2023-11:16:20] [I] [TRT] [GpuLayer] CONVOLUTION: Conv_25 + Relu_26
[08/10/2023-11:16:20] [I] [TRT] [GpuLayer] CONVOLUTION: Conv_27 + Relu_28
[08/10/2023-11:16:20] [I] [TRT] [GpuLayer] CONVOLUTION: Conv_44 + Relu_45
[08/10/2023-11:16:20] [I] [TRT] [GpuLayer] CONVOLUTION: Conv_46 + Relu_47
[08/10/2023-11:16:20] [I] [TRT] [GpuLayer] CONVOLUTION: Conv_48 + Relu_49
[08/10/2023-11:16:20] [I] [TRT] [GpuLayer] CONVOLUTION: Conv_50 + Relu_51
[08/10/2023-11:16:20] [I] [TRT] [GpuLayer] CONVOLUTION: Conv_52 + Relu_53
[08/10/2023-11:16:20] [I] [TRT] [GpuLayer] CONVOLUTION: Conv_54 + Relu_55
[08/10/2023-11:16:20] [I] [TRT] [GpuLayer] DECONVOLUTION: ConvTranspose_56 + BatchNormalization_57 + Relu_58
[08/10/2023-11:16:20] [I] [TRT] [GpuLayer] COPY: onnx::Concat_647 copy
[08/10/2023-11:16:20] [I] [TRT] [GpuLayer] CONVOLUTION: Conv_60 + Relu_61
[08/10/2023-11:16:20] [I] [TRT] [GpuLayer] CONVOLUTION: Conv_62 + Relu_63 || Conv_65 + Relu_66 || Conv_68 + Relu_69 || Conv_71 + Relu_72 || Conv_74 + Relu_75 || Conv_77 + Relu_78 || Conv_80 + Relu_81 || Conv_83 + Relu_84
[08/10/2023-11:16:20] [I] [TRT] [GpuLayer] CONVOLUTION: Conv_64
[08/10/2023-11:16:20] [I] [TRT] [GpuLayer] CONVOLUTION: Conv_67
[08/10/2023-11:16:20] [I] [TRT] [GpuLayer] CONVOLUTION: Conv_70
[08/10/2023-11:16:20] [I] [TRT] [GpuLayer] CONVOLUTION: Conv_73
[08/10/2023-11:16:20] [I] [TRT] [GpuLayer] CONVOLUTION: Conv_76
[08/10/2023-11:16:20] [I] [TRT] [GpuLayer] CONVOLUTION: Conv_79
[08/10/2023-11:16:20] [I] [TRT] [GpuLayer] CONVOLUTION: Conv_82
[08/10/2023-11:16:20] [I] [TRT] [GpuLayer] CONVOLUTION: Conv_85
[08/10/2023-11:16:20] [I] [TRT] [GpuLayer] CONVOLUTION: Conv_86 + Relu_87 || Conv_89 + Relu_90 || Conv_92 + Relu_93 || Conv_95 + Relu_96 || Conv_98 + Relu_99 || Conv_101 + Relu_102 || Conv_104 + Relu_105 || Conv_107 + Relu_108
[08/10/2023-11:16:20] [I] [TRT] [GpuLayer] CONVOLUTION: Conv_88
[08/10/2023-11:16:20] [I] [TRT] [GpuLayer] CONVOLUTION: Conv_91
[08/10/2023-11:16:20] [I] [TRT] [GpuLayer] CONVOLUTION: Conv_94
[08/10/2023-11:16:20] [I] [TRT] [GpuLayer] CONVOLUTION: Conv_97
[08/10/2023-11:16:20] [I] [TRT] [GpuLayer] CONVOLUTION: Conv_100
[08/10/2023-11:16:20] [I] [TRT] [GpuLayer] CONVOLUTION: Conv_103
[08/10/2023-11:16:20] [I] [TRT] [GpuLayer] CONVOLUTION: Conv_106
[08/10/2023-11:16:20] [I] [TRT] [GpuLayer] CONVOLUTION: Conv_109
[08/10/2023-11:16:20] [I] [TRT] [GpuLayer] CONVOLUTION: Conv_110 + Relu_111 || Conv_113 + Relu_114 || Conv_116 + Relu_117 || Conv_119 + Relu_120 || Conv_122 + Relu_123 || Conv_125 + Relu_126 || Conv_128 + Relu_129 || Conv_131 + Relu_132
[08/10/2023-11:16:20] [I] [TRT] [GpuLayer] CONVOLUTION: Conv_112
[08/10/2023-11:16:20] [I] [TRT] [GpuLayer] CONVOLUTION: Conv_115
[08/10/2023-11:16:20] [I] [TRT] [GpuLayer] CONVOLUTION: Conv_118
[08/10/2023-11:16:20] [I] [TRT] [GpuLayer] CONVOLUTION: Conv_121
[08/10/2023-11:16:20] [I] [TRT] [GpuLayer] CONVOLUTION: Conv_124
[08/10/2023-11:16:20] [I] [TRT] [GpuLayer] CONVOLUTION: Conv_127
[08/10/2023-11:16:20] [I] [TRT] [GpuLayer] CONVOLUTION: Conv_130
[08/10/2023-11:16:20] [I] [TRT] [GpuLayer] CONVOLUTION: Conv_133
[08/10/2023-11:16:20] [I] [TRT] [GpuLayer] CONVOLUTION: Conv_134 + Relu_135 || Conv_137 + Relu_138 || Conv_140 + Relu_141 || Conv_143 + Relu_144 || Conv_146 + Relu_147 || Conv_149 + Relu_150 || Conv_152 + Relu_153 || Conv_155 + Relu_156
[08/10/2023-11:16:20] [I] [TRT] [GpuLayer] CONVOLUTION: Conv_136
[08/10/2023-11:16:20] [I] [TRT] [GpuLayer] CONVOLUTION: Conv_139
[08/10/2023-11:16:20] [I] [TRT] [GpuLayer] CONVOLUTION: Conv_142
[08/10/2023-11:16:20] [I] [TRT] [GpuLayer] CONVOLUTION: Conv_145
[08/10/2023-11:16:20] [I] [TRT] [GpuLayer] CONVOLUTION: Conv_148
[08/10/2023-11:16:20] [I] [TRT] [GpuLayer] CONVOLUTION: Conv_151
[08/10/2023-11:16:20] [I] [TRT] [GpuLayer] CONVOLUTION: Conv_154
[08/10/2023-11:16:20] [I] [TRT] [GpuLayer] CONVOLUTION: Conv_157
[08/10/2023-11:16:20] [I] [TRT] [GpuLayer] CONVOLUTION: Conv_158 + Relu_159 || Conv_161 + Relu_162 || Conv_164 + Relu_165 || Conv_167 + Relu_168
[08/10/2023-11:16:20] [I] [TRT] [GpuLayer] CONVOLUTION: Conv_160
[08/10/2023-11:16:20] [I] [TRT] [GpuLayer] CONVOLUTION: Conv_163
[08/10/2023-11:16:20] [I] [TRT] [GpuLayer] CONVOLUTION: Conv_166
[08/10/2023-11:16:20] [I] [TRT] [GpuLayer] CONVOLUTION: Conv_169
[08/10/2023-11:16:23] [V] [TRT] Using cublasLt as a tactic source
[08/10/2023-11:16:23] [I] [TRT] [MemUsageChange] Init cuBLAS/cuBLASLt: CPU +534, GPU +806, now: CPU 1177, GPU 6936 (MiB)
[08/10/2023-11:16:23] [V] [TRT] Using cuDNN as a tactic source
[08/10/2023-11:16:23] [I] [TRT] [MemUsageChange] Init cuDNN: CPU +86, GPU +142, now: CPU 1263, GPU 7078 (MiB)
[08/10/2023-11:16:23] [I] [TRT] Local timing cache in use. Profiling results in this builder pass will not be stored.
[08/10/2023-11:16:23] [V] [TRT] Constructing optimization profile number 0 [1/1].
[08/10/2023-11:16:23] [V] [TRT] Reserving memory for host IO tensors. Host: 0 bytes
[08/10/2023-11:16:23] [V] [TRT] =============== Computing reformatting costs
[08/10/2023-11:16:23] [V] [TRT] *************** Autotuning Reformat: Half(8294400,32400,180,1) -> Float(8294400,32400,180,1) ***************
[08/10/2023-11:16:23] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(input -> <out>) (Reformat)
[08/10/2023-11:16:23] [V] [TRT] Tactic: 0x00000000000003e8 Time: 1.0168
[08/10/2023-11:16:23] [V] [TRT] Tactic: 0x00000000000003ea Time: 1.37268
[08/10/2023-11:16:24] [V] [TRT] Tactic: 0x0000000000000000 Time: 2.41574
[08/10/2023-11:16:24] [V] [TRT] Fastest Tactic: 0x00000000000003e8 Time: 1.0168
[08/10/2023-11:16:24] [V] [TRT] *************** Autotuning Reformat: Half(8294400,32400,180,1) -> Float(8294400,1,46080,256) ***************
[08/10/2023-11:16:24] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(input -> <out>) (Reformat)
[08/10/2023-11:16:24] [V] [TRT] Tactic: 0x00000000000003e8 Time: 4.49254
[08/10/2023-11:16:24] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.605349
[08/10/2023-11:16:24] [V] [TRT] Tactic: 0x0000000000000000 Time: 2.2717
[08/10/2023-11:16:24] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 0.605349
[08/10/2023-11:16:24] [V] [TRT] *************** Autotuning Reformat: Half(8294400,32400,180,1) -> Float(2073600,1:4,11520,64) ***************
[08/10/2023-11:16:24] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(input -> <out>) (Reformat)
[08/10/2023-11:16:24] [V] [TRT] Tactic: 0x00000000000003e8 Time: 2.28139
[08/10/2023-11:16:24] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.533472
[08/10/2023-11:16:24] [V] [TRT] Tactic: 0x0000000000000000 Time: 2.23554
[08/10/2023-11:16:24] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 0.533472
[08/10/2023-11:16:24] [V] [TRT] *************** Autotuning Reformat: Half(8294400,32400,180,1) -> Half(4147200,32400:2,180,1) ***************
[08/10/2023-11:16:24] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(input -> <out>) (Reformat)
[08/10/2023-11:16:24] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.842985
[08/10/2023-11:16:24] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.458875
[08/10/2023-11:16:24] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.633545
[08/10/2023-11:16:24] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 0.458875
[08/10/2023-11:16:24] [V] [TRT] *************** Autotuning Reformat: Half(8294400,32400,180,1) -> Half(1036800,1:8,5760,32) ***************
[08/10/2023-11:16:24] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(input -> <out>) (Reformat)
[08/10/2023-11:16:24] [V] [TRT] Tactic: 0x00000000000003e8 Time: 2.17402
[08/10/2023-11:16:24] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.407639
[08/10/2023-11:16:24] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.639355
[08/10/2023-11:16:24] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 0.407639
[08/10/2023-11:16:24] [V] [TRT] *************** Autotuning Reformat: Half(8294400,32400,180,1) -> Half(518400,1:16,2880,16) ***************
[08/10/2023-11:16:24] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(input -> <out>) (Reformat)
[08/10/2023-11:16:24] [V] [TRT] Tactic: 0x00000000000003e8 Time: 2.21746
[08/10/2023-11:16:24] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.406825
[08/10/2023-11:16:24] [V] [TRT] Tactic: 0x0000000000000000 Time: 2.18
[08/10/2023-11:16:24] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 0.406825
[08/10/2023-11:16:24] [V] [TRT] =============== Computing reformatting costs
[08/10/2023-11:16:24] [V] [TRT] *************** Autotuning Reformat: Float(4147200,32400,180,1) -> Float(4147200,1,23040,128) ***************
[08/10/2023-11:16:24] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(input.12 -> <out>) (Reformat)
[08/10/2023-11:16:24] [V] [TRT] Tactic: 0x00000000000003e8 Time: 1.25478
[08/10/2023-11:16:24] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.296485
[08/10/2023-11:16:24] [V] [TRT] Tactic: 0x0000000000000000 Time: 1.24635
[08/10/2023-11:16:24] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 0.296485
[08/10/2023-11:16:24] [V] [TRT] *************** Autotuning Reformat: Float(4147200,32400,180,1) -> Float(1036800,1:4,5760,32) ***************
[08/10/2023-11:16:24] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(input.12 -> <out>) (Reformat)
[08/10/2023-11:16:24] [V] [TRT] Tactic: 0x00000000000003e8 Time: 1.2906
[08/10/2023-11:16:24] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.289774
[08/10/2023-11:16:24] [V] [TRT] Tactic: 0x0000000000000000 Time: 1.28062
[08/10/2023-11:16:24] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 0.289774
[08/10/2023-11:16:24] [V] [TRT] *************** Autotuning Reformat: Float(4147200,32400,180,1) -> Half(4147200,32400,180,1) ***************
[08/10/2023-11:16:24] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(input.12 -> <out>) (Reformat)
[08/10/2023-11:16:24] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.322999
[08/10/2023-11:16:24] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.355127
[08/10/2023-11:16:24] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.501134
[08/10/2023-11:16:24] [V] [TRT] Fastest Tactic: 0x00000000000003e8 Time: 0.322999
[08/10/2023-11:16:24] [V] [TRT] *************** Autotuning Reformat: Float(4147200,32400,180,1) -> Half(2073600,32400:2,180,1) ***************
[08/10/2023-11:16:24] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(input.12 -> <out>) (Reformat)
[08/10/2023-11:16:24] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.430217
[08/10/2023-11:16:24] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.385175
[08/10/2023-11:16:24] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.319013
[08/10/2023-11:16:24] [V] [TRT] Fastest Tactic: 0x0000000000000000 Time: 0.319013
[08/10/2023-11:16:24] [V] [TRT] *************** Autotuning Reformat: Float(4147200,32400,180,1) -> Half(518400,1:8,2880,16) ***************
[08/10/2023-11:16:24] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(input.12 -> <out>) (Reformat)
[08/10/2023-11:16:24] [V] [TRT] Tactic: 0x00000000000003e8 Time: 1.16666
[08/10/2023-11:16:24] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.242779
[08/10/2023-11:16:24] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.314327
[08/10/2023-11:16:24] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 0.242779
[08/10/2023-11:16:24] [V] [TRT] *************** Autotuning Reformat: Float(4147200,32400,180,1) -> Half(259200,1:16,1440,8) ***************
[08/10/2023-11:16:24] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(input.12 -> <out>) (Reformat)
[08/10/2023-11:16:24] [V] [TRT] Tactic: 0x00000000000003e8 Time: 1.17022
[08/10/2023-11:16:24] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.244631
[08/10/2023-11:16:24] [V] [TRT] Tactic: 0x0000000000000000 Time: 1.16788
[08/10/2023-11:16:24] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 0.244631
[08/10/2023-11:16:24] [V] [TRT] *************** Autotuning Reformat: Float(4147200,1,23040,128) -> Float(4147200,32400,180,1) ***************
[08/10/2023-11:16:24] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(input.12 -> <out>) (Reformat)
[08/10/2023-11:16:24] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.835694
[08/10/2023-11:16:24] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.293765
[08/10/2023-11:16:24] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.911767
[08/10/2023-11:16:24] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 0.293765
[08/10/2023-11:16:24] [V] [TRT] *************** Autotuning Reformat: Float(4147200,1,23040,128) -> Float(1036800,1:4,5760,32) ***************
[08/10/2023-11:16:24] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(input.12 -> <out>) (Reformat)
[08/10/2023-11:16:24] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.436965
[08/10/2023-11:16:24] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.291589
[08/10/2023-11:16:24] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.436562
[08/10/2023-11:16:24] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 0.291589
[08/10/2023-11:16:24] [V] [TRT] *************** Autotuning Reformat: Float(4147200,1,23040,128) -> Half(4147200,32400,180,1) ***************
[08/10/2023-11:16:24] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(input.12 -> <out>) (Reformat)
[08/10/2023-11:16:24] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.735671
[08/10/2023-11:16:24] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.267017
[08/10/2023-11:16:24] [V] [TRT] Tactic: 0x0000000000000000 Time: 1.0471
[08/10/2023-11:16:24] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 0.267017
[08/10/2023-11:16:24] [V] [TRT] *************** Autotuning Reformat: Float(4147200,1,23040,128) -> Half(2073600,32400:2,180,1) ***************
[08/10/2023-11:16:24] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(input.12 -> <out>) (Reformat)
[08/10/2023-11:16:24] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.778062
[08/10/2023-11:16:24] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.347707
[08/10/2023-11:16:24] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.873317
[08/10/2023-11:16:24] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 0.347707
[08/10/2023-11:16:24] [V] [TRT] *************** Autotuning Reformat: Float(4147200,1,23040,128) -> Half(518400,1:8,2880,16) ***************
[08/10/2023-11:16:24] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(input.12 -> <out>) (Reformat)
[08/10/2023-11:16:24] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.426496
[08/10/2023-11:16:24] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.215858
[08/10/2023-11:16:24] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.425253
[08/10/2023-11:16:24] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 0.215858
[08/10/2023-11:16:24] [V] [TRT] *************** Autotuning Reformat: Float(4147200,1,23040,128) -> Half(259200,1:16,1440,8) ***************
[08/10/2023-11:16:24] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(input.12 -> <out>) (Reformat)
[08/10/2023-11:16:24] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.425957
[08/10/2023-11:16:24] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.215602
[08/10/2023-11:16:24] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.426231
[08/10/2023-11:16:24] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 0.215602
[08/10/2023-11:16:24] [V] [TRT] *************** Autotuning Reformat: Float(1036800,1:4,5760,32) -> Float(4147200,32400,180,1) ***************
[08/10/2023-11:16:24] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(input.12 -> <out>) (Reformat)
[08/10/2023-11:16:24] [V] [TRT] Tactic: 0x00000000000003e8 Time: 1.14561
[08/10/2023-11:16:24] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.288503
[08/10/2023-11:16:24] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.957184
[08/10/2023-11:16:24] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 0.288503
[08/10/2023-11:16:24] [V] [TRT] *************** Autotuning Reformat: Float(1036800,1:4,5760,32) -> Float(4147200,1,23040,128) ***************
[08/10/2023-11:16:24] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(input.12 -> <out>) (Reformat)
[08/10/2023-11:16:24] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.434862
[08/10/2023-11:16:24] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.405303
[08/10/2023-11:16:24] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.434309
[08/10/2023-11:16:24] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 0.405303
[08/10/2023-11:16:24] [V] [TRT] *************** Autotuning Reformat: Float(1036800,1:4,5760,32) -> Half(4147200,32400,180,1) ***************
[08/10/2023-11:16:24] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(input.12 -> <out>) (Reformat)
[08/10/2023-11:16:24] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.693125
[08/10/2023-11:16:24] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.266176
[08/10/2023-11:16:24] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.86816
[08/10/2023-11:16:24] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 0.266176
[08/10/2023-11:16:24] [V] [TRT] *************** Autotuning Reformat: Float(1036800,1:4,5760,32) -> Half(2073600,32400:2,180,1) ***************
[08/10/2023-11:16:24] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(input.12 -> <out>) (Reformat)
[08/10/2023-11:16:24] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.909595
[08/10/2023-11:16:24] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.365696
[08/10/2023-11:16:24] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.807781
[08/10/2023-11:16:24] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 0.365696
[08/10/2023-11:16:24] [V] [TRT] *************** Autotuning Reformat: Float(1036800,1:4,5760,32) -> Half(518400,1:8,2880,16) ***************
[08/10/2023-11:16:24] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(input.12 -> <out>) (Reformat)
[08/10/2023-11:16:24] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.521879
[08/10/2023-11:16:24] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.219835
[08/10/2023-11:16:24] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.51803
[08/10/2023-11:16:24] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 0.219835
[08/10/2023-11:16:24] [V] [TRT] *************** Autotuning Reformat: Float(1036800,1:4,5760,32) -> Half(259200,1:16,1440,8) ***************
[08/10/2023-11:16:24] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(input.12 -> <out>) (Reformat)
[08/10/2023-11:16:24] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.519685
[08/10/2023-11:16:24] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.219899
[08/10/2023-11:16:24] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.519333
[08/10/2023-11:16:24] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 0.219899
[08/10/2023-11:16:24] [V] [TRT] *************** Autotuning Reformat: Half(4147200,32400,180,1) -> Float(4147200,32400,180,1) ***************
[08/10/2023-11:16:24] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(input.12 -> <out>) (Reformat)
[08/10/2023-11:16:24] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.33541
[08/10/2023-11:16:24] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.33072
[08/10/2023-11:16:24] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.418377
[08/10/2023-11:16:24] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 0.33072
[08/10/2023-11:16:24] [V] [TRT] *************** Autotuning Reformat: Half(4147200,32400,180,1) -> Float(4147200,1,23040,128) ***************
[08/10/2023-11:16:24] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(input.12 -> <out>) (Reformat)
[08/10/2023-11:16:24] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.944302
[08/10/2023-11:16:24] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.248128
[08/10/2023-11:16:24] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.922702
[08/10/2023-11:16:24] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 0.248128
[08/10/2023-11:16:24] [V] [TRT] *************** Autotuning Reformat: Half(4147200,32400,180,1) -> Float(1036800,1:4,5760,32) ***************
[08/10/2023-11:16:24] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(input.12 -> <out>) (Reformat)
[08/10/2023-11:16:24] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.946167
[08/10/2023-11:16:24] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.242514
[08/10/2023-11:16:24] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.932928
[08/10/2023-11:16:24] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 0.242514
[08/10/2023-11:16:24] [V] [TRT] *************** Autotuning Reformat: Half(4147200,32400,180,1) -> Half(2073600,32400:2,180,1) ***************
[08/10/2023-11:16:24] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(input.12 -> <out>) (Reformat)
[08/10/2023-11:16:24] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.38923
[08/10/2023-11:16:24] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.212923
[08/10/2023-11:16:24] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.306514
[08/10/2023-11:16:24] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 0.212923
[08/10/2023-11:16:24] [V] [TRT] *************** Autotuning Reformat: Half(4147200,32400,180,1) -> Half(518400,1:8,2880,16) ***************
[08/10/2023-11:16:24] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(input.12 -> <out>) (Reformat)
[08/10/2023-11:16:24] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.966309
[08/10/2023-11:16:24] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.189696
[08/10/2023-11:16:24] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.304398
[08/10/2023-11:16:24] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 0.189696
[08/10/2023-11:16:24] [V] [TRT] *************** Autotuning Reformat: Half(4147200,32400,180,1) -> Half(259200,1:16,1440,8) ***************
[08/10/2023-11:16:24] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(input.12 -> <out>) (Reformat)
[08/10/2023-11:16:24] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.964617
[08/10/2023-11:16:24] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.189883
[08/10/2023-11:16:24] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.981307
[08/10/2023-11:16:24] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 0.189883
[08/10/2023-11:16:24] [V] [TRT] *************** Autotuning Reformat: Half(2073600,32400:2,180,1) -> Float(4147200,32400,180,1) ***************
[08/10/2023-11:16:24] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(input.12 -> <out>) (Reformat)
[08/10/2023-11:16:24] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.416037
[08/10/2023-11:16:24] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.28507
[08/10/2023-11:16:24] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.273783
[08/10/2023-11:16:24] [V] [TRT] Fastest Tactic: 0x0000000000000000 Time: 0.273783
[08/10/2023-11:16:24] [V] [TRT] *************** Autotuning Reformat: Half(2073600,32400:2,180,1) -> Float(4147200,1,23040,128) ***************
[08/10/2023-11:16:24] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(input.12 -> <out>) (Reformat)
[08/10/2023-11:16:24] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.625202
[08/10/2023-11:16:24] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.242853
[08/10/2023-11:16:24] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.628091
[08/10/2023-11:16:24] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 0.242853
[08/10/2023-11:16:24] [V] [TRT] *************** Autotuning Reformat: Half(2073600,32400:2,180,1) -> Float(1036800,1:4,5760,32) ***************
[08/10/2023-11:16:24] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(input.12 -> <out>) (Reformat)
[08/10/2023-11:16:24] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.691739
[08/10/2023-11:16:24] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.24389
[08/10/2023-11:16:24] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.696261
[08/10/2023-11:16:24] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 0.24389
[08/10/2023-11:16:24] [V] [TRT] *************** Autotuning Reformat: Half(2073600,32400:2,180,1) -> Half(4147200,32400,180,1) ***************
[08/10/2023-11:16:24] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(input.12 -> <out>) (Reformat)
[08/10/2023-11:16:24] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.366651
[08/10/2023-11:16:24] [V] [TRT] Tactic: 0x00000000000003ea Time: 1.36757
[08/10/2023-11:16:24] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.24571
[08/10/2023-11:16:24] [V] [TRT] Fastest Tactic: 0x0000000000000000 Time: 0.24571
[08/10/2023-11:16:24] [V] [TRT] *************** Autotuning Reformat: Half(2073600,32400:2,180,1) -> Half(518400,1:8,2880,16) ***************
[08/10/2023-11:16:24] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(input.12 -> <out>) (Reformat)
[08/10/2023-11:16:24] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.697797
[08/10/2023-11:16:24] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.23328
[08/10/2023-11:16:24] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.272791
[08/10/2023-11:16:24] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 0.23328
[08/10/2023-11:16:24] [V] [TRT] *************** Autotuning Reformat: Half(2073600,32400:2,180,1) -> Half(259200,1:16,1440,8) ***************
[08/10/2023-11:16:24] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(input.12 -> <out>) (Reformat)
[08/10/2023-11:16:24] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.698473
[08/10/2023-11:16:24] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.23445
[08/10/2023-11:16:24] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.697143
[08/10/2023-11:16:24] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 0.23445
[08/10/2023-11:16:24] [V] [TRT] *************** Autotuning Reformat: Half(518400,1:8,2880,16) -> Float(4147200,32400,180,1) ***************
[08/10/2023-11:16:24] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(input.12 -> <out>) (Reformat)
[08/10/2023-11:16:24] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.720745
[08/10/2023-11:16:24] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.254391
[08/10/2023-11:16:24] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.271278
[08/10/2023-11:16:24] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 0.254391
[08/10/2023-11:16:24] [V] [TRT] *************** Autotuning Reformat: Half(518400,1:8,2880,16) -> Float(4147200,1,23040,128) ***************
[08/10/2023-11:16:24] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(input.12 -> <out>) (Reformat)
[08/10/2023-11:16:24] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.432722
[08/10/2023-11:16:24] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.219081
[08/10/2023-11:16:24] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.432073
[08/10/2023-11:16:24] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 0.219081
[08/10/2023-11:16:24] [V] [TRT] *************** Autotuning Reformat: Half(518400,1:8,2880,16) -> Float(1036800,1:4,5760,32) ***************
[08/10/2023-11:16:24] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(input.12 -> <out>) (Reformat)
[08/10/2023-11:16:24] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.503054
[08/10/2023-11:16:24] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.219826
[08/10/2023-11:16:25] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.504114
[08/10/2023-11:16:25] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 0.219826
[08/10/2023-11:16:25] [V] [TRT] *************** Autotuning Reformat: Half(518400,1:8,2880,16) -> Half(4147200,32400,180,1) ***************
[08/10/2023-11:16:25] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(input.12 -> <out>) (Reformat)
[08/10/2023-11:16:25] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.724133
[08/10/2023-11:16:25] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.999547
[08/10/2023-11:16:25] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.215374
[08/10/2023-11:16:25] [V] [TRT] Fastest Tactic: 0x0000000000000000 Time: 0.215374
[08/10/2023-11:16:25] [V] [TRT] *************** Autotuning Reformat: Half(518400,1:8,2880,16) -> Half(2073600,32400:2,180,1) ***************
[08/10/2023-11:16:25] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(input.12 -> <out>) (Reformat)
[08/10/2023-11:16:25] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.689413
[08/10/2023-11:16:25] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.313609
[08/10/2023-11:16:25] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.205893
[08/10/2023-11:16:25] [V] [TRT] Fastest Tactic: 0x0000000000000000 Time: 0.205893
[08/10/2023-11:16:25] [V] [TRT] *************** Autotuning Reformat: Half(518400,1:8,2880,16) -> Half(259200,1:16,1440,8) ***************
[08/10/2023-11:16:25] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(input.12 -> <out>) (Reformat)
[08/10/2023-11:16:25] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.456119
[08/10/2023-11:16:25] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.202359
[08/10/2023-11:16:25] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.45579
[08/10/2023-11:16:25] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 0.202359
[08/10/2023-11:16:25] [V] [TRT] *************** Autotuning Reformat: Half(259200,1:16,1440,8) -> Float(4147200,32400,180,1) ***************
[08/10/2023-11:16:25] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(input.12 -> <out>) (Reformat)
[08/10/2023-11:16:25] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.76677
[08/10/2023-11:16:25] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.254437
[08/10/2023-11:16:25] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.871072
[08/10/2023-11:16:25] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 0.254437
[08/10/2023-11:16:25] [V] [TRT] *************** Autotuning Reformat: Half(259200,1:16,1440,8) -> Float(4147200,1,23040,128) ***************
[08/10/2023-11:16:25] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(input.12 -> <out>) (Reformat)
[08/10/2023-11:16:25] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.434405
[08/10/2023-11:16:25] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.219835
[08/10/2023-11:16:25] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.434359
[08/10/2023-11:16:25] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 0.219835
[08/10/2023-11:16:25] [V] [TRT] *************** Autotuning Reformat: Half(259200,1:16,1440,8) -> Float(1036800,1:4,5760,32) ***************
[08/10/2023-11:16:25] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(input.12 -> <out>) (Reformat)
[08/10/2023-11:16:25] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.492443
[08/10/2023-11:16:25] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.219575
[08/10/2023-11:16:25] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.495845
[08/10/2023-11:16:25] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 0.219575
[08/10/2023-11:16:25] [V] [TRT] *************** Autotuning Reformat: Half(259200,1:16,1440,8) -> Half(4147200,32400,180,1) ***************
[08/10/2023-11:16:25] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(input.12 -> <out>) (Reformat)
[08/10/2023-11:16:25] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.636663
[08/10/2023-11:16:25] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.977006
[08/10/2023-11:16:25] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.715511
[08/10/2023-11:16:25] [V] [TRT] Fastest Tactic: 0x00000000000003e8 Time: 0.636663
[08/10/2023-11:16:25] [V] [TRT] *************** Autotuning Reformat: Half(259200,1:16,1440,8) -> Half(2073600,32400:2,180,1) ***************
[08/10/2023-11:16:25] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(input.12 -> <out>) (Reformat)
[08/10/2023-11:16:25] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.717413
[08/10/2023-11:16:25] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.313353
[08/10/2023-11:16:25] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.68832
[08/10/2023-11:16:25] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 0.313353
[08/10/2023-11:16:25] [V] [TRT] *************** Autotuning Reformat: Half(259200,1:16,1440,8) -> Half(518400,1:8,2880,16) ***************
[08/10/2023-11:16:25] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(input.12 -> <out>) (Reformat)
[08/10/2023-11:16:25] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.456713
[08/10/2023-11:16:25] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.204128
[08/10/2023-11:16:25] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.44683
[08/10/2023-11:16:25] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 0.204128
[08/10/2023-11:16:25] [V] [TRT] =============== Computing reformatting costs
[08/10/2023-11:16:25] [V] [TRT] *************** Autotuning Reformat: Float(4147200,32400,180,1) -> Float(4147200,1,23040,128) ***************
[08/10/2023-11:16:25] [V] [TRT] *************** Autotuning Reformat: Float(4147200,32400,180,1) -> Float(1036800,1:4,5760,32) ***************
[08/10/2023-11:16:25] [V] [TRT] *************** Autotuning Reformat: Float(4147200,32400,180,1) -> Half(4147200,32400,180,1) ***************
[08/10/2023-11:16:25] [V] [TRT] *************** Autotuning Reformat: Float(4147200,32400,180,1) -> Half(2073600,32400:2,180,1) ***************
[08/10/2023-11:16:25] [V] [TRT] *************** Autotuning Reformat: Float(4147200,32400,180,1) -> Half(518400,1:8,2880,16) ***************
[08/10/2023-11:16:25] [V] [TRT] *************** Autotuning Reformat: Float(4147200,32400,180,1) -> Half(259200,1:16,1440,8) ***************
[08/10/2023-11:16:25] [V] [TRT] *************** Autotuning Reformat: Float(4147200,1,23040,128) -> Float(4147200,32400,180,1) ***************
[08/10/2023-11:16:25] [V] [TRT] *************** Autotuning Reformat: Float(4147200,1,23040,128) -> Float(1036800,1:4,5760,32) ***************
[08/10/2023-11:16:25] [V] [TRT] *************** Autotuning Reformat: Float(4147200,1,23040,128) -> Half(4147200,32400,180,1) ***************
[08/10/2023-11:16:25] [V] [TRT] *************** Autotuning Reformat: Float(4147200,1,23040,128) -> Half(2073600,32400:2,180,1) ***************
[08/10/2023-11:16:25] [V] [TRT] *************** Autotuning Reformat: Float(4147200,1,23040,128) -> Half(518400,1:8,2880,16) ***************
[08/10/2023-11:16:25] [V] [TRT] *************** Autotuning Reformat: Float(4147200,1,23040,128) -> Half(259200,1:16,1440,8) ***************
[08/10/2023-11:16:25] [V] [TRT] *************** Autotuning Reformat: Float(1036800,1:4,5760,32) -> Float(4147200,32400,180,1) ***************
[08/10/2023-11:16:25] [V] [TRT] *************** Autotuning Reformat: Float(1036800,1:4,5760,32) -> Float(4147200,1,23040,128) ***************
[08/10/2023-11:16:25] [V] [TRT] *************** Autotuning Reformat: Float(1036800,1:4,5760,32) -> Half(4147200,32400,180,1) ***************
[08/10/2023-11:16:25] [V] [TRT] *************** Autotuning Reformat: Float(1036800,1:4,5760,32) -> Half(2073600,32400:2,180,1) ***************
[08/10/2023-11:16:25] [V] [TRT] *************** Autotuning Reformat: Float(1036800,1:4,5760,32) -> Half(518400,1:8,2880,16) ***************
[08/10/2023-11:16:25] [V] [TRT] *************** Autotuning Reformat: Float(1036800,1:4,5760,32) -> Half(259200,1:16,1440,8) ***************
[08/10/2023-11:16:25] [V] [TRT] *************** Autotuning Reformat: Half(4147200,32400,180,1) -> Float(4147200,32400,180,1) ***************
[08/10/2023-11:16:25] [V] [TRT] *************** Autotuning Reformat: Half(4147200,32400,180,1) -> Float(4147200,1,23040,128) ***************
[08/10/2023-11:16:25] [V] [TRT] *************** Autotuning Reformat: Half(4147200,32400,180,1) -> Float(1036800,1:4,5760,32) ***************
[08/10/2023-11:16:25] [V] [TRT] *************** Autotuning Reformat: Half(4147200,32400,180,1) -> Half(2073600,32400:2,180,1) ***************
[08/10/2023-11:16:25] [V] [TRT] *************** Autotuning Reformat: Half(4147200,32400,180,1) -> Half(518400,1:8,2880,16) ***************
[08/10/2023-11:16:25] [V] [TRT] *************** Autotuning Reformat: Half(4147200,32400,180,1) -> Half(259200,1:16,1440,8) ***************
[08/10/2023-11:16:25] [V] [TRT] *************** Autotuning Reformat: Half(2073600,32400:2,180,1) -> Float(4147200,32400,180,1) ***************
[08/10/2023-11:16:25] [V] [TRT] *************** Autotuning Reformat: Half(2073600,32400:2,180,1) -> Float(4147200,1,23040,128) ***************
[08/10/2023-11:16:25] [V] [TRT] *************** Autotuning Reformat: Half(2073600,32400:2,180,1) -> Float(1036800,1:4,5760,32) ***************
[08/10/2023-11:16:25] [V] [TRT] *************** Autotuning Reformat: Half(2073600,32400:2,180,1) -> Half(4147200,32400,180,1) ***************
[08/10/2023-11:16:25] [V] [TRT] *************** Autotuning Reformat: Half(2073600,32400:2,180,1) -> Half(518400,1:8,2880,16) ***************
[08/10/2023-11:16:25] [V] [TRT] *************** Autotuning Reformat: Half(2073600,32400:2,180,1) -> Half(259200,1:16,1440,8) ***************
[08/10/2023-11:16:25] [V] [TRT] *************** Autotuning Reformat: Half(518400,1:8,2880,16) -> Float(4147200,32400,180,1) ***************
[08/10/2023-11:16:25] [V] [TRT] *************** Autotuning Reformat: Half(518400,1:8,2880,16) -> Float(4147200,1,23040,128) ***************
[08/10/2023-11:16:25] [V] [TRT] *************** Autotuning Reformat: Half(518400,1:8,2880,16) -> Float(1036800,1:4,5760,32) ***************
[08/10/2023-11:16:25] [V] [TRT] *************** Autotuning Reformat: Half(518400,1:8,2880,16) -> Half(4147200,32400,180,1) ***************
[08/10/2023-11:16:25] [V] [TRT] *************** Autotuning Reformat: Half(518400,1:8,2880,16) -> Half(2073600,32400:2,180,1) ***************
[08/10/2023-11:16:25] [V] [TRT] *************** Autotuning Reformat: Half(518400,1:8,2880,16) -> Half(259200,1:16,1440,8) ***************
[08/10/2023-11:16:25] [V] [TRT] *************** Autotuning Reformat: Half(259200,1:16,1440,8) -> Float(4147200,32400,180,1) ***************
[08/10/2023-11:16:25] [V] [TRT] *************** Autotuning Reformat: Half(259200,1:16,1440,8) -> Float(4147200,1,23040,128) ***************
[08/10/2023-11:16:25] [V] [TRT] *************** Autotuning Reformat: Half(259200,1:16,1440,8) -> Float(1036800,1:4,5760,32) ***************
[08/10/2023-11:16:25] [V] [TRT] *************** Autotuning Reformat: Half(259200,1:16,1440,8) -> Half(4147200,32400,180,1) ***************
[08/10/2023-11:16:25] [V] [TRT] *************** Autotuning Reformat: Half(259200,1:16,1440,8) -> Half(2073600,32400:2,180,1) ***************
[08/10/2023-11:16:25] [V] [TRT] *************** Autotuning Reformat: Half(259200,1:16,1440,8) -> Half(518400,1:8,2880,16) ***************
[08/10/2023-11:16:25] [V] [TRT] =============== Computing reformatting costs
[08/10/2023-11:16:25] [V] [TRT] *************** Autotuning Reformat: Float(4147200,32400,180,1) -> Float(4147200,1,23040,128) ***************
[08/10/2023-11:16:25] [V] [TRT] *************** Autotuning Reformat: Float(4147200,32400,180,1) -> Float(1036800,1:4,5760,32) ***************
[08/10/2023-11:16:25] [V] [TRT] *************** Autotuning Reformat: Float(4147200,32400,180,1) -> Half(4147200,32400,180,1) ***************
[08/10/2023-11:16:25] [V] [TRT] *************** Autotuning Reformat: Float(4147200,32400,180,1) -> Half(2073600,32400:2,180,1) ***************
[08/10/2023-11:16:25] [V] [TRT] *************** Autotuning Reformat: Float(4147200,32400,180,1) -> Half(518400,1:8,2880,16) ***************
[08/10/2023-11:16:25] [V] [TRT] *************** Autotuning Reformat: Float(4147200,32400,180,1) -> Half(259200,1:16,1440,8) ***************
[08/10/2023-11:16:25] [V] [TRT] *************** Autotuning Reformat: Float(4147200,1,23040,128) -> Float(4147200,32400,180,1) ***************
[08/10/2023-11:16:25] [V] [TRT] *************** Autotuning Reformat: Float(4147200,1,23040,128) -> Float(1036800,1:4,5760,32) ***************
[08/10/2023-11:16:25] [V] [TRT] *************** Autotuning Reformat: Float(4147200,1,23040,128) -> Half(4147200,32400,180,1) ***************
[08/10/2023-11:16:25] [V] [TRT] *************** Autotuning Reformat: Float(4147200,1,23040,128) -> Half(2073600,32400:2,180,1) ***************
[08/10/2023-11:16:25] [V] [TRT] *************** Autotuning Reformat: Float(4147200,1,23040,128) -> Half(518400,1:8,2880,16) ***************
[08/10/2023-11:16:25] [V] [TRT] *************** Autotuning Reformat: Float(4147200,1,23040,128) -> Half(259200,1:16,1440,8) ***************
[08/10/2023-11:16:25] [V] [TRT] *************** Autotuning Reformat: Float(1036800,1:4,5760,32) -> Float(4147200,32400,180,1) ***************
[08/10/2023-11:16:25] [V] [TRT] *************** Autotuning Reformat: Float(1036800,1:4,5760,32) -> Float(4147200,1,23040,128) ***************
[08/10/2023-11:16:25] [V] [TRT] *************** Autotuning Reformat: Float(1036800,1:4,5760,32) -> Half(4147200,32400,180,1) ***************
[08/10/2023-11:16:25] [V] [TRT] *************** Autotuning Reformat: Float(1036800,1:4,5760,32) -> Half(2073600,32400:2,180,1) ***************
[08/10/2023-11:16:25] [V] [TRT] *************** Autotuning Reformat: Float(1036800,1:4,5760,32) -> Half(518400,1:8,2880,16) ***************
[08/10/2023-11:16:25] [V] [TRT] *************** Autotuning Reformat: Float(1036800,1:4,5760,32) -> Half(259200,1:16,1440,8) ***************
[08/10/2023-11:16:25] [V] [TRT] *************** Autotuning Reformat: Half(4147200,32400,180,1) -> Float(4147200,32400,180,1) ***************
[08/10/2023-11:16:25] [V] [TRT] *************** Autotuning Reformat: Half(4147200,32400,180,1) -> Float(4147200,1,23040,128) ***************
[08/10/2023-11:16:25] [V] [TRT] *************** Autotuning Reformat: Half(4147200,32400,180,1) -> Float(1036800,1:4,5760,32) ***************
[08/10/2023-11:16:25] [V] [TRT] *************** Autotuning Reformat: Half(4147200,32400,180,1) -> Half(2073600,32400:2,180,1) ***************
[08/10/2023-11:16:25] [V] [TRT] *************** Autotuning Reformat: Half(4147200,32400,180,1) -> Half(518400,1:8,2880,16) ***************
[08/10/2023-11:16:25] [V] [TRT] *************** Autotuning Reformat: Half(4147200,32400,180,1) -> Half(259200,1:16,1440,8) ***************
[08/10/2023-11:16:25] [V] [TRT] *************** Autotuning Reformat: Half(2073600,32400:2,180,1) -> Float(4147200,32400,180,1) ***************
[08/10/2023-11:16:25] [V] [TRT] *************** Autotuning Reformat: Half(2073600,32400:2,180,1) -> Float(4147200,1,23040,128) ***************
[08/10/2023-11:16:25] [V] [TRT] *************** Autotuning Reformat: Half(2073600,32400:2,180,1) -> Float(1036800,1:4,5760,32) ***************
[08/10/2023-11:16:25] [V] [TRT] *************** Autotuning Reformat: Half(2073600,32400:2,180,1) -> Half(4147200,32400,180,1) ***************
[08/10/2023-11:16:25] [V] [TRT] *************** Autotuning Reformat: Half(2073600,32400:2,180,1) -> Half(518400,1:8,2880,16) ***************
[08/10/2023-11:16:25] [V] [TRT] *************** Autotuning Reformat: Half(2073600,32400:2,180,1) -> Half(259200,1:16,1440,8) ***************
[08/10/2023-11:16:25] [V] [TRT] *************** Autotuning Reformat: Half(518400,1:8,2880,16) -> Float(4147200,32400,180,1) ***************
[08/10/2023-11:16:25] [V] [TRT] *************** Autotuning Reformat: Half(518400,1:8,2880,16) -> Float(4147200,1,23040,128) ***************
[08/10/2023-11:16:25] [V] [TRT] *************** Autotuning Reformat: Half(518400,1:8,2880,16) -> Float(1036800,1:4,5760,32) ***************
[08/10/2023-11:16:25] [V] [TRT] *************** Autotuning Reformat: Half(518400,1:8,2880,16) -> Half(4147200,32400,180,1) ***************
[08/10/2023-11:16:25] [V] [TRT] *************** Autotuning Reformat: Half(518400,1:8,2880,16) -> Half(2073600,32400:2,180,1) ***************
[08/10/2023-11:16:25] [V] [TRT] *************** Autotuning Reformat: Half(518400,1:8,2880,16) -> Half(259200,1:16,1440,8) ***************
[08/10/2023-11:16:25] [V] [TRT] *************** Autotuning Reformat: Half(259200,1:16,1440,8) -> Float(4147200,32400,180,1) ***************
[08/10/2023-11:16:25] [V] [TRT] *************** Autotuning Reformat: Half(259200,1:16,1440,8) -> Float(4147200,1,23040,128) ***************
[08/10/2023-11:16:25] [V] [TRT] *************** Autotuning Reformat: Half(259200,1:16,1440,8) -> Float(1036800,1:4,5760,32) ***************
[08/10/2023-11:16:25] [V] [TRT] *************** Autotuning Reformat: Half(259200,1:16,1440,8) -> Half(4147200,32400,180,1) ***************
[08/10/2023-11:16:25] [V] [TRT] *************** Autotuning Reformat: Half(259200,1:16,1440,8) -> Half(2073600,32400:2,180,1) ***************
[08/10/2023-11:16:25] [V] [TRT] *************** Autotuning Reformat: Half(259200,1:16,1440,8) -> Half(518400,1:8,2880,16) ***************
[08/10/2023-11:16:25] [V] [TRT] =============== Computing reformatting costs
[08/10/2023-11:16:25] [V] [TRT] *************** Autotuning Reformat: Float(4147200,32400,180,1) -> Float(4147200,1,23040,128) ***************
[08/10/2023-11:16:25] [V] [TRT] *************** Autotuning Reformat: Float(4147200,32400,180,1) -> Float(1036800,1:4,5760,32) ***************
[08/10/2023-11:16:25] [V] [TRT] *************** Autotuning Reformat: Float(4147200,32400,180,1) -> Half(4147200,32400,180,1) ***************
[08/10/2023-11:16:25] [V] [TRT] *************** Autotuning Reformat: Float(4147200,32400,180,1) -> Half(2073600,32400:2,180,1) ***************
[08/10/2023-11:16:25] [V] [TRT] *************** Autotuning Reformat: Float(4147200,32400,180,1) -> Half(518400,1:8,2880,16) ***************
[08/10/2023-11:16:25] [V] [TRT] *************** Autotuning Reformat: Float(4147200,32400,180,1) -> Half(259200,1:16,1440,8) ***************
[08/10/2023-11:16:25] [V] [TRT] *************** Autotuning Reformat: Float(4147200,1,23040,128) -> Float(4147200,32400,180,1) ***************
[08/10/2023-11:16:25] [V] [TRT] *************** Autotuning Reformat: Float(4147200,1,23040,128) -> Float(1036800,1:4,5760,32) ***************
[08/10/2023-11:16:25] [V] [TRT] *************** Autotuning Reformat: Float(4147200,1,23040,128) -> Half(4147200,32400,180,1) ***************
[08/10/2023-11:16:25] [V] [TRT] *************** Autotuning Reformat: Float(4147200,1,23040,128) -> Half(2073600,32400:2,180,1) ***************
[08/10/2023-11:16:25] [V] [TRT] *************** Autotuning Reformat: Float(4147200,1,23040,128) -> Half(518400,1:8,2880,16) ***************
[08/10/2023-11:16:25] [V] [TRT] *************** Autotuning Reformat: Float(4147200,1,23040,128) -> Half(259200,1:16,1440,8) ***************
[08/10/2023-11:16:25] [V] [TRT] *************** Autotuning Reformat: Float(1036800,1:4,5760,32) -> Float(4147200,32400,180,1) ***************
[08/10/2023-11:16:25] [V] [TRT] *************** Autotuning Reformat: Float(1036800,1:4,5760,32) -> Float(4147200,1,23040,128) ***************
[08/10/2023-11:16:25] [V] [TRT] *************** Autotuning Reformat: Float(1036800,1:4,5760,32) -> Half(4147200,32400,180,1) ***************
[08/10/2023-11:16:25] [V] [TRT] *************** Autotuning Reformat: Float(1036800,1:4,5760,32) -> Half(2073600,32400:2,180,1) ***************
[08/10/2023-11:16:25] [V] [TRT] *************** Autotuning Reformat: Float(1036800,1:4,5760,32) -> Half(518400,1:8,2880,16) ***************
[08/10/2023-11:16:25] [V] [TRT] *************** Autotuning Reformat: Float(1036800,1:4,5760,32) -> Half(259200,1:16,1440,8) ***************
[08/10/2023-11:16:25] [V] [TRT] *************** Autotuning Reformat: Half(4147200,32400,180,1) -> Float(4147200,32400,180,1) ***************
[08/10/2023-11:16:25] [V] [TRT] *************** Autotuning Reformat: Half(4147200,32400,180,1) -> Float(4147200,1,23040,128) ***************
[08/10/2023-11:16:25] [V] [TRT] *************** Autotuning Reformat: Half(4147200,32400,180,1) -> Float(1036800,1:4,5760,32) ***************
[08/10/2023-11:16:25] [V] [TRT] *************** Autotuning Reformat: Half(4147200,32400,180,1) -> Half(2073600,32400:2,180,1) ***************
[08/10/2023-11:16:25] [V] [TRT] *************** Autotuning Reformat: Half(4147200,32400,180,1) -> Half(518400,1:8,2880,16) ***************
[08/10/2023-11:16:25] [V] [TRT] *************** Autotuning Reformat: Half(4147200,32400,180,1) -> Half(259200,1:16,1440,8) ***************
[08/10/2023-11:16:25] [V] [TRT] *************** Autotuning Reformat: Half(2073600,32400:2,180,1) -> Float(4147200,32400,180,1) ***************
[08/10/2023-11:16:25] [V] [TRT] *************** Autotuning Reformat: Half(2073600,32400:2,180,1) -> Float(4147200,1,23040,128) ***************
[08/10/2023-11:16:25] [V] [TRT] *************** Autotuning Reformat: Half(2073600,32400:2,180,1) -> Float(1036800,1:4,5760,32) ***************
[08/10/2023-11:16:25] [V] [TRT] *************** Autotuning Reformat: Half(2073600,32400:2,180,1) -> Half(4147200,32400,180,1) ***************
[08/10/2023-11:16:25] [V] [TRT] *************** Autotuning Reformat: Half(2073600,32400:2,180,1) -> Half(518400,1:8,2880,16) ***************
[08/10/2023-11:16:25] [V] [TRT] *************** Autotuning Reformat: Half(2073600,32400:2,180,1) -> Half(259200,1:16,1440,8) ***************
[08/10/2023-11:16:25] [V] [TRT] *************** Autotuning Reformat: Half(518400,1:8,2880,16) -> Float(4147200,32400,180,1) ***************
[08/10/2023-11:16:25] [V] [TRT] *************** Autotuning Reformat: Half(518400,1:8,2880,16) -> Float(4147200,1,23040,128) ***************
[08/10/2023-11:16:25] [V] [TRT] *************** Autotuning Reformat: Half(518400,1:8,2880,16) -> Float(1036800,1:4,5760,32) ***************
[08/10/2023-11:16:25] [V] [TRT] *************** Autotuning Reformat: Half(518400,1:8,2880,16) -> Half(4147200,32400,180,1) ***************
[08/10/2023-11:16:25] [V] [TRT] *************** Autotuning Reformat: Half(518400,1:8,2880,16) -> Half(2073600,32400:2,180,1) ***************
[08/10/2023-11:16:25] [V] [TRT] *************** Autotuning Reformat: Half(518400,1:8,2880,16) -> Half(259200,1:16,1440,8) ***************
[08/10/2023-11:16:25] [V] [TRT] *************** Autotuning Reformat: Half(259200,1:16,1440,8) -> Float(4147200,32400,180,1) ***************
[08/10/2023-11:16:25] [V] [TRT] *************** Autotuning Reformat: Half(259200,1:16,1440,8) -> Float(4147200,1,23040,128) ***************
[08/10/2023-11:16:25] [V] [TRT] *************** Autotuning Reformat: Half(259200,1:16,1440,8) -> Float(1036800,1:4,5760,32) ***************
[08/10/2023-11:16:25] [V] [TRT] *************** Autotuning Reformat: Half(259200,1:16,1440,8) -> Half(4147200,32400,180,1) ***************
[08/10/2023-11:16:25] [V] [TRT] *************** Autotuning Reformat: Half(259200,1:16,1440,8) -> Half(2073600,32400:2,180,1) ***************
[08/10/2023-11:16:25] [V] [TRT] *************** Autotuning Reformat: Half(259200,1:16,1440,8) -> Half(518400,1:8,2880,16) ***************
[08/10/2023-11:16:25] [V] [TRT] =============== Computing reformatting costs
[08/10/2023-11:16:25] [V] [TRT] *************** Autotuning Reformat: Float(4147200,32400,180,1) -> Float(4147200,1,23040,128) ***************
[08/10/2023-11:16:25] [V] [TRT] *************** Autotuning Reformat: Float(4147200,32400,180,1) -> Float(1036800,1:4,5760,32) ***************
[08/10/2023-11:16:25] [V] [TRT] *************** Autotuning Reformat: Float(4147200,32400,180,1) -> Half(4147200,32400,180,1) ***************
[08/10/2023-11:16:25] [V] [TRT] *************** Autotuning Reformat: Float(4147200,32400,180,1) -> Half(2073600,32400:2,180,1) ***************
[08/10/2023-11:16:25] [V] [TRT] *************** Autotuning Reformat: Float(4147200,32400,180,1) -> Half(518400,1:8,2880,16) ***************
[08/10/2023-11:16:25] [V] [TRT] *************** Autotuning Reformat: Float(4147200,32400,180,1) -> Half(259200,1:16,1440,8) ***************
[08/10/2023-11:16:25] [V] [TRT] *************** Autotuning Reformat: Float(4147200,1,23040,128) -> Float(4147200,32400,180,1) ***************
[08/10/2023-11:16:25] [V] [TRT] *************** Autotuning Reformat: Float(4147200,1,23040,128) -> Float(1036800,1:4,5760,32) ***************
[08/10/2023-11:16:25] [V] [TRT] *************** Autotuning Reformat: Float(4147200,1,23040,128) -> Half(4147200,32400,180,1) ***************
[08/10/2023-11:16:25] [V] [TRT] *************** Autotuning Reformat: Float(4147200,1,23040,128) -> Half(2073600,32400:2,180,1) ***************
[08/10/2023-11:16:25] [V] [TRT] *************** Autotuning Reformat: Float(4147200,1,23040,128) -> Half(518400,1:8,2880,16) ***************
[08/10/2023-11:16:25] [V] [TRT] *************** Autotuning Reformat: Float(4147200,1,23040,128) -> Half(259200,1:16,1440,8) ***************
[08/10/2023-11:16:25] [V] [TRT] *************** Autotuning Reformat: Float(1036800,1:4,5760,32) -> Float(4147200,32400,180,1) ***************
[08/10/2023-11:16:25] [V] [TRT] *************** Autotuning Reformat: Float(1036800,1:4,5760,32) -> Float(4147200,1,23040,128) ***************
[08/10/2023-11:16:25] [V] [TRT] *************** Autotuning Reformat: Float(1036800,1:4,5760,32) -> Half(4147200,32400,180,1) ***************
[08/10/2023-11:16:25] [V] [TRT] *************** Autotuning Reformat: Float(1036800,1:4,5760,32) -> Half(2073600,32400:2,180,1) ***************
[08/10/2023-11:16:25] [V] [TRT] *************** Autotuning Reformat: Float(1036800,1:4,5760,32) -> Half(518400,1:8,2880,16) ***************
[08/10/2023-11:16:25] [V] [TRT] *************** Autotuning Reformat: Float(1036800,1:4,5760,32) -> Half(259200,1:16,1440,8) ***************
[08/10/2023-11:16:25] [V] [TRT] *************** Autotuning Reformat: Half(4147200,32400,180,1) -> Float(4147200,32400,180,1) ***************
[08/10/2023-11:16:25] [V] [TRT] *************** Autotuning Reformat: Half(4147200,32400,180,1) -> Float(4147200,1,23040,128) ***************
[08/10/2023-11:16:25] [V] [TRT] *************** Autotuning Reformat: Half(4147200,32400,180,1) -> Float(1036800,1:4,5760,32) ***************
[08/10/2023-11:16:25] [V] [TRT] *************** Autotuning Reformat: Half(4147200,32400,180,1) -> Half(2073600,32400:2,180,1) ***************
[08/10/2023-11:16:25] [V] [TRT] *************** Autotuning Reformat: Half(4147200,32400,180,1) -> Half(518400,1:8,2880,16) ***************
[08/10/2023-11:16:25] [V] [TRT] *************** Autotuning Reformat: Half(4147200,32400,180,1) -> Half(259200,1:16,1440,8) ***************
[08/10/2023-11:16:25] [V] [TRT] *************** Autotuning Reformat: Half(2073600,32400:2,180,1) -> Float(4147200,32400,180,1) ***************
[08/10/2023-11:16:25] [V] [TRT] *************** Autotuning Reformat: Half(2073600,32400:2,180,1) -> Float(4147200,1,23040,128) ***************
[08/10/2023-11:16:25] [V] [TRT] *************** Autotuning Reformat: Half(2073600,32400:2,180,1) -> Float(1036800,1:4,5760,32) ***************
[08/10/2023-11:16:25] [V] [TRT] *************** Autotuning Reformat: Half(2073600,32400:2,180,1) -> Half(4147200,32400,180,1) ***************
[08/10/2023-11:16:25] [V] [TRT] *************** Autotuning Reformat: Half(2073600,32400:2,180,1) -> Half(518400,1:8,2880,16) ***************
[08/10/2023-11:16:25] [V] [TRT] *************** Autotuning Reformat: Half(2073600,32400:2,180,1) -> Half(259200,1:16,1440,8) ***************
[08/10/2023-11:16:25] [V] [TRT] *************** Autotuning Reformat: Half(518400,1:8,2880,16) -> Float(4147200,32400,180,1) ***************
[08/10/2023-11:16:25] [V] [TRT] *************** Autotuning Reformat: Half(518400,1:8,2880,16) -> Float(4147200,1,23040,128) ***************
[08/10/2023-11:16:25] [V] [TRT] *************** Autotuning Reformat: Half(518400,1:8,2880,16) -> Float(1036800,1:4,5760,32) ***************
[08/10/2023-11:16:25] [V] [TRT] *************** Autotuning Reformat: Half(518400,1:8,2880,16) -> Half(4147200,32400,180,1) ***************
[08/10/2023-11:16:25] [V] [TRT] *************** Autotuning Reformat: Half(518400,1:8,2880,16) -> Half(2073600,32400:2,180,1) ***************
[08/10/2023-11:16:25] [V] [TRT] *************** Autotuning Reformat: Half(518400,1:8,2880,16) -> Half(259200,1:16,1440,8) ***************
[08/10/2023-11:16:25] [V] [TRT] *************** Autotuning Reformat: Half(259200,1:16,1440,8) -> Float(4147200,32400,180,1) ***************
[08/10/2023-11:16:25] [V] [TRT] *************** Autotuning Reformat: Half(259200,1:16,1440,8) -> Float(4147200,1,23040,128) ***************
[08/10/2023-11:16:25] [V] [TRT] *************** Autotuning Reformat: Half(259200,1:16,1440,8) -> Float(1036800,1:4,5760,32) ***************
[08/10/2023-11:16:25] [V] [TRT] *************** Autotuning Reformat: Half(259200,1:16,1440,8) -> Half(4147200,32400,180,1) ***************
[08/10/2023-11:16:25] [V] [TRT] *************** Autotuning Reformat: Half(259200,1:16,1440,8) -> Half(2073600,32400:2,180,1) ***************
[08/10/2023-11:16:25] [V] [TRT] *************** Autotuning Reformat: Half(259200,1:16,1440,8) -> Half(518400,1:8,2880,16) ***************
[08/10/2023-11:16:25] [V] [TRT] =============== Computing reformatting costs
[08/10/2023-11:16:25] [V] [TRT] *************** Autotuning Reformat: Float(4147200,32400,180,1) -> Float(4147200,1,23040,128) ***************
[08/10/2023-11:16:25] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(<in> -> input.72) (Reformat)
[08/10/2023-11:16:25] [V] [TRT] Tactic: 0x00000000000003e8 Time: 1.46357
[08/10/2023-11:16:25] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.243086
[08/10/2023-11:16:25] [V] [TRT] Tactic: 0x0000000000000000 Time: 1.15972
[08/10/2023-11:16:25] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 0.243086
[08/10/2023-11:16:25] [V] [TRT] *************** Autotuning Reformat: Float(4147200,32400,180,1) -> Float(1036800,1:4,5760,32) ***************
[08/10/2023-11:16:25] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(<in> -> input.72) (Reformat)
[08/10/2023-11:16:25] [V] [TRT] Tactic: 0x00000000000003e8 Time: 1.17747
[08/10/2023-11:16:25] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.249577
[08/10/2023-11:16:25] [V] [TRT] Tactic: 0x0000000000000000 Time: 1.18879
[08/10/2023-11:16:25] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 0.249577
[08/10/2023-11:16:25] [V] [TRT] *************** Autotuning Reformat: Float(4147200,32400,180,1) -> Half(4147200,32400,180,1) ***************
[08/10/2023-11:16:25] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(<in> -> input.72) (Reformat)
[08/10/2023-11:16:25] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.307607
[08/10/2023-11:16:25] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.319438
[08/10/2023-11:16:25] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.448128
[08/10/2023-11:16:25] [V] [TRT] Fastest Tactic: 0x00000000000003e8 Time: 0.307607
[08/10/2023-11:16:25] [V] [TRT] *************** Autotuning Reformat: Float(4147200,32400,180,1) -> Half(2073600,32400:2,180,1) ***************
[08/10/2023-11:16:25] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(<in> -> input.72) (Reformat)
[08/10/2023-11:16:25] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.418789
[08/10/2023-11:16:25] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.369239
[08/10/2023-11:16:25] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.31584
[08/10/2023-11:16:25] [V] [TRT] Fastest Tactic: 0x0000000000000000 Time: 0.31584
[08/10/2023-11:16:25] [V] [TRT] *************** Autotuning Reformat: Float(4147200,32400,180,1) -> Half(518400,1:8,2880,16) ***************
[08/10/2023-11:16:25] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(<in> -> input.72) (Reformat)
[08/10/2023-11:16:25] [V] [TRT] Tactic: 0x00000000000003e8 Time: 1.07896
[08/10/2023-11:16:25] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.23189
[08/10/2023-11:16:25] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.302139
[08/10/2023-11:16:25] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 0.23189
[08/10/2023-11:16:25] [V] [TRT] *************** Autotuning Reformat: Float(4147200,32400,180,1) -> Half(259200,1:16,1440,8) ***************
[08/10/2023-11:16:25] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(<in> -> input.72) (Reformat)
[08/10/2023-11:16:25] [V] [TRT] Tactic: 0x00000000000003e8 Time: 1.0924
[08/10/2023-11:16:25] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.23205
[08/10/2023-11:16:25] [V] [TRT] Tactic: 0x0000000000000000 Time: 1.07949
[08/10/2023-11:16:25] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 0.23205
[08/10/2023-11:16:25] [V] [TRT] *************** Autotuning Reformat: Float(4147200,1,23040,128) -> Float(4147200,32400,180,1) ***************
[08/10/2023-11:16:25] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(<in> -> input.72) (Reformat)
[08/10/2023-11:16:25] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.8784
[08/10/2023-11:16:25] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.287319
[08/10/2023-11:16:25] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.93312
[08/10/2023-11:16:25] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 0.287319
[08/10/2023-11:16:25] [V] [TRT] *************** Autotuning Reformat: Float(4147200,1,23040,128) -> Float(1036800,1:4,5760,32) ***************
[08/10/2023-11:16:25] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(<in> -> input.72) (Reformat)
[08/10/2023-11:16:25] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.431342
[08/10/2023-11:16:25] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.290679
[08/10/2023-11:16:25] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.430094
[08/10/2023-11:16:25] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 0.290679
[08/10/2023-11:16:25] [V] [TRT] *************** Autotuning Reformat: Float(4147200,1,23040,128) -> Half(4147200,32400,180,1) ***************
[08/10/2023-11:16:25] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(<in> -> input.72) (Reformat)
[08/10/2023-11:16:25] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.756178
[08/10/2023-11:16:25] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.260617
[08/10/2023-11:16:25] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.718414
[08/10/2023-11:16:25] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 0.260617
[08/10/2023-11:16:25] [V] [TRT] *************** Autotuning Reformat: Float(4147200,1,23040,128) -> Half(2073600,32400:2,180,1) ***************
[08/10/2023-11:16:25] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(<in> -> input.72) (Reformat)
[08/10/2023-11:16:25] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.963762
[08/10/2023-11:16:25] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.342779
[08/10/2023-11:16:25] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.742501
[08/10/2023-11:16:25] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 0.342779
[08/10/2023-11:16:25] [V] [TRT] *************** Autotuning Reformat: Float(4147200,1,23040,128) -> Half(518400,1:8,2880,16) ***************
[08/10/2023-11:16:25] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(<in> -> input.72) (Reformat)
[08/10/2023-11:16:25] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.416302
[08/10/2023-11:16:25] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.202862
[08/10/2023-11:16:25] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.414574
[08/10/2023-11:16:25] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 0.202862
[08/10/2023-11:16:25] [V] [TRT] *************** Autotuning Reformat: Float(4147200,1,23040,128) -> Half(259200,1:16,1440,8) ***************
[08/10/2023-11:16:25] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(<in> -> input.72) (Reformat)
[08/10/2023-11:16:25] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.415561
[08/10/2023-11:16:25] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.202962
[08/10/2023-11:16:25] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.414651
[08/10/2023-11:16:25] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 0.202962
[08/10/2023-11:16:25] [V] [TRT] *************** Autotuning Reformat: Float(1036800,1:4,5760,32) -> Float(4147200,32400,180,1) ***************
[08/10/2023-11:16:25] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(<in> -> input.72) (Reformat)
[08/10/2023-11:16:25] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.72949
[08/10/2023-11:16:25] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.269829
[08/10/2023-11:16:25] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.718057
[08/10/2023-11:16:25] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 0.269829
[08/10/2023-11:16:25] [V] [TRT] *************** Autotuning Reformat: Float(1036800,1:4,5760,32) -> Float(4147200,1,23040,128) ***************
[08/10/2023-11:16:25] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(<in> -> input.72) (Reformat)
[08/10/2023-11:16:25] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.40283
[08/10/2023-11:16:25] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.210711
[08/10/2023-11:16:25] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.403401
[08/10/2023-11:16:25] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 0.210711
[08/10/2023-11:16:25] [V] [TRT] *************** Autotuning Reformat: Float(1036800,1:4,5760,32) -> Half(4147200,32400,180,1) ***************
[08/10/2023-11:16:25] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(<in> -> input.72) (Reformat)
[08/10/2023-11:16:25] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.695515
[08/10/2023-11:16:25] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.258491
[08/10/2023-11:16:25] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.662249
[08/10/2023-11:16:25] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 0.258491
[08/10/2023-11:16:25] [V] [TRT] *************** Autotuning Reformat: Float(1036800,1:4,5760,32) -> Half(2073600,32400:2,180,1) ***************
[08/10/2023-11:16:25] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(<in> -> input.72) (Reformat)
[08/10/2023-11:16:25] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.724777
[08/10/2023-11:16:25] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.340247
[08/10/2023-11:16:25] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.671314
[08/10/2023-11:16:25] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 0.340247
[08/10/2023-11:16:25] [V] [TRT] *************** Autotuning Reformat: Float(1036800,1:4,5760,32) -> Half(518400,1:8,2880,16) ***************
[08/10/2023-11:16:25] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(<in> -> input.72) (Reformat)
[08/10/2023-11:16:25] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.481984
[08/10/2023-11:16:25] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.198706
[08/10/2023-11:16:25] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.4816
[08/10/2023-11:16:25] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 0.198706
[08/10/2023-11:16:25] [V] [TRT] *************** Autotuning Reformat: Float(1036800,1:4,5760,32) -> Half(259200,1:16,1440,8) ***************
[08/10/2023-11:16:25] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(<in> -> input.72) (Reformat)
[08/10/2023-11:16:25] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.480814
[08/10/2023-11:16:25] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.197554
[08/10/2023-11:16:25] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.480882
[08/10/2023-11:16:25] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 0.197554
[08/10/2023-11:16:25] [V] [TRT] *************** Autotuning Reformat: Half(4147200,32400,180,1) -> Float(4147200,32400,180,1) ***************
[08/10/2023-11:16:25] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(<in> -> input.72) (Reformat)
[08/10/2023-11:16:25] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.272535
[08/10/2023-11:16:25] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.30779
[08/10/2023-11:16:25] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.394313
[08/10/2023-11:16:25] [V] [TRT] Fastest Tactic: 0x00000000000003e8 Time: 0.272535
[08/10/2023-11:16:25] [V] [TRT] *************** Autotuning Reformat: Half(4147200,32400,180,1) -> Float(4147200,1,23040,128) ***************
[08/10/2023-11:16:25] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(<in> -> input.72) (Reformat)
[08/10/2023-11:16:25] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.959986
[08/10/2023-11:16:25] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.225266
[08/10/2023-11:16:25] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.970194
[08/10/2023-11:16:25] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 0.225266
[08/10/2023-11:16:25] [V] [TRT] *************** Autotuning Reformat: Half(4147200,32400,180,1) -> Float(1036800,1:4,5760,32) ***************
[08/10/2023-11:16:25] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(<in> -> input.72) (Reformat)
[08/10/2023-11:16:25] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.991141
[08/10/2023-11:16:25] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.225202
[08/10/2023-11:16:25] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.973943
[08/10/2023-11:16:25] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 0.225202
[08/10/2023-11:16:25] [V] [TRT] *************** Autotuning Reformat: Half(4147200,32400,180,1) -> Half(2073600,32400:2,180,1) ***************
[08/10/2023-11:16:25] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(<in> -> input.72) (Reformat)
[08/10/2023-11:16:25] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.361842
[08/10/2023-11:16:25] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.20496
[08/10/2023-11:16:25] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.257371
[08/10/2023-11:16:25] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 0.20496
[08/10/2023-11:16:25] [V] [TRT] *************** Autotuning Reformat: Half(4147200,32400,180,1) -> Half(518400,1:8,2880,16) ***************
[08/10/2023-11:16:25] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(<in> -> input.72) (Reformat)
[08/10/2023-11:16:25] [V] [TRT] Tactic: 0x00000000000003e8 Time: 1.03587
[08/10/2023-11:16:25] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.18736
[08/10/2023-11:16:25] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.269179
[08/10/2023-11:16:25] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 0.18736
[08/10/2023-11:16:25] [V] [TRT] *************** Autotuning Reformat: Half(4147200,32400,180,1) -> Half(259200,1:16,1440,8) ***************
[08/10/2023-11:16:25] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(<in> -> input.72) (Reformat)
[08/10/2023-11:16:25] [V] [TRT] Tactic: 0x00000000000003e8 Time: 1.04142
[08/10/2023-11:16:25] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.187026
[08/10/2023-11:16:25] [V] [TRT] Tactic: 0x0000000000000000 Time: 1.03873
[08/10/2023-11:16:25] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 0.187026
[08/10/2023-11:16:25] [V] [TRT] *************** Autotuning Reformat: Half(2073600,32400:2,180,1) -> Float(4147200,32400,180,1) ***************
[08/10/2023-11:16:25] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(<in> -> input.72) (Reformat)
[08/10/2023-11:16:25] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.397906
[08/10/2023-11:16:25] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.292421
[08/10/2023-11:16:25] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.272608
[08/10/2023-11:16:25] [V] [TRT] Fastest Tactic: 0x0000000000000000 Time: 0.272608
[08/10/2023-11:16:25] [V] [TRT] *************** Autotuning Reformat: Half(2073600,32400:2,180,1) -> Float(4147200,1,23040,128) ***************
[08/10/2023-11:16:25] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(<in> -> input.72) (Reformat)
[08/10/2023-11:16:25] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.629445
[08/10/2023-11:16:25] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.246423
[08/10/2023-11:16:25] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.721467
[08/10/2023-11:16:25] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 0.246423
[08/10/2023-11:16:25] [V] [TRT] *************** Autotuning Reformat: Half(2073600,32400:2,180,1) -> Float(1036800,1:4,5760,32) ***************
[08/10/2023-11:16:25] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(<in> -> input.72) (Reformat)
[08/10/2023-11:16:25] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.691845
[08/10/2023-11:16:25] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.244695
[08/10/2023-11:16:25] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.691968
[08/10/2023-11:16:25] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 0.244695
[08/10/2023-11:16:25] [V] [TRT] *************** Autotuning Reformat: Half(2073600,32400:2,180,1) -> Half(4147200,32400,180,1) ***************
[08/10/2023-11:16:25] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(<in> -> input.72) (Reformat)
[08/10/2023-11:16:25] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.366313
[08/10/2023-11:16:25] [V] [TRT] Tactic: 0x00000000000003ea Time: 1.36623
[08/10/2023-11:16:25] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.245477
[08/10/2023-11:16:25] [V] [TRT] Fastest Tactic: 0x0000000000000000 Time: 0.245477
[08/10/2023-11:16:25] [V] [TRT] *************** Autotuning Reformat: Half(2073600,32400:2,180,1) -> Half(518400,1:8,2880,16) ***************
[08/10/2023-11:16:25] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(<in> -> input.72) (Reformat)
[08/10/2023-11:16:25] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.697454
[08/10/2023-11:16:25] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.233595
[08/10/2023-11:16:25] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.271063
[08/10/2023-11:16:25] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 0.233595
[08/10/2023-11:16:25] [V] [TRT] *************** Autotuning Reformat: Half(2073600,32400:2,180,1) -> Half(259200,1:16,1440,8) ***************
[08/10/2023-11:16:25] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(<in> -> input.72) (Reformat)
[08/10/2023-11:16:25] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.699881
[08/10/2023-11:16:25] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.234962
[08/10/2023-11:16:25] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.696517
[08/10/2023-11:16:25] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 0.234962
[08/10/2023-11:16:25] [V] [TRT] *************** Autotuning Reformat: Half(518400,1:8,2880,16) -> Float(4147200,32400,180,1) ***************
[08/10/2023-11:16:25] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(<in> -> input.72) (Reformat)
[08/10/2023-11:16:25] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.662171
[08/10/2023-11:16:25] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.254322
[08/10/2023-11:16:25] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.271817
[08/10/2023-11:16:25] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 0.254322
[08/10/2023-11:16:25] [V] [TRT] *************** Autotuning Reformat: Half(518400,1:8,2880,16) -> Float(4147200,1,23040,128) ***************
[08/10/2023-11:16:25] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(<in> -> input.72) (Reformat)
[08/10/2023-11:16:25] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.432507
[08/10/2023-11:16:25] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.220773
[08/10/2023-11:16:25] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.440969
[08/10/2023-11:16:25] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 0.220773
[08/10/2023-11:16:25] [V] [TRT] *************** Autotuning Reformat: Half(518400,1:8,2880,16) -> Float(1036800,1:4,5760,32) ***************
[08/10/2023-11:16:25] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(<in> -> input.72) (Reformat)
[08/10/2023-11:16:25] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.502789
[08/10/2023-11:16:25] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.220562
[08/10/2023-11:16:25] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.503643
[08/10/2023-11:16:25] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 0.220562
[08/10/2023-11:16:25] [V] [TRT] *************** Autotuning Reformat: Half(518400,1:8,2880,16) -> Half(4147200,32400,180,1) ***************
[08/10/2023-11:16:25] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(<in> -> input.72) (Reformat)
[08/10/2023-11:16:25] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.729984
[08/10/2023-11:16:25] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.997673
[08/10/2023-11:16:25] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.216215
[08/10/2023-11:16:25] [V] [TRT] Fastest Tactic: 0x0000000000000000 Time: 0.216215
[08/10/2023-11:16:25] [V] [TRT] *************** Autotuning Reformat: Half(518400,1:8,2880,16) -> Half(2073600,32400:2,180,1) ***************
[08/10/2023-11:16:25] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(<in> -> input.72) (Reformat)
[08/10/2023-11:16:25] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.664434
[08/10/2023-11:16:25] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.313074
[08/10/2023-11:16:25] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.213801
[08/10/2023-11:16:25] [V] [TRT] Fastest Tactic: 0x0000000000000000 Time: 0.213801
[08/10/2023-11:16:25] [V] [TRT] *************** Autotuning Reformat: Half(518400,1:8,2880,16) -> Half(259200,1:16,1440,8) ***************
[08/10/2023-11:16:25] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(<in> -> input.72) (Reformat)
[08/10/2023-11:16:25] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.448736
[08/10/2023-11:16:25] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.198231
[08/10/2023-11:16:25] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.447803
[08/10/2023-11:16:25] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 0.198231
[08/10/2023-11:16:25] [V] [TRT] *************** Autotuning Reformat: Half(259200,1:16,1440,8) -> Float(4147200,32400,180,1) ***************
[08/10/2023-11:16:25] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(<in> -> input.72) (Reformat)
[08/10/2023-11:16:25] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.66528
[08/10/2023-11:16:25] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.251355
[08/10/2023-11:16:26] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.654245
[08/10/2023-11:16:26] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 0.251355
[08/10/2023-11:16:26] [V] [TRT] *************** Autotuning Reformat: Half(259200,1:16,1440,8) -> Float(4147200,1,23040,128) ***************
[08/10/2023-11:16:26] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(<in> -> input.72) (Reformat)
[08/10/2023-11:16:26] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.409509
[08/10/2023-11:16:26] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.199881
[08/10/2023-11:16:26] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.409394
[08/10/2023-11:16:26] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 0.199881
[08/10/2023-11:16:26] [V] [TRT] *************** Autotuning Reformat: Half(259200,1:16,1440,8) -> Float(1036800,1:4,5760,32) ***************
[08/10/2023-11:16:26] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(<in> -> input.72) (Reformat)
[08/10/2023-11:16:26] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.483767
[08/10/2023-11:16:26] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.199927
[08/10/2023-11:16:26] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.483717
[08/10/2023-11:16:26] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 0.199927
[08/10/2023-11:16:26] [V] [TRT] *************** Autotuning Reformat: Half(259200,1:16,1440,8) -> Half(4147200,32400,180,1) ***************
[08/10/2023-11:16:26] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(<in> -> input.72) (Reformat)
[08/10/2023-11:16:26] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.629317
[08/10/2023-11:16:26] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.971173
[08/10/2023-11:16:26] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.627785
[08/10/2023-11:16:26] [V] [TRT] Fastest Tactic: 0x0000000000000000 Time: 0.627785
[08/10/2023-11:16:26] [V] [TRT] *************** Autotuning Reformat: Half(259200,1:16,1440,8) -> Half(2073600,32400:2,180,1) ***************
[08/10/2023-11:16:26] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(<in> -> input.72) (Reformat)
[08/10/2023-11:16:26] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.67083
[08/10/2023-11:16:26] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.307589
[08/10/2023-11:16:26] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.681193
[08/10/2023-11:16:26] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 0.307589
[08/10/2023-11:16:26] [V] [TRT] *************** Autotuning Reformat: Half(259200,1:16,1440,8) -> Half(518400,1:8,2880,16) ***************
[08/10/2023-11:16:26] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(<in> -> input.72) (Reformat)
[08/10/2023-11:16:26] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.447863
[08/10/2023-11:16:26] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.198464
[08/10/2023-11:16:26] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.447374
[08/10/2023-11:16:26] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 0.198464
[08/10/2023-11:16:26] [V] [TRT] =============== Computing reformatting costs
[08/10/2023-11:16:26] [V] [TRT] *************** Autotuning Reformat: Float(4147200,32400,180,1) -> Float(4147200,1,23040,128) ***************
[08/10/2023-11:16:26] [V] [TRT] *************** Autotuning Reformat: Float(4147200,32400,180,1) -> Float(1036800,1:4,5760,32) ***************
[08/10/2023-11:16:26] [V] [TRT] *************** Autotuning Reformat: Float(4147200,32400,180,1) -> Half(4147200,32400,180,1) ***************
[08/10/2023-11:16:26] [V] [TRT] *************** Autotuning Reformat: Float(4147200,32400,180,1) -> Half(2073600,32400:2,180,1) ***************
[08/10/2023-11:16:26] [V] [TRT] *************** Autotuning Reformat: Float(4147200,32400,180,1) -> Half(518400,1:8,2880,16) ***************
[08/10/2023-11:16:26] [V] [TRT] *************** Autotuning Reformat: Float(4147200,32400,180,1) -> Half(259200,1:16,1440,8) ***************
[08/10/2023-11:16:26] [V] [TRT] *************** Autotuning Reformat: Float(4147200,1,23040,128) -> Float(4147200,32400,180,1) ***************
[08/10/2023-11:16:26] [V] [TRT] *************** Autotuning Reformat: Float(4147200,1,23040,128) -> Float(1036800,1:4,5760,32) ***************
[08/10/2023-11:16:26] [V] [TRT] *************** Autotuning Reformat: Float(4147200,1,23040,128) -> Half(4147200,32400,180,1) ***************
[08/10/2023-11:16:26] [V] [TRT] *************** Autotuning Reformat: Float(4147200,1,23040,128) -> Half(2073600,32400:2,180,1) ***************
[08/10/2023-11:16:26] [V] [TRT] *************** Autotuning Reformat: Float(4147200,1,23040,128) -> Half(518400,1:8,2880,16) ***************
[08/10/2023-11:16:26] [V] [TRT] *************** Autotuning Reformat: Float(4147200,1,23040,128) -> Half(259200,1:16,1440,8) ***************
[08/10/2023-11:16:26] [V] [TRT] *************** Autotuning Reformat: Float(1036800,1:4,5760,32) -> Float(4147200,32400,180,1) ***************
[08/10/2023-11:16:26] [V] [TRT] *************** Autotuning Reformat: Float(1036800,1:4,5760,32) -> Float(4147200,1,23040,128) ***************
[08/10/2023-11:16:26] [V] [TRT] *************** Autotuning Reformat: Float(1036800,1:4,5760,32) -> Half(4147200,32400,180,1) ***************
[08/10/2023-11:16:26] [V] [TRT] *************** Autotuning Reformat: Float(1036800,1:4,5760,32) -> Half(2073600,32400:2,180,1) ***************
[08/10/2023-11:16:26] [V] [TRT] *************** Autotuning Reformat: Float(1036800,1:4,5760,32) -> Half(518400,1:8,2880,16) ***************
[08/10/2023-11:16:26] [V] [TRT] *************** Autotuning Reformat: Float(1036800,1:4,5760,32) -> Half(259200,1:16,1440,8) ***************
[08/10/2023-11:16:26] [V] [TRT] *************** Autotuning Reformat: Half(4147200,32400,180,1) -> Float(4147200,32400,180,1) ***************
[08/10/2023-11:16:26] [V] [TRT] *************** Autotuning Reformat: Half(4147200,32400,180,1) -> Float(4147200,1,23040,128) ***************
[08/10/2023-11:16:26] [V] [TRT] *************** Autotuning Reformat: Half(4147200,32400,180,1) -> Float(1036800,1:4,5760,32) ***************
[08/10/2023-11:16:26] [V] [TRT] *************** Autotuning Reformat: Half(4147200,32400,180,1) -> Half(2073600,32400:2,180,1) ***************
[08/10/2023-11:16:26] [V] [TRT] *************** Autotuning Reformat: Half(4147200,32400,180,1) -> Half(518400,1:8,2880,16) ***************
[08/10/2023-11:16:26] [V] [TRT] *************** Autotuning Reformat: Half(4147200,32400,180,1) -> Half(259200,1:16,1440,8) ***************
[08/10/2023-11:16:26] [V] [TRT] *************** Autotuning Reformat: Half(2073600,32400:2,180,1) -> Float(4147200,32400,180,1) ***************
[08/10/2023-11:16:26] [V] [TRT] *************** Autotuning Reformat: Half(2073600,32400:2,180,1) -> Float(4147200,1,23040,128) ***************
[08/10/2023-11:16:26] [V] [TRT] *************** Autotuning Reformat: Half(2073600,32400:2,180,1) -> Float(1036800,1:4,5760,32) ***************
[08/10/2023-11:16:26] [V] [TRT] *************** Autotuning Reformat: Half(2073600,32400:2,180,1) -> Half(4147200,32400,180,1) ***************
[08/10/2023-11:16:26] [V] [TRT] *************** Autotuning Reformat: Half(2073600,32400:2,180,1) -> Half(518400,1:8,2880,16) ***************
[08/10/2023-11:16:26] [V] [TRT] *************** Autotuning Reformat: Half(2073600,32400:2,180,1) -> Half(259200,1:16,1440,8) ***************
[08/10/2023-11:16:26] [V] [TRT] *************** Autotuning Reformat: Half(518400,1:8,2880,16) -> Float(4147200,32400,180,1) ***************
[08/10/2023-11:16:26] [V] [TRT] *************** Autotuning Reformat: Half(518400,1:8,2880,16) -> Float(4147200,1,23040,128) ***************
[08/10/2023-11:16:26] [V] [TRT] *************** Autotuning Reformat: Half(518400,1:8,2880,16) -> Float(1036800,1:4,5760,32) ***************
[08/10/2023-11:16:26] [V] [TRT] *************** Autotuning Reformat: Half(518400,1:8,2880,16) -> Half(4147200,32400,180,1) ***************
[08/10/2023-11:16:26] [V] [TRT] *************** Autotuning Reformat: Half(518400,1:8,2880,16) -> Half(2073600,32400:2,180,1) ***************
[08/10/2023-11:16:26] [V] [TRT] *************** Autotuning Reformat: Half(518400,1:8,2880,16) -> Half(259200,1:16,1440,8) ***************
[08/10/2023-11:16:26] [V] [TRT] *************** Autotuning Reformat: Half(259200,1:16,1440,8) -> Float(4147200,32400,180,1) ***************
[08/10/2023-11:16:26] [V] [TRT] *************** Autotuning Reformat: Half(259200,1:16,1440,8) -> Float(4147200,1,23040,128) ***************
[08/10/2023-11:16:26] [V] [TRT] *************** Autotuning Reformat: Half(259200,1:16,1440,8) -> Float(1036800,1:4,5760,32) ***************
[08/10/2023-11:16:26] [V] [TRT] *************** Autotuning Reformat: Half(259200,1:16,1440,8) -> Half(4147200,32400,180,1) ***************
[08/10/2023-11:16:26] [V] [TRT] *************** Autotuning Reformat: Half(259200,1:16,1440,8) -> Half(2073600,32400:2,180,1) ***************
[08/10/2023-11:16:26] [V] [TRT] *************** Autotuning Reformat: Half(259200,1:16,1440,8) -> Half(518400,1:8,2880,16) ***************
[08/10/2023-11:16:26] [V] [TRT] =============== Computing reformatting costs
[08/10/2023-11:16:26] [V] [TRT] *************** Autotuning Reformat: Float(16588800,32400,180,1) -> Float(16588800,1,92160,512) ***************
[08/10/2023-11:16:26] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(<in> -> onnx::Concat_602) (Reformat)
[08/10/2023-11:16:26] [V] [TRT] Tactic: 0x00000000000003e8 Time: 2.60692
[08/10/2023-11:16:26] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.483611
[08/10/2023-11:16:26] [V] [TRT] Tactic: 0x0000000000000000 Time: 2.66083
[08/10/2023-11:16:26] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 0.483611
[08/10/2023-11:16:26] [V] [TRT] *************** Autotuning Reformat: Float(16588800,32400,180,1) -> Float(4147200,1:4,23040,128) ***************
[08/10/2023-11:16:26] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(<in> -> onnx::Concat_602) (Reformat)
[08/10/2023-11:16:26] [V] [TRT] Tactic: 0x00000000000003e8 Time: 2.52111
[08/10/2023-11:16:26] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.498725
[08/10/2023-11:16:26] [V] [TRT] Tactic: 0x0000000000000000 Time: 2.41615
[08/10/2023-11:16:26] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 0.498725
[08/10/2023-11:16:26] [V] [TRT] *************** Autotuning Reformat: Float(16588800,32400,180,1) -> Half(16588800,32400,180,1) ***************
[08/10/2023-11:16:26] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(<in> -> onnx::Concat_602) (Reformat)
[08/10/2023-11:16:26] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.419671
[08/10/2023-11:16:26] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.729614
[08/10/2023-11:16:26] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.413861
[08/10/2023-11:16:26] [V] [TRT] Fastest Tactic: 0x0000000000000000 Time: 0.413861
[08/10/2023-11:16:26] [V] [TRT] *************** Autotuning Reformat: Float(16588800,32400,180,1) -> Half(8294400,32400:2,180,1) ***************
[08/10/2023-11:16:26] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(<in> -> onnx::Concat_602) (Reformat)
[08/10/2023-11:16:26] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.810866
[08/10/2023-11:16:26] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.719205
[08/10/2023-11:16:26] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.618181
[08/10/2023-11:16:26] [V] [TRT] Fastest Tactic: 0x0000000000000000 Time: 0.618181
[08/10/2023-11:16:26] [V] [TRT] *************** Autotuning Reformat: Float(16588800,32400,180,1) -> Half(2073600,1:8,11520,64) ***************
[08/10/2023-11:16:26] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(<in> -> onnx::Concat_602) (Reformat)
[08/10/2023-11:16:26] [V] [TRT] Tactic: 0x00000000000003e8 Time: 2.37637
[08/10/2023-11:16:26] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.459534
[08/10/2023-11:16:26] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.598921
[08/10/2023-11:16:26] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 0.459534
[08/10/2023-11:16:26] [V] [TRT] *************** Autotuning Reformat: Float(16588800,32400,180,1) -> Half(1036800,1:16,5760,32) ***************
[08/10/2023-11:16:26] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(<in> -> onnx::Concat_602) (Reformat)
[08/10/2023-11:16:26] [V] [TRT] Tactic: 0x00000000000003e8 Time: 2.32747
[08/10/2023-11:16:26] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.451762
[08/10/2023-11:16:26] [V] [TRT] Tactic: 0x0000000000000000 Time: 2.32835
[08/10/2023-11:16:26] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 0.451762
[08/10/2023-11:16:26] [V] [TRT] *************** Autotuning Reformat: Float(16588800,1,92160,512) -> Float(16588800,32400,180,1) ***************
[08/10/2023-11:16:26] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(<in> -> onnx::Concat_602) (Reformat)
[08/10/2023-11:16:26] [V] [TRT] Tactic: 0x00000000000003e8 Time: 2.23184
[08/10/2023-11:16:26] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.560905
[08/10/2023-11:16:26] [V] [TRT] Tactic: 0x0000000000000000 Time: 2.34307
[08/10/2023-11:16:26] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 0.560905
[08/10/2023-11:16:26] [V] [TRT] *************** Autotuning Reformat: Float(16588800,1,92160,512) -> Float(4147200,1:4,23040,128) ***************
[08/10/2023-11:16:26] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(<in> -> onnx::Concat_602) (Reformat)
[08/10/2023-11:16:26] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.845582
[08/10/2023-11:16:26] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.572535
[08/10/2023-11:16:26] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.845033
[08/10/2023-11:16:26] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 0.572535
[08/10/2023-11:16:26] [V] [TRT] *************** Autotuning Reformat: Float(16588800,1,92160,512) -> Half(16588800,32400,180,1) ***************
[08/10/2023-11:16:26] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(<in> -> onnx::Concat_602) (Reformat)
[08/10/2023-11:16:26] [V] [TRT] Tactic: 0x00000000000003e8 Time: 1.96867
[08/10/2023-11:16:26] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.536192
[08/10/2023-11:16:26] [V] [TRT] Tactic: 0x0000000000000000 Time: 1.3691
[08/10/2023-11:16:26] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 0.536192
[08/10/2023-11:16:26] [V] [TRT] *************** Autotuning Reformat: Float(16588800,1,92160,512) -> Half(8294400,32400:2,180,1) ***************
[08/10/2023-11:16:26] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(<in> -> onnx::Concat_602) (Reformat)
[08/10/2023-11:16:26] [V] [TRT] Tactic: 0x00000000000003e8 Time: 1.43711
[08/10/2023-11:16:26] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.704773
[08/10/2023-11:16:26] [V] [TRT] Tactic: 0x0000000000000000 Time: 1.38616
[08/10/2023-11:16:26] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 0.704773
[08/10/2023-11:16:26] [V] [TRT] *************** Autotuning Reformat: Float(16588800,1,92160,512) -> Half(2073600,1:8,11520,64) ***************
[08/10/2023-11:16:26] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(<in> -> onnx::Concat_602) (Reformat)
[08/10/2023-11:16:26] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.805623
[08/10/2023-11:16:26] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.386286
[08/10/2023-11:16:26] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.803456
[08/10/2023-11:16:26] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 0.386286
[08/10/2023-11:16:26] [V] [TRT] *************** Autotuning Reformat: Float(16588800,1,92160,512) -> Half(1036800,1:16,5760,32) ***************
[08/10/2023-11:16:26] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(<in> -> onnx::Concat_602) (Reformat)
[08/10/2023-11:16:26] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.803456
[08/10/2023-11:16:26] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.386697
[08/10/2023-11:16:26] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.803415
[08/10/2023-11:16:26] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 0.386697
[08/10/2023-11:16:26] [V] [TRT] *************** Autotuning Reformat: Float(4147200,1:4,23040,128) -> Float(16588800,32400,180,1) ***************
[08/10/2023-11:16:26] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(<in> -> onnx::Concat_602) (Reformat)
[08/10/2023-11:16:26] [V] [TRT] Tactic: 0x00000000000003e8 Time: 1.69991
[08/10/2023-11:16:26] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.523931
[08/10/2023-11:16:26] [V] [TRT] Tactic: 0x0000000000000000 Time: 1.67221
[08/10/2023-11:16:26] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 0.523931
[08/10/2023-11:16:26] [V] [TRT] *************** Autotuning Reformat: Float(4147200,1:4,23040,128) -> Float(16588800,1,92160,512) ***************
[08/10/2023-11:16:26] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(<in> -> onnx::Concat_602) (Reformat)
[08/10/2023-11:16:26] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.814235
[08/10/2023-11:16:26] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.412622
[08/10/2023-11:16:26] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.797001
[08/10/2023-11:16:26] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 0.412622
[08/10/2023-11:16:26] [V] [TRT] *************** Autotuning Reformat: Float(4147200,1:4,23040,128) -> Half(16588800,32400,180,1) ***************
[08/10/2023-11:16:26] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(<in> -> onnx::Concat_602) (Reformat)
[08/10/2023-11:16:26] [V] [TRT] Tactic: 0x00000000000003e8 Time: 1.35353
[08/10/2023-11:16:26] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.520585
[08/10/2023-11:16:26] [V] [TRT] Tactic: 0x0000000000000000 Time: 1.73197
[08/10/2023-11:16:26] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 0.520585
[08/10/2023-11:16:26] [V] [TRT] *************** Autotuning Reformat: Float(4147200,1:4,23040,128) -> Half(8294400,32400:2,180,1) ***************
[08/10/2023-11:16:26] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(<in> -> onnx::Concat_602) (Reformat)
[08/10/2023-11:16:26] [V] [TRT] Tactic: 0x00000000000003e8 Time: 1.48594
[08/10/2023-11:16:26] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.706779
[08/10/2023-11:16:26] [V] [TRT] Tactic: 0x0000000000000000 Time: 1.75061
[08/10/2023-11:16:26] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 0.706779
[08/10/2023-11:16:26] [V] [TRT] *************** Autotuning Reformat: Float(4147200,1:4,23040,128) -> Half(2073600,1:8,11520,64) ***************
[08/10/2023-11:16:26] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(<in> -> onnx::Concat_602) (Reformat)
[08/10/2023-11:16:26] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.953815
[08/10/2023-11:16:26] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.385906
[08/10/2023-11:16:26] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.954501
[08/10/2023-11:16:26] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 0.385906
[08/10/2023-11:16:26] [V] [TRT] *************** Autotuning Reformat: Float(4147200,1:4,23040,128) -> Half(1036800,1:16,5760,32) ***************
[08/10/2023-11:16:26] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(<in> -> onnx::Concat_602) (Reformat)
[08/10/2023-11:16:27] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.954633
[08/10/2023-11:16:27] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.386048
[08/10/2023-11:16:27] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.954226
[08/10/2023-11:16:27] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 0.386048
[08/10/2023-11:16:27] [V] [TRT] *************** Autotuning Reformat: Half(16588800,32400,180,1) -> Float(16588800,32400,180,1) ***************
[08/10/2023-11:16:27] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(<in> -> onnx::Concat_602) (Reformat)
[08/10/2023-11:16:27] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.311547
[08/10/2023-11:16:27] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.742871
[08/10/2023-11:16:27] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.313321
[08/10/2023-11:16:27] [V] [TRT] Fastest Tactic: 0x00000000000003e8 Time: 0.311547
[08/10/2023-11:16:27] [V] [TRT] *************** Autotuning Reformat: Half(16588800,32400,180,1) -> Float(16588800,1,92160,512) ***************
[08/10/2023-11:16:27] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(<in> -> onnx::Concat_602) (Reformat)
[08/10/2023-11:16:27] [V] [TRT] Tactic: 0x00000000000003e8 Time: 1.98365
[08/10/2023-11:16:27] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.440768
[08/10/2023-11:16:27] [V] [TRT] Tactic: 0x0000000000000000 Time: 1.93902
[08/10/2023-11:16:27] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 0.440768
[08/10/2023-11:16:27] [V] [TRT] *************** Autotuning Reformat: Half(16588800,32400,180,1) -> Float(4147200,1:4,23040,128) ***************
[08/10/2023-11:16:27] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(<in> -> onnx::Concat_602) (Reformat)
[08/10/2023-11:16:27] [V] [TRT] Tactic: 0x00000000000003e8 Time: 1.94673
[08/10/2023-11:16:27] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.44027
[08/10/2023-11:16:27] [V] [TRT] Tactic: 0x0000000000000000 Time: 1.95002
[08/10/2023-11:16:27] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 0.44027
[08/10/2023-11:16:27] [V] [TRT] *************** Autotuning Reformat: Half(16588800,32400,180,1) -> Half(8294400,32400:2,180,1) ***************
[08/10/2023-11:16:27] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(<in> -> onnx::Concat_602) (Reformat)
[08/10/2023-11:16:27] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.716827
[08/10/2023-11:16:27] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.398213
[08/10/2023-11:16:27] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.504695
[08/10/2023-11:16:27] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 0.398213
[08/10/2023-11:16:27] [V] [TRT] *************** Autotuning Reformat: Half(16588800,32400,180,1) -> Half(2073600,1:8,11520,64) ***************
[08/10/2023-11:16:27] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(<in> -> onnx::Concat_602) (Reformat)
[08/10/2023-11:16:27] [V] [TRT] Tactic: 0x00000000000003e8 Time: 1.96207
[08/10/2023-11:16:27] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.351429
[08/10/2023-11:16:27] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.51861
[08/10/2023-11:16:27] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 0.351429
[08/10/2023-11:16:27] [V] [TRT] *************** Autotuning Reformat: Half(16588800,32400,180,1) -> Half(1036800,1:16,5760,32) ***************
[08/10/2023-11:16:27] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(<in> -> onnx::Concat_602) (Reformat)
[08/10/2023-11:16:27] [V] [TRT] Tactic: 0x00000000000003e8 Time: 2.00321
[08/10/2023-11:16:27] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.360818
[08/10/2023-11:16:27] [V] [TRT] Tactic: 0x0000000000000000 Time: 2.0168
[08/10/2023-11:16:27] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 0.360818
[08/10/2023-11:16:27] [V] [TRT] *************** Autotuning Reformat: Half(8294400,32400:2,180,1) -> Float(16588800,32400,180,1) ***************
[08/10/2023-11:16:27] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(<in> -> onnx::Concat_602) (Reformat)
[08/10/2023-11:16:27] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.773902
[08/10/2023-11:16:27] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.541074
[08/10/2023-11:16:27] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.772082
[08/10/2023-11:16:27] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 0.541074
[08/10/2023-11:16:27] [V] [TRT] *************** Autotuning Reformat: Half(8294400,32400:2,180,1) -> Float(16588800,1,92160,512) ***************
[08/10/2023-11:16:27] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(<in> -> onnx::Concat_602) (Reformat)
[08/10/2023-11:16:27] [V] [TRT] Tactic: 0x00000000000003e8 Time: 1.51233
[08/10/2023-11:16:27] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.451749
[08/10/2023-11:16:27] [V] [TRT] Tactic: 0x0000000000000000 Time: 1.50907
[08/10/2023-11:16:27] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 0.451749
[08/10/2023-11:16:27] [V] [TRT] *************** Autotuning Reformat: Half(8294400,32400:2,180,1) -> Float(4147200,1:4,23040,128) ***************
[08/10/2023-11:16:27] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(<in> -> onnx::Concat_602) (Reformat)
[08/10/2023-11:16:27] [V] [TRT] Tactic: 0x00000000000003e8 Time: 1.64109
[08/10/2023-11:16:27] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.448343
[08/10/2023-11:16:27] [V] [TRT] Tactic: 0x0000000000000000 Time: 1.63767
[08/10/2023-11:16:27] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 0.448343
[08/10/2023-11:16:27] [V] [TRT] *************** Autotuning Reformat: Half(8294400,32400:2,180,1) -> Half(16588800,32400,180,1) ***************
[08/10/2023-11:16:27] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(<in> -> onnx::Concat_602) (Reformat)
[08/10/2023-11:16:27] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.697746
[08/10/2023-11:16:27] [V] [TRT] Tactic: 0x00000000000003ea Time: 2.75165
[08/10/2023-11:16:27] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.697915
[08/10/2023-11:16:27] [V] [TRT] Fastest Tactic: 0x00000000000003e8 Time: 0.697746
[08/10/2023-11:16:27] [V] [TRT] *************** Autotuning Reformat: Half(8294400,32400:2,180,1) -> Half(2073600,1:8,11520,64) ***************
[08/10/2023-11:16:27] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(<in> -> onnx::Concat_602) (Reformat)
[08/10/2023-11:16:27] [V] [TRT] Tactic: 0x00000000000003e8 Time: 1.60445
[08/10/2023-11:16:27] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.458862
[08/10/2023-11:16:27] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.4384
[08/10/2023-11:16:27] [V] [TRT] Fastest Tactic: 0x0000000000000000 Time: 0.4384
[08/10/2023-11:16:27] [V] [TRT] *************** Autotuning Reformat: Half(8294400,32400:2,180,1) -> Half(1036800,1:16,5760,32) ***************
[08/10/2023-11:16:27] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(<in> -> onnx::Concat_602) (Reformat)
[08/10/2023-11:16:27] [V] [TRT] Tactic: 0x00000000000003e8 Time: 1.64536
[08/10/2023-11:16:27] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.457536
[08/10/2023-11:16:27] [V] [TRT] Tactic: 0x0000000000000000 Time: 1.61114
[08/10/2023-11:16:27] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 0.457536
[08/10/2023-11:16:27] [V] [TRT] *************** Autotuning Reformat: Half(2073600,1:8,11520,64) -> Float(16588800,32400,180,1) ***************
[08/10/2023-11:16:27] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(<in> -> onnx::Concat_602) (Reformat)
[08/10/2023-11:16:27] [V] [TRT] Tactic: 0x00000000000003e8 Time: 1.35317
[08/10/2023-11:16:27] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.489344
[08/10/2023-11:16:27] [V] [TRT] Tactic: 0x0000000000000000 Time: 1.30735
[08/10/2023-11:16:27] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 0.489344
[08/10/2023-11:16:27] [V] [TRT] *************** Autotuning Reformat: Half(2073600,1:8,11520,64) -> Float(16588800,1,92160,512) ***************
[08/10/2023-11:16:27] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(<in> -> onnx::Concat_602) (Reformat)
[08/10/2023-11:16:27] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.809307
[08/10/2023-11:16:27] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.388288
[08/10/2023-11:16:27] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.809216
[08/10/2023-11:16:27] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 0.388288
[08/10/2023-11:16:27] [V] [TRT] *************** Autotuning Reformat: Half(2073600,1:8,11520,64) -> Float(4147200,1:4,23040,128) ***************
[08/10/2023-11:16:27] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(<in> -> onnx::Concat_602) (Reformat)
[08/10/2023-11:16:27] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.958098
[08/10/2023-11:16:27] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.388608
[08/10/2023-11:16:27] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.95824
[08/10/2023-11:16:27] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 0.388608
[08/10/2023-11:16:27] [V] [TRT] *************** Autotuning Reformat: Half(2073600,1:8,11520,64) -> Half(16588800,32400,180,1) ***************
[08/10/2023-11:16:27] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(<in> -> onnx::Concat_602) (Reformat)
[08/10/2023-11:16:27] [V] [TRT] Tactic: 0x00000000000003e8 Time: 1.24454
[08/10/2023-11:16:27] [V] [TRT] Tactic: 0x00000000000003ea Time: 1.96843
[08/10/2023-11:16:27] [V] [TRT] Tactic: 0x0000000000000000 Time: 1.27826
[08/10/2023-11:16:27] [V] [TRT] Fastest Tactic: 0x00000000000003e8 Time: 1.24454
[08/10/2023-11:16:27] [V] [TRT] *************** Autotuning Reformat: Half(2073600,1:8,11520,64) -> Half(8294400,32400:2,180,1) ***************
[08/10/2023-11:16:27] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(<in> -> onnx::Concat_602) (Reformat)
[08/10/2023-11:16:27] [V] [TRT] Tactic: 0x00000000000003e8 Time: 1.37371
[08/10/2023-11:16:27] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.635515
[08/10/2023-11:16:27] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.335803
[08/10/2023-11:16:27] [V] [TRT] Fastest Tactic: 0x0000000000000000 Time: 0.335803
[08/10/2023-11:16:27] [V] [TRT] *************** Autotuning Reformat: Half(2073600,1:8,11520,64) -> Half(1036800,1:16,5760,32) ***************
[08/10/2023-11:16:27] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(<in> -> onnx::Concat_602) (Reformat)
[08/10/2023-11:16:27] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.889499
[08/10/2023-11:16:27] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.388361
[08/10/2023-11:16:27] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.889056
[08/10/2023-11:16:27] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 0.388361
[08/10/2023-11:16:27] [V] [TRT] *************** Autotuning Reformat: Half(1036800,1:16,5760,32) -> Float(16588800,32400,180,1) ***************
[08/10/2023-11:16:27] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(<in> -> onnx::Concat_602) (Reformat)
[08/10/2023-11:16:27] [V] [TRT] Tactic: 0x00000000000003e8 Time: 1.28554
[08/10/2023-11:16:27] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.489943
[08/10/2023-11:16:27] [V] [TRT] Tactic: 0x0000000000000000 Time: 1.3753
[08/10/2023-11:16:27] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 0.489943
[08/10/2023-11:16:27] [V] [TRT] *************** Autotuning Reformat: Half(1036800,1:16,5760,32) -> Float(16588800,1,92160,512) ***************
[08/10/2023-11:16:27] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(<in> -> onnx::Concat_602) (Reformat)
[08/10/2023-11:16:27] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.809559
[08/10/2023-11:16:27] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.388183
[08/10/2023-11:16:27] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.809545
[08/10/2023-11:16:27] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 0.388183
[08/10/2023-11:16:27] [V] [TRT] *************** Autotuning Reformat: Half(1036800,1:16,5760,32) -> Float(4147200,1:4,23040,128) ***************
[08/10/2023-11:16:27] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(<in> -> onnx::Concat_602) (Reformat)
[08/10/2023-11:16:27] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.957522
[08/10/2023-11:16:27] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.388169
[08/10/2023-11:16:27] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.957335
[08/10/2023-11:16:27] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 0.388169
[08/10/2023-11:16:27] [V] [TRT] *************** Autotuning Reformat: Half(1036800,1:16,5760,32) -> Half(16588800,32400,180,1) ***************
[08/10/2023-11:16:27] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(<in> -> onnx::Concat_602) (Reformat)
[08/10/2023-11:16:27] [V] [TRT] Tactic: 0x00000000000003e8 Time: 1.25365
[08/10/2023-11:16:27] [V] [TRT] Tactic: 0x00000000000003ea Time: 1.96674
[08/10/2023-11:16:27] [V] [TRT] Tactic: 0x0000000000000000 Time: 1.27515
[08/10/2023-11:16:27] [V] [TRT] Fastest Tactic: 0x00000000000003e8 Time: 1.25365
[08/10/2023-11:16:27] [V] [TRT] *************** Autotuning Reformat: Half(1036800,1:16,5760,32) -> Half(8294400,32400:2,180,1) ***************
[08/10/2023-11:16:27] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(<in> -> onnx::Concat_602) (Reformat)
[08/10/2023-11:16:27] [V] [TRT] Tactic: 0x00000000000003e8 Time: 1.46801
[08/10/2023-11:16:27] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.631685
[08/10/2023-11:16:28] [V] [TRT] Tactic: 0x0000000000000000 Time: 1.32833
[08/10/2023-11:16:28] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 0.631685
[08/10/2023-11:16:28] [V] [TRT] *************** Autotuning Reformat: Half(1036800,1:16,5760,32) -> Half(2073600,1:8,11520,64) ***************
[08/10/2023-11:16:28] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(<in> -> onnx::Concat_602) (Reformat)
[08/10/2023-11:16:28] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.888302
[08/10/2023-11:16:28] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.388526
[08/10/2023-11:16:28] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.887301
[08/10/2023-11:16:28] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 0.388526
[08/10/2023-11:16:28] [V] [TRT] =============== Computing reformatting costs
[08/10/2023-11:16:28] [V] [TRT] *************** Autotuning Reformat: Float(4147200,32400,180,1) -> Float(4147200,1,23040,128) ***************
[08/10/2023-11:16:28] [V] [TRT] *************** Autotuning Reformat: Float(4147200,32400,180,1) -> Float(1036800,1:4,5760,32) ***************
[08/10/2023-11:16:28] [V] [TRT] *************** Autotuning Reformat: Float(4147200,32400,180,1) -> Half(4147200,32400,180,1) ***************
[08/10/2023-11:16:28] [V] [TRT] *************** Autotuning Reformat: Float(4147200,32400,180,1) -> Half(2073600,32400:2,180,1) ***************
[08/10/2023-11:16:28] [V] [TRT] *************** Autotuning Reformat: Float(4147200,32400,180,1) -> Half(518400,1:8,2880,16) ***************
[08/10/2023-11:16:28] [V] [TRT] *************** Autotuning Reformat: Float(4147200,32400,180,1) -> Half(259200,1:16,1440,8) ***************
[08/10/2023-11:16:28] [V] [TRT] *************** Autotuning Reformat: Float(4147200,1,23040,128) -> Float(4147200,32400,180,1) ***************
[08/10/2023-11:16:28] [V] [TRT] *************** Autotuning Reformat: Float(4147200,1,23040,128) -> Float(1036800,1:4,5760,32) ***************
[08/10/2023-11:16:28] [V] [TRT] *************** Autotuning Reformat: Float(4147200,1,23040,128) -> Half(4147200,32400,180,1) ***************
[08/10/2023-11:16:28] [V] [TRT] *************** Autotuning Reformat: Float(4147200,1,23040,128) -> Half(2073600,32400:2,180,1) ***************
[08/10/2023-11:16:28] [V] [TRT] *************** Autotuning Reformat: Float(4147200,1,23040,128) -> Half(518400,1:8,2880,16) ***************
[08/10/2023-11:16:28] [V] [TRT] *************** Autotuning Reformat: Float(4147200,1,23040,128) -> Half(259200,1:16,1440,8) ***************
[08/10/2023-11:16:28] [V] [TRT] *************** Autotuning Reformat: Float(1036800,1:4,5760,32) -> Float(4147200,32400,180,1) ***************
[08/10/2023-11:16:28] [V] [TRT] *************** Autotuning Reformat: Float(1036800,1:4,5760,32) -> Float(4147200,1,23040,128) ***************
[08/10/2023-11:16:28] [V] [TRT] *************** Autotuning Reformat: Float(1036800,1:4,5760,32) -> Half(4147200,32400,180,1) ***************
[08/10/2023-11:16:28] [V] [TRT] *************** Autotuning Reformat: Float(1036800,1:4,5760,32) -> Half(2073600,32400:2,180,1) ***************
[08/10/2023-11:16:28] [V] [TRT] *************** Autotuning Reformat: Float(1036800,1:4,5760,32) -> Half(518400,1:8,2880,16) ***************
[08/10/2023-11:16:28] [V] [TRT] *************** Autotuning Reformat: Float(1036800,1:4,5760,32) -> Half(259200,1:16,1440,8) ***************
[08/10/2023-11:16:28] [V] [TRT] *************** Autotuning Reformat: Half(4147200,32400,180,1) -> Float(4147200,32400,180,1) ***************
[08/10/2023-11:16:28] [V] [TRT] *************** Autotuning Reformat: Half(4147200,32400,180,1) -> Float(4147200,1,23040,128) ***************
[08/10/2023-11:16:28] [V] [TRT] *************** Autotuning Reformat: Half(4147200,32400,180,1) -> Float(1036800,1:4,5760,32) ***************
[08/10/2023-11:16:28] [V] [TRT] *************** Autotuning Reformat: Half(4147200,32400,180,1) -> Half(2073600,32400:2,180,1) ***************
[08/10/2023-11:16:28] [V] [TRT] *************** Autotuning Reformat: Half(4147200,32400,180,1) -> Half(518400,1:8,2880,16) ***************
[08/10/2023-11:16:28] [V] [TRT] *************** Autotuning Reformat: Half(4147200,32400,180,1) -> Half(259200,1:16,1440,8) ***************
[08/10/2023-11:16:28] [V] [TRT] *************** Autotuning Reformat: Half(2073600,32400:2,180,1) -> Float(4147200,32400,180,1) ***************
[08/10/2023-11:16:28] [V] [TRT] *************** Autotuning Reformat: Half(2073600,32400:2,180,1) -> Float(4147200,1,23040,128) ***************
[08/10/2023-11:16:28] [V] [TRT] *************** Autotuning Reformat: Half(2073600,32400:2,180,1) -> Float(1036800,1:4,5760,32) ***************
[08/10/2023-11:16:28] [V] [TRT] *************** Autotuning Reformat: Half(2073600,32400:2,180,1) -> Half(4147200,32400,180,1) ***************
[08/10/2023-11:16:28] [V] [TRT] *************** Autotuning Reformat: Half(2073600,32400:2,180,1) -> Half(518400,1:8,2880,16) ***************
[08/10/2023-11:16:28] [V] [TRT] *************** Autotuning Reformat: Half(2073600,32400:2,180,1) -> Half(259200,1:16,1440,8) ***************
[08/10/2023-11:16:28] [V] [TRT] *************** Autotuning Reformat: Half(518400,1:8,2880,16) -> Float(4147200,32400,180,1) ***************
[08/10/2023-11:16:28] [V] [TRT] *************** Autotuning Reformat: Half(518400,1:8,2880,16) -> Float(4147200,1,23040,128) ***************
[08/10/2023-11:16:28] [V] [TRT] *************** Autotuning Reformat: Half(518400,1:8,2880,16) -> Float(1036800,1:4,5760,32) ***************
[08/10/2023-11:16:28] [V] [TRT] *************** Autotuning Reformat: Half(518400,1:8,2880,16) -> Half(4147200,32400,180,1) ***************
[08/10/2023-11:16:28] [V] [TRT] *************** Autotuning Reformat: Half(518400,1:8,2880,16) -> Half(2073600,32400:2,180,1) ***************
[08/10/2023-11:16:28] [V] [TRT] *************** Autotuning Reformat: Half(518400,1:8,2880,16) -> Half(259200,1:16,1440,8) ***************
[08/10/2023-11:16:28] [V] [TRT] *************** Autotuning Reformat: Half(259200,1:16,1440,8) -> Float(4147200,32400,180,1) ***************
[08/10/2023-11:16:28] [V] [TRT] *************** Autotuning Reformat: Half(259200,1:16,1440,8) -> Float(4147200,1,23040,128) ***************
[08/10/2023-11:16:28] [V] [TRT] *************** Autotuning Reformat: Half(259200,1:16,1440,8) -> Float(1036800,1:4,5760,32) ***************
[08/10/2023-11:16:28] [V] [TRT] *************** Autotuning Reformat: Half(259200,1:16,1440,8) -> Half(4147200,32400,180,1) ***************
[08/10/2023-11:16:28] [V] [TRT] *************** Autotuning Reformat: Half(259200,1:16,1440,8) -> Half(2073600,32400:2,180,1) ***************
[08/10/2023-11:16:28] [V] [TRT] *************** Autotuning Reformat: Half(259200,1:16,1440,8) -> Half(518400,1:8,2880,16) ***************
[08/10/2023-11:16:28] [V] [TRT] =============== Computing reformatting costs
[08/10/2023-11:16:28] [V] [TRT] *************** Autotuning Reformat: Float(2073600,8100,90,1) -> Float(2073600,1,23040,256) ***************
[08/10/2023-11:16:28] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(input.96 -> <out>) (Reformat)
[08/10/2023-11:16:28] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.710825
[08/10/2023-11:16:28] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.128603
[08/10/2023-11:16:28] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.72128
[08/10/2023-11:16:28] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 0.128603
[08/10/2023-11:16:28] [V] [TRT] *************** Autotuning Reformat: Float(2073600,8100,90,1) -> Float(518400,1:4,5760,64) ***************
[08/10/2023-11:16:28] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(input.96 -> <out>) (Reformat)
[08/10/2023-11:16:28] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.671995
[08/10/2023-11:16:28] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.128341
[08/10/2023-11:16:28] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.649147
[08/10/2023-11:16:28] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 0.128341
[08/10/2023-11:16:28] [V] [TRT] *************** Autotuning Reformat: Float(2073600,8100,90,1) -> Half(2073600,8100,90,1) ***************
[08/10/2023-11:16:28] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(input.96 -> <out>) (Reformat)
[08/10/2023-11:16:28] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.130167
[08/10/2023-11:16:28] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.159493
[08/10/2023-11:16:28] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.222098
[08/10/2023-11:16:28] [V] [TRT] Fastest Tactic: 0x00000000000003e8 Time: 0.130167
[08/10/2023-11:16:28] [V] [TRT] *************** Autotuning Reformat: Float(2073600,8100,90,1) -> Half(1036800,8100:2,90,1) ***************
[08/10/2023-11:16:28] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(input.96 -> <out>) (Reformat)
[08/10/2023-11:16:28] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.197655
[08/10/2023-11:16:28] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.185385
[08/10/2023-11:16:28] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.136263
[08/10/2023-11:16:28] [V] [TRT] Fastest Tactic: 0x0000000000000000 Time: 0.136263
[08/10/2023-11:16:28] [V] [TRT] *************** Autotuning Reformat: Float(2073600,8100,90,1) -> Half(259200,1:8,2880,32) ***************
[08/10/2023-11:16:28] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(input.96 -> <out>) (Reformat)
[08/10/2023-11:16:28] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.617294
[08/10/2023-11:16:28] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.116768
[08/10/2023-11:16:28] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.140843
[08/10/2023-11:16:28] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 0.116768
[08/10/2023-11:16:28] [V] [TRT] *************** Autotuning Reformat: Float(2073600,8100,90,1) -> Half(129600,1:16,1440,16) ***************
[08/10/2023-11:16:28] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(input.96 -> <out>) (Reformat)
[08/10/2023-11:16:28] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.621664
[08/10/2023-11:16:28] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.116729
[08/10/2023-11:16:28] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.625262
[08/10/2023-11:16:28] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 0.116729
[08/10/2023-11:16:28] [V] [TRT] *************** Autotuning Reformat: Float(2073600,1,23040,256) -> Float(2073600,8100,90,1) ***************
[08/10/2023-11:16:28] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(input.96 -> <out>) (Reformat)
[08/10/2023-11:16:28] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.306619
[08/10/2023-11:16:28] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.158299
[08/10/2023-11:16:28] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.312457
[08/10/2023-11:16:28] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 0.158299
[08/10/2023-11:16:28] [V] [TRT] *************** Autotuning Reformat: Float(2073600,1,23040,256) -> Float(518400,1:4,5760,64) ***************
[08/10/2023-11:16:28] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(input.96 -> <out>) (Reformat)
[08/10/2023-11:16:28] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.211351
[08/10/2023-11:16:28] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.116117
[08/10/2023-11:16:28] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.211378
[08/10/2023-11:16:28] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 0.116117
[08/10/2023-11:16:28] [V] [TRT] *************** Autotuning Reformat: Float(2073600,1,23040,256) -> Half(2073600,8100,90,1) ***************
[08/10/2023-11:16:28] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(input.96 -> <out>) (Reformat)
[08/10/2023-11:16:28] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.306286
[08/10/2023-11:16:28] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.179058
[08/10/2023-11:16:28] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.307086
[08/10/2023-11:16:28] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 0.179058
[08/10/2023-11:16:28] [V] [TRT] *************** Autotuning Reformat: Float(2073600,1,23040,256) -> Half(1036800,8100:2,90,1) ***************
[08/10/2023-11:16:28] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(input.96 -> <out>) (Reformat)
[08/10/2023-11:16:28] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.312407
[08/10/2023-11:16:28] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.185449
[08/10/2023-11:16:28] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.309792
[08/10/2023-11:16:28] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 0.185449
[08/10/2023-11:16:28] [V] [TRT] *************** Autotuning Reformat: Float(2073600,1,23040,256) -> Half(259200,1:8,2880,32) ***************
[08/10/2023-11:16:28] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(input.96 -> <out>) (Reformat)
[08/10/2023-11:16:28] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.21179
[08/10/2023-11:16:28] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.10725
[08/10/2023-11:16:28] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.210802
[08/10/2023-11:16:28] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 0.10725
[08/10/2023-11:16:28] [V] [TRT] *************** Autotuning Reformat: Float(2073600,1,23040,256) -> Half(129600,1:16,1440,16) ***************
[08/10/2023-11:16:28] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(input.96 -> <out>) (Reformat)
[08/10/2023-11:16:28] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.207717
[08/10/2023-11:16:28] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.104453
[08/10/2023-11:16:28] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.207707
[08/10/2023-11:16:28] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 0.104453
[08/10/2023-11:16:28] [V] [TRT] *************** Autotuning Reformat: Float(518400,1:4,5760,64) -> Float(2073600,8100,90,1) ***************
[08/10/2023-11:16:28] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(input.96 -> <out>) (Reformat)
[08/10/2023-11:16:28] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.308494
[08/10/2023-11:16:28] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.154555
[08/10/2023-11:16:28] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.307758
[08/10/2023-11:16:28] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 0.154555
[08/10/2023-11:16:28] [V] [TRT] *************** Autotuning Reformat: Float(518400,1:4,5760,64) -> Float(2073600,1,23040,256) ***************
[08/10/2023-11:16:28] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(input.96 -> <out>) (Reformat)
[08/10/2023-11:16:28] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.206432
[08/10/2023-11:16:28] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.11491
[08/10/2023-11:16:28] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.205353
[08/10/2023-11:16:28] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 0.11491
[08/10/2023-11:16:28] [V] [TRT] *************** Autotuning Reformat: Float(518400,1:4,5760,64) -> Half(2073600,8100,90,1) ***************
[08/10/2023-11:16:28] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(input.96 -> <out>) (Reformat)
[08/10/2023-11:16:28] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.301358
[08/10/2023-11:16:28] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.17451
[08/10/2023-11:16:28] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.300713
[08/10/2023-11:16:28] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 0.17451
[08/10/2023-11:16:28] [V] [TRT] *************** Autotuning Reformat: Float(518400,1:4,5760,64) -> Half(1036800,8100:2,90,1) ***************
[08/10/2023-11:16:28] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(input.96 -> <out>) (Reformat)
[08/10/2023-11:16:28] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.308443
[08/10/2023-11:16:28] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.185847
[08/10/2023-11:16:28] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.308105
[08/10/2023-11:16:28] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 0.185847
[08/10/2023-11:16:28] [V] [TRT] *************** Autotuning Reformat: Float(518400,1:4,5760,64) -> Half(259200,1:8,2880,32) ***************
[08/10/2023-11:16:28] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(input.96 -> <out>) (Reformat)
[08/10/2023-11:16:28] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.245568
[08/10/2023-11:16:28] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.104743
[08/10/2023-11:16:28] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.245673
[08/10/2023-11:16:28] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 0.104743
[08/10/2023-11:16:28] [V] [TRT] *************** Autotuning Reformat: Float(518400,1:4,5760,64) -> Half(129600,1:16,1440,16) ***************
[08/10/2023-11:16:28] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(input.96 -> <out>) (Reformat)
[08/10/2023-11:16:28] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.245243
[08/10/2023-11:16:28] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.104469
[08/10/2023-11:16:28] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.245125
[08/10/2023-11:16:28] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 0.104469
[08/10/2023-11:16:28] [V] [TRT] *************** Autotuning Reformat: Half(2073600,8100,90,1) -> Float(2073600,8100,90,1) ***************
[08/10/2023-11:16:28] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(input.96 -> <out>) (Reformat)
[08/10/2023-11:16:28] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.138901
[08/10/2023-11:16:28] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.159237
[08/10/2023-11:16:28] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.201623
[08/10/2023-11:16:28] [V] [TRT] Fastest Tactic: 0x00000000000003e8 Time: 0.138901
[08/10/2023-11:16:28] [V] [TRT] *************** Autotuning Reformat: Half(2073600,8100,90,1) -> Float(2073600,1,23040,256) ***************
[08/10/2023-11:16:28] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(input.96 -> <out>) (Reformat)
[08/10/2023-11:16:28] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.361481
[08/10/2023-11:16:28] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.118866
[08/10/2023-11:16:28] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.357239
[08/10/2023-11:16:28] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 0.118866
[08/10/2023-11:16:28] [V] [TRT] *************** Autotuning Reformat: Half(2073600,8100,90,1) -> Float(518400,1:4,5760,64) ***************
[08/10/2023-11:16:28] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(input.96 -> <out>) (Reformat)
[08/10/2023-11:16:28] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.380059
[08/10/2023-11:16:28] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.118341
[08/10/2023-11:16:28] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.382286
[08/10/2023-11:16:28] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 0.118341
[08/10/2023-11:16:28] [V] [TRT] *************** Autotuning Reformat: Half(2073600,8100,90,1) -> Half(1036800,8100:2,90,1) ***************
[08/10/2023-11:16:28] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(input.96 -> <out>) (Reformat)
[08/10/2023-11:16:28] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.184078
[08/10/2023-11:16:28] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.108352
[08/10/2023-11:16:28] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.130839
[08/10/2023-11:16:28] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 0.108352
[08/10/2023-11:16:28] [V] [TRT] *************** Autotuning Reformat: Half(2073600,8100,90,1) -> Half(259200,1:8,2880,32) ***************
[08/10/2023-11:16:28] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(input.96 -> <out>) (Reformat)
[08/10/2023-11:16:28] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.381559
[08/10/2023-11:16:28] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.0939931
[08/10/2023-11:16:28] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.133771
[08/10/2023-11:16:28] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 0.0939931
[08/10/2023-11:16:28] [V] [TRT] *************** Autotuning Reformat: Half(2073600,8100,90,1) -> Half(129600,1:16,1440,16) ***************
[08/10/2023-11:16:28] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(input.96 -> <out>) (Reformat)
[08/10/2023-11:16:28] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.382734
[08/10/2023-11:16:28] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.0938331
[08/10/2023-11:16:28] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.383803
[08/10/2023-11:16:28] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 0.0938331
[08/10/2023-11:16:28] [V] [TRT] *************** Autotuning Reformat: Half(1036800,8100:2,90,1) -> Float(2073600,8100,90,1) ***************
[08/10/2023-11:16:28] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(input.96 -> <out>) (Reformat)
[08/10/2023-11:16:28] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.197938
[08/10/2023-11:16:28] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.183255
[08/10/2023-11:16:28] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.131266
[08/10/2023-11:16:28] [V] [TRT] Fastest Tactic: 0x0000000000000000 Time: 0.131266
[08/10/2023-11:16:28] [V] [TRT] *************** Autotuning Reformat: Half(1036800,8100:2,90,1) -> Float(2073600,1,23040,256) ***************
[08/10/2023-11:16:28] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(input.96 -> <out>) (Reformat)
[08/10/2023-11:16:28] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.379945
[08/10/2023-11:16:28] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.120311
[08/10/2023-11:16:28] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.377842
[08/10/2023-11:16:28] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 0.120311
[08/10/2023-11:16:28] [V] [TRT] *************** Autotuning Reformat: Half(1036800,8100:2,90,1) -> Float(518400,1:4,5760,64) ***************
[08/10/2023-11:16:28] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(input.96 -> <out>) (Reformat)
[08/10/2023-11:16:28] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.41312
[08/10/2023-11:16:28] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.119685
[08/10/2023-11:16:28] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.418098
[08/10/2023-11:16:28] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 0.119685
[08/10/2023-11:16:28] [V] [TRT] *************** Autotuning Reformat: Half(1036800,8100:2,90,1) -> Half(2073600,8100,90,1) ***************
[08/10/2023-11:16:28] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(input.96 -> <out>) (Reformat)
[08/10/2023-11:16:28] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.184782
[08/10/2023-11:16:28] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.492183
[08/10/2023-11:16:28] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.120786
[08/10/2023-11:16:28] [V] [TRT] Fastest Tactic: 0x0000000000000000 Time: 0.120786
[08/10/2023-11:16:28] [V] [TRT] *************** Autotuning Reformat: Half(1036800,8100:2,90,1) -> Half(259200,1:8,2880,32) ***************
[08/10/2023-11:16:28] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(input.96 -> <out>) (Reformat)
[08/10/2023-11:16:28] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.417367
[08/10/2023-11:16:28] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.121513
[08/10/2023-11:16:28] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.116731
[08/10/2023-11:16:28] [V] [TRT] Fastest Tactic: 0x0000000000000000 Time: 0.116731
[08/10/2023-11:16:28] [V] [TRT] *************** Autotuning Reformat: Half(1036800,8100:2,90,1) -> Half(129600,1:16,1440,16) ***************
[08/10/2023-11:16:28] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(input.96 -> <out>) (Reformat)
[08/10/2023-11:16:28] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.416146
[08/10/2023-11:16:28] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.121403
[08/10/2023-11:16:28] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.416146
[08/10/2023-11:16:28] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 0.121403
[08/10/2023-11:16:28] [V] [TRT] *************** Autotuning Reformat: Half(259200,1:8,2880,32) -> Float(2073600,8100,90,1) ***************
[08/10/2023-11:16:28] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(input.96 -> <out>) (Reformat)
[08/10/2023-11:16:28] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.283081
[08/10/2023-11:16:28] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.170661
[08/10/2023-11:16:28] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.117557
[08/10/2023-11:16:28] [V] [TRT] Fastest Tactic: 0x0000000000000000 Time: 0.117557
[08/10/2023-11:16:28] [V] [TRT] *************** Autotuning Reformat: Half(259200,1:8,2880,32) -> Float(2073600,1,23040,256) ***************
[08/10/2023-11:16:28] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(input.96 -> <out>) (Reformat)
[08/10/2023-11:16:28] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.208923
[08/10/2023-11:16:28] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.104768
[08/10/2023-11:16:28] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.209207
[08/10/2023-11:16:28] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 0.104768
[08/10/2023-11:16:28] [V] [TRT] *************** Autotuning Reformat: Half(259200,1:8,2880,32) -> Float(518400,1:4,5760,64) ***************
[08/10/2023-11:16:28] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(input.96 -> <out>) (Reformat)
[08/10/2023-11:16:28] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.24683
[08/10/2023-11:16:28] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.105152
[08/10/2023-11:16:28] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.246697
[08/10/2023-11:16:28] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 0.105152
[08/10/2023-11:16:28] [V] [TRT] *************** Autotuning Reformat: Half(259200,1:8,2880,32) -> Half(2073600,8100,90,1) ***************
[08/10/2023-11:16:28] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(input.96 -> <out>) (Reformat)
[08/10/2023-11:16:28] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.277582
[08/10/2023-11:16:28] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.424558
[08/10/2023-11:16:28] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.0967383
[08/10/2023-11:16:28] [V] [TRT] Fastest Tactic: 0x0000000000000000 Time: 0.0967383
[08/10/2023-11:16:28] [V] [TRT] *************** Autotuning Reformat: Half(259200,1:8,2880,32) -> Half(1036800,8100:2,90,1) ***************
[08/10/2023-11:16:28] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(input.96 -> <out>) (Reformat)
[08/10/2023-11:16:28] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.28389
[08/10/2023-11:16:28] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.163931
[08/10/2023-11:16:28] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.0913989
[08/10/2023-11:16:28] [V] [TRT] Fastest Tactic: 0x0000000000000000 Time: 0.0913989
[08/10/2023-11:16:28] [V] [TRT] *************** Autotuning Reformat: Half(259200,1:8,2880,32) -> Half(129600,1:16,1440,16) ***************
[08/10/2023-11:16:28] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(input.96 -> <out>) (Reformat)
[08/10/2023-11:16:28] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.228151
[08/10/2023-11:16:28] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.104311
[08/10/2023-11:16:28] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.227246
[08/10/2023-11:16:28] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 0.104311
[08/10/2023-11:16:28] [V] [TRT] *************** Autotuning Reformat: Half(129600,1:16,1440,16) -> Float(2073600,8100,90,1) ***************
[08/10/2023-11:16:28] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(input.96 -> <out>) (Reformat)
[08/10/2023-11:16:28] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.274917
[08/10/2023-11:16:28] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.165426
[08/10/2023-11:16:28] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.27659
[08/10/2023-11:16:28] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 0.165426
[08/10/2023-11:16:28] [V] [TRT] *************** Autotuning Reformat: Half(129600,1:16,1440,16) -> Float(2073600,1,23040,256) ***************
[08/10/2023-11:16:28] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(input.96 -> <out>) (Reformat)
[08/10/2023-11:16:28] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.208923
[08/10/2023-11:16:28] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.104944
[08/10/2023-11:16:28] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.20875
[08/10/2023-11:16:28] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 0.104944
[08/10/2023-11:16:28] [V] [TRT] *************** Autotuning Reformat: Half(129600,1:16,1440,16) -> Float(518400,1:4,5760,64) ***************
[08/10/2023-11:16:28] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(input.96 -> <out>) (Reformat)
[08/10/2023-11:16:28] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.245024
[08/10/2023-11:16:28] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.105083
[08/10/2023-11:16:28] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.245266
[08/10/2023-11:16:28] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 0.105083
[08/10/2023-11:16:28] [V] [TRT] *************** Autotuning Reformat: Half(129600,1:16,1440,16) -> Half(2073600,8100,90,1) ***************
[08/10/2023-11:16:28] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(input.96 -> <out>) (Reformat)
[08/10/2023-11:16:28] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.276046
[08/10/2023-11:16:28] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.419369
[08/10/2023-11:16:28] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.276942
[08/10/2023-11:16:28] [V] [TRT] Fastest Tactic: 0x00000000000003e8 Time: 0.276046
[08/10/2023-11:16:28] [V] [TRT] *************** Autotuning Reformat: Half(129600,1:16,1440,16) -> Half(1036800,8100:2,90,1) ***************
[08/10/2023-11:16:28] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(input.96 -> <out>) (Reformat)
[08/10/2023-11:16:28] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.283227
[08/10/2023-11:16:28] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.167392
[08/10/2023-11:16:28] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.282473
[08/10/2023-11:16:28] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 0.167392
[08/10/2023-11:16:28] [V] [TRT] *************** Autotuning Reformat: Half(129600,1:16,1440,16) -> Half(259200,1:8,2880,32) ***************
[08/10/2023-11:16:28] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(input.96 -> <out>) (Reformat)
[08/10/2023-11:16:28] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.228101
[08/10/2023-11:16:28] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.103662
[08/10/2023-11:16:28] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.227447
[08/10/2023-11:16:28] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 0.103662
[08/10/2023-11:16:28] [V] [TRT] =============== Computing reformatting costs
[08/10/2023-11:16:28] [V] [TRT] *************** Autotuning Reformat: Float(2073600,8100,90,1) -> Float(2073600,1,23040,256) ***************
[08/10/2023-11:16:28] [V] [TRT] *************** Autotuning Reformat: Float(2073600,8100,90,1) -> Float(518400,1:4,5760,64) ***************
[08/10/2023-11:16:28] [V] [TRT] *************** Autotuning Reformat: Float(2073600,8100,90,1) -> Half(2073600,8100,90,1) ***************
[08/10/2023-11:16:28] [V] [TRT] *************** Autotuning Reformat: Float(2073600,8100,90,1) -> Half(1036800,8100:2,90,1) ***************
[08/10/2023-11:16:28] [V] [TRT] *************** Autotuning Reformat: Float(2073600,8100,90,1) -> Half(259200,1:8,2880,32) ***************
[08/10/2023-11:16:28] [V] [TRT] *************** Autotuning Reformat: Float(2073600,8100,90,1) -> Half(129600,1:16,1440,16) ***************
[08/10/2023-11:16:28] [V] [TRT] *************** Autotuning Reformat: Float(2073600,1,23040,256) -> Float(2073600,8100,90,1) ***************
[08/10/2023-11:16:28] [V] [TRT] *************** Autotuning Reformat: Float(2073600,1,23040,256) -> Float(518400,1:4,5760,64) ***************
[08/10/2023-11:16:28] [V] [TRT] *************** Autotuning Reformat: Float(2073600,1,23040,256) -> Half(2073600,8100,90,1) ***************
[08/10/2023-11:16:28] [V] [TRT] *************** Autotuning Reformat: Float(2073600,1,23040,256) -> Half(1036800,8100:2,90,1) ***************
[08/10/2023-11:16:28] [V] [TRT] *************** Autotuning Reformat: Float(2073600,1,23040,256) -> Half(259200,1:8,2880,32) ***************
[08/10/2023-11:16:28] [V] [TRT] *************** Autotuning Reformat: Float(2073600,1,23040,256) -> Half(129600,1:16,1440,16) ***************
[08/10/2023-11:16:28] [V] [TRT] *************** Autotuning Reformat: Float(518400,1:4,5760,64) -> Float(2073600,8100,90,1) ***************
[08/10/2023-11:16:28] [V] [TRT] *************** Autotuning Reformat: Float(518400,1:4,5760,64) -> Float(2073600,1,23040,256) ***************
[08/10/2023-11:16:28] [V] [TRT] *************** Autotuning Reformat: Float(518400,1:4,5760,64) -> Half(2073600,8100,90,1) ***************
[08/10/2023-11:16:28] [V] [TRT] *************** Autotuning Reformat: Float(518400,1:4,5760,64) -> Half(1036800,8100:2,90,1) ***************
[08/10/2023-11:16:28] [V] [TRT] *************** Autotuning Reformat: Float(518400,1:4,5760,64) -> Half(259200,1:8,2880,32) ***************
[08/10/2023-11:16:28] [V] [TRT] *************** Autotuning Reformat: Float(518400,1:4,5760,64) -> Half(129600,1:16,1440,16) ***************
[08/10/2023-11:16:28] [V] [TRT] *************** Autotuning Reformat: Half(2073600,8100,90,1) -> Float(2073600,8100,90,1) ***************
[08/10/2023-11:16:28] [V] [TRT] *************** Autotuning Reformat: Half(2073600,8100,90,1) -> Float(2073600,1,23040,256) ***************
[08/10/2023-11:16:28] [V] [TRT] *************** Autotuning Reformat: Half(2073600,8100,90,1) -> Float(518400,1:4,5760,64) ***************
[08/10/2023-11:16:28] [V] [TRT] *************** Autotuning Reformat: Half(2073600,8100,90,1) -> Half(1036800,8100:2,90,1) ***************
[08/10/2023-11:16:28] [V] [TRT] *************** Autotuning Reformat: Half(2073600,8100,90,1) -> Half(259200,1:8,2880,32) ***************
[08/10/2023-11:16:28] [V] [TRT] *************** Autotuning Reformat: Half(2073600,8100,90,1) -> Half(129600,1:16,1440,16) ***************
[08/10/2023-11:16:28] [V] [TRT] *************** Autotuning Reformat: Half(1036800,8100:2,90,1) -> Float(2073600,8100,90,1) ***************
[08/10/2023-11:16:28] [V] [TRT] *************** Autotuning Reformat: Half(1036800,8100:2,90,1) -> Float(2073600,1,23040,256) ***************
[08/10/2023-11:16:28] [V] [TRT] *************** Autotuning Reformat: Half(1036800,8100:2,90,1) -> Float(518400,1:4,5760,64) ***************
[08/10/2023-11:16:28] [V] [TRT] *************** Autotuning Reformat: Half(1036800,8100:2,90,1) -> Half(2073600,8100,90,1) ***************
[08/10/2023-11:16:28] [V] [TRT] *************** Autotuning Reformat: Half(1036800,8100:2,90,1) -> Half(259200,1:8,2880,32) ***************
[08/10/2023-11:16:28] [V] [TRT] *************** Autotuning Reformat: Half(1036800,8100:2,90,1) -> Half(129600,1:16,1440,16) ***************
[08/10/2023-11:16:28] [V] [TRT] *************** Autotuning Reformat: Half(259200,1:8,2880,32) -> Float(2073600,8100,90,1) ***************
[08/10/2023-11:16:28] [V] [TRT] *************** Autotuning Reformat: Half(259200,1:8,2880,32) -> Float(2073600,1,23040,256) ***************
[08/10/2023-11:16:28] [V] [TRT] *************** Autotuning Reformat: Half(259200,1:8,2880,32) -> Float(518400,1:4,5760,64) ***************
[08/10/2023-11:16:28] [V] [TRT] *************** Autotuning Reformat: Half(259200,1:8,2880,32) -> Half(2073600,8100,90,1) ***************
[08/10/2023-11:16:28] [V] [TRT] *************** Autotuning Reformat: Half(259200,1:8,2880,32) -> Half(1036800,8100:2,90,1) ***************
[08/10/2023-11:16:28] [V] [TRT] *************** Autotuning Reformat: Half(259200,1:8,2880,32) -> Half(129600,1:16,1440,16) ***************
[08/10/2023-11:16:28] [V] [TRT] *************** Autotuning Reformat: Half(129600,1:16,1440,16) -> Float(2073600,8100,90,1) ***************
[08/10/2023-11:16:28] [V] [TRT] *************** Autotuning Reformat: Half(129600,1:16,1440,16) -> Float(2073600,1,23040,256) ***************
[08/10/2023-11:16:28] [V] [TRT] *************** Autotuning Reformat: Half(129600,1:16,1440,16) -> Float(518400,1:4,5760,64) ***************
[08/10/2023-11:16:28] [V] [TRT] *************** Autotuning Reformat: Half(129600,1:16,1440,16) -> Half(2073600,8100,90,1) ***************
[08/10/2023-11:16:28] [V] [TRT] *************** Autotuning Reformat: Half(129600,1:16,1440,16) -> Half(1036800,8100:2,90,1) ***************
[08/10/2023-11:16:28] [V] [TRT] *************** Autotuning Reformat: Half(129600,1:16,1440,16) -> Half(259200,1:8,2880,32) ***************
[08/10/2023-11:16:28] [V] [TRT] =============== Computing reformatting costs
[08/10/2023-11:16:28] [V] [TRT] *************** Autotuning Reformat: Float(2073600,8100,90,1) -> Float(2073600,1,23040,256) ***************
[08/10/2023-11:16:28] [V] [TRT] *************** Autotuning Reformat: Float(2073600,8100,90,1) -> Float(518400,1:4,5760,64) ***************
[08/10/2023-11:16:28] [V] [TRT] *************** Autotuning Reformat: Float(2073600,8100,90,1) -> Half(2073600,8100,90,1) ***************
[08/10/2023-11:16:28] [V] [TRT] *************** Autotuning Reformat: Float(2073600,8100,90,1) -> Half(1036800,8100:2,90,1) ***************
[08/10/2023-11:16:28] [V] [TRT] *************** Autotuning Reformat: Float(2073600,8100,90,1) -> Half(259200,1:8,2880,32) ***************
[08/10/2023-11:16:28] [V] [TRT] *************** Autotuning Reformat: Float(2073600,8100,90,1) -> Half(129600,1:16,1440,16) ***************
[08/10/2023-11:16:28] [V] [TRT] *************** Autotuning Reformat: Float(2073600,1,23040,256) -> Float(2073600,8100,90,1) ***************
[08/10/2023-11:16:28] [V] [TRT] *************** Autotuning Reformat: Float(2073600,1,23040,256) -> Float(518400,1:4,5760,64) ***************
[08/10/2023-11:16:28] [V] [TRT] *************** Autotuning Reformat: Float(2073600,1,23040,256) -> Half(2073600,8100,90,1) ***************
[08/10/2023-11:16:28] [V] [TRT] *************** Autotuning Reformat: Float(2073600,1,23040,256) -> Half(1036800,8100:2,90,1) ***************
[08/10/2023-11:16:28] [V] [TRT] *************** Autotuning Reformat: Float(2073600,1,23040,256) -> Half(259200,1:8,2880,32) ***************
[08/10/2023-11:16:28] [V] [TRT] *************** Autotuning Reformat: Float(2073600,1,23040,256) -> Half(129600,1:16,1440,16) ***************
[08/10/2023-11:16:28] [V] [TRT] *************** Autotuning Reformat: Float(518400,1:4,5760,64) -> Float(2073600,8100,90,1) ***************
[08/10/2023-11:16:28] [V] [TRT] *************** Autotuning Reformat: Float(518400,1:4,5760,64) -> Float(2073600,1,23040,256) ***************
[08/10/2023-11:16:28] [V] [TRT] *************** Autotuning Reformat: Float(518400,1:4,5760,64) -> Half(2073600,8100,90,1) ***************
[08/10/2023-11:16:28] [V] [TRT] *************** Autotuning Reformat: Float(518400,1:4,5760,64) -> Half(1036800,8100:2,90,1) ***************
[08/10/2023-11:16:28] [V] [TRT] *************** Autotuning Reformat: Float(518400,1:4,5760,64) -> Half(259200,1:8,2880,32) ***************
[08/10/2023-11:16:28] [V] [TRT] *************** Autotuning Reformat: Float(518400,1:4,5760,64) -> Half(129600,1:16,1440,16) ***************
[08/10/2023-11:16:28] [V] [TRT] *************** Autotuning Reformat: Half(2073600,8100,90,1) -> Float(2073600,8100,90,1) ***************
[08/10/2023-11:16:28] [V] [TRT] *************** Autotuning Reformat: Half(2073600,8100,90,1) -> Float(2073600,1,23040,256) ***************
[08/10/2023-11:16:28] [V] [TRT] *************** Autotuning Reformat: Half(2073600,8100,90,1) -> Float(518400,1:4,5760,64) ***************
[08/10/2023-11:16:28] [V] [TRT] *************** Autotuning Reformat: Half(2073600,8100,90,1) -> Half(1036800,8100:2,90,1) ***************
[08/10/2023-11:16:28] [V] [TRT] *************** Autotuning Reformat: Half(2073600,8100,90,1) -> Half(259200,1:8,2880,32) ***************
[08/10/2023-11:16:28] [V] [TRT] *************** Autotuning Reformat: Half(2073600,8100,90,1) -> Half(129600,1:16,1440,16) ***************
[08/10/2023-11:16:28] [V] [TRT] *************** Autotuning Reformat: Half(1036800,8100:2,90,1) -> Float(2073600,8100,90,1) ***************
[08/10/2023-11:16:28] [V] [TRT] *************** Autotuning Reformat: Half(1036800,8100:2,90,1) -> Float(2073600,1,23040,256) ***************
[08/10/2023-11:16:28] [V] [TRT] *************** Autotuning Reformat: Half(1036800,8100:2,90,1) -> Float(518400,1:4,5760,64) ***************
[08/10/2023-11:16:28] [V] [TRT] *************** Autotuning Reformat: Half(1036800,8100:2,90,1) -> Half(2073600,8100,90,1) ***************
[08/10/2023-11:16:28] [V] [TRT] *************** Autotuning Reformat: Half(1036800,8100:2,90,1) -> Half(259200,1:8,2880,32) ***************
[08/10/2023-11:16:28] [V] [TRT] *************** Autotuning Reformat: Half(1036800,8100:2,90,1) -> Half(129600,1:16,1440,16) ***************
[08/10/2023-11:16:28] [V] [TRT] *************** Autotuning Reformat: Half(259200,1:8,2880,32) -> Float(2073600,8100,90,1) ***************
[08/10/2023-11:16:28] [V] [TRT] *************** Autotuning Reformat: Half(259200,1:8,2880,32) -> Float(2073600,1,23040,256) ***************
[08/10/2023-11:16:28] [V] [TRT] *************** Autotuning Reformat: Half(259200,1:8,2880,32) -> Float(518400,1:4,5760,64) ***************
[08/10/2023-11:16:28] [V] [TRT] *************** Autotuning Reformat: Half(259200,1:8,2880,32) -> Half(2073600,8100,90,1) ***************
[08/10/2023-11:16:28] [V] [TRT] *************** Autotuning Reformat: Half(259200,1:8,2880,32) -> Half(1036800,8100:2,90,1) ***************
[08/10/2023-11:16:28] [V] [TRT] *************** Autotuning Reformat: Half(259200,1:8,2880,32) -> Half(129600,1:16,1440,16) ***************
[08/10/2023-11:16:28] [V] [TRT] *************** Autotuning Reformat: Half(129600,1:16,1440,16) -> Float(2073600,8100,90,1) ***************
[08/10/2023-11:16:28] [V] [TRT] *************** Autotuning Reformat: Half(129600,1:16,1440,16) -> Float(2073600,1,23040,256) ***************
[08/10/2023-11:16:28] [V] [TRT] *************** Autotuning Reformat: Half(129600,1:16,1440,16) -> Float(518400,1:4,5760,64) ***************
[08/10/2023-11:16:28] [V] [TRT] *************** Autotuning Reformat: Half(129600,1:16,1440,16) -> Half(2073600,8100,90,1) ***************
[08/10/2023-11:16:28] [V] [TRT] *************** Autotuning Reformat: Half(129600,1:16,1440,16) -> Half(1036800,8100:2,90,1) ***************
[08/10/2023-11:16:28] [V] [TRT] *************** Autotuning Reformat: Half(129600,1:16,1440,16) -> Half(259200,1:8,2880,32) ***************
[08/10/2023-11:16:28] [V] [TRT] =============== Computing reformatting costs
[08/10/2023-11:16:28] [V] [TRT] *************** Autotuning Reformat: Float(2073600,8100,90,1) -> Float(2073600,1,23040,256) ***************
[08/10/2023-11:16:28] [V] [TRT] *************** Autotuning Reformat: Float(2073600,8100,90,1) -> Float(518400,1:4,5760,64) ***************
[08/10/2023-11:16:28] [V] [TRT] *************** Autotuning Reformat: Float(2073600,8100,90,1) -> Half(2073600,8100,90,1) ***************
[08/10/2023-11:16:28] [V] [TRT] *************** Autotuning Reformat: Float(2073600,8100,90,1) -> Half(1036800,8100:2,90,1) ***************
[08/10/2023-11:16:28] [V] [TRT] *************** Autotuning Reformat: Float(2073600,8100,90,1) -> Half(259200,1:8,2880,32) ***************
[08/10/2023-11:16:28] [V] [TRT] *************** Autotuning Reformat: Float(2073600,8100,90,1) -> Half(129600,1:16,1440,16) ***************
[08/10/2023-11:16:28] [V] [TRT] *************** Autotuning Reformat: Float(2073600,1,23040,256) -> Float(2073600,8100,90,1) ***************
[08/10/2023-11:16:28] [V] [TRT] *************** Autotuning Reformat: Float(2073600,1,23040,256) -> Float(518400,1:4,5760,64) ***************
[08/10/2023-11:16:28] [V] [TRT] *************** Autotuning Reformat: Float(2073600,1,23040,256) -> Half(2073600,8100,90,1) ***************
[08/10/2023-11:16:28] [V] [TRT] *************** Autotuning Reformat: Float(2073600,1,23040,256) -> Half(1036800,8100:2,90,1) ***************
[08/10/2023-11:16:28] [V] [TRT] *************** Autotuning Reformat: Float(2073600,1,23040,256) -> Half(259200,1:8,2880,32) ***************
[08/10/2023-11:16:28] [V] [TRT] *************** Autotuning Reformat: Float(2073600,1,23040,256) -> Half(129600,1:16,1440,16) ***************
[08/10/2023-11:16:28] [V] [TRT] *************** Autotuning Reformat: Float(518400,1:4,5760,64) -> Float(2073600,8100,90,1) ***************
[08/10/2023-11:16:28] [V] [TRT] *************** Autotuning Reformat: Float(518400,1:4,5760,64) -> Float(2073600,1,23040,256) ***************
[08/10/2023-11:16:28] [V] [TRT] *************** Autotuning Reformat: Float(518400,1:4,5760,64) -> Half(2073600,8100,90,1) ***************
[08/10/2023-11:16:28] [V] [TRT] *************** Autotuning Reformat: Float(518400,1:4,5760,64) -> Half(1036800,8100:2,90,1) ***************
[08/10/2023-11:16:28] [V] [TRT] *************** Autotuning Reformat: Float(518400,1:4,5760,64) -> Half(259200,1:8,2880,32) ***************
[08/10/2023-11:16:28] [V] [TRT] *************** Autotuning Reformat: Float(518400,1:4,5760,64) -> Half(129600,1:16,1440,16) ***************
[08/10/2023-11:16:28] [V] [TRT] *************** Autotuning Reformat: Half(2073600,8100,90,1) -> Float(2073600,8100,90,1) ***************
[08/10/2023-11:16:28] [V] [TRT] *************** Autotuning Reformat: Half(2073600,8100,90,1) -> Float(2073600,1,23040,256) ***************
[08/10/2023-11:16:28] [V] [TRT] *************** Autotuning Reformat: Half(2073600,8100,90,1) -> Float(518400,1:4,5760,64) ***************
[08/10/2023-11:16:28] [V] [TRT] *************** Autotuning Reformat: Half(2073600,8100,90,1) -> Half(1036800,8100:2,90,1) ***************
[08/10/2023-11:16:28] [V] [TRT] *************** Autotuning Reformat: Half(2073600,8100,90,1) -> Half(259200,1:8,2880,32) ***************
[08/10/2023-11:16:28] [V] [TRT] *************** Autotuning Reformat: Half(2073600,8100,90,1) -> Half(129600,1:16,1440,16) ***************
[08/10/2023-11:16:28] [V] [TRT] *************** Autotuning Reformat: Half(1036800,8100:2,90,1) -> Float(2073600,8100,90,1) ***************
[08/10/2023-11:16:28] [V] [TRT] *************** Autotuning Reformat: Half(1036800,8100:2,90,1) -> Float(2073600,1,23040,256) ***************
[08/10/2023-11:16:28] [V] [TRT] *************** Autotuning Reformat: Half(1036800,8100:2,90,1) -> Float(518400,1:4,5760,64) ***************
[08/10/2023-11:16:28] [V] [TRT] *************** Autotuning Reformat: Half(1036800,8100:2,90,1) -> Half(2073600,8100,90,1) ***************
[08/10/2023-11:16:28] [V] [TRT] *************** Autotuning Reformat: Half(1036800,8100:2,90,1) -> Half(259200,1:8,2880,32) ***************
[08/10/2023-11:16:28] [V] [TRT] *************** Autotuning Reformat: Half(1036800,8100:2,90,1) -> Half(129600,1:16,1440,16) ***************
[08/10/2023-11:16:28] [V] [TRT] *************** Autotuning Reformat: Half(259200,1:8,2880,32) -> Float(2073600,8100,90,1) ***************
[08/10/2023-11:16:28] [V] [TRT] *************** Autotuning Reformat: Half(259200,1:8,2880,32) -> Float(2073600,1,23040,256) ***************
[08/10/2023-11:16:28] [V] [TRT] *************** Autotuning Reformat: Half(259200,1:8,2880,32) -> Float(518400,1:4,5760,64) ***************
[08/10/2023-11:16:28] [V] [TRT] *************** Autotuning Reformat: Half(259200,1:8,2880,32) -> Half(2073600,8100,90,1) ***************
[08/10/2023-11:16:28] [V] [TRT] *************** Autotuning Reformat: Half(259200,1:8,2880,32) -> Half(1036800,8100:2,90,1) ***************
[08/10/2023-11:16:28] [V] [TRT] *************** Autotuning Reformat: Half(259200,1:8,2880,32) -> Half(129600,1:16,1440,16) ***************
[08/10/2023-11:16:28] [V] [TRT] *************** Autotuning Reformat: Half(129600,1:16,1440,16) -> Float(2073600,8100,90,1) ***************
[08/10/2023-11:16:28] [V] [TRT] *************** Autotuning Reformat: Half(129600,1:16,1440,16) -> Float(2073600,1,23040,256) ***************
[08/10/2023-11:16:28] [V] [TRT] *************** Autotuning Reformat: Half(129600,1:16,1440,16) -> Float(518400,1:4,5760,64) ***************
[08/10/2023-11:16:28] [V] [TRT] *************** Autotuning Reformat: Half(129600,1:16,1440,16) -> Half(2073600,8100,90,1) ***************
[08/10/2023-11:16:28] [V] [TRT] *************** Autotuning Reformat: Half(129600,1:16,1440,16) -> Half(1036800,8100:2,90,1) ***************
[08/10/2023-11:16:28] [V] [TRT] *************** Autotuning Reformat: Half(129600,1:16,1440,16) -> Half(259200,1:8,2880,32) ***************
[08/10/2023-11:16:28] [V] [TRT] =============== Computing reformatting costs
[08/10/2023-11:16:28] [V] [TRT] *************** Autotuning Reformat: Float(2073600,8100,90,1) -> Float(2073600,1,23040,256) ***************
[08/10/2023-11:16:28] [V] [TRT] *************** Autotuning Reformat: Float(2073600,8100,90,1) -> Float(518400,1:4,5760,64) ***************
[08/10/2023-11:16:28] [V] [TRT] *************** Autotuning Reformat: Float(2073600,8100,90,1) -> Half(2073600,8100,90,1) ***************
[08/10/2023-11:16:28] [V] [TRT] *************** Autotuning Reformat: Float(2073600,8100,90,1) -> Half(1036800,8100:2,90,1) ***************
[08/10/2023-11:16:28] [V] [TRT] *************** Autotuning Reformat: Float(2073600,8100,90,1) -> Half(259200,1:8,2880,32) ***************
[08/10/2023-11:16:28] [V] [TRT] *************** Autotuning Reformat: Float(2073600,8100,90,1) -> Half(129600,1:16,1440,16) ***************
[08/10/2023-11:16:28] [V] [TRT] *************** Autotuning Reformat: Float(2073600,1,23040,256) -> Float(2073600,8100,90,1) ***************
[08/10/2023-11:16:28] [V] [TRT] *************** Autotuning Reformat: Float(2073600,1,23040,256) -> Float(518400,1:4,5760,64) ***************
[08/10/2023-11:16:28] [V] [TRT] *************** Autotuning Reformat: Float(2073600,1,23040,256) -> Half(2073600,8100,90,1) ***************
[08/10/2023-11:16:28] [V] [TRT] *************** Autotuning Reformat: Float(2073600,1,23040,256) -> Half(1036800,8100:2,90,1) ***************
[08/10/2023-11:16:28] [V] [TRT] *************** Autotuning Reformat: Float(2073600,1,23040,256) -> Half(259200,1:8,2880,32) ***************
[08/10/2023-11:16:28] [V] [TRT] *************** Autotuning Reformat: Float(2073600,1,23040,256) -> Half(129600,1:16,1440,16) ***************
[08/10/2023-11:16:28] [V] [TRT] *************** Autotuning Reformat: Float(518400,1:4,5760,64) -> Float(2073600,8100,90,1) ***************
[08/10/2023-11:16:28] [V] [TRT] *************** Autotuning Reformat: Float(518400,1:4,5760,64) -> Float(2073600,1,23040,256) ***************
[08/10/2023-11:16:28] [V] [TRT] *************** Autotuning Reformat: Float(518400,1:4,5760,64) -> Half(2073600,8100,90,1) ***************
[08/10/2023-11:16:28] [V] [TRT] *************** Autotuning Reformat: Float(518400,1:4,5760,64) -> Half(1036800,8100:2,90,1) ***************
[08/10/2023-11:16:28] [V] [TRT] *************** Autotuning Reformat: Float(518400,1:4,5760,64) -> Half(259200,1:8,2880,32) ***************
[08/10/2023-11:16:28] [V] [TRT] *************** Autotuning Reformat: Float(518400,1:4,5760,64) -> Half(129600,1:16,1440,16) ***************
[08/10/2023-11:16:28] [V] [TRT] *************** Autotuning Reformat: Half(2073600,8100,90,1) -> Float(2073600,8100,90,1) ***************
[08/10/2023-11:16:28] [V] [TRT] *************** Autotuning Reformat: Half(2073600,8100,90,1) -> Float(2073600,1,23040,256) ***************
[08/10/2023-11:16:28] [V] [TRT] *************** Autotuning Reformat: Half(2073600,8100,90,1) -> Float(518400,1:4,5760,64) ***************
[08/10/2023-11:16:28] [V] [TRT] *************** Autotuning Reformat: Half(2073600,8100,90,1) -> Half(1036800,8100:2,90,1) ***************
[08/10/2023-11:16:28] [V] [TRT] *************** Autotuning Reformat: Half(2073600,8100,90,1) -> Half(259200,1:8,2880,32) ***************
[08/10/2023-11:16:28] [V] [TRT] *************** Autotuning Reformat: Half(2073600,8100,90,1) -> Half(129600,1:16,1440,16) ***************
[08/10/2023-11:16:28] [V] [TRT] *************** Autotuning Reformat: Half(1036800,8100:2,90,1) -> Float(2073600,8100,90,1) ***************
[08/10/2023-11:16:28] [V] [TRT] *************** Autotuning Reformat: Half(1036800,8100:2,90,1) -> Float(2073600,1,23040,256) ***************
[08/10/2023-11:16:28] [V] [TRT] *************** Autotuning Reformat: Half(1036800,8100:2,90,1) -> Float(518400,1:4,5760,64) ***************
[08/10/2023-11:16:28] [V] [TRT] *************** Autotuning Reformat: Half(1036800,8100:2,90,1) -> Half(2073600,8100,90,1) ***************
[08/10/2023-11:16:28] [V] [TRT] *************** Autotuning Reformat: Half(1036800,8100:2,90,1) -> Half(259200,1:8,2880,32) ***************
[08/10/2023-11:16:28] [V] [TRT] *************** Autotuning Reformat: Half(1036800,8100:2,90,1) -> Half(129600,1:16,1440,16) ***************
[08/10/2023-11:16:28] [V] [TRT] *************** Autotuning Reformat: Half(259200,1:8,2880,32) -> Float(2073600,8100,90,1) ***************
[08/10/2023-11:16:28] [V] [TRT] *************** Autotuning Reformat: Half(259200,1:8,2880,32) -> Float(2073600,1,23040,256) ***************
[08/10/2023-11:16:28] [V] [TRT] *************** Autotuning Reformat: Half(259200,1:8,2880,32) -> Float(518400,1:4,5760,64) ***************
[08/10/2023-11:16:28] [V] [TRT] *************** Autotuning Reformat: Half(259200,1:8,2880,32) -> Half(2073600,8100,90,1) ***************
[08/10/2023-11:16:28] [V] [TRT] *************** Autotuning Reformat: Half(259200,1:8,2880,32) -> Half(1036800,8100:2,90,1) ***************
[08/10/2023-11:16:28] [V] [TRT] *************** Autotuning Reformat: Half(259200,1:8,2880,32) -> Half(129600,1:16,1440,16) ***************
[08/10/2023-11:16:28] [V] [TRT] *************** Autotuning Reformat: Half(129600,1:16,1440,16) -> Float(2073600,8100,90,1) ***************
[08/10/2023-11:16:28] [V] [TRT] *************** Autotuning Reformat: Half(129600,1:16,1440,16) -> Float(2073600,1,23040,256) ***************
[08/10/2023-11:16:28] [V] [TRT] *************** Autotuning Reformat: Half(129600,1:16,1440,16) -> Float(518400,1:4,5760,64) ***************
[08/10/2023-11:16:28] [V] [TRT] *************** Autotuning Reformat: Half(129600,1:16,1440,16) -> Half(2073600,8100,90,1) ***************
[08/10/2023-11:16:28] [V] [TRT] *************** Autotuning Reformat: Half(129600,1:16,1440,16) -> Half(1036800,8100:2,90,1) ***************
[08/10/2023-11:16:28] [V] [TRT] *************** Autotuning Reformat: Half(129600,1:16,1440,16) -> Half(259200,1:8,2880,32) ***************
[08/10/2023-11:16:28] [V] [TRT] =============== Computing reformatting costs
[08/10/2023-11:16:28] [V] [TRT] *************** Autotuning Reformat: Float(2073600,8100,90,1) -> Float(2073600,1,23040,256) ***************
[08/10/2023-11:16:28] [V] [TRT] *************** Autotuning Reformat: Float(2073600,8100,90,1) -> Float(518400,1:4,5760,64) ***************
[08/10/2023-11:16:28] [V] [TRT] *************** Autotuning Reformat: Float(2073600,8100,90,1) -> Half(2073600,8100,90,1) ***************
[08/10/2023-11:16:28] [V] [TRT] *************** Autotuning Reformat: Float(2073600,8100,90,1) -> Half(1036800,8100:2,90,1) ***************
[08/10/2023-11:16:28] [V] [TRT] *************** Autotuning Reformat: Float(2073600,8100,90,1) -> Half(259200,1:8,2880,32) ***************
[08/10/2023-11:16:28] [V] [TRT] *************** Autotuning Reformat: Float(2073600,8100,90,1) -> Half(129600,1:16,1440,16) ***************
[08/10/2023-11:16:28] [V] [TRT] *************** Autotuning Reformat: Float(2073600,1,23040,256) -> Float(2073600,8100,90,1) ***************
[08/10/2023-11:16:28] [V] [TRT] *************** Autotuning Reformat: Float(2073600,1,23040,256) -> Float(518400,1:4,5760,64) ***************
[08/10/2023-11:16:28] [V] [TRT] *************** Autotuning Reformat: Float(2073600,1,23040,256) -> Half(2073600,8100,90,1) ***************
[08/10/2023-11:16:28] [V] [TRT] *************** Autotuning Reformat: Float(2073600,1,23040,256) -> Half(1036800,8100:2,90,1) ***************
[08/10/2023-11:16:28] [V] [TRT] *************** Autotuning Reformat: Float(2073600,1,23040,256) -> Half(259200,1:8,2880,32) ***************
[08/10/2023-11:16:28] [V] [TRT] *************** Autotuning Reformat: Float(2073600,1,23040,256) -> Half(129600,1:16,1440,16) ***************
[08/10/2023-11:16:28] [V] [TRT] *************** Autotuning Reformat: Float(518400,1:4,5760,64) -> Float(2073600,8100,90,1) ***************
[08/10/2023-11:16:28] [V] [TRT] *************** Autotuning Reformat: Float(518400,1:4,5760,64) -> Float(2073600,1,23040,256) ***************
[08/10/2023-11:16:28] [V] [TRT] *************** Autotuning Reformat: Float(518400,1:4,5760,64) -> Half(2073600,8100,90,1) ***************
[08/10/2023-11:16:28] [V] [TRT] *************** Autotuning Reformat: Float(518400,1:4,5760,64) -> Half(1036800,8100:2,90,1) ***************
[08/10/2023-11:16:28] [V] [TRT] *************** Autotuning Reformat: Float(518400,1:4,5760,64) -> Half(259200,1:8,2880,32) ***************
[08/10/2023-11:16:28] [V] [TRT] *************** Autotuning Reformat: Float(518400,1:4,5760,64) -> Half(129600,1:16,1440,16) ***************
[08/10/2023-11:16:28] [V] [TRT] *************** Autotuning Reformat: Half(2073600,8100,90,1) -> Float(2073600,8100,90,1) ***************
[08/10/2023-11:16:28] [V] [TRT] *************** Autotuning Reformat: Half(2073600,8100,90,1) -> Float(2073600,1,23040,256) ***************
[08/10/2023-11:16:28] [V] [TRT] *************** Autotuning Reformat: Half(2073600,8100,90,1) -> Float(518400,1:4,5760,64) ***************
[08/10/2023-11:16:28] [V] [TRT] *************** Autotuning Reformat: Half(2073600,8100,90,1) -> Half(1036800,8100:2,90,1) ***************
[08/10/2023-11:16:28] [V] [TRT] *************** Autotuning Reformat: Half(2073600,8100,90,1) -> Half(259200,1:8,2880,32) ***************
[08/10/2023-11:16:28] [V] [TRT] *************** Autotuning Reformat: Half(2073600,8100,90,1) -> Half(129600,1:16,1440,16) ***************
[08/10/2023-11:16:28] [V] [TRT] *************** Autotuning Reformat: Half(1036800,8100:2,90,1) -> Float(2073600,8100,90,1) ***************
[08/10/2023-11:16:28] [V] [TRT] *************** Autotuning Reformat: Half(1036800,8100:2,90,1) -> Float(2073600,1,23040,256) ***************
[08/10/2023-11:16:28] [V] [TRT] *************** Autotuning Reformat: Half(1036800,8100:2,90,1) -> Float(518400,1:4,5760,64) ***************
[08/10/2023-11:16:28] [V] [TRT] *************** Autotuning Reformat: Half(1036800,8100:2,90,1) -> Half(2073600,8100,90,1) ***************
[08/10/2023-11:16:28] [V] [TRT] *************** Autotuning Reformat: Half(1036800,8100:2,90,1) -> Half(259200,1:8,2880,32) ***************
[08/10/2023-11:16:28] [V] [TRT] *************** Autotuning Reformat: Half(1036800,8100:2,90,1) -> Half(129600,1:16,1440,16) ***************
[08/10/2023-11:16:28] [V] [TRT] *************** Autotuning Reformat: Half(259200,1:8,2880,32) -> Float(2073600,8100,90,1) ***************
[08/10/2023-11:16:28] [V] [TRT] *************** Autotuning Reformat: Half(259200,1:8,2880,32) -> Float(2073600,1,23040,256) ***************
[08/10/2023-11:16:28] [V] [TRT] *************** Autotuning Reformat: Half(259200,1:8,2880,32) -> Float(518400,1:4,5760,64) ***************
[08/10/2023-11:16:28] [V] [TRT] *************** Autotuning Reformat: Half(259200,1:8,2880,32) -> Half(2073600,8100,90,1) ***************
[08/10/2023-11:16:28] [V] [TRT] *************** Autotuning Reformat: Half(259200,1:8,2880,32) -> Half(1036800,8100:2,90,1) ***************
[08/10/2023-11:16:28] [V] [TRT] *************** Autotuning Reformat: Half(259200,1:8,2880,32) -> Half(129600,1:16,1440,16) ***************
[08/10/2023-11:16:28] [V] [TRT] *************** Autotuning Reformat: Half(129600,1:16,1440,16) -> Float(2073600,8100,90,1) ***************
[08/10/2023-11:16:28] [V] [TRT] *************** Autotuning Reformat: Half(129600,1:16,1440,16) -> Float(2073600,1,23040,256) ***************
[08/10/2023-11:16:28] [V] [TRT] *************** Autotuning Reformat: Half(129600,1:16,1440,16) -> Float(518400,1:4,5760,64) ***************
[08/10/2023-11:16:28] [V] [TRT] *************** Autotuning Reformat: Half(129600,1:16,1440,16) -> Half(2073600,8100,90,1) ***************
[08/10/2023-11:16:28] [V] [TRT] *************** Autotuning Reformat: Half(129600,1:16,1440,16) -> Half(1036800,8100:2,90,1) ***************
[08/10/2023-11:16:28] [V] [TRT] *************** Autotuning Reformat: Half(129600,1:16,1440,16) -> Half(259200,1:8,2880,32) ***************
[08/10/2023-11:16:28] [V] [TRT] =============== Computing reformatting costs
[08/10/2023-11:16:28] [V] [TRT] *************** Autotuning Reformat: Float(8294400,32400,180,1) -> Float(16588800,32400,180,1) ***************
[08/10/2023-11:16:28] [V] [TRT] --------------- Timing Runner: onnx::Concat_647 copy (Reformat)
[08/10/2023-11:16:28] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.582336
[08/10/2023-11:16:28] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.765239
[08/10/2023-11:16:28] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.436197
[08/10/2023-11:16:28] [V] [TRT] Fastest Tactic: 0x0000000000000000 Time: 0.436197
[08/10/2023-11:16:28] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x0000000000000000
[08/10/2023-11:16:28] [V] [TRT] *************** Autotuning Reformat: Float(8294400,32400,180,1) -> Float(16588800,1,92160,512) ***************
[08/10/2023-11:16:28] [V] [TRT] --------------- Timing Runner: onnx::Concat_647 copy (Reformat)
[08/10/2023-11:16:28] [V] [TRT] Tactic: 0x00000000000003e8 Time: 2.67784
[08/10/2023-11:16:28] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.474587
[08/10/2023-11:16:28] [V] [TRT] Tactic: 0x0000000000000000 Time: 2.64469
[08/10/2023-11:16:28] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 0.474587
[08/10/2023-11:16:28] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x00000000000003ea
[08/10/2023-11:16:28] [V] [TRT] *************** Autotuning Reformat: Float(8294400,32400,180,1) -> Float(4147200,1:4,23040,128) ***************
[08/10/2023-11:16:28] [V] [TRT] --------------- Timing Runner: onnx::Concat_647 copy (Reformat)
[08/10/2023-11:16:28] [V] [TRT] Tactic: 0x00000000000003e8 Time: 2.53582
[08/10/2023-11:16:28] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.473499
[08/10/2023-11:16:28] [V] [TRT] Tactic: 0x0000000000000000 Time: 2.51024
[08/10/2023-11:16:28] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 0.473499
[08/10/2023-11:16:28] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x00000000000003ea
[08/10/2023-11:16:28] [V] [TRT] *************** Autotuning Reformat: Float(8294400,32400,180,1) -> Half(16588800,32400,180,1) ***************
[08/10/2023-11:16:28] [V] [TRT] --------------- Timing Runner: onnx::Concat_647 copy (Reformat)
[08/10/2023-11:16:28] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.559497
[08/10/2023-11:16:28] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.725408
[08/10/2023-11:16:28] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.558249
[08/10/2023-11:16:28] [V] [TRT] Fastest Tactic: 0x0000000000000000 Time: 0.558249
[08/10/2023-11:16:28] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x0000000000000000
[08/10/2023-11:16:28] [V] [TRT] *************** Autotuning Reformat: Float(8294400,32400,180,1) -> Half(8294400,32400:2,180,1) ***************
[08/10/2023-11:16:28] [V] [TRT] --------------- Timing Runner: onnx::Concat_647 copy (Reformat)
[08/10/2023-11:16:28] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.768462
[08/10/2023-11:16:28] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.705509
[08/10/2023-11:16:28] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.523547
[08/10/2023-11:16:28] [V] [TRT] Fastest Tactic: 0x0000000000000000 Time: 0.523547
[08/10/2023-11:16:28] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x0000000000000000
[08/10/2023-11:16:28] [V] [TRT] *************** Autotuning Reformat: Float(8294400,32400,180,1) -> Half(2073600,1:8,11520,64) ***************
[08/10/2023-11:16:28] [V] [TRT] --------------- Timing Runner: onnx::Concat_647 copy (Reformat)
[08/10/2023-11:16:28] [V] [TRT] Tactic: 0x00000000000003e8 Time: 2.40648
[08/10/2023-11:16:28] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.447278
[08/10/2023-11:16:28] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.531195
[08/10/2023-11:16:28] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 0.447278
[08/10/2023-11:16:28] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x00000000000003ea
[08/10/2023-11:16:28] [V] [TRT] *************** Autotuning Reformat: Float(8294400,32400,180,1) -> Half(1036800,1:16,5760,32) ***************
[08/10/2023-11:16:28] [V] [TRT] --------------- Timing Runner: onnx::Concat_647 copy (Reformat)
[08/10/2023-11:16:28] [V] [TRT] Tactic: 0x00000000000003e8 Time: 2.45718
[08/10/2023-11:16:28] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.448283
[08/10/2023-11:16:28] [V] [TRT] Tactic: 0x0000000000000000 Time: 2.44739
[08/10/2023-11:16:28] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 0.448283
[08/10/2023-11:16:28] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x00000000000003ea
[08/10/2023-11:16:28] [V] [TRT] *************** Autotuning Reformat: Float(8294400,1,46080,256) -> Float(16588800,32400,180,1) ***************
[08/10/2023-11:16:28] [V] [TRT] --------------- Timing Runner: onnx::Concat_647 copy (Reformat)
[08/10/2023-11:16:28] [V] [TRT] Tactic: 0x00000000000003e8 Time: 1.75809
[08/10/2023-11:16:28] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.523995
[08/10/2023-11:16:28] [V] [TRT] Tactic: 0x0000000000000000 Time: 1.70847
[08/10/2023-11:16:28] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 0.523995
[08/10/2023-11:16:28] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x00000000000003ea
[08/10/2023-11:16:28] [V] [TRT] *************** Autotuning Reformat: Float(8294400,1,46080,256) -> Float(16588800,1,92160,512) ***************
[08/10/2023-11:16:28] [V] [TRT] --------------- Timing Runner: onnx::Concat_647 copy (Reformat)
[08/10/2023-11:16:28] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.699744
[08/10/2023-11:16:29] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.658615
[08/10/2023-11:16:29] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.697765
[08/10/2023-11:16:29] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 0.658615
[08/10/2023-11:16:29] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x00000000000003ea
[08/10/2023-11:16:29] [V] [TRT] *************** Autotuning Reformat: Float(8294400,1,46080,256) -> Float(4147200,1:4,23040,128) ***************
[08/10/2023-11:16:29] [V] [TRT] --------------- Timing Runner: onnx::Concat_647 copy (Reformat)
[08/10/2023-11:16:29] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.800942
[08/10/2023-11:16:29] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.415154
[08/10/2023-11:16:29] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.818889
[08/10/2023-11:16:29] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 0.415154
[08/10/2023-11:16:29] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x00000000000003ea
[08/10/2023-11:16:29] [V] [TRT] *************** Autotuning Reformat: Float(8294400,1,46080,256) -> Half(16588800,32400,180,1) ***************
[08/10/2023-11:16:29] [V] [TRT] --------------- Timing Runner: onnx::Concat_647 copy (Reformat)
[08/10/2023-11:16:29] [V] [TRT] Tactic: 0x00000000000003e8 Time: 1.63824
[08/10/2023-11:16:29] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.526715
[08/10/2023-11:16:29] [V] [TRT] Tactic: 0x0000000000000000 Time: 1.67748
[08/10/2023-11:16:29] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 0.526715
[08/10/2023-11:16:29] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x00000000000003ea
[08/10/2023-11:16:29] [V] [TRT] *************** Autotuning Reformat: Float(8294400,1,46080,256) -> Half(8294400,32400:2,180,1) ***************
[08/10/2023-11:16:29] [V] [TRT] --------------- Timing Runner: onnx::Concat_647 copy (Reformat)
[08/10/2023-11:16:29] [V] [TRT] Tactic: 0x00000000000003e8 Time: 1.80626
[08/10/2023-11:16:29] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.701106
[08/10/2023-11:16:29] [V] [TRT] Tactic: 0x0000000000000000 Time: 1.71784
[08/10/2023-11:16:29] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 0.701106
[08/10/2023-11:16:29] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x00000000000003ea
[08/10/2023-11:16:29] [V] [TRT] *************** Autotuning Reformat: Float(8294400,1,46080,256) -> Half(2073600,1:8,11520,64) ***************
[08/10/2023-11:16:29] [V] [TRT] --------------- Timing Runner: onnx::Concat_647 copy (Reformat)
[08/10/2023-11:16:29] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.805371
[08/10/2023-11:16:29] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.387744
[08/10/2023-11:16:29] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.803424
[08/10/2023-11:16:29] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 0.387744
[08/10/2023-11:16:29] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x00000000000003ea
[08/10/2023-11:16:29] [V] [TRT] *************** Autotuning Reformat: Float(8294400,1,46080,256) -> Half(1036800,1:16,5760,32) ***************
[08/10/2023-11:16:29] [V] [TRT] --------------- Timing Runner: onnx::Concat_647 copy (Reformat)
[08/10/2023-11:16:29] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.803918
[08/10/2023-11:16:29] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.386683
[08/10/2023-11:16:29] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.803991
[08/10/2023-11:16:29] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 0.386683
[08/10/2023-11:16:29] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x00000000000003ea
[08/10/2023-11:16:29] [V] [TRT] *************** Autotuning Reformat: Float(2073600,1:4,11520,64) -> Float(16588800,32400,180,1) ***************
[08/10/2023-11:16:29] [V] [TRT] --------------- Timing Runner: onnx::Concat_647 copy (Reformat)
[08/10/2023-11:16:29] [V] [TRT] Tactic: 0x00000000000003e8 Time: 1.8553
[08/10/2023-11:16:29] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.522569
[08/10/2023-11:16:29] [V] [TRT] Tactic: 0x0000000000000000 Time: 1.50202
[08/10/2023-11:16:29] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 0.522569
[08/10/2023-11:16:29] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x00000000000003ea
[08/10/2023-11:16:29] [V] [TRT] *************** Autotuning Reformat: Float(2073600,1:4,11520,64) -> Float(16588800,1,92160,512) ***************
[08/10/2023-11:16:29] [V] [TRT] --------------- Timing Runner: onnx::Concat_647 copy (Reformat)
[08/10/2023-11:16:29] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.815817
[08/10/2023-11:16:29] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.414043
[08/10/2023-11:16:29] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.799497
[08/10/2023-11:16:29] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 0.414043
[08/10/2023-11:16:29] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x00000000000003ea
[08/10/2023-11:16:29] [V] [TRT] *************** Autotuning Reformat: Float(2073600,1:4,11520,64) -> Float(4147200,1:4,23040,128) ***************
[08/10/2023-11:16:29] [V] [TRT] --------------- Timing Runner: onnx::Concat_647 copy (Reformat)
[08/10/2023-11:16:29] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.946203
[08/10/2023-11:16:29] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.408562
[08/10/2023-11:16:29] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.944946
[08/10/2023-11:16:29] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 0.408562
[08/10/2023-11:16:29] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x00000000000003ea
[08/10/2023-11:16:29] [V] [TRT] *************** Autotuning Reformat: Float(2073600,1:4,11520,64) -> Half(16588800,32400,180,1) ***************
[08/10/2023-11:16:29] [V] [TRT] --------------- Timing Runner: onnx::Concat_647 copy (Reformat)
[08/10/2023-11:16:29] [V] [TRT] Tactic: 0x00000000000003e8 Time: 1.61259
[08/10/2023-11:16:29] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.518642
[08/10/2023-11:16:29] [V] [TRT] Tactic: 0x0000000000000000 Time: 1.34869
[08/10/2023-11:16:29] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 0.518642
[08/10/2023-11:16:29] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x00000000000003ea
[08/10/2023-11:16:29] [V] [TRT] *************** Autotuning Reformat: Float(2073600,1:4,11520,64) -> Half(8294400,32400:2,180,1) ***************
[08/10/2023-11:16:29] [V] [TRT] --------------- Timing Runner: onnx::Concat_647 copy (Reformat)
[08/10/2023-11:16:29] [V] [TRT] Tactic: 0x00000000000003e8 Time: 1.93891
[08/10/2023-11:16:29] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.685426
[08/10/2023-11:16:29] [V] [TRT] Tactic: 0x0000000000000000 Time: 1.61804
[08/10/2023-11:16:29] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 0.685426
[08/10/2023-11:16:29] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x00000000000003ea
[08/10/2023-11:16:29] [V] [TRT] *************** Autotuning Reformat: Float(2073600,1:4,11520,64) -> Half(2073600,1:8,11520,64) ***************
[08/10/2023-11:16:29] [V] [TRT] --------------- Timing Runner: onnx::Concat_647 copy (Reformat)
[08/10/2023-11:16:29] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.977042
[08/10/2023-11:16:29] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.39595
[08/10/2023-11:16:29] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.975849
[08/10/2023-11:16:29] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 0.39595
[08/10/2023-11:16:29] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x00000000000003ea
[08/10/2023-11:16:29] [V] [TRT] *************** Autotuning Reformat: Float(2073600,1:4,11520,64) -> Half(1036800,1:16,5760,32) ***************
[08/10/2023-11:16:29] [V] [TRT] --------------- Timing Runner: onnx::Concat_647 copy (Reformat)
[08/10/2023-11:16:29] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.973641
[08/10/2023-11:16:29] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.387378
[08/10/2023-11:16:29] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.955259
[08/10/2023-11:16:29] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 0.387378
[08/10/2023-11:16:29] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x00000000000003ea
[08/10/2023-11:16:29] [V] [TRT] *************** Autotuning Reformat: Half(8294400,32400,180,1) -> Float(16588800,32400,180,1) ***************
[08/10/2023-11:16:29] [V] [TRT] --------------- Timing Runner: onnx::Concat_647 copy (Reformat)
[08/10/2023-11:16:29] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.587511
[08/10/2023-11:16:29] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.726912
[08/10/2023-11:16:29] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.586752
[08/10/2023-11:16:29] [V] [TRT] Fastest Tactic: 0x0000000000000000 Time: 0.586752
[08/10/2023-11:16:29] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x0000000000000000
[08/10/2023-11:16:29] [V] [TRT] *************** Autotuning Reformat: Half(8294400,32400,180,1) -> Float(16588800,1,92160,512) ***************
[08/10/2023-11:16:29] [V] [TRT] --------------- Timing Runner: onnx::Concat_647 copy (Reformat)
[08/10/2023-11:16:29] [V] [TRT] Tactic: 0x00000000000003e8 Time: 1.93121
[08/10/2023-11:16:29] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.441787
[08/10/2023-11:16:29] [V] [TRT] Tactic: 0x0000000000000000 Time: 1.94574
[08/10/2023-11:16:29] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 0.441787
[08/10/2023-11:16:29] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x00000000000003ea
[08/10/2023-11:16:29] [V] [TRT] *************** Autotuning Reformat: Half(8294400,32400,180,1) -> Float(4147200,1:4,23040,128) ***************
[08/10/2023-11:16:29] [V] [TRT] --------------- Timing Runner: onnx::Concat_647 copy (Reformat)
[08/10/2023-11:16:29] [V] [TRT] Tactic: 0x00000000000003e8 Time: 1.94159
[08/10/2023-11:16:29] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.439177
[08/10/2023-11:16:29] [V] [TRT] Tactic: 0x0000000000000000 Time: 1.9393
[08/10/2023-11:16:29] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 0.439177
[08/10/2023-11:16:29] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x00000000000003ea
[08/10/2023-11:16:29] [V] [TRT] *************** Autotuning Reformat: Half(8294400,32400,180,1) -> Half(16588800,32400,180,1) ***************
[08/10/2023-11:16:29] [V] [TRT] --------------- Timing Runner: onnx::Concat_647 copy (Reformat)
[08/10/2023-11:16:29] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.524773
[08/10/2023-11:16:29] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.545033
[08/10/2023-11:16:29] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.221088
[08/10/2023-11:16:29] [V] [TRT] Fastest Tactic: 0x0000000000000000 Time: 0.221088
[08/10/2023-11:16:29] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x0000000000000000
[08/10/2023-11:16:29] [V] [TRT] *************** Autotuning Reformat: Half(8294400,32400,180,1) -> Half(8294400,32400:2,180,1) ***************
[08/10/2023-11:16:29] [V] [TRT] --------------- Timing Runner: onnx::Concat_647 copy (Reformat)
[08/10/2023-11:16:29] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.733947
[08/10/2023-11:16:29] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.408393
[08/10/2023-11:16:29] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.512965
[08/10/2023-11:16:29] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 0.408393
[08/10/2023-11:16:29] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x00000000000003ea
[08/10/2023-11:16:29] [V] [TRT] *************** Autotuning Reformat: Half(8294400,32400,180,1) -> Half(2073600,1:8,11520,64) ***************
[08/10/2023-11:16:29] [V] [TRT] --------------- Timing Runner: onnx::Concat_647 copy (Reformat)
[08/10/2023-11:16:29] [V] [TRT] Tactic: 0x00000000000003e8 Time: 2.02237
[08/10/2023-11:16:29] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.360215
[08/10/2023-11:16:29] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.523328
[08/10/2023-11:16:29] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 0.360215
[08/10/2023-11:16:29] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x00000000000003ea
[08/10/2023-11:16:29] [V] [TRT] *************** Autotuning Reformat: Half(8294400,32400,180,1) -> Half(1036800,1:16,5760,32) ***************
[08/10/2023-11:16:29] [V] [TRT] --------------- Timing Runner: onnx::Concat_647 copy (Reformat)
[08/10/2023-11:16:29] [V] [TRT] Tactic: 0x00000000000003e8 Time: 1.96175
[08/10/2023-11:16:29] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.352846
[08/10/2023-11:16:29] [V] [TRT] Tactic: 0x0000000000000000 Time: 1.95264
[08/10/2023-11:16:29] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 0.352846
[08/10/2023-11:16:29] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x00000000000003ea
[08/10/2023-11:16:29] [V] [TRT] *************** Autotuning Reformat: Half(4147200,32400:2,180,1) -> Float(16588800,32400,180,1) ***************
[08/10/2023-11:16:29] [V] [TRT] --------------- Timing Runner: onnx::Concat_647 copy (Reformat)
[08/10/2023-11:16:29] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.771378
[08/10/2023-11:16:29] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.537061
[08/10/2023-11:16:29] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.772654
[08/10/2023-11:16:29] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 0.537061
[08/10/2023-11:16:29] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x00000000000003ea
[08/10/2023-11:16:29] [V] [TRT] *************** Autotuning Reformat: Half(4147200,32400:2,180,1) -> Float(16588800,1,92160,512) ***************
[08/10/2023-11:16:29] [V] [TRT] --------------- Timing Runner: onnx::Concat_647 copy (Reformat)
[08/10/2023-11:16:29] [V] [TRT] Tactic: 0x00000000000003e8 Time: 1.50413
[08/10/2023-11:16:29] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.449673
[08/10/2023-11:16:29] [V] [TRT] Tactic: 0x0000000000000000 Time: 1.50918
[08/10/2023-11:16:29] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 0.449673
[08/10/2023-11:16:29] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x00000000000003ea
[08/10/2023-11:16:29] [V] [TRT] *************** Autotuning Reformat: Half(4147200,32400:2,180,1) -> Float(4147200,1:4,23040,128) ***************
[08/10/2023-11:16:29] [V] [TRT] --------------- Timing Runner: onnx::Concat_647 copy (Reformat)
[08/10/2023-11:16:29] [V] [TRT] Tactic: 0x00000000000003e8 Time: 1.68131
[08/10/2023-11:16:29] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.461184
[08/10/2023-11:16:29] [V] [TRT] Tactic: 0x0000000000000000 Time: 1.67977
[08/10/2023-11:16:29] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 0.461184
[08/10/2023-11:16:29] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x00000000000003ea
[08/10/2023-11:16:29] [V] [TRT] *************** Autotuning Reformat: Half(4147200,32400:2,180,1) -> Half(16588800,32400,180,1) ***************
[08/10/2023-11:16:29] [V] [TRT] --------------- Timing Runner: onnx::Concat_647 copy (Reformat)
[08/10/2023-11:16:30] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.699067
[08/10/2023-11:16:30] [V] [TRT] Tactic: 0x00000000000003ea Time: 2.7542
[08/10/2023-11:16:30] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.699214
[08/10/2023-11:16:30] [V] [TRT] Fastest Tactic: 0x00000000000003e8 Time: 0.699067
[08/10/2023-11:16:30] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x00000000000003e8
[08/10/2023-11:16:30] [V] [TRT] *************** Autotuning Reformat: Half(4147200,32400:2,180,1) -> Half(8294400,32400:2,180,1) ***************
[08/10/2023-11:16:30] [V] [TRT] --------------- Timing Runner: onnx::Concat_647 copy (Reformat)
[08/10/2023-11:16:30] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.326537
[08/10/2023-11:16:30] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.736773
[08/10/2023-11:16:30] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.220293
[08/10/2023-11:16:30] [V] [TRT] Fastest Tactic: 0x0000000000000000 Time: 0.220293
[08/10/2023-11:16:30] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x0000000000000000
[08/10/2023-11:16:30] [V] [TRT] *************** Autotuning Reformat: Half(4147200,32400:2,180,1) -> Half(2073600,1:8,11520,64) ***************
[08/10/2023-11:16:30] [V] [TRT] --------------- Timing Runner: onnx::Concat_647 copy (Reformat)
[08/10/2023-11:16:30] [V] [TRT] Tactic: 0x00000000000003e8 Time: 1.6111
[08/10/2023-11:16:30] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.447717
[08/10/2023-11:16:30] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.432896
[08/10/2023-11:16:30] [V] [TRT] Fastest Tactic: 0x0000000000000000 Time: 0.432896
[08/10/2023-11:16:30] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x0000000000000000
[08/10/2023-11:16:30] [V] [TRT] *************** Autotuning Reformat: Half(4147200,32400:2,180,1) -> Half(1036800,1:16,5760,32) ***************
[08/10/2023-11:16:30] [V] [TRT] --------------- Timing Runner: onnx::Concat_647 copy (Reformat)
[08/10/2023-11:16:30] [V] [TRT] Tactic: 0x00000000000003e8 Time: 1.60438
[08/10/2023-11:16:30] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.446898
[08/10/2023-11:16:30] [V] [TRT] Tactic: 0x0000000000000000 Time: 1.60272
[08/10/2023-11:16:30] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 0.446898
[08/10/2023-11:16:30] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x00000000000003ea
[08/10/2023-11:16:30] [V] [TRT] *************** Autotuning Reformat: Half(1036800,1:8,5760,32) -> Float(16588800,32400,180,1) ***************
[08/10/2023-11:16:30] [V] [TRT] --------------- Timing Runner: onnx::Concat_647 copy (Reformat)
[08/10/2023-11:16:30] [V] [TRT] Tactic: 0x00000000000003e8 Time: 1.30278
[08/10/2023-11:16:30] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.48773
[08/10/2023-11:16:30] [V] [TRT] Tactic: 0x0000000000000000 Time: 1.33361
[08/10/2023-11:16:30] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 0.48773
[08/10/2023-11:16:30] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x00000000000003ea
[08/10/2023-11:16:30] [V] [TRT] *************** Autotuning Reformat: Half(1036800,1:8,5760,32) -> Float(16588800,1,92160,512) ***************
[08/10/2023-11:16:30] [V] [TRT] --------------- Timing Runner: onnx::Concat_647 copy (Reformat)
[08/10/2023-11:16:30] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.828018
[08/10/2023-11:16:30] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.397079
[08/10/2023-11:16:30] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.827666
[08/10/2023-11:16:30] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 0.397079
[08/10/2023-11:16:30] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x00000000000003ea
[08/10/2023-11:16:30] [V] [TRT] *************** Autotuning Reformat: Half(1036800,1:8,5760,32) -> Float(4147200,1:4,23040,128) ***************
[08/10/2023-11:16:30] [V] [TRT] --------------- Timing Runner: onnx::Concat_647 copy (Reformat)
[08/10/2023-11:16:30] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.969938
[08/10/2023-11:16:30] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.389111
[08/10/2023-11:16:30] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.958985
[08/10/2023-11:16:30] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 0.389111
[08/10/2023-11:16:30] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x00000000000003ea
[08/10/2023-11:16:30] [V] [TRT] *************** Autotuning Reformat: Half(1036800,1:8,5760,32) -> Half(16588800,32400,180,1) ***************
[08/10/2023-11:16:30] [V] [TRT] --------------- Timing Runner: onnx::Concat_647 copy (Reformat)
[08/10/2023-11:16:30] [V] [TRT] Tactic: 0x00000000000003e8 Time: 1.26172
[08/10/2023-11:16:30] [V] [TRT] Tactic: 0x00000000000003ea Time: 1.92032
[08/10/2023-11:16:30] [V] [TRT] Tactic: 0x0000000000000000 Time: 1.24477
[08/10/2023-11:16:30] [V] [TRT] Fastest Tactic: 0x0000000000000000 Time: 1.24477
[08/10/2023-11:16:30] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x0000000000000000
[08/10/2023-11:16:30] [V] [TRT] *************** Autotuning Reformat: Half(1036800,1:8,5760,32) -> Half(8294400,32400:2,180,1) ***************
[08/10/2023-11:16:30] [V] [TRT] --------------- Timing Runner: onnx::Concat_647 copy (Reformat)
[08/10/2023-11:16:30] [V] [TRT] Tactic: 0x00000000000003e8 Time: 1.28464
[08/10/2023-11:16:30] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.629408
[08/10/2023-11:16:30] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.335077
[08/10/2023-11:16:30] [V] [TRT] Fastest Tactic: 0x0000000000000000 Time: 0.335077
[08/10/2023-11:16:30] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x0000000000000000
[08/10/2023-11:16:30] [V] [TRT] *************** Autotuning Reformat: Half(1036800,1:8,5760,32) -> Half(2073600,1:8,11520,64) ***************
[08/10/2023-11:16:30] [V] [TRT] --------------- Timing Runner: onnx::Concat_647 copy (Reformat)
[08/10/2023-11:16:30] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.88752
[08/10/2023-11:16:30] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.387145
[08/10/2023-11:16:30] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.88731
[08/10/2023-11:16:30] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 0.387145
[08/10/2023-11:16:30] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x00000000000003ea
[08/10/2023-11:16:30] [V] [TRT] *************** Autotuning Reformat: Half(1036800,1:8,5760,32) -> Half(1036800,1:16,5760,32) ***************
[08/10/2023-11:16:30] [V] [TRT] --------------- Timing Runner: onnx::Concat_647 copy (Reformat)
[08/10/2023-11:16:30] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.90597
[08/10/2023-11:16:30] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.396581
[08/10/2023-11:16:30] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.907515
[08/10/2023-11:16:30] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 0.396581
[08/10/2023-11:16:30] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x00000000000003ea
[08/10/2023-11:16:30] [V] [TRT] *************** Autotuning Reformat: Half(518400,1:16,2880,16) -> Float(16588800,32400,180,1) ***************
[08/10/2023-11:16:30] [V] [TRT] --------------- Timing Runner: onnx::Concat_647 copy (Reformat)
[08/10/2023-11:16:30] [V] [TRT] Tactic: 0x00000000000003e8 Time: 1.34632
[08/10/2023-11:16:30] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.491351
[08/10/2023-11:16:30] [V] [TRT] Tactic: 0x0000000000000000 Time: 1.31342
[08/10/2023-11:16:30] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 0.491351
[08/10/2023-11:16:30] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x00000000000003ea
[08/10/2023-11:16:30] [V] [TRT] *************** Autotuning Reformat: Half(518400,1:16,2880,16) -> Float(16588800,1,92160,512) ***************
[08/10/2023-11:16:30] [V] [TRT] --------------- Timing Runner: onnx::Concat_647 copy (Reformat)
[08/10/2023-11:16:30] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.811424
[08/10/2023-11:16:30] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.389975
[08/10/2023-11:16:30] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.811013
[08/10/2023-11:16:30] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 0.389975
[08/10/2023-11:16:30] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x00000000000003ea
[08/10/2023-11:16:30] [V] [TRT] *************** Autotuning Reformat: Half(518400,1:16,2880,16) -> Float(4147200,1:4,23040,128) ***************
[08/10/2023-11:16:30] [V] [TRT] --------------- Timing Runner: onnx::Concat_647 copy (Reformat)
[08/10/2023-11:16:30] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.959383
[08/10/2023-11:16:30] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.389454
[08/10/2023-11:16:30] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.95744
[08/10/2023-11:16:30] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 0.389454
[08/10/2023-11:16:30] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x00000000000003ea
[08/10/2023-11:16:30] [V] [TRT] *************** Autotuning Reformat: Half(518400,1:16,2880,16) -> Half(16588800,32400,180,1) ***************
[08/10/2023-11:16:30] [V] [TRT] --------------- Timing Runner: onnx::Concat_647 copy (Reformat)
[08/10/2023-11:16:30] [V] [TRT] Tactic: 0x00000000000003e8 Time: 1.27141
[08/10/2023-11:16:30] [V] [TRT] Tactic: 0x00000000000003ea Time: 1.91794
[08/10/2023-11:16:30] [V] [TRT] Tactic: 0x0000000000000000 Time: 1.25803
[08/10/2023-11:16:30] [V] [TRT] Fastest Tactic: 0x0000000000000000 Time: 1.25803
[08/10/2023-11:16:30] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x0000000000000000
[08/10/2023-11:16:30] [V] [TRT] *************** Autotuning Reformat: Half(518400,1:16,2880,16) -> Half(8294400,32400:2,180,1) ***************
[08/10/2023-11:16:30] [V] [TRT] --------------- Timing Runner: onnx::Concat_647 copy (Reformat)
[08/10/2023-11:16:30] [V] [TRT] Tactic: 0x00000000000003e8 Time: 1.47134
[08/10/2023-11:16:30] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.636325
[08/10/2023-11:16:30] [V] [TRT] Tactic: 0x0000000000000000 Time: 1.30252
[08/10/2023-11:16:30] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 0.636325
[08/10/2023-11:16:30] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x00000000000003ea
[08/10/2023-11:16:30] [V] [TRT] *************** Autotuning Reformat: Half(518400,1:16,2880,16) -> Half(2073600,1:8,11520,64) ***************
[08/10/2023-11:16:30] [V] [TRT] --------------- Timing Runner: onnx::Concat_647 copy (Reformat)
[08/10/2023-11:16:30] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.908946
[08/10/2023-11:16:30] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.396215
[08/10/2023-11:16:30] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.907945
[08/10/2023-11:16:30] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 0.396215
[08/10/2023-11:16:30] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x00000000000003ea
[08/10/2023-11:16:30] [V] [TRT] *************** Autotuning Reformat: Half(518400,1:16,2880,16) -> Half(1036800,1:16,5760,32) ***************
[08/10/2023-11:16:30] [V] [TRT] --------------- Timing Runner: onnx::Concat_647 copy (Reformat)
[08/10/2023-11:16:30] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.907611
[08/10/2023-11:16:30] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.39589
[08/10/2023-11:16:30] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.888471
[08/10/2023-11:16:30] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 0.39589
[08/10/2023-11:16:30] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x00000000000003ea
[08/10/2023-11:16:30] [V] [TRT] =============== Computing reformatting costs
[08/10/2023-11:16:30] [V] [TRT] *************** Autotuning Reformat: Float(16588800,32400,180,1) -> Float(16588800,1,92160,512) ***************
[08/10/2023-11:16:30] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(input.164 -> <out>) (Reformat)
[08/10/2023-11:16:30] [V] [TRT] Tactic: 0x00000000000003e8 Time: 5.48769
[08/10/2023-11:16:30] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.932485
[08/10/2023-11:16:30] [V] [TRT] Tactic: 0x0000000000000000 Time: 5.47195
[08/10/2023-11:16:30] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 0.932485
[08/10/2023-11:16:30] [V] [TRT] *************** Autotuning Reformat: Float(16588800,32400,180,1) -> Float(4147200,1:4,23040,128) ***************
[08/10/2023-11:16:30] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(input.164 -> <out>) (Reformat)
[08/10/2023-11:16:30] [V] [TRT] Tactic: 0x00000000000003e8 Time: 5.68271
[08/10/2023-11:16:30] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.934779
[08/10/2023-11:16:30] [V] [TRT] Tactic: 0x0000000000000000 Time: 5.57065
[08/10/2023-11:16:30] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 0.934779
[08/10/2023-11:16:30] [V] [TRT] *************** Autotuning Reformat: Float(16588800,32400,180,1) -> Half(16588800,32400,180,1) ***************
[08/10/2023-11:16:30] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(input.164 -> <out>) (Reformat)
[08/10/2023-11:16:30] [V] [TRT] Tactic: 0x00000000000003e8 Time: 1.00426
[08/10/2023-11:16:31] [V] [TRT] Tactic: 0x00000000000003ea Time: 1.20616
[08/10/2023-11:16:31] [V] [TRT] Tactic: 0x0000000000000000 Time: 1.72059
[08/10/2023-11:16:31] [V] [TRT] Fastest Tactic: 0x00000000000003e8 Time: 1.00426
[08/10/2023-11:16:31] [V] [TRT] *************** Autotuning Reformat: Float(16588800,32400,180,1) -> Half(8294400,32400:2,180,1) ***************
[08/10/2023-11:16:31] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(input.164 -> <out>) (Reformat)
[08/10/2023-11:16:31] [V] [TRT] Tactic: 0x00000000000003e8 Time: 1.52795
[08/10/2023-11:16:31] [V] [TRT] Tactic: 0x00000000000003ea Time: 1.40106
[08/10/2023-11:16:31] [V] [TRT] Tactic: 0x0000000000000000 Time: 1.04104
[08/10/2023-11:16:31] [V] [TRT] Fastest Tactic: 0x0000000000000000 Time: 1.04104
[08/10/2023-11:16:31] [V] [TRT] *************** Autotuning Reformat: Float(16588800,32400,180,1) -> Half(2073600,1:8,11520,64) ***************
[08/10/2023-11:16:31] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(input.164 -> <out>) (Reformat)
[08/10/2023-11:16:31] [V] [TRT] Tactic: 0x00000000000003e8 Time: 5.50548
[08/10/2023-11:16:31] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.866098
[08/10/2023-11:16:31] [V] [TRT] Tactic: 0x0000000000000000 Time: 1.03672
[08/10/2023-11:16:31] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 0.866098
[08/10/2023-11:16:31] [V] [TRT] *************** Autotuning Reformat: Float(16588800,32400,180,1) -> Half(1036800,1:16,5760,32) ***************
[08/10/2023-11:16:31] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(input.164 -> <out>) (Reformat)
[08/10/2023-11:16:31] [V] [TRT] Tactic: 0x00000000000003e8 Time: 5.40797
[08/10/2023-11:16:31] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.863945
[08/10/2023-11:16:31] [V] [TRT] Tactic: 0x0000000000000000 Time: 5.36788
[08/10/2023-11:16:31] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 0.863945
[08/10/2023-11:16:31] [V] [TRT] *************** Autotuning Reformat: Float(16588800,1,92160,512) -> Float(16588800,32400,180,1) ***************
[08/10/2023-11:16:31] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(input.164 -> <out>) (Reformat)
[08/10/2023-11:16:31] [V] [TRT] Tactic: 0x00000000000003e8 Time: 3.65195
[08/10/2023-11:16:31] [V] [TRT] Tactic: 0x00000000000003ea Time: 1.05476
[08/10/2023-11:16:31] [V] [TRT] Tactic: 0x0000000000000000 Time: 3.61639
[08/10/2023-11:16:31] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 1.05476
[08/10/2023-11:16:31] [V] [TRT] *************** Autotuning Reformat: Float(16588800,1,92160,512) -> Float(4147200,1:4,23040,128) ***************
[08/10/2023-11:16:31] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(input.164 -> <out>) (Reformat)
[08/10/2023-11:16:31] [V] [TRT] Tactic: 0x00000000000003e8 Time: 1.59585
[08/10/2023-11:16:31] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.798139
[08/10/2023-11:16:31] [V] [TRT] Tactic: 0x0000000000000000 Time: 1.59532
[08/10/2023-11:16:31] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 0.798139
[08/10/2023-11:16:31] [V] [TRT] *************** Autotuning Reformat: Float(16588800,1,92160,512) -> Half(16588800,32400,180,1) ***************
[08/10/2023-11:16:31] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(input.164 -> <out>) (Reformat)
[08/10/2023-11:16:31] [V] [TRT] Tactic: 0x00000000000003e8 Time: 3.21424
[08/10/2023-11:16:31] [V] [TRT] Tactic: 0x00000000000003ea Time: 1.0297
[08/10/2023-11:16:31] [V] [TRT] Tactic: 0x0000000000000000 Time: 3.30572
[08/10/2023-11:16:31] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 1.0297
[08/10/2023-11:16:31] [V] [TRT] *************** Autotuning Reformat: Float(16588800,1,92160,512) -> Half(8294400,32400:2,180,1) ***************
[08/10/2023-11:16:31] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(input.164 -> <out>) (Reformat)
[08/10/2023-11:16:31] [V] [TRT] Tactic: 0x00000000000003e8 Time: 3.3656
[08/10/2023-11:16:31] [V] [TRT] Tactic: 0x00000000000003ea Time: 1.40708
[08/10/2023-11:16:31] [V] [TRT] Tactic: 0x0000000000000000 Time: 3.70349
[08/10/2023-11:16:31] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 1.40708
[08/10/2023-11:16:31] [V] [TRT] *************** Autotuning Reformat: Float(16588800,1,92160,512) -> Half(2073600,1:8,11520,64) ***************
[08/10/2023-11:16:31] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(input.164 -> <out>) (Reformat)
[08/10/2023-11:16:31] [V] [TRT] Tactic: 0x00000000000003e8 Time: 1.59953
[08/10/2023-11:16:31] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.762103
[08/10/2023-11:16:31] [V] [TRT] Tactic: 0x0000000000000000 Time: 1.60019
[08/10/2023-11:16:31] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 0.762103
[08/10/2023-11:16:31] [V] [TRT] *************** Autotuning Reformat: Float(16588800,1,92160,512) -> Half(1036800,1:16,5760,32) ***************
[08/10/2023-11:16:31] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(input.164 -> <out>) (Reformat)
[08/10/2023-11:16:31] [V] [TRT] Tactic: 0x00000000000003e8 Time: 1.59998
[08/10/2023-11:16:31] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.763017
[08/10/2023-11:16:31] [V] [TRT] Tactic: 0x0000000000000000 Time: 1.62812
[08/10/2023-11:16:31] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 0.763017
[08/10/2023-11:16:31] [V] [TRT] *************** Autotuning Reformat: Float(4147200,1:4,23040,128) -> Float(16588800,32400,180,1) ***************
[08/10/2023-11:16:31] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(input.164 -> <out>) (Reformat)
[08/10/2023-11:16:31] [V] [TRT] Tactic: 0x00000000000003e8 Time: 3.69884
[08/10/2023-11:16:31] [V] [TRT] Tactic: 0x00000000000003ea Time: 1.03414
[08/10/2023-11:16:31] [V] [TRT] Tactic: 0x0000000000000000 Time: 3.68312
[08/10/2023-11:16:31] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 1.03414
[08/10/2023-11:16:31] [V] [TRT] *************** Autotuning Reformat: Float(4147200,1:4,23040,128) -> Float(16588800,1,92160,512) ***************
[08/10/2023-11:16:31] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(input.164 -> <out>) (Reformat)
[08/10/2023-11:16:31] [V] [TRT] Tactic: 0x00000000000003e8 Time: 1.58602
[08/10/2023-11:16:31] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.796814
[08/10/2023-11:16:31] [V] [TRT] Tactic: 0x0000000000000000 Time: 1.58447
[08/10/2023-11:16:31] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 0.796814
[08/10/2023-11:16:31] [V] [TRT] *************** Autotuning Reformat: Float(4147200,1:4,23040,128) -> Half(16588800,32400,180,1) ***************
[08/10/2023-11:16:31] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(input.164 -> <out>) (Reformat)
[08/10/2023-11:16:31] [V] [TRT] Tactic: 0x00000000000003e8 Time: 3.44269
[08/10/2023-11:16:31] [V] [TRT] Tactic: 0x00000000000003ea Time: 1.03082
[08/10/2023-11:16:32] [V] [TRT] Tactic: 0x0000000000000000 Time: 3.36765
[08/10/2023-11:16:32] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 1.03082
[08/10/2023-11:16:32] [V] [TRT] *************** Autotuning Reformat: Float(4147200,1:4,23040,128) -> Half(8294400,32400:2,180,1) ***************
[08/10/2023-11:16:32] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(input.164 -> <out>) (Reformat)
[08/10/2023-11:16:32] [V] [TRT] Tactic: 0x00000000000003e8 Time: 3.51048
[08/10/2023-11:16:32] [V] [TRT] Tactic: 0x00000000000003ea Time: 1.38661
[08/10/2023-11:16:32] [V] [TRT] Tactic: 0x0000000000000000 Time: 3.53756
[08/10/2023-11:16:32] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 1.38661
[08/10/2023-11:16:32] [V] [TRT] *************** Autotuning Reformat: Float(4147200,1:4,23040,128) -> Half(2073600,1:8,11520,64) ***************
[08/10/2023-11:16:32] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(input.164 -> <out>) (Reformat)
[08/10/2023-11:16:32] [V] [TRT] Tactic: 0x00000000000003e8 Time: 1.89981
[08/10/2023-11:16:32] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.762359
[08/10/2023-11:16:32] [V] [TRT] Tactic: 0x0000000000000000 Time: 1.89992
[08/10/2023-11:16:32] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 0.762359
[08/10/2023-11:16:32] [V] [TRT] *************** Autotuning Reformat: Float(4147200,1:4,23040,128) -> Half(1036800,1:16,5760,32) ***************
[08/10/2023-11:16:32] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(input.164 -> <out>) (Reformat)
[08/10/2023-11:16:32] [V] [TRT] Tactic: 0x00000000000003e8 Time: 1.91217
[08/10/2023-11:16:32] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.780965
[08/10/2023-11:16:32] [V] [TRT] Tactic: 0x0000000000000000 Time: 1.94304
[08/10/2023-11:16:32] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 0.780965
[08/10/2023-11:16:32] [V] [TRT] *************** Autotuning Reformat: Half(16588800,32400,180,1) -> Float(16588800,32400,180,1) ***************
[08/10/2023-11:16:32] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(input.164 -> <out>) (Reformat)
[08/10/2023-11:16:32] [V] [TRT] Tactic: 0x00000000000003e8 Time: 1.07163
[08/10/2023-11:16:32] [V] [TRT] Tactic: 0x00000000000003ea Time: 1.20471
[08/10/2023-11:16:32] [V] [TRT] Tactic: 0x0000000000000000 Time: 1.55771
[08/10/2023-11:16:32] [V] [TRT] Fastest Tactic: 0x00000000000003e8 Time: 1.07163
[08/10/2023-11:16:32] [V] [TRT] *************** Autotuning Reformat: Half(16588800,32400,180,1) -> Float(16588800,1,92160,512) ***************
[08/10/2023-11:16:32] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(input.164 -> <out>) (Reformat)
[08/10/2023-11:16:32] [V] [TRT] Tactic: 0x00000000000003e8 Time: 4.5763
[08/10/2023-11:16:32] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.870007
[08/10/2023-11:16:32] [V] [TRT] Tactic: 0x0000000000000000 Time: 4.5563
[08/10/2023-11:16:32] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 0.870007
[08/10/2023-11:16:32] [V] [TRT] *************** Autotuning Reformat: Half(16588800,32400,180,1) -> Float(4147200,1:4,23040,128) ***************
[08/10/2023-11:16:32] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(input.164 -> <out>) (Reformat)
[08/10/2023-11:16:32] [V] [TRT] Tactic: 0x00000000000003e8 Time: 4.61655
[08/10/2023-11:16:32] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.889198
[08/10/2023-11:16:32] [V] [TRT] Tactic: 0x0000000000000000 Time: 4.66798
[08/10/2023-11:16:32] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 0.889198
[08/10/2023-11:16:32] [V] [TRT] *************** Autotuning Reformat: Half(16588800,32400,180,1) -> Half(8294400,32400:2,180,1) ***************
[08/10/2023-11:16:32] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(input.164 -> <out>) (Reformat)
[08/10/2023-11:16:32] [V] [TRT] Tactic: 0x00000000000003e8 Time: 1.4281
[08/10/2023-11:16:32] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.786414
[08/10/2023-11:16:32] [V] [TRT] Tactic: 0x0000000000000000 Time: 1.01094
[08/10/2023-11:16:32] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 0.786414
[08/10/2023-11:16:32] [V] [TRT] *************** Autotuning Reformat: Half(16588800,32400,180,1) -> Half(2073600,1:8,11520,64) ***************
[08/10/2023-11:16:32] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(input.164 -> <out>) (Reformat)
[08/10/2023-11:16:32] [V] [TRT] Tactic: 0x00000000000003e8 Time: 4.66976
[08/10/2023-11:16:32] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.692677
[08/10/2023-11:16:32] [V] [TRT] Tactic: 0x0000000000000000 Time: 1.02389
[08/10/2023-11:16:32] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 0.692677
[08/10/2023-11:16:32] [V] [TRT] *************** Autotuning Reformat: Half(16588800,32400,180,1) -> Half(1036800,1:16,5760,32) ***************
[08/10/2023-11:16:32] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(input.164 -> <out>) (Reformat)
[08/10/2023-11:16:32] [V] [TRT] Tactic: 0x00000000000003e8 Time: 4.74683
[08/10/2023-11:16:32] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.709413
[08/10/2023-11:16:32] [V] [TRT] Tactic: 0x0000000000000000 Time: 4.68714
[08/10/2023-11:16:32] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 0.709413
[08/10/2023-11:16:32] [V] [TRT] *************** Autotuning Reformat: Half(8294400,32400:2,180,1) -> Float(16588800,32400,180,1) ***************
[08/10/2023-11:16:32] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(input.164 -> <out>) (Reformat)
[08/10/2023-11:16:32] [V] [TRT] Tactic: 0x00000000000003e8 Time: 1.53429
[08/10/2023-11:16:32] [V] [TRT] Tactic: 0x00000000000003ea Time: 1.06547
[08/10/2023-11:16:32] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.916379
[08/10/2023-11:16:32] [V] [TRT] Fastest Tactic: 0x0000000000000000 Time: 0.916379
[08/10/2023-11:16:32] [V] [TRT] *************** Autotuning Reformat: Half(8294400,32400:2,180,1) -> Float(16588800,1,92160,512) ***************
[08/10/2023-11:16:32] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(input.164 -> <out>) (Reformat)
[08/10/2023-11:16:32] [V] [TRT] Tactic: 0x00000000000003e8 Time: 3.13744
[08/10/2023-11:16:32] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.890144
[08/10/2023-11:16:32] [V] [TRT] Tactic: 0x0000000000000000 Time: 3.13612
[08/10/2023-11:16:32] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 0.890144
[08/10/2023-11:16:32] [V] [TRT] *************** Autotuning Reformat: Half(8294400,32400:2,180,1) -> Float(4147200,1:4,23040,128) ***************
[08/10/2023-11:16:32] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(input.164 -> <out>) (Reformat)
[08/10/2023-11:16:32] [V] [TRT] Tactic: 0x00000000000003e8 Time: 3.3868
[08/10/2023-11:16:32] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.909673
[08/10/2023-11:16:32] [V] [TRT] Tactic: 0x0000000000000000 Time: 3.47669
[08/10/2023-11:16:32] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 0.909673
[08/10/2023-11:16:32] [V] [TRT] *************** Autotuning Reformat: Half(8294400,32400:2,180,1) -> Half(16588800,32400,180,1) ***************
[08/10/2023-11:16:32] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(input.164 -> <out>) (Reformat)
[08/10/2023-11:16:32] [V] [TRT] Tactic: 0x00000000000003e8 Time: 1.39232
[08/10/2023-11:16:33] [V] [TRT] Tactic: 0x00000000000003ea Time: 5.54383
[08/10/2023-11:16:33] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.856827
[08/10/2023-11:16:33] [V] [TRT] Fastest Tactic: 0x0000000000000000 Time: 0.856827
[08/10/2023-11:16:33] [V] [TRT] *************** Autotuning Reformat: Half(8294400,32400:2,180,1) -> Half(2073600,1:8,11520,64) ***************
[08/10/2023-11:16:33] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(input.164 -> <out>) (Reformat)
[08/10/2023-11:16:33] [V] [TRT] Tactic: 0x00000000000003e8 Time: 3.31363
[08/10/2023-11:16:33] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.883639
[08/10/2023-11:16:33] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.865239
[08/10/2023-11:16:33] [V] [TRT] Fastest Tactic: 0x0000000000000000 Time: 0.865239
[08/10/2023-11:16:33] [V] [TRT] *************** Autotuning Reformat: Half(8294400,32400:2,180,1) -> Half(1036800,1:16,5760,32) ***************
[08/10/2023-11:16:33] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(input.164 -> <out>) (Reformat)
[08/10/2023-11:16:33] [V] [TRT] Tactic: 0x00000000000003e8 Time: 3.35271
[08/10/2023-11:16:33] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.904814
[08/10/2023-11:16:33] [V] [TRT] Tactic: 0x0000000000000000 Time: 3.37797
[08/10/2023-11:16:33] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 0.904814
[08/10/2023-11:16:33] [V] [TRT] *************** Autotuning Reformat: Half(2073600,1:8,11520,64) -> Float(16588800,32400,180,1) ***************
[08/10/2023-11:16:33] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(input.164 -> <out>) (Reformat)
[08/10/2023-11:16:33] [V] [TRT] Tactic: 0x00000000000003e8 Time: 3.29656
[08/10/2023-11:16:33] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.986757
[08/10/2023-11:16:33] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.813623
[08/10/2023-11:16:33] [V] [TRT] Fastest Tactic: 0x0000000000000000 Time: 0.813623
[08/10/2023-11:16:33] [V] [TRT] *************** Autotuning Reformat: Half(2073600,1:8,11520,64) -> Float(16588800,1,92160,512) ***************
[08/10/2023-11:16:33] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(input.164 -> <out>) (Reformat)
[08/10/2023-11:16:33] [V] [TRT] Tactic: 0x00000000000003e8 Time: 1.61307
[08/10/2023-11:16:33] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.772622
[08/10/2023-11:16:33] [V] [TRT] Tactic: 0x0000000000000000 Time: 1.61142
[08/10/2023-11:16:33] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 0.772622
[08/10/2023-11:16:33] [V] [TRT] *************** Autotuning Reformat: Half(2073600,1:8,11520,64) -> Float(4147200,1:4,23040,128) ***************
[08/10/2023-11:16:33] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(input.164 -> <out>) (Reformat)
[08/10/2023-11:16:33] [V] [TRT] Tactic: 0x00000000000003e8 Time: 1.90713
[08/10/2023-11:16:33] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.772078
[08/10/2023-11:16:33] [V] [TRT] Tactic: 0x0000000000000000 Time: 1.90746
[08/10/2023-11:16:33] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 0.772078
[08/10/2023-11:16:33] [V] [TRT] *************** Autotuning Reformat: Half(2073600,1:8,11520,64) -> Half(16588800,32400,180,1) ***************
[08/10/2023-11:16:33] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(input.164 -> <out>) (Reformat)
[08/10/2023-11:16:33] [V] [TRT] Tactic: 0x00000000000003e8 Time: 2.57823
[08/10/2023-11:16:33] [V] [TRT] Tactic: 0x00000000000003ea Time: 3.88908
[08/10/2023-11:16:33] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.670126
[08/10/2023-11:16:33] [V] [TRT] Fastest Tactic: 0x0000000000000000 Time: 0.670126
[08/10/2023-11:16:33] [V] [TRT] *************** Autotuning Reformat: Half(2073600,1:8,11520,64) -> Half(8294400,32400:2,180,1) ***************
[08/10/2023-11:16:33] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(input.164 -> <out>) (Reformat)
[08/10/2023-11:16:33] [V] [TRT] Tactic: 0x00000000000003e8 Time: 3.09235
[08/10/2023-11:16:33] [V] [TRT] Tactic: 0x00000000000003ea Time: 1.27503
[08/10/2023-11:16:33] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.654738
[08/10/2023-11:16:33] [V] [TRT] Fastest Tactic: 0x0000000000000000 Time: 0.654738
[08/10/2023-11:16:33] [V] [TRT] *************** Autotuning Reformat: Half(2073600,1:8,11520,64) -> Half(1036800,1:16,5760,32) ***************
[08/10/2023-11:16:33] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(input.164 -> <out>) (Reformat)
[08/10/2023-11:16:33] [V] [TRT] Tactic: 0x00000000000003e8 Time: 1.76609
[08/10/2023-11:16:33] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.766683
[08/10/2023-11:16:33] [V] [TRT] Tactic: 0x0000000000000000 Time: 1.76585
[08/10/2023-11:16:33] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 0.766683
[08/10/2023-11:16:33] [V] [TRT] *************** Autotuning Reformat: Half(1036800,1:16,5760,32) -> Float(16588800,32400,180,1) ***************
[08/10/2023-11:16:33] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(input.164 -> <out>) (Reformat)
[08/10/2023-11:16:33] [V] [TRT] Tactic: 0x00000000000003e8 Time: 3.62846
[08/10/2023-11:16:33] [V] [TRT] Tactic: 0x00000000000003ea Time: 1.00659
[08/10/2023-11:16:33] [V] [TRT] Tactic: 0x0000000000000000 Time: 3.12294
[08/10/2023-11:16:33] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 1.00659
[08/10/2023-11:16:33] [V] [TRT] *************** Autotuning Reformat: Half(1036800,1:16,5760,32) -> Float(16588800,1,92160,512) ***************
[08/10/2023-11:16:33] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(input.164 -> <out>) (Reformat)
[08/10/2023-11:16:33] [V] [TRT] Tactic: 0x00000000000003e8 Time: 1.61364
[08/10/2023-11:16:33] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.774779
[08/10/2023-11:16:33] [V] [TRT] Tactic: 0x0000000000000000 Time: 1.61402
[08/10/2023-11:16:33] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 0.774779
[08/10/2023-11:16:33] [V] [TRT] *************** Autotuning Reformat: Half(1036800,1:16,5760,32) -> Float(4147200,1:4,23040,128) ***************
[08/10/2023-11:16:33] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(input.164 -> <out>) (Reformat)
[08/10/2023-11:16:33] [V] [TRT] Tactic: 0x00000000000003e8 Time: 1.91039
[08/10/2023-11:16:33] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.774729
[08/10/2023-11:16:33] [V] [TRT] Tactic: 0x0000000000000000 Time: 1.90699
[08/10/2023-11:16:33] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 0.774729
[08/10/2023-11:16:33] [V] [TRT] *************** Autotuning Reformat: Half(1036800,1:16,5760,32) -> Half(16588800,32400,180,1) ***************
[08/10/2023-11:16:33] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(input.164 -> <out>) (Reformat)
[08/10/2023-11:16:33] [V] [TRT] Tactic: 0x00000000000003e8 Time: 3.3299
[08/10/2023-11:16:33] [V] [TRT] Tactic: 0x00000000000003ea Time: 3.82702
[08/10/2023-11:16:33] [V] [TRT] Tactic: 0x0000000000000000 Time: 3.26429
[08/10/2023-11:16:33] [V] [TRT] Fastest Tactic: 0x0000000000000000 Time: 3.26429
[08/10/2023-11:16:33] [V] [TRT] *************** Autotuning Reformat: Half(1036800,1:16,5760,32) -> Half(8294400,32400:2,180,1) ***************
[08/10/2023-11:16:33] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(input.164 -> <out>) (Reformat)
[08/10/2023-11:16:33] [V] [TRT] Tactic: 0x00000000000003e8 Time: 2.7596
[08/10/2023-11:16:33] [V] [TRT] Tactic: 0x00000000000003ea Time: 1.25119
[08/10/2023-11:16:33] [V] [TRT] Tactic: 0x0000000000000000 Time: 3.44484
[08/10/2023-11:16:33] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 1.25119
[08/10/2023-11:16:33] [V] [TRT] *************** Autotuning Reformat: Half(1036800,1:16,5760,32) -> Half(2073600,1:8,11520,64) ***************
[08/10/2023-11:16:33] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(input.164 -> <out>) (Reformat)
[08/10/2023-11:16:33] [V] [TRT] Tactic: 0x00000000000003e8 Time: 1.76663
[08/10/2023-11:16:33] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.764887
[08/10/2023-11:16:33] [V] [TRT] Tactic: 0x0000000000000000 Time: 1.76665
[08/10/2023-11:16:33] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 0.764887
[08/10/2023-11:16:33] [V] [TRT] =============== Computing reformatting costs
[08/10/2023-11:16:33] [V] [TRT] *************** Autotuning Reformat: Float(2073600,32400,180,1) -> Float(2073600,1,11520,64) ***************
[08/10/2023-11:16:33] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(<in> -> onnx::Conv_651) (Reformat)
[08/10/2023-11:16:33] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.379607
[08/10/2023-11:16:33] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.128217
[08/10/2023-11:16:33] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.380229
[08/10/2023-11:16:33] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 0.128217
[08/10/2023-11:16:33] [V] [TRT] *************** Autotuning Reformat: Float(2073600,32400,180,1) -> Float(518400,1:4,2880,16) ***************
[08/10/2023-11:16:33] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(<in> -> onnx::Conv_651) (Reformat)
[08/10/2023-11:16:33] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.3984
[08/10/2023-11:16:33] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.128165
[08/10/2023-11:16:33] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.397394
[08/10/2023-11:16:33] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 0.128165
[08/10/2023-11:16:33] [V] [TRT] *************** Autotuning Reformat: Float(2073600,32400,180,1) -> Half(2073600,32400,180,1) ***************
[08/10/2023-11:16:33] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(<in> -> onnx::Conv_651) (Reformat)
[08/10/2023-11:16:33] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.130894
[08/10/2023-11:16:33] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.158661
[08/10/2023-11:16:33] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.223072
[08/10/2023-11:16:33] [V] [TRT] Fastest Tactic: 0x00000000000003e8 Time: 0.130894
[08/10/2023-11:16:33] [V] [TRT] *************** Autotuning Reformat: Float(2073600,32400,180,1) -> Half(1036800,32400:2,180,1) ***************
[08/10/2023-11:16:33] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(<in> -> onnx::Conv_651) (Reformat)
[08/10/2023-11:16:33] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.198455
[08/10/2023-11:16:33] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.177509
[08/10/2023-11:16:33] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.13891
[08/10/2023-11:16:33] [V] [TRT] Fastest Tactic: 0x0000000000000000 Time: 0.13891
[08/10/2023-11:16:33] [V] [TRT] *************** Autotuning Reformat: Float(2073600,32400,180,1) -> Half(259200,1:8,1440,8) ***************
[08/10/2023-11:16:33] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(<in> -> onnx::Conv_651) (Reformat)
[08/10/2023-11:16:33] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.416064
[08/10/2023-11:16:34] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.119499
[08/10/2023-11:16:34] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.139454
[08/10/2023-11:16:34] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 0.119499
[08/10/2023-11:16:34] [V] [TRT] *************** Autotuning Reformat: Float(2073600,32400,180,1) -> Half(129600,1:16,720,4) ***************
[08/10/2023-11:16:34] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(<in> -> onnx::Conv_651) (Reformat)
[08/10/2023-11:16:34] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.41771
[08/10/2023-11:16:34] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.119502
[08/10/2023-11:16:34] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.417248
[08/10/2023-11:16:34] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 0.119502
[08/10/2023-11:16:34] [V] [TRT] *************** Autotuning Reformat: Float(2073600,1,11520,64) -> Float(2073600,32400,180,1) ***************
[08/10/2023-11:16:34] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(<in> -> onnx::Conv_651) (Reformat)
[08/10/2023-11:16:34] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.369509
[08/10/2023-11:16:34] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.145303
[08/10/2023-11:16:34] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.391675
[08/10/2023-11:16:34] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 0.145303
[08/10/2023-11:16:34] [V] [TRT] *************** Autotuning Reformat: Float(2073600,1,11520,64) -> Float(518400,1:4,2880,16) ***************
[08/10/2023-11:16:34] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(<in> -> onnx::Conv_651) (Reformat)
[08/10/2023-11:16:34] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.211337
[08/10/2023-11:16:34] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.116229
[08/10/2023-11:16:34] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.207182
[08/10/2023-11:16:34] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 0.116229
[08/10/2023-11:16:34] [V] [TRT] *************** Autotuning Reformat: Float(2073600,1,11520,64) -> Half(2073600,32400,180,1) ***************
[08/10/2023-11:16:34] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(<in> -> onnx::Conv_651) (Reformat)
[08/10/2023-11:16:34] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.334222
[08/10/2023-11:16:34] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.134489
[08/10/2023-11:16:34] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.328306
[08/10/2023-11:16:34] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 0.134489
[08/10/2023-11:16:34] [V] [TRT] *************** Autotuning Reformat: Float(2073600,1,11520,64) -> Half(1036800,32400:2,180,1) ***************
[08/10/2023-11:16:34] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(<in> -> onnx::Conv_651) (Reformat)
[08/10/2023-11:16:34] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.378871
[08/10/2023-11:16:34] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.172206
[08/10/2023-11:16:34] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.363776
[08/10/2023-11:16:34] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 0.172206
[08/10/2023-11:16:34] [V] [TRT] *************** Autotuning Reformat: Float(2073600,1,11520,64) -> Half(259200,1:8,1440,8) ***************
[08/10/2023-11:16:34] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(<in> -> onnx::Conv_651) (Reformat)
[08/10/2023-11:16:34] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.208343
[08/10/2023-11:16:34] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.104203
[08/10/2023-11:16:34] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.207328
[08/10/2023-11:16:34] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 0.104203
[08/10/2023-11:16:34] [V] [TRT] *************** Autotuning Reformat: Float(2073600,1,11520,64) -> Half(129600,1:16,720,4) ***************
[08/10/2023-11:16:34] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(<in> -> onnx::Conv_651) (Reformat)
[08/10/2023-11:16:34] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.207707
[08/10/2023-11:16:34] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.104101
[08/10/2023-11:16:34] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.206555
[08/10/2023-11:16:34] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 0.104101
[08/10/2023-11:16:34] [V] [TRT] *************** Autotuning Reformat: Float(518400,1:4,2880,16) -> Float(2073600,32400,180,1) ***************
[08/10/2023-11:16:34] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(<in> -> onnx::Conv_651) (Reformat)
[08/10/2023-11:16:34] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.374903
[08/10/2023-11:16:34] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.141536
[08/10/2023-11:16:34] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.377947
[08/10/2023-11:16:34] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 0.141536
[08/10/2023-11:16:34] [V] [TRT] *************** Autotuning Reformat: Float(518400,1:4,2880,16) -> Float(2073600,1,11520,64) ***************
[08/10/2023-11:16:34] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(<in> -> onnx::Conv_651) (Reformat)
[08/10/2023-11:16:34] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.206834
[08/10/2023-11:16:34] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.114985
[08/10/2023-11:16:34] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.20571
[08/10/2023-11:16:34] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 0.114985
[08/10/2023-11:16:34] [V] [TRT] *************** Autotuning Reformat: Float(518400,1:4,2880,16) -> Half(2073600,32400,180,1) ***************
[08/10/2023-11:16:34] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(<in> -> onnx::Conv_651) (Reformat)
[08/10/2023-11:16:34] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.325874
[08/10/2023-11:16:34] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.134917
[08/10/2023-11:16:34] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.336123
[08/10/2023-11:16:34] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 0.134917
[08/10/2023-11:16:34] [V] [TRT] *************** Autotuning Reformat: Float(518400,1:4,2880,16) -> Half(1036800,32400:2,180,1) ***************
[08/10/2023-11:16:34] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(<in> -> onnx::Conv_651) (Reformat)
[08/10/2023-11:16:34] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.357198
[08/10/2023-11:16:34] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.170327
[08/10/2023-11:16:34] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.335049
[08/10/2023-11:16:34] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 0.170327
[08/10/2023-11:16:34] [V] [TRT] *************** Autotuning Reformat: Float(518400,1:4,2880,16) -> Half(259200,1:8,1440,8) ***************
[08/10/2023-11:16:34] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(<in> -> onnx::Conv_651) (Reformat)
[08/10/2023-11:16:34] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.244946
[08/10/2023-11:16:34] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.103849
[08/10/2023-11:16:34] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.244571
[08/10/2023-11:16:34] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 0.103849
[08/10/2023-11:16:34] [V] [TRT] *************** Autotuning Reformat: Float(518400,1:4,2880,16) -> Half(129600,1:16,720,4) ***************
[08/10/2023-11:16:34] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(<in> -> onnx::Conv_651) (Reformat)
[08/10/2023-11:16:34] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.244096
[08/10/2023-11:16:34] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.103808
[08/10/2023-11:16:34] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.244466
[08/10/2023-11:16:34] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 0.103808
[08/10/2023-11:16:34] [V] [TRT] *************** Autotuning Reformat: Half(2073600,32400,180,1) -> Float(2073600,32400,180,1) ***************
[08/10/2023-11:16:34] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(<in> -> onnx::Conv_651) (Reformat)
[08/10/2023-11:16:34] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.138597
[08/10/2023-11:16:34] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.158437
[08/10/2023-11:16:34] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.200571
[08/10/2023-11:16:34] [V] [TRT] Fastest Tactic: 0x00000000000003e8 Time: 0.138597
[08/10/2023-11:16:34] [V] [TRT] *************** Autotuning Reformat: Half(2073600,32400,180,1) -> Float(2073600,1,11520,64) ***************
[08/10/2023-11:16:34] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(<in> -> onnx::Conv_651) (Reformat)
[08/10/2023-11:16:34] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.319717
[08/10/2023-11:16:34] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.118032
[08/10/2023-11:16:34] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.320064
[08/10/2023-11:16:34] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 0.118032
[08/10/2023-11:16:34] [V] [TRT] *************** Autotuning Reformat: Half(2073600,32400,180,1) -> Float(518400,1:4,2880,16) ***************
[08/10/2023-11:16:34] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(<in> -> onnx::Conv_651) (Reformat)
[08/10/2023-11:16:34] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.351616
[08/10/2023-11:16:34] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.11744
[08/10/2023-11:16:34] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.350985
[08/10/2023-11:16:34] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 0.11744
[08/10/2023-11:16:34] [V] [TRT] *************** Autotuning Reformat: Half(2073600,32400,180,1) -> Half(1036800,32400:2,180,1) ***************
[08/10/2023-11:16:34] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(<in> -> onnx::Conv_651) (Reformat)
[08/10/2023-11:16:34] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.185266
[08/10/2023-11:16:34] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.10773
[08/10/2023-11:16:34] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.131531
[08/10/2023-11:16:34] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 0.10773
[08/10/2023-11:16:34] [V] [TRT] *************** Autotuning Reformat: Half(2073600,32400,180,1) -> Half(259200,1:8,1440,8) ***************
[08/10/2023-11:16:34] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(<in> -> onnx::Conv_651) (Reformat)
[08/10/2023-11:16:34] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.351237
[08/10/2023-11:16:34] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.0964091
[08/10/2023-11:16:34] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.136434
[08/10/2023-11:16:34] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 0.0964091
[08/10/2023-11:16:34] [V] [TRT] *************** Autotuning Reformat: Half(2073600,32400,180,1) -> Half(129600,1:16,720,4) ***************
[08/10/2023-11:16:34] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(<in> -> onnx::Conv_651) (Reformat)
[08/10/2023-11:16:34] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.350766
[08/10/2023-11:16:34] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.0967109
[08/10/2023-11:16:34] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.350574
[08/10/2023-11:16:34] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 0.0967109
[08/10/2023-11:16:34] [V] [TRT] *************** Autotuning Reformat: Half(1036800,32400:2,180,1) -> Float(2073600,32400,180,1) ***************
[08/10/2023-11:16:34] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(<in> -> onnx::Conv_651) (Reformat)
[08/10/2023-11:16:34] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.198258
[08/10/2023-11:16:34] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.143936
[08/10/2023-11:16:34] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.120016
[08/10/2023-11:16:34] [V] [TRT] Fastest Tactic: 0x0000000000000000 Time: 0.120016
[08/10/2023-11:16:34] [V] [TRT] *************** Autotuning Reformat: Half(1036800,32400:2,180,1) -> Float(2073600,1,11520,64) ***************
[08/10/2023-11:16:34] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(<in> -> onnx::Conv_651) (Reformat)
[08/10/2023-11:16:34] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.231269
[08/10/2023-11:16:34] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.122978
[08/10/2023-11:16:34] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.233413
[08/10/2023-11:16:34] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 0.122978
[08/10/2023-11:16:34] [V] [TRT] *************** Autotuning Reformat: Half(1036800,32400:2,180,1) -> Float(518400,1:4,2880,16) ***************
[08/10/2023-11:16:34] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(<in> -> onnx::Conv_651) (Reformat)
[08/10/2023-11:16:34] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.253664
[08/10/2023-11:16:34] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.123602
[08/10/2023-11:16:34] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.25472
[08/10/2023-11:16:34] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 0.123602
[08/10/2023-11:16:34] [V] [TRT] *************** Autotuning Reformat: Half(1036800,32400:2,180,1) -> Half(2073600,32400,180,1) ***************
[08/10/2023-11:16:34] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(<in> -> onnx::Conv_651) (Reformat)
[08/10/2023-11:16:34] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.185618
[08/10/2023-11:16:34] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.540667
[08/10/2023-11:16:34] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.114857
[08/10/2023-11:16:34] [V] [TRT] Fastest Tactic: 0x0000000000000000 Time: 0.114857
[08/10/2023-11:16:34] [V] [TRT] *************** Autotuning Reformat: Half(1036800,32400:2,180,1) -> Half(259200,1:8,1440,8) ***************
[08/10/2023-11:16:34] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(<in> -> onnx::Conv_651) (Reformat)
[08/10/2023-11:16:34] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.242167
[08/10/2023-11:16:34] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.121602
[08/10/2023-11:16:34] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.11549
[08/10/2023-11:16:34] [V] [TRT] Fastest Tactic: 0x0000000000000000 Time: 0.11549
[08/10/2023-11:16:34] [V] [TRT] *************** Autotuning Reformat: Half(1036800,32400:2,180,1) -> Half(129600,1:16,720,4) ***************
[08/10/2023-11:16:34] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(<in> -> onnx::Conv_651) (Reformat)
[08/10/2023-11:16:34] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.242816
[08/10/2023-11:16:34] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.122014
[08/10/2023-11:16:34] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.240539
[08/10/2023-11:16:34] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 0.122014
[08/10/2023-11:16:34] [V] [TRT] *************** Autotuning Reformat: Half(259200,1:8,1440,8) -> Float(2073600,32400,180,1) ***************
[08/10/2023-11:16:34] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(<in> -> onnx::Conv_651) (Reformat)
[08/10/2023-11:16:34] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.369061
[08/10/2023-11:16:34] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.132233
[08/10/2023-11:16:34] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.113573
[08/10/2023-11:16:34] [V] [TRT] Fastest Tactic: 0x0000000000000000 Time: 0.113573
[08/10/2023-11:16:34] [V] [TRT] *************** Autotuning Reformat: Half(259200,1:8,1440,8) -> Float(2073600,1,11520,64) ***************
[08/10/2023-11:16:34] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(<in> -> onnx::Conv_651) (Reformat)
[08/10/2023-11:16:34] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.209646
[08/10/2023-11:16:34] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.105783
[08/10/2023-11:16:34] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.209518
[08/10/2023-11:16:34] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 0.105783
[08/10/2023-11:16:34] [V] [TRT] *************** Autotuning Reformat: Half(259200,1:8,1440,8) -> Float(518400,1:4,2880,16) ***************
[08/10/2023-11:16:34] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(<in> -> onnx::Conv_651) (Reformat)
[08/10/2023-11:16:34] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.246098
[08/10/2023-11:16:34] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.105728
[08/10/2023-11:16:34] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.246007
[08/10/2023-11:16:34] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 0.105728
[08/10/2023-11:16:34] [V] [TRT] *************** Autotuning Reformat: Half(259200,1:8,1440,8) -> Half(2073600,32400,180,1) ***************
[08/10/2023-11:16:34] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(<in> -> onnx::Conv_651) (Reformat)
[08/10/2023-11:16:34] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.305687
[08/10/2023-11:16:34] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.443099
[08/10/2023-11:16:34] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.0934011
[08/10/2023-11:16:34] [V] [TRT] Fastest Tactic: 0x0000000000000000 Time: 0.0934011
[08/10/2023-11:16:34] [V] [TRT] *************** Autotuning Reformat: Half(259200,1:8,1440,8) -> Half(1036800,32400:2,180,1) ***************
[08/10/2023-11:16:34] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(<in> -> onnx::Conv_651) (Reformat)
[08/10/2023-11:16:34] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.341819
[08/10/2023-11:16:34] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.159337
[08/10/2023-11:16:34] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.0911383
[08/10/2023-11:16:34] [V] [TRT] Fastest Tactic: 0x0000000000000000 Time: 0.0911383
[08/10/2023-11:16:34] [V] [TRT] *************** Autotuning Reformat: Half(259200,1:8,1440,8) -> Half(129600,1:16,720,4) ***************
[08/10/2023-11:16:34] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(<in> -> onnx::Conv_651) (Reformat)
[08/10/2023-11:16:34] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.227621
[08/10/2023-11:16:34] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.103513
[08/10/2023-11:16:34] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.227657
[08/10/2023-11:16:34] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 0.103513
[08/10/2023-11:16:34] [V] [TRT] *************** Autotuning Reformat: Half(129600,1:16,720,4) -> Float(2073600,32400,180,1) ***************
[08/10/2023-11:16:34] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(<in> -> onnx::Conv_651) (Reformat)
[08/10/2023-11:16:34] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.33264
[08/10/2023-11:16:34] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.132827
[08/10/2023-11:16:34] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.345682
[08/10/2023-11:16:34] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 0.132827
[08/10/2023-11:16:34] [V] [TRT] *************** Autotuning Reformat: Half(129600,1:16,720,4) -> Float(2073600,1,11520,64) ***************
[08/10/2023-11:16:34] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(<in> -> onnx::Conv_651) (Reformat)
[08/10/2023-11:16:34] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.20896
[08/10/2023-11:16:34] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.104441
[08/10/2023-11:16:34] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.209024
[08/10/2023-11:16:34] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 0.104441
[08/10/2023-11:16:34] [V] [TRT] *************** Autotuning Reformat: Half(129600,1:16,720,4) -> Float(518400,1:4,2880,16) ***************
[08/10/2023-11:16:34] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(<in> -> onnx::Conv_651) (Reformat)
[08/10/2023-11:16:34] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.245783
[08/10/2023-11:16:34] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.104907
[08/10/2023-11:16:34] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.24571
[08/10/2023-11:16:34] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 0.104907
[08/10/2023-11:16:34] [V] [TRT] *************** Autotuning Reformat: Half(129600,1:16,720,4) -> Half(2073600,32400,180,1) ***************
[08/10/2023-11:16:34] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(<in> -> onnx::Conv_651) (Reformat)
[08/10/2023-11:16:34] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.311442
[08/10/2023-11:16:34] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.442258
[08/10/2023-11:16:34] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.31504
[08/10/2023-11:16:34] [V] [TRT] Fastest Tactic: 0x00000000000003e8 Time: 0.311442
[08/10/2023-11:16:34] [V] [TRT] *************** Autotuning Reformat: Half(129600,1:16,720,4) -> Half(1036800,32400:2,180,1) ***************
[08/10/2023-11:16:34] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(<in> -> onnx::Conv_651) (Reformat)
[08/10/2023-11:16:34] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.337362
[08/10/2023-11:16:34] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.159054
[08/10/2023-11:16:34] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.348256
[08/10/2023-11:16:34] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 0.159054
[08/10/2023-11:16:34] [V] [TRT] *************** Autotuning Reformat: Half(129600,1:16,720,4) -> Half(259200,1:8,1440,8) ***************
[08/10/2023-11:16:34] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(<in> -> onnx::Conv_651) (Reformat)
[08/10/2023-11:16:34] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.226949
[08/10/2023-11:16:34] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.103726
[08/10/2023-11:16:34] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.228027
[08/10/2023-11:16:34] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 0.103726
[08/10/2023-11:16:34] [V] [TRT] =============== Computing reformatting costs
[08/10/2023-11:16:34] [V] [TRT] *************** Autotuning Reformat: Float(2073600,32400,180,1) -> Float(2073600,1,11520,64) ***************
[08/10/2023-11:16:34] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(onnx::Conv_651 -> <out>) (Reformat)
[08/10/2023-11:16:34] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.380498
[08/10/2023-11:16:34] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.128613
[08/10/2023-11:16:34] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.379639
[08/10/2023-11:16:34] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 0.128613
[08/10/2023-11:16:34] [V] [TRT] *************** Autotuning Reformat: Float(2073600,32400,180,1) -> Float(518400,1:4,2880,16) ***************
[08/10/2023-11:16:34] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(onnx::Conv_651 -> <out>) (Reformat)
[08/10/2023-11:16:34] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.411621
[08/10/2023-11:16:34] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.132112
[08/10/2023-11:16:34] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.40933
[08/10/2023-11:16:34] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 0.132112
[08/10/2023-11:16:34] [V] [TRT] *************** Autotuning Reformat: Float(2073600,32400,180,1) -> Half(2073600,32400,180,1) ***************
[08/10/2023-11:16:34] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(onnx::Conv_651 -> <out>) (Reformat)
[08/10/2023-11:16:34] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.132603
[08/10/2023-11:16:34] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.162117
[08/10/2023-11:16:34] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.227346
[08/10/2023-11:16:34] [V] [TRT] Fastest Tactic: 0x00000000000003e8 Time: 0.132603
[08/10/2023-11:16:34] [V] [TRT] *************** Autotuning Reformat: Float(2073600,32400,180,1) -> Half(1036800,32400:2,180,1) ***************
[08/10/2023-11:16:34] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(onnx::Conv_651 -> <out>) (Reformat)
[08/10/2023-11:16:34] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.203337
[08/10/2023-11:16:34] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.183502
[08/10/2023-11:16:34] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.136992
[08/10/2023-11:16:34] [V] [TRT] Fastest Tactic: 0x0000000000000000 Time: 0.136992
[08/10/2023-11:16:34] [V] [TRT] *************** Autotuning Reformat: Float(2073600,32400,180,1) -> Half(259200,1:8,1440,8) ***************
[08/10/2023-11:16:34] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(onnx::Conv_651 -> <out>) (Reformat)
[08/10/2023-11:16:34] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.419726
[08/10/2023-11:16:34] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.11957
[08/10/2023-11:16:34] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.141767
[08/10/2023-11:16:34] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 0.11957
[08/10/2023-11:16:34] [V] [TRT] *************** Autotuning Reformat: Float(2073600,32400,180,1) -> Half(129600,1:16,720,4) ***************
[08/10/2023-11:16:34] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(onnx::Conv_651 -> <out>) (Reformat)
[08/10/2023-11:16:34] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.409317
[08/10/2023-11:16:34] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.117289
[08/10/2023-11:16:34] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.407931
[08/10/2023-11:16:34] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 0.117289
[08/10/2023-11:16:34] [V] [TRT] *************** Autotuning Reformat: Float(2073600,1,11520,64) -> Float(2073600,32400,180,1) ***************
[08/10/2023-11:16:34] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(onnx::Conv_651 -> <out>) (Reformat)
[08/10/2023-11:16:34] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.346272
[08/10/2023-11:16:34] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.141406
[08/10/2023-11:16:34] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.338277
[08/10/2023-11:16:34] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 0.141406
[08/10/2023-11:16:34] [V] [TRT] *************** Autotuning Reformat: Float(2073600,1,11520,64) -> Float(518400,1:4,2880,16) ***************
[08/10/2023-11:16:34] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(onnx::Conv_651 -> <out>) (Reformat)
[08/10/2023-11:16:34] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.20688
[08/10/2023-11:16:34] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.114885
[08/10/2023-11:16:34] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.206505
[08/10/2023-11:16:34] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 0.114885
[08/10/2023-11:16:34] [V] [TRT] *************** Autotuning Reformat: Float(2073600,1,11520,64) -> Half(2073600,32400,180,1) ***************
[08/10/2023-11:16:34] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(onnx::Conv_651 -> <out>) (Reformat)
[08/10/2023-11:16:34] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.344933
[08/10/2023-11:16:34] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.13389
[08/10/2023-11:16:34] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.328672
[08/10/2023-11:16:34] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 0.13389
[08/10/2023-11:16:34] [V] [TRT] *************** Autotuning Reformat: Float(2073600,1,11520,64) -> Half(1036800,32400:2,180,1) ***************
[08/10/2023-11:16:34] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(onnx::Conv_651 -> <out>) (Reformat)
[08/10/2023-11:16:34] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.386331
[08/10/2023-11:16:34] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.17205
[08/10/2023-11:16:34] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.383451
[08/10/2023-11:16:34] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 0.17205
[08/10/2023-11:16:34] [V] [TRT] *************** Autotuning Reformat: Float(2073600,1,11520,64) -> Half(259200,1:8,1440,8) ***************
[08/10/2023-11:16:34] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(onnx::Conv_651 -> <out>) (Reformat)
[08/10/2023-11:16:34] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.207799
[08/10/2023-11:16:34] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.103895
[08/10/2023-11:16:34] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.207296
[08/10/2023-11:16:34] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 0.103895
[08/10/2023-11:16:34] [V] [TRT] *************** Autotuning Reformat: Float(2073600,1,11520,64) -> Half(129600,1:16,720,4) ***************
[08/10/2023-11:16:34] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(onnx::Conv_651 -> <out>) (Reformat)
[08/10/2023-11:16:34] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.207474
[08/10/2023-11:16:34] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.104078
[08/10/2023-11:16:34] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.20757
[08/10/2023-11:16:34] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 0.104078
[08/10/2023-11:16:34] [V] [TRT] *************** Autotuning Reformat: Float(518400,1:4,2880,16) -> Float(2073600,32400,180,1) ***************
[08/10/2023-11:16:34] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(onnx::Conv_651 -> <out>) (Reformat)
[08/10/2023-11:16:34] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.343013
[08/10/2023-11:16:34] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.142007
[08/10/2023-11:16:34] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.373861
[08/10/2023-11:16:34] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 0.142007
[08/10/2023-11:16:34] [V] [TRT] *************** Autotuning Reformat: Float(518400,1:4,2880,16) -> Float(2073600,1,11520,64) ***************
[08/10/2023-11:16:34] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(onnx::Conv_651 -> <out>) (Reformat)
[08/10/2023-11:16:34] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.205829
[08/10/2023-11:16:34] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.114343
[08/10/2023-11:16:34] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.205147
[08/10/2023-11:16:34] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 0.114343
[08/10/2023-11:16:34] [V] [TRT] *************** Autotuning Reformat: Float(518400,1:4,2880,16) -> Half(2073600,32400,180,1) ***************
[08/10/2023-11:16:34] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(onnx::Conv_651 -> <out>) (Reformat)
[08/10/2023-11:16:34] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.341957
[08/10/2023-11:16:34] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.134615
[08/10/2023-11:16:34] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.348155
[08/10/2023-11:16:34] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 0.134615
[08/10/2023-11:16:34] [V] [TRT] *************** Autotuning Reformat: Float(518400,1:4,2880,16) -> Half(1036800,32400:2,180,1) ***************
[08/10/2023-11:16:34] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(onnx::Conv_651 -> <out>) (Reformat)
[08/10/2023-11:16:34] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.378464
[08/10/2023-11:16:34] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.170313
[08/10/2023-11:16:34] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.377184
[08/10/2023-11:16:34] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 0.170313
[08/10/2023-11:16:34] [V] [TRT] *************** Autotuning Reformat: Float(518400,1:4,2880,16) -> Half(259200,1:8,1440,8) ***************
[08/10/2023-11:16:34] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(onnx::Conv_651 -> <out>) (Reformat)
[08/10/2023-11:16:34] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.244498
[08/10/2023-11:16:34] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.103616
[08/10/2023-11:16:34] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.244517
[08/10/2023-11:16:34] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 0.103616
[08/10/2023-11:16:34] [V] [TRT] *************** Autotuning Reformat: Float(518400,1:4,2880,16) -> Half(129600,1:16,720,4) ***************
[08/10/2023-11:16:34] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(onnx::Conv_651 -> <out>) (Reformat)
[08/10/2023-11:16:34] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.245125
[08/10/2023-11:16:34] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.103506
[08/10/2023-11:16:34] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.244274
[08/10/2023-11:16:34] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 0.103506
[08/10/2023-11:16:34] [V] [TRT] *************** Autotuning Reformat: Half(2073600,32400,180,1) -> Float(2073600,32400,180,1) ***************
[08/10/2023-11:16:34] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(onnx::Conv_651 -> <out>) (Reformat)
[08/10/2023-11:16:34] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.138809
[08/10/2023-11:16:34] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.158885
[08/10/2023-11:16:34] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.200745
[08/10/2023-11:16:34] [V] [TRT] Fastest Tactic: 0x00000000000003e8 Time: 0.138809
[08/10/2023-11:16:34] [V] [TRT] *************** Autotuning Reformat: Half(2073600,32400,180,1) -> Float(2073600,1,11520,64) ***************
[08/10/2023-11:16:34] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(onnx::Conv_651 -> <out>) (Reformat)
[08/10/2023-11:16:34] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.321952
[08/10/2023-11:16:34] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.117787
[08/10/2023-11:16:34] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.319822
[08/10/2023-11:16:34] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 0.117787
[08/10/2023-11:16:34] [V] [TRT] *************** Autotuning Reformat: Half(2073600,32400,180,1) -> Float(518400,1:4,2880,16) ***************
[08/10/2023-11:16:34] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(onnx::Conv_651 -> <out>) (Reformat)
[08/10/2023-11:16:34] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.349957
[08/10/2023-11:16:34] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.117712
[08/10/2023-11:16:34] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.351177
[08/10/2023-11:16:34] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 0.117712
[08/10/2023-11:16:34] [V] [TRT] *************** Autotuning Reformat: Half(2073600,32400,180,1) -> Half(1036800,32400:2,180,1) ***************
[08/10/2023-11:16:34] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(onnx::Conv_651 -> <out>) (Reformat)
[08/10/2023-11:16:34] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.184544
[08/10/2023-11:16:34] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.107113
[08/10/2023-11:16:34] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.132731
[08/10/2023-11:16:34] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 0.107113
[08/10/2023-11:16:34] [V] [TRT] *************** Autotuning Reformat: Half(2073600,32400,180,1) -> Half(259200,1:8,1440,8) ***************
[08/10/2023-11:16:34] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(onnx::Conv_651 -> <out>) (Reformat)
[08/10/2023-11:16:34] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.361865
[08/10/2023-11:16:34] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.099952
[08/10/2023-11:16:34] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.137751
[08/10/2023-11:16:34] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 0.099952
[08/10/2023-11:16:34] [V] [TRT] *************** Autotuning Reformat: Half(2073600,32400,180,1) -> Half(129600,1:16,720,4) ***************
[08/10/2023-11:16:34] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(onnx::Conv_651 -> <out>) (Reformat)
[08/10/2023-11:16:34] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.362011
[08/10/2023-11:16:34] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.0990811
[08/10/2023-11:16:34] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.361221
[08/10/2023-11:16:34] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 0.0990811
[08/10/2023-11:16:34] [V] [TRT] *************** Autotuning Reformat: Half(1036800,32400:2,180,1) -> Float(2073600,32400,180,1) ***************
[08/10/2023-11:16:34] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(onnx::Conv_651 -> <out>) (Reformat)
[08/10/2023-11:16:34] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.203365
[08/10/2023-11:16:34] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.149042
[08/10/2023-11:16:34] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.122091
[08/10/2023-11:16:34] [V] [TRT] Fastest Tactic: 0x0000000000000000 Time: 0.122091
[08/10/2023-11:16:34] [V] [TRT] *************** Autotuning Reformat: Half(1036800,32400:2,180,1) -> Float(2073600,1,11520,64) ***************
[08/10/2023-11:16:34] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(onnx::Conv_651 -> <out>) (Reformat)
[08/10/2023-11:16:34] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.232672
[08/10/2023-11:16:34] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.122448
[08/10/2023-11:16:34] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.233234
[08/10/2023-11:16:34] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 0.122448
[08/10/2023-11:16:34] [V] [TRT] *************** Autotuning Reformat: Half(1036800,32400:2,180,1) -> Float(518400,1:4,2880,16) ***************
[08/10/2023-11:16:34] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(onnx::Conv_651 -> <out>) (Reformat)
[08/10/2023-11:16:34] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.254519
[08/10/2023-11:16:34] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.120208
[08/10/2023-11:16:34] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.249001
[08/10/2023-11:16:34] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 0.120208
[08/10/2023-11:16:34] [V] [TRT] *************** Autotuning Reformat: Half(1036800,32400:2,180,1) -> Half(2073600,32400,180,1) ***************
[08/10/2023-11:16:34] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(onnx::Conv_651 -> <out>) (Reformat)
[08/10/2023-11:16:34] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.180965
[08/10/2023-11:16:34] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.531035
[08/10/2023-11:16:34] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.112898
[08/10/2023-11:16:34] [V] [TRT] Fastest Tactic: 0x0000000000000000 Time: 0.112898
[08/10/2023-11:16:34] [V] [TRT] *************** Autotuning Reformat: Half(1036800,32400:2,180,1) -> Half(259200,1:8,1440,8) ***************
[08/10/2023-11:16:34] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(onnx::Conv_651 -> <out>) (Reformat)
[08/10/2023-11:16:34] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.237248
[08/10/2023-11:16:34] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.119269
[08/10/2023-11:16:34] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.11392
[08/10/2023-11:16:34] [V] [TRT] Fastest Tactic: 0x0000000000000000 Time: 0.11392
[08/10/2023-11:16:34] [V] [TRT] *************** Autotuning Reformat: Half(1036800,32400:2,180,1) -> Half(129600,1:16,720,4) ***************
[08/10/2023-11:16:34] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(onnx::Conv_651 -> <out>) (Reformat)
[08/10/2023-11:16:34] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.237193
[08/10/2023-11:16:34] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.11928
[08/10/2023-11:16:34] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.237211
[08/10/2023-11:16:34] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 0.11928
[08/10/2023-11:16:34] [V] [TRT] *************** Autotuning Reformat: Half(259200,1:8,1440,8) -> Float(2073600,32400,180,1) ***************
[08/10/2023-11:16:34] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(onnx::Conv_651 -> <out>) (Reformat)
[08/10/2023-11:16:34] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.324352
[08/10/2023-11:16:34] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.131438
[08/10/2023-11:16:34] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.112725
[08/10/2023-11:16:34] [V] [TRT] Fastest Tactic: 0x0000000000000000 Time: 0.112725
[08/10/2023-11:16:34] [V] [TRT] *************** Autotuning Reformat: Half(259200,1:8,1440,8) -> Float(2073600,1,11520,64) ***************
[08/10/2023-11:16:34] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(onnx::Conv_651 -> <out>) (Reformat)
[08/10/2023-11:16:34] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.208878
[08/10/2023-11:16:34] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.105051
[08/10/2023-11:16:34] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.208786
[08/10/2023-11:16:34] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 0.105051
[08/10/2023-11:16:34] [V] [TRT] *************** Autotuning Reformat: Half(259200,1:8,1440,8) -> Float(518400,1:4,2880,16) ***************
[08/10/2023-11:16:34] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(onnx::Conv_651 -> <out>) (Reformat)
[08/10/2023-11:16:34] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.24592
[08/10/2023-11:16:34] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.104126
[08/10/2023-11:16:34] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.245262
[08/10/2023-11:16:34] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 0.104126
[08/10/2023-11:16:34] [V] [TRT] *************** Autotuning Reformat: Half(259200,1:8,1440,8) -> Half(2073600,32400,180,1) ***************
[08/10/2023-11:16:34] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(onnx::Conv_651 -> <out>) (Reformat)
[08/10/2023-11:16:34] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.315433
[08/10/2023-11:16:34] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.443054
[08/10/2023-11:16:34] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.0932411
[08/10/2023-11:16:34] [V] [TRT] Fastest Tactic: 0x0000000000000000 Time: 0.0932411
[08/10/2023-11:16:34] [V] [TRT] *************** Autotuning Reformat: Half(259200,1:8,1440,8) -> Half(1036800,32400:2,180,1) ***************
[08/10/2023-11:16:34] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(onnx::Conv_651 -> <out>) (Reformat)
[08/10/2023-11:16:34] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.325335
[08/10/2023-11:16:34] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.158167
[08/10/2023-11:16:34] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.0917943
[08/10/2023-11:16:34] [V] [TRT] Fastest Tactic: 0x0000000000000000 Time: 0.0917943
[08/10/2023-11:16:34] [V] [TRT] *************** Autotuning Reformat: Half(259200,1:8,1440,8) -> Half(129600,1:16,720,4) ***************
[08/10/2023-11:16:34] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(onnx::Conv_651 -> <out>) (Reformat)
[08/10/2023-11:16:34] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.227895
[08/10/2023-11:16:34] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.103776
[08/10/2023-11:16:34] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.227182
[08/10/2023-11:16:34] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 0.103776
[08/10/2023-11:16:34] [V] [TRT] *************** Autotuning Reformat: Half(129600,1:16,720,4) -> Float(2073600,32400,180,1) ***************
[08/10/2023-11:16:34] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(onnx::Conv_651 -> <out>) (Reformat)
[08/10/2023-11:16:34] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.361678
[08/10/2023-11:16:34] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.132386
[08/10/2023-11:16:34] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.376818
[08/10/2023-11:16:34] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 0.132386
[08/10/2023-11:16:34] [V] [TRT] *************** Autotuning Reformat: Half(129600,1:16,720,4) -> Float(2073600,1,11520,64) ***************
[08/10/2023-11:16:34] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(onnx::Conv_651 -> <out>) (Reformat)
[08/10/2023-11:16:34] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.208805
[08/10/2023-11:16:34] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.10445
[08/10/2023-11:16:34] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.208594
[08/10/2023-11:16:34] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 0.10445
[08/10/2023-11:16:34] [V] [TRT] *************** Autotuning Reformat: Half(129600,1:16,720,4) -> Float(518400,1:4,2880,16) ***************
[08/10/2023-11:16:34] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(onnx::Conv_651 -> <out>) (Reformat)
[08/10/2023-11:16:34] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.245463
[08/10/2023-11:16:34] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.104384
[08/10/2023-11:16:34] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.244923
[08/10/2023-11:16:34] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 0.104384
[08/10/2023-11:16:34] [V] [TRT] *************** Autotuning Reformat: Half(129600,1:16,720,4) -> Half(2073600,32400,180,1) ***************
[08/10/2023-11:16:34] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(onnx::Conv_651 -> <out>) (Reformat)
[08/10/2023-11:16:34] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.304384
[08/10/2023-11:16:34] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.442638
[08/10/2023-11:16:34] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.348786
[08/10/2023-11:16:34] [V] [TRT] Fastest Tactic: 0x00000000000003e8 Time: 0.304384
[08/10/2023-11:16:34] [V] [TRT] *************** Autotuning Reformat: Half(129600,1:16,720,4) -> Half(1036800,32400:2,180,1) ***************
[08/10/2023-11:16:34] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(onnx::Conv_651 -> <out>) (Reformat)
[08/10/2023-11:16:34] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.325714
[08/10/2023-11:16:34] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.163195
[08/10/2023-11:16:34] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.34213
[08/10/2023-11:16:34] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 0.163195
[08/10/2023-11:16:34] [V] [TRT] *************** Autotuning Reformat: Half(129600,1:16,720,4) -> Half(259200,1:8,1440,8) ***************
[08/10/2023-11:16:34] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(onnx::Conv_651 -> <out>) (Reformat)
[08/10/2023-11:16:34] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.233371
[08/10/2023-11:16:34] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.106379
[08/10/2023-11:16:34] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.233083
[08/10/2023-11:16:34] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 0.106379
[08/10/2023-11:16:34] [V] [TRT] =============== Computing reformatting costs
[08/10/2023-11:16:34] [V] [TRT] *************** Autotuning Reformat: Float(16588800,32400,180,1) -> Float(16588800,1,92160,512) ***************
[08/10/2023-11:16:34] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(<in> -> Conv_62 + Relu_63 || Conv_65 + Relu_66 || Conv_68 + Relu_69 || Conv_71 + Relu_72 || Conv_74 + Relu_75 || Conv_77 + Relu_78 || Conv_80 + Relu_81 || Conv_83 + Relu_84) (Reformat)
[08/10/2023-11:16:34] [V] [TRT] Tactic: 0x00000000000003e8 Time: 5.51132
[08/10/2023-11:16:34] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.935296
[08/10/2023-11:16:35] [V] [TRT] Tactic: 0x0000000000000000 Time: 5.49401
[08/10/2023-11:16:35] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 0.935296
[08/10/2023-11:16:35] [V] [TRT] *************** Autotuning Reformat: Float(16588800,32400,180,1) -> Float(4147200,1:4,23040,128) ***************
[08/10/2023-11:16:35] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(<in> -> Conv_62 + Relu_63 || Conv_65 + Relu_66 || Conv_68 + Relu_69 || Conv_71 + Relu_72 || Conv_74 + Relu_75 || Conv_77 + Relu_78 || Conv_80 + Relu_81 || Conv_83 + Relu_84) (Reformat)
[08/10/2023-11:16:35] [V] [TRT] Tactic: 0x00000000000003e8 Time: 5.55378
[08/10/2023-11:16:35] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.932869
[08/10/2023-11:16:35] [V] [TRT] Tactic: 0x0000000000000000 Time: 5.68587
[08/10/2023-11:16:35] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 0.932869
[08/10/2023-11:16:35] [V] [TRT] *************** Autotuning Reformat: Float(16588800,32400,180,1) -> Half(16588800,32400,180,1) ***************
[08/10/2023-11:16:35] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(<in> -> Conv_62 + Relu_63 || Conv_65 + Relu_66 || Conv_68 + Relu_69 || Conv_71 + Relu_72 || Conv_74 + Relu_75 || Conv_77 + Relu_78 || Conv_80 + Relu_81 || Conv_83 + Relu_84) (Reformat)
[08/10/2023-11:16:35] [V] [TRT] Tactic: 0x00000000000003e8 Time: 1.0029
[08/10/2023-11:16:35] [V] [TRT] Tactic: 0x00000000000003ea Time: 1.20852
[08/10/2023-11:16:35] [V] [TRT] Tactic: 0x0000000000000000 Time: 1.71785
[08/10/2023-11:16:35] [V] [TRT] Fastest Tactic: 0x00000000000003e8 Time: 1.0029
[08/10/2023-11:16:35] [V] [TRT] *************** Autotuning Reformat: Float(16588800,32400,180,1) -> Half(8294400,32400:2,180,1) ***************
[08/10/2023-11:16:35] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(<in> -> Conv_62 + Relu_63 || Conv_65 + Relu_66 || Conv_68 + Relu_69 || Conv_71 + Relu_72 || Conv_74 + Relu_75 || Conv_77 + Relu_78 || Conv_80 + Relu_81 || Conv_83 + Relu_84) (Reformat)
[08/10/2023-11:16:35] [V] [TRT] Tactic: 0x00000000000003e8 Time: 1.52869
[08/10/2023-11:16:35] [V] [TRT] Tactic: 0x00000000000003ea Time: 1.39552
[08/10/2023-11:16:35] [V] [TRT] Tactic: 0x0000000000000000 Time: 1.03594
[08/10/2023-11:16:35] [V] [TRT] Fastest Tactic: 0x0000000000000000 Time: 1.03594
[08/10/2023-11:16:35] [V] [TRT] *************** Autotuning Reformat: Float(16588800,32400,180,1) -> Half(2073600,1:8,11520,64) ***************
[08/10/2023-11:16:35] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(<in> -> Conv_62 + Relu_63 || Conv_65 + Relu_66 || Conv_68 + Relu_69 || Conv_71 + Relu_72 || Conv_74 + Relu_75 || Conv_77 + Relu_78 || Conv_80 + Relu_81 || Conv_83 + Relu_84) (Reformat)
[08/10/2023-11:16:35] [V] [TRT] Tactic: 0x00000000000003e8 Time: 5.46349
[08/10/2023-11:16:35] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.88421
[08/10/2023-11:16:35] [V] [TRT] Tactic: 0x0000000000000000 Time: 1.04736
[08/10/2023-11:16:35] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 0.88421
[08/10/2023-11:16:35] [V] [TRT] *************** Autotuning Reformat: Float(16588800,32400,180,1) -> Half(1036800,1:16,5760,32) ***************
[08/10/2023-11:16:35] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(<in> -> Conv_62 + Relu_63 || Conv_65 + Relu_66 || Conv_68 + Relu_69 || Conv_71 + Relu_72 || Conv_74 + Relu_75 || Conv_77 + Relu_78 || Conv_80 + Relu_81 || Conv_83 + Relu_84) (Reformat)
[08/10/2023-11:16:35] [V] [TRT] Tactic: 0x00000000000003e8 Time: 5.36356
[08/10/2023-11:16:35] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.867003
[08/10/2023-11:16:35] [V] [TRT] Tactic: 0x0000000000000000 Time: 5.3419
[08/10/2023-11:16:35] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 0.867003
[08/10/2023-11:16:35] [V] [TRT] *************** Autotuning Reformat: Float(16588800,1,92160,512) -> Float(16588800,32400,180,1) ***************
[08/10/2023-11:16:35] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(<in> -> Conv_62 + Relu_63 || Conv_65 + Relu_66 || Conv_68 + Relu_69 || Conv_71 + Relu_72 || Conv_74 + Relu_75 || Conv_77 + Relu_78 || Conv_80 + Relu_81 || Conv_83 + Relu_84) (Reformat)
[08/10/2023-11:16:35] [V] [TRT] Tactic: 0x00000000000003e8 Time: 3.51038
[08/10/2023-11:16:35] [V] [TRT] Tactic: 0x00000000000003ea Time: 1.03227
[08/10/2023-11:16:35] [V] [TRT] Tactic: 0x0000000000000000 Time: 3.53389
[08/10/2023-11:16:35] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 1.03227
[08/10/2023-11:16:35] [V] [TRT] *************** Autotuning Reformat: Float(16588800,1,92160,512) -> Float(4147200,1:4,23040,128) ***************
[08/10/2023-11:16:35] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(<in> -> Conv_62 + Relu_63 || Conv_65 + Relu_66 || Conv_68 + Relu_69 || Conv_71 + Relu_72 || Conv_74 + Relu_75 || Conv_77 + Relu_78 || Conv_80 + Relu_81 || Conv_83 + Relu_84) (Reformat)
[08/10/2023-11:16:35] [V] [TRT] Tactic: 0x00000000000003e8 Time: 1.63051
[08/10/2023-11:16:35] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.799538
[08/10/2023-11:16:35] [V] [TRT] Tactic: 0x0000000000000000 Time: 1.59534
[08/10/2023-11:16:35] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 0.799538
[08/10/2023-11:16:35] [V] [TRT] *************** Autotuning Reformat: Float(16588800,1,92160,512) -> Half(16588800,32400,180,1) ***************
[08/10/2023-11:16:35] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(<in> -> Conv_62 + Relu_63 || Conv_65 + Relu_66 || Conv_68 + Relu_69 || Conv_71 + Relu_72 || Conv_74 + Relu_75 || Conv_77 + Relu_78 || Conv_80 + Relu_81 || Conv_83 + Relu_84) (Reformat)
[08/10/2023-11:16:35] [V] [TRT] Tactic: 0x00000000000003e8 Time: 3.40924
[08/10/2023-11:16:35] [V] [TRT] Tactic: 0x00000000000003ea Time: 1.03039
[08/10/2023-11:16:35] [V] [TRT] Tactic: 0x0000000000000000 Time: 3.19895
[08/10/2023-11:16:35] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 1.03039
[08/10/2023-11:16:35] [V] [TRT] *************** Autotuning Reformat: Float(16588800,1,92160,512) -> Half(8294400,32400:2,180,1) ***************
[08/10/2023-11:16:35] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(<in> -> Conv_62 + Relu_63 || Conv_65 + Relu_66 || Conv_68 + Relu_69 || Conv_71 + Relu_72 || Conv_74 + Relu_75 || Conv_77 + Relu_78 || Conv_80 + Relu_81 || Conv_83 + Relu_84) (Reformat)
[08/10/2023-11:16:35] [V] [TRT] Tactic: 0x00000000000003e8 Time: 3.25512
[08/10/2023-11:16:35] [V] [TRT] Tactic: 0x00000000000003ea Time: 1.43685
[08/10/2023-11:16:35] [V] [TRT] Tactic: 0x0000000000000000 Time: 3.42834
[08/10/2023-11:16:35] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 1.43685
[08/10/2023-11:16:35] [V] [TRT] *************** Autotuning Reformat: Float(16588800,1,92160,512) -> Half(2073600,1:8,11520,64) ***************
[08/10/2023-11:16:35] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(<in> -> Conv_62 + Relu_63 || Conv_65 + Relu_66 || Conv_68 + Relu_69 || Conv_71 + Relu_72 || Conv_74 + Relu_75 || Conv_77 + Relu_78 || Conv_80 + Relu_81 || Conv_83 + Relu_84) (Reformat)
[08/10/2023-11:16:35] [V] [TRT] Tactic: 0x00000000000003e8 Time: 1.60213
[08/10/2023-11:16:35] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.763433
[08/10/2023-11:16:35] [V] [TRT] Tactic: 0x0000000000000000 Time: 1.60282
[08/10/2023-11:16:35] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 0.763433
[08/10/2023-11:16:35] [V] [TRT] *************** Autotuning Reformat: Float(16588800,1,92160,512) -> Half(1036800,1:16,5760,32) ***************
[08/10/2023-11:16:35] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(<in> -> Conv_62 + Relu_63 || Conv_65 + Relu_66 || Conv_68 + Relu_69 || Conv_71 + Relu_72 || Conv_74 + Relu_75 || Conv_77 + Relu_78 || Conv_80 + Relu_81 || Conv_83 + Relu_84) (Reformat)
[08/10/2023-11:16:35] [V] [TRT] Tactic: 0x00000000000003e8 Time: 1.60201
[08/10/2023-11:16:35] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.764407
[08/10/2023-11:16:35] [V] [TRT] Tactic: 0x0000000000000000 Time: 1.59981
[08/10/2023-11:16:35] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 0.764407
[08/10/2023-11:16:35] [V] [TRT] *************** Autotuning Reformat: Float(4147200,1:4,23040,128) -> Float(16588800,32400,180,1) ***************
[08/10/2023-11:16:35] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(<in> -> Conv_62 + Relu_63 || Conv_65 + Relu_66 || Conv_68 + Relu_69 || Conv_71 + Relu_72 || Conv_74 + Relu_75 || Conv_77 + Relu_78 || Conv_80 + Relu_81 || Conv_83 + Relu_84) (Reformat)
[08/10/2023-11:16:36] [V] [TRT] Tactic: 0x00000000000003e8 Time: 3.37828
[08/10/2023-11:16:36] [V] [TRT] Tactic: 0x00000000000003ea Time: 1.03109
[08/10/2023-11:16:36] [V] [TRT] Tactic: 0x0000000000000000 Time: 3.71382
[08/10/2023-11:16:36] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 1.03109
[08/10/2023-11:16:36] [V] [TRT] *************** Autotuning Reformat: Float(4147200,1:4,23040,128) -> Float(16588800,1,92160,512) ***************
[08/10/2023-11:16:36] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(<in> -> Conv_62 + Relu_63 || Conv_65 + Relu_66 || Conv_68 + Relu_69 || Conv_71 + Relu_72 || Conv_74 + Relu_75 || Conv_77 + Relu_78 || Conv_80 + Relu_81 || Conv_83 + Relu_84) (Reformat)
[08/10/2023-11:16:36] [V] [TRT] Tactic: 0x00000000000003e8 Time: 1.61778
[08/10/2023-11:16:36] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.79728
[08/10/2023-11:16:36] [V] [TRT] Tactic: 0x0000000000000000 Time: 1.58649
[08/10/2023-11:16:36] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 0.79728
[08/10/2023-11:16:36] [V] [TRT] *************** Autotuning Reformat: Float(4147200,1:4,23040,128) -> Half(16588800,32400,180,1) ***************
[08/10/2023-11:16:36] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(<in> -> Conv_62 + Relu_63 || Conv_65 + Relu_66 || Conv_68 + Relu_69 || Conv_71 + Relu_72 || Conv_74 + Relu_75 || Conv_77 + Relu_78 || Conv_80 + Relu_81 || Conv_83 + Relu_84) (Reformat)
[08/10/2023-11:16:36] [V] [TRT] Tactic: 0x00000000000003e8 Time: 2.71442
[08/10/2023-11:16:36] [V] [TRT] Tactic: 0x00000000000003ea Time: 1.03269
[08/10/2023-11:16:36] [V] [TRT] Tactic: 0x0000000000000000 Time: 3.08992
[08/10/2023-11:16:36] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 1.03269
[08/10/2023-11:16:36] [V] [TRT] *************** Autotuning Reformat: Float(4147200,1:4,23040,128) -> Half(8294400,32400:2,180,1) ***************
[08/10/2023-11:16:36] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(<in> -> Conv_62 + Relu_63 || Conv_65 + Relu_66 || Conv_68 + Relu_69 || Conv_71 + Relu_72 || Conv_74 + Relu_75 || Conv_77 + Relu_78 || Conv_80 + Relu_81 || Conv_83 + Relu_84) (Reformat)
[08/10/2023-11:16:36] [V] [TRT] Tactic: 0x00000000000003e8 Time: 3.2433
[08/10/2023-11:16:36] [V] [TRT] Tactic: 0x00000000000003ea Time: 1.41807
[08/10/2023-11:16:36] [V] [TRT] Tactic: 0x0000000000000000 Time: 2.95225
[08/10/2023-11:16:36] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 1.41807
[08/10/2023-11:16:36] [V] [TRT] *************** Autotuning Reformat: Float(4147200,1:4,23040,128) -> Half(2073600,1:8,11520,64) ***************
[08/10/2023-11:16:36] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(<in> -> Conv_62 + Relu_63 || Conv_65 + Relu_66 || Conv_68 + Relu_69 || Conv_71 + Relu_72 || Conv_74 + Relu_75 || Conv_77 + Relu_78 || Conv_80 + Relu_81 || Conv_83 + Relu_84) (Reformat)
[08/10/2023-11:16:36] [V] [TRT] Tactic: 0x00000000000003e8 Time: 1.90331
[08/10/2023-11:16:36] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.763232
[08/10/2023-11:16:36] [V] [TRT] Tactic: 0x0000000000000000 Time: 1.90338
[08/10/2023-11:16:36] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 0.763232
[08/10/2023-11:16:36] [V] [TRT] *************** Autotuning Reformat: Float(4147200,1:4,23040,128) -> Half(1036800,1:16,5760,32) ***************
[08/10/2023-11:16:36] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(<in> -> Conv_62 + Relu_63 || Conv_65 + Relu_66 || Conv_68 + Relu_69 || Conv_71 + Relu_72 || Conv_74 + Relu_75 || Conv_77 + Relu_78 || Conv_80 + Relu_81 || Conv_83 + Relu_84) (Reformat)
[08/10/2023-11:16:36] [V] [TRT] Tactic: 0x00000000000003e8 Time: 1.90273
[08/10/2023-11:16:36] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.763735
[08/10/2023-11:16:36] [V] [TRT] Tactic: 0x0000000000000000 Time: 1.89983
[08/10/2023-11:16:36] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 0.763735
[08/10/2023-11:16:36] [V] [TRT] *************** Autotuning Reformat: Half(16588800,32400,180,1) -> Float(16588800,32400,180,1) ***************
[08/10/2023-11:16:36] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(<in> -> Conv_62 + Relu_63 || Conv_65 + Relu_66 || Conv_68 + Relu_69 || Conv_71 + Relu_72 || Conv_74 + Relu_75 || Conv_77 + Relu_78 || Conv_80 + Relu_81 || Conv_83 + Relu_84) (Reformat)
[08/10/2023-11:16:36] [V] [TRT] Tactic: 0x00000000000003e8 Time: 1.06887
[08/10/2023-11:16:36] [V] [TRT] Tactic: 0x00000000000003ea Time: 1.20393
[08/10/2023-11:16:36] [V] [TRT] Tactic: 0x0000000000000000 Time: 1.55609
[08/10/2023-11:16:36] [V] [TRT] Fastest Tactic: 0x00000000000003e8 Time: 1.06887
[08/10/2023-11:16:36] [V] [TRT] *************** Autotuning Reformat: Half(16588800,32400,180,1) -> Float(16588800,1,92160,512) ***************
[08/10/2023-11:16:36] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(<in> -> Conv_62 + Relu_63 || Conv_65 + Relu_66 || Conv_68 + Relu_69 || Conv_71 + Relu_72 || Conv_74 + Relu_75 || Conv_77 + Relu_78 || Conv_80 + Relu_81 || Conv_83 + Relu_84) (Reformat)
[08/10/2023-11:16:36] [V] [TRT] Tactic: 0x00000000000003e8 Time: 4.64316
[08/10/2023-11:16:36] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.890121
[08/10/2023-11:16:36] [V] [TRT] Tactic: 0x0000000000000000 Time: 4.60271
[08/10/2023-11:16:36] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 0.890121
[08/10/2023-11:16:36] [V] [TRT] *************** Autotuning Reformat: Half(16588800,32400,180,1) -> Float(4147200,1:4,23040,128) ***************
[08/10/2023-11:16:36] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(<in> -> Conv_62 + Relu_63 || Conv_65 + Relu_66 || Conv_68 + Relu_69 || Conv_71 + Relu_72 || Conv_74 + Relu_75 || Conv_77 + Relu_78 || Conv_80 + Relu_81 || Conv_83 + Relu_84) (Reformat)
[08/10/2023-11:16:36] [V] [TRT] Tactic: 0x00000000000003e8 Time: 4.61632
[08/10/2023-11:16:36] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.869234
[08/10/2023-11:16:36] [V] [TRT] Tactic: 0x0000000000000000 Time: 4.59756
[08/10/2023-11:16:36] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 0.869234
[08/10/2023-11:16:36] [V] [TRT] *************** Autotuning Reformat: Half(16588800,32400,180,1) -> Half(8294400,32400:2,180,1) ***************
[08/10/2023-11:16:36] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(<in> -> Conv_62 + Relu_63 || Conv_65 + Relu_66 || Conv_68 + Relu_69 || Conv_71 + Relu_72 || Conv_74 + Relu_75 || Conv_77 + Relu_78 || Conv_80 + Relu_81 || Conv_83 + Relu_84) (Reformat)
[08/10/2023-11:16:36] [V] [TRT] Tactic: 0x00000000000003e8 Time: 1.42609
[08/10/2023-11:16:36] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.803255
[08/10/2023-11:16:36] [V] [TRT] Tactic: 0x0000000000000000 Time: 1.01664
[08/10/2023-11:16:36] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 0.803255
[08/10/2023-11:16:36] [V] [TRT] *************** Autotuning Reformat: Half(16588800,32400,180,1) -> Half(2073600,1:8,11520,64) ***************
[08/10/2023-11:16:36] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(<in> -> Conv_62 + Relu_63 || Conv_65 + Relu_66 || Conv_68 + Relu_69 || Conv_71 + Relu_72 || Conv_74 + Relu_75 || Conv_77 + Relu_78 || Conv_80 + Relu_81 || Conv_83 + Relu_84) (Reformat)
[08/10/2023-11:16:36] [V] [TRT] Tactic: 0x00000000000003e8 Time: 4.74928
[08/10/2023-11:16:36] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.693627
[08/10/2023-11:16:36] [V] [TRT] Tactic: 0x0000000000000000 Time: 1.0262
[08/10/2023-11:16:36] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 0.693627
[08/10/2023-11:16:36] [V] [TRT] *************** Autotuning Reformat: Half(16588800,32400,180,1) -> Half(1036800,1:16,5760,32) ***************
[08/10/2023-11:16:36] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(<in> -> Conv_62 + Relu_63 || Conv_65 + Relu_66 || Conv_68 + Relu_69 || Conv_71 + Relu_72 || Conv_74 + Relu_75 || Conv_77 + Relu_78 || Conv_80 + Relu_81 || Conv_83 + Relu_84) (Reformat)
[08/10/2023-11:16:36] [V] [TRT] Tactic: 0x00000000000003e8 Time: 4.6723
[08/10/2023-11:16:36] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.792887
[08/10/2023-11:16:36] [V] [TRT] Tactic: 0x0000000000000000 Time: 4.8571
[08/10/2023-11:16:36] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 0.792887
[08/10/2023-11:16:36] [V] [TRT] *************** Autotuning Reformat: Half(8294400,32400:2,180,1) -> Float(16588800,32400,180,1) ***************
[08/10/2023-11:16:36] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(<in> -> Conv_62 + Relu_63 || Conv_65 + Relu_66 || Conv_68 + Relu_69 || Conv_71 + Relu_72 || Conv_74 + Relu_75 || Conv_77 + Relu_78 || Conv_80 + Relu_81 || Conv_83 + Relu_84) (Reformat)
[08/10/2023-11:16:36] [V] [TRT] Tactic: 0x00000000000003e8 Time: 1.53149
[08/10/2023-11:16:36] [V] [TRT] Tactic: 0x00000000000003ea Time: 1.0617
[08/10/2023-11:16:36] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.913275
[08/10/2023-11:16:36] [V] [TRT] Fastest Tactic: 0x0000000000000000 Time: 0.913275
[08/10/2023-11:16:36] [V] [TRT] *************** Autotuning Reformat: Half(8294400,32400:2,180,1) -> Float(16588800,1,92160,512) ***************
[08/10/2023-11:16:36] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(<in> -> Conv_62 + Relu_63 || Conv_65 + Relu_66 || Conv_68 + Relu_69 || Conv_71 + Relu_72 || Conv_74 + Relu_75 || Conv_77 + Relu_78 || Conv_80 + Relu_81 || Conv_83 + Relu_84) (Reformat)
[08/10/2023-11:16:36] [V] [TRT] Tactic: 0x00000000000003e8 Time: 3.19547
[08/10/2023-11:16:36] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.908448
[08/10/2023-11:16:37] [V] [TRT] Tactic: 0x0000000000000000 Time: 3.17946
[08/10/2023-11:16:37] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 0.908448
[08/10/2023-11:16:37] [V] [TRT] *************** Autotuning Reformat: Half(8294400,32400:2,180,1) -> Float(4147200,1:4,23040,128) ***************
[08/10/2023-11:16:37] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(<in> -> Conv_62 + Relu_63 || Conv_65 + Relu_66 || Conv_68 + Relu_69 || Conv_71 + Relu_72 || Conv_74 + Relu_75 || Conv_77 + Relu_78 || Conv_80 + Relu_81 || Conv_83 + Relu_84) (Reformat)
[08/10/2023-11:16:37] [V] [TRT] Tactic: 0x00000000000003e8 Time: 3.3926
[08/10/2023-11:16:37] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.888741
[08/10/2023-11:16:37] [V] [TRT] Tactic: 0x0000000000000000 Time: 3.38157
[08/10/2023-11:16:37] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 0.888741
[08/10/2023-11:16:37] [V] [TRT] *************** Autotuning Reformat: Half(8294400,32400:2,180,1) -> Half(16588800,32400,180,1) ***************
[08/10/2023-11:16:37] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(<in> -> Conv_62 + Relu_63 || Conv_65 + Relu_66 || Conv_68 + Relu_69 || Conv_71 + Relu_72 || Conv_74 + Relu_75 || Conv_77 + Relu_78 || Conv_80 + Relu_81 || Conv_83 + Relu_84) (Reformat)
[08/10/2023-11:16:37] [V] [TRT] Tactic: 0x00000000000003e8 Time: 1.38891
[08/10/2023-11:16:37] [V] [TRT] Tactic: 0x00000000000003ea Time: 5.57622
[08/10/2023-11:16:37] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.871227
[08/10/2023-11:16:37] [V] [TRT] Fastest Tactic: 0x0000000000000000 Time: 0.871227
[08/10/2023-11:16:37] [V] [TRT] *************** Autotuning Reformat: Half(8294400,32400:2,180,1) -> Half(2073600,1:8,11520,64) ***************
[08/10/2023-11:16:37] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(<in> -> Conv_62 + Relu_63 || Conv_65 + Relu_66 || Conv_68 + Relu_69 || Conv_71 + Relu_72 || Conv_74 + Relu_75 || Conv_77 + Relu_78 || Conv_80 + Relu_81 || Conv_83 + Relu_84) (Reformat)
[08/10/2023-11:16:37] [V] [TRT] Tactic: 0x00000000000003e8 Time: 3.37663
[08/10/2023-11:16:37] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.885097
[08/10/2023-11:16:37] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.862455
[08/10/2023-11:16:37] [V] [TRT] Fastest Tactic: 0x0000000000000000 Time: 0.862455
[08/10/2023-11:16:37] [V] [TRT] *************** Autotuning Reformat: Half(8294400,32400:2,180,1) -> Half(1036800,1:16,5760,32) ***************
[08/10/2023-11:16:37] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(<in> -> Conv_62 + Relu_63 || Conv_65 + Relu_66 || Conv_68 + Relu_69 || Conv_71 + Relu_72 || Conv_74 + Relu_75 || Conv_77 + Relu_78 || Conv_80 + Relu_81 || Conv_83 + Relu_84) (Reformat)
[08/10/2023-11:16:37] [V] [TRT] Tactic: 0x00000000000003e8 Time: 3.31607
[08/10/2023-11:16:37] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.885367
[08/10/2023-11:16:37] [V] [TRT] Tactic: 0x0000000000000000 Time: 3.31661
[08/10/2023-11:16:37] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 0.885367
[08/10/2023-11:16:37] [V] [TRT] *************** Autotuning Reformat: Half(2073600,1:8,11520,64) -> Float(16588800,32400,180,1) ***************
[08/10/2023-11:16:37] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(<in> -> Conv_62 + Relu_63 || Conv_65 + Relu_66 || Conv_68 + Relu_69 || Conv_71 + Relu_72 || Conv_74 + Relu_75 || Conv_77 + Relu_78 || Conv_80 + Relu_81 || Conv_83 + Relu_84) (Reformat)
[08/10/2023-11:16:37] [V] [TRT] Tactic: 0x00000000000003e8 Time: 3.38696
[08/10/2023-11:16:37] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.987424
[08/10/2023-11:16:37] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.812969
[08/10/2023-11:16:37] [V] [TRT] Fastest Tactic: 0x0000000000000000 Time: 0.812969
[08/10/2023-11:16:37] [V] [TRT] *************** Autotuning Reformat: Half(2073600,1:8,11520,64) -> Float(16588800,1,92160,512) ***************
[08/10/2023-11:16:37] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(<in> -> Conv_62 + Relu_63 || Conv_65 + Relu_66 || Conv_68 + Relu_69 || Conv_71 + Relu_72 || Conv_74 + Relu_75 || Conv_77 + Relu_78 || Conv_80 + Relu_81 || Conv_83 + Relu_84) (Reformat)
[08/10/2023-11:16:37] [V] [TRT] Tactic: 0x00000000000003e8 Time: 1.61176
[08/10/2023-11:16:37] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.777257
[08/10/2023-11:16:37] [V] [TRT] Tactic: 0x0000000000000000 Time: 1.6472
[08/10/2023-11:16:37] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 0.777257
[08/10/2023-11:16:37] [V] [TRT] *************** Autotuning Reformat: Half(2073600,1:8,11520,64) -> Float(4147200,1:4,23040,128) ***************
[08/10/2023-11:16:37] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(<in> -> Conv_62 + Relu_63 || Conv_65 + Relu_66 || Conv_68 + Relu_69 || Conv_71 + Relu_72 || Conv_74 + Relu_75 || Conv_77 + Relu_78 || Conv_80 + Relu_81 || Conv_83 + Relu_84) (Reformat)
[08/10/2023-11:16:37] [V] [TRT] Tactic: 0x00000000000003e8 Time: 1.94964
[08/10/2023-11:16:37] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.773673
[08/10/2023-11:16:37] [V] [TRT] Tactic: 0x0000000000000000 Time: 1.90989
[08/10/2023-11:16:37] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 0.773673
[08/10/2023-11:16:37] [V] [TRT] *************** Autotuning Reformat: Half(2073600,1:8,11520,64) -> Half(16588800,32400,180,1) ***************
[08/10/2023-11:16:37] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(<in> -> Conv_62 + Relu_63 || Conv_65 + Relu_66 || Conv_68 + Relu_69 || Conv_71 + Relu_72 || Conv_74 + Relu_75 || Conv_77 + Relu_78 || Conv_80 + Relu_81 || Conv_83 + Relu_84) (Reformat)
[08/10/2023-11:16:37] [V] [TRT] Tactic: 0x00000000000003e8 Time: 3.11351
[08/10/2023-11:16:37] [V] [TRT] Tactic: 0x00000000000003ea Time: 3.82436
[08/10/2023-11:16:37] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.671438
[08/10/2023-11:16:37] [V] [TRT] Fastest Tactic: 0x0000000000000000 Time: 0.671438
[08/10/2023-11:16:37] [V] [TRT] *************** Autotuning Reformat: Half(2073600,1:8,11520,64) -> Half(8294400,32400:2,180,1) ***************
[08/10/2023-11:16:37] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(<in> -> Conv_62 + Relu_63 || Conv_65 + Relu_66 || Conv_68 + Relu_69 || Conv_71 + Relu_72 || Conv_74 + Relu_75 || Conv_77 + Relu_78 || Conv_80 + Relu_81 || Conv_83 + Relu_84) (Reformat)
[08/10/2023-11:16:37] [V] [TRT] Tactic: 0x00000000000003e8 Time: 3.2219
[08/10/2023-11:16:37] [V] [TRT] Tactic: 0x00000000000003ea Time: 1.27488
[08/10/2023-11:16:37] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.665833
[08/10/2023-11:16:37] [V] [TRT] Fastest Tactic: 0x0000000000000000 Time: 0.665833
[08/10/2023-11:16:37] [V] [TRT] *************** Autotuning Reformat: Half(2073600,1:8,11520,64) -> Half(1036800,1:16,5760,32) ***************
[08/10/2023-11:16:37] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(<in> -> Conv_62 + Relu_63 || Conv_65 + Relu_66 || Conv_68 + Relu_69 || Conv_71 + Relu_72 || Conv_74 + Relu_75 || Conv_77 + Relu_78 || Conv_80 + Relu_81 || Conv_83 + Relu_84) (Reformat)
[08/10/2023-11:16:37] [V] [TRT] Tactic: 0x00000000000003e8 Time: 1.80712
[08/10/2023-11:16:37] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.782802
[08/10/2023-11:16:37] [V] [TRT] Tactic: 0x0000000000000000 Time: 1.76875
[08/10/2023-11:16:37] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 0.782802
[08/10/2023-11:16:37] [V] [TRT] *************** Autotuning Reformat: Half(1036800,1:16,5760,32) -> Float(16588800,32400,180,1) ***************
[08/10/2023-11:16:37] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(<in> -> Conv_62 + Relu_63 || Conv_65 + Relu_66 || Conv_68 + Relu_69 || Conv_71 + Relu_72 || Conv_74 + Relu_75 || Conv_77 + Relu_78 || Conv_80 + Relu_81 || Conv_83 + Relu_84) (Reformat)
[08/10/2023-11:16:37] [V] [TRT] Tactic: 0x00000000000003e8 Time: 3.30385
[08/10/2023-11:16:37] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.988091
[08/10/2023-11:16:37] [V] [TRT] Tactic: 0x0000000000000000 Time: 3.40544
[08/10/2023-11:16:37] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 0.988091
[08/10/2023-11:16:37] [V] [TRT] *************** Autotuning Reformat: Half(1036800,1:16,5760,32) -> Float(16588800,1,92160,512) ***************
[08/10/2023-11:16:37] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(<in> -> Conv_62 + Relu_63 || Conv_65 + Relu_66 || Conv_68 + Relu_69 || Conv_71 + Relu_72 || Conv_74 + Relu_75 || Conv_77 + Relu_78 || Conv_80 + Relu_81 || Conv_83 + Relu_84) (Reformat)
[08/10/2023-11:16:37] [V] [TRT] Tactic: 0x00000000000003e8 Time: 1.61071
[08/10/2023-11:16:37] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.772942
[08/10/2023-11:16:37] [V] [TRT] Tactic: 0x0000000000000000 Time: 1.61104
[08/10/2023-11:16:37] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 0.772942
[08/10/2023-11:16:37] [V] [TRT] *************** Autotuning Reformat: Half(1036800,1:16,5760,32) -> Float(4147200,1:4,23040,128) ***************
[08/10/2023-11:16:37] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(<in> -> Conv_62 + Relu_63 || Conv_65 + Relu_66 || Conv_68 + Relu_69 || Conv_71 + Relu_72 || Conv_74 + Relu_75 || Conv_77 + Relu_78 || Conv_80 + Relu_81 || Conv_83 + Relu_84) (Reformat)
[08/10/2023-11:16:37] [V] [TRT] Tactic: 0x00000000000003e8 Time: 1.9069
[08/10/2023-11:16:37] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.772896
[08/10/2023-11:16:37] [V] [TRT] Tactic: 0x0000000000000000 Time: 1.94708
[08/10/2023-11:16:37] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 0.772896
[08/10/2023-11:16:37] [V] [TRT] *************** Autotuning Reformat: Half(1036800,1:16,5760,32) -> Half(16588800,32400,180,1) ***************
[08/10/2023-11:16:37] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(<in> -> Conv_62 + Relu_63 || Conv_65 + Relu_66 || Conv_68 + Relu_69 || Conv_71 + Relu_72 || Conv_74 + Relu_75 || Conv_77 + Relu_78 || Conv_80 + Relu_81 || Conv_83 + Relu_84) (Reformat)
[08/10/2023-11:16:37] [V] [TRT] Tactic: 0x00000000000003e8 Time: 3.19491
[08/10/2023-11:16:37] [V] [TRT] Tactic: 0x00000000000003ea Time: 3.82928
[08/10/2023-11:16:37] [V] [TRT] Tactic: 0x0000000000000000 Time: 3.172
[08/10/2023-11:16:37] [V] [TRT] Fastest Tactic: 0x0000000000000000 Time: 3.172
[08/10/2023-11:16:37] [V] [TRT] *************** Autotuning Reformat: Half(1036800,1:16,5760,32) -> Half(8294400,32400:2,180,1) ***************
[08/10/2023-11:16:37] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(<in> -> Conv_62 + Relu_63 || Conv_65 + Relu_66 || Conv_68 + Relu_69 || Conv_71 + Relu_72 || Conv_74 + Relu_75 || Conv_77 + Relu_78 || Conv_80 + Relu_81 || Conv_83 + Relu_84) (Reformat)
[08/10/2023-11:16:37] [V] [TRT] Tactic: 0x00000000000003e8 Time: 2.92352
[08/10/2023-11:16:37] [V] [TRT] Tactic: 0x00000000000003ea Time: 1.27211
[08/10/2023-11:16:38] [V] [TRT] Tactic: 0x0000000000000000 Time: 3.39797
[08/10/2023-11:16:38] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 1.27211
[08/10/2023-11:16:38] [V] [TRT] *************** Autotuning Reformat: Half(1036800,1:16,5760,32) -> Half(2073600,1:8,11520,64) ***************
[08/10/2023-11:16:38] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(<in> -> Conv_62 + Relu_63 || Conv_65 + Relu_66 || Conv_68 + Relu_69 || Conv_71 + Relu_72 || Conv_74 + Relu_75 || Conv_77 + Relu_78 || Conv_80 + Relu_81 || Conv_83 + Relu_84) (Reformat)
[08/10/2023-11:16:38] [V] [TRT] Tactic: 0x00000000000003e8 Time: 1.78593
[08/10/2023-11:16:38] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.783762
[08/10/2023-11:16:38] [V] [TRT] Tactic: 0x0000000000000000 Time: 1.808
[08/10/2023-11:16:38] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 0.783762
[08/10/2023-11:16:38] [V] [TRT] =============== Computing reformatting costs
[08/10/2023-11:16:38] [V] [TRT] *************** Autotuning Reformat: Float(16588800,32400,180,1) -> Float(16588800,1,92160,512) ***************
[08/10/2023-11:16:38] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(input.184 -> <out>) (Reformat)
[08/10/2023-11:16:38] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.388608
[08/10/2023-11:16:38] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.129003
[08/10/2023-11:16:38] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.380873
[08/10/2023-11:16:38] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 0.129003
[08/10/2023-11:16:38] [V] [TRT] *************** Autotuning Reformat: Float(16588800,32400,180,1) -> Float(4147200,1:4,23040,128) ***************
[08/10/2023-11:16:38] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(input.184 -> <out>) (Reformat)
[08/10/2023-11:16:38] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.399909
[08/10/2023-11:16:38] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.129184
[08/10/2023-11:16:38] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.39936
[08/10/2023-11:16:38] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 0.129184
[08/10/2023-11:16:38] [V] [TRT] *************** Autotuning Reformat: Float(16588800,32400,180,1) -> Half(16588800,32400,180,1) ***************
[08/10/2023-11:16:38] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(input.184 -> <out>) (Reformat)
[08/10/2023-11:16:38] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.145769
[08/10/2023-11:16:38] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.18891
[08/10/2023-11:16:38] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.145957
[08/10/2023-11:16:38] [V] [TRT] Fastest Tactic: 0x00000000000003e8 Time: 0.145769
[08/10/2023-11:16:38] [V] [TRT] *************** Autotuning Reformat: Float(16588800,32400,180,1) -> Half(8294400,32400:2,180,1) ***************
[08/10/2023-11:16:38] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(input.184 -> <out>) (Reformat)
[08/10/2023-11:16:38] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.198798
[08/10/2023-11:16:38] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.178053
[08/10/2023-11:16:38] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.197701
[08/10/2023-11:16:38] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 0.178053
[08/10/2023-11:16:38] [V] [TRT] *************** Autotuning Reformat: Float(16588800,32400,180,1) -> Half(2073600,1:8,11520,64) ***************
[08/10/2023-11:16:38] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(input.184 -> <out>) (Reformat)
[08/10/2023-11:16:38] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.405111
[08/10/2023-11:16:38] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.116622
[08/10/2023-11:16:38] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.405431
[08/10/2023-11:16:38] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 0.116622
[08/10/2023-11:16:38] [V] [TRT] *************** Autotuning Reformat: Float(16588800,32400,180,1) -> Half(1036800,1:16,5760,32) ***************
[08/10/2023-11:16:38] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(input.184 -> <out>) (Reformat)
[08/10/2023-11:16:38] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.405467
[08/10/2023-11:16:38] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.116869
[08/10/2023-11:16:38] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.405312
[08/10/2023-11:16:38] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 0.116869
[08/10/2023-11:16:38] [V] [TRT] *************** Autotuning Reformat: Float(16588800,1,92160,512) -> Float(16588800,32400,180,1) ***************
[08/10/2023-11:16:38] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(input.184 -> <out>) (Reformat)
[08/10/2023-11:16:38] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.456768
[08/10/2023-11:16:38] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.145906
[08/10/2023-11:16:38] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.433664
[08/10/2023-11:16:38] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 0.145906
[08/10/2023-11:16:38] [V] [TRT] *************** Autotuning Reformat: Float(16588800,1,92160,512) -> Float(4147200,1:4,23040,128) ***************
[08/10/2023-11:16:38] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(input.184 -> <out>) (Reformat)
[08/10/2023-11:16:38] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.210441
[08/10/2023-11:16:38] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.115266
[08/10/2023-11:16:38] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.210793
[08/10/2023-11:16:38] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 0.115266
[08/10/2023-11:16:38] [V] [TRT] *************** Autotuning Reformat: Float(16588800,1,92160,512) -> Half(16588800,32400,180,1) ***************
[08/10/2023-11:16:38] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(input.184 -> <out>) (Reformat)
[08/10/2023-11:16:38] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.388599
[08/10/2023-11:16:38] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.13941
[08/10/2023-11:16:38] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.351493
[08/10/2023-11:16:38] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 0.13941
[08/10/2023-11:16:38] [V] [TRT] *************** Autotuning Reformat: Float(16588800,1,92160,512) -> Half(8294400,32400:2,180,1) ***************
[08/10/2023-11:16:38] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(input.184 -> <out>) (Reformat)
[08/10/2023-11:16:38] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.443685
[08/10/2023-11:16:38] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.174907
[08/10/2023-11:16:38] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.414409
[08/10/2023-11:16:38] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 0.174907
[08/10/2023-11:16:38] [V] [TRT] *************** Autotuning Reformat: Float(16588800,1,92160,512) -> Half(2073600,1:8,11520,64) ***************
[08/10/2023-11:16:38] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(input.184 -> <out>) (Reformat)
[08/10/2023-11:16:38] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.207296
[08/10/2023-11:16:38] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.103399
[08/10/2023-11:16:38] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.207095
[08/10/2023-11:16:38] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 0.103399
[08/10/2023-11:16:38] [V] [TRT] *************** Autotuning Reformat: Float(16588800,1,92160,512) -> Half(1036800,1:16,5760,32) ***************
[08/10/2023-11:16:38] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(input.184 -> <out>) (Reformat)
[08/10/2023-11:16:38] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.207223
[08/10/2023-11:16:38] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.103842
[08/10/2023-11:16:38] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.207099
[08/10/2023-11:16:38] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 0.103842
[08/10/2023-11:16:38] [V] [TRT] *************** Autotuning Reformat: Float(4147200,1:4,23040,128) -> Float(16588800,32400,180,1) ***************
[08/10/2023-11:16:38] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(input.184 -> <out>) (Reformat)
[08/10/2023-11:16:38] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.432091
[08/10/2023-11:16:38] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.144018
[08/10/2023-11:16:38] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.40709
[08/10/2023-11:16:38] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 0.144018
[08/10/2023-11:16:38] [V] [TRT] *************** Autotuning Reformat: Float(4147200,1:4,23040,128) -> Float(16588800,1,92160,512) ***************
[08/10/2023-11:16:38] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(input.184 -> <out>) (Reformat)
[08/10/2023-11:16:38] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.209856
[08/10/2023-11:16:38] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.115886
[08/10/2023-11:16:38] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.205285
[08/10/2023-11:16:38] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 0.115886
[08/10/2023-11:16:38] [V] [TRT] *************** Autotuning Reformat: Float(4147200,1:4,23040,128) -> Half(16588800,32400,180,1) ***************
[08/10/2023-11:16:38] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(input.184 -> <out>) (Reformat)
[08/10/2023-11:16:38] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.391703
[08/10/2023-11:16:38] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.139223
[08/10/2023-11:16:38] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.363826
[08/10/2023-11:16:38] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 0.139223
[08/10/2023-11:16:38] [V] [TRT] *************** Autotuning Reformat: Float(4147200,1:4,23040,128) -> Half(8294400,32400:2,180,1) ***************
[08/10/2023-11:16:38] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(input.184 -> <out>) (Reformat)
[08/10/2023-11:16:38] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.414011
[08/10/2023-11:16:38] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.175909
[08/10/2023-11:16:38] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.422514
[08/10/2023-11:16:38] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 0.175909
[08/10/2023-11:16:38] [V] [TRT] *************** Autotuning Reformat: Float(4147200,1:4,23040,128) -> Half(2073600,1:8,11520,64) ***************
[08/10/2023-11:16:38] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(input.184 -> <out>) (Reformat)
[08/10/2023-11:16:38] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.245097
[08/10/2023-11:16:38] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.104135
[08/10/2023-11:16:38] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.244325
[08/10/2023-11:16:38] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 0.104135
[08/10/2023-11:16:38] [V] [TRT] *************** Autotuning Reformat: Float(4147200,1:4,23040,128) -> Half(1036800,1:16,5760,32) ***************
[08/10/2023-11:16:38] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(input.184 -> <out>) (Reformat)
[08/10/2023-11:16:38] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.244517
[08/10/2023-11:16:38] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.103847
[08/10/2023-11:16:38] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.244393
[08/10/2023-11:16:38] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 0.103847
[08/10/2023-11:16:38] [V] [TRT] *************** Autotuning Reformat: Half(16588800,32400,180,1) -> Float(16588800,32400,180,1) ***************
[08/10/2023-11:16:38] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(input.184 -> <out>) (Reformat)
[08/10/2023-11:16:38] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.152544
[08/10/2023-11:16:38] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.188539
[08/10/2023-11:16:38] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.152512
[08/10/2023-11:16:38] [V] [TRT] Fastest Tactic: 0x0000000000000000 Time: 0.152512
[08/10/2023-11:16:38] [V] [TRT] *************** Autotuning Reformat: Half(16588800,32400,180,1) -> Float(16588800,1,92160,512) ***************
[08/10/2023-11:16:38] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(input.184 -> <out>) (Reformat)
[08/10/2023-11:16:38] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.319922
[08/10/2023-11:16:38] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.117607
[08/10/2023-11:16:38] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.319653
[08/10/2023-11:16:38] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 0.117607
[08/10/2023-11:16:38] [V] [TRT] *************** Autotuning Reformat: Half(16588800,32400,180,1) -> Float(4147200,1:4,23040,128) ***************
[08/10/2023-11:16:38] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(input.184 -> <out>) (Reformat)
[08/10/2023-11:16:38] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.349531
[08/10/2023-11:16:38] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.117753
[08/10/2023-11:16:38] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.349573
[08/10/2023-11:16:38] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 0.117753
[08/10/2023-11:16:38] [V] [TRT] *************** Autotuning Reformat: Half(16588800,32400,180,1) -> Half(8294400,32400:2,180,1) ***************
[08/10/2023-11:16:38] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(input.184 -> <out>) (Reformat)
[08/10/2023-11:16:38] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.189751
[08/10/2023-11:16:38] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.109977
[08/10/2023-11:16:38] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.18901
[08/10/2023-11:16:38] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 0.109977
[08/10/2023-11:16:38] [V] [TRT] *************** Autotuning Reformat: Half(16588800,32400,180,1) -> Half(2073600,1:8,11520,64) ***************
[08/10/2023-11:16:38] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(input.184 -> <out>) (Reformat)
[08/10/2023-11:16:38] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.358478
[08/10/2023-11:16:38] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.0986309
[08/10/2023-11:16:38] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.359058
[08/10/2023-11:16:38] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 0.0986309
[08/10/2023-11:16:38] [V] [TRT] *************** Autotuning Reformat: Half(16588800,32400,180,1) -> Half(1036800,1:16,5760,32) ***************
[08/10/2023-11:16:38] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(input.184 -> <out>) (Reformat)
[08/10/2023-11:16:38] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.360402
[08/10/2023-11:16:38] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.0996023
[08/10/2023-11:16:38] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.352142
[08/10/2023-11:16:38] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 0.0996023
[08/10/2023-11:16:38] [V] [TRT] *************** Autotuning Reformat: Half(8294400,32400:2,180,1) -> Float(16588800,32400,180,1) ***************
[08/10/2023-11:16:38] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(input.184 -> <out>) (Reformat)
[08/10/2023-11:16:38] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.198725
[08/10/2023-11:16:38] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.144247
[08/10/2023-11:16:38] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.120901
[08/10/2023-11:16:38] [V] [TRT] Fastest Tactic: 0x0000000000000000 Time: 0.120901
[08/10/2023-11:16:38] [V] [TRT] *************** Autotuning Reformat: Half(8294400,32400:2,180,1) -> Float(16588800,1,92160,512) ***************
[08/10/2023-11:16:38] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(input.184 -> <out>) (Reformat)
[08/10/2023-11:16:38] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.228805
[08/10/2023-11:16:38] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.119726
[08/10/2023-11:16:38] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.23243
[08/10/2023-11:16:38] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 0.119726
[08/10/2023-11:16:38] [V] [TRT] *************** Autotuning Reformat: Half(8294400,32400:2,180,1) -> Float(4147200,1:4,23040,128) ***************
[08/10/2023-11:16:38] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(input.184 -> <out>) (Reformat)
[08/10/2023-11:16:38] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.247909
[08/10/2023-11:16:38] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.119621
[08/10/2023-11:16:38] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.247698
[08/10/2023-11:16:38] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 0.119621
[08/10/2023-11:16:38] [V] [TRT] *************** Autotuning Reformat: Half(8294400,32400:2,180,1) -> Half(16588800,32400,180,1) ***************
[08/10/2023-11:16:38] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(input.184 -> <out>) (Reformat)
[08/10/2023-11:16:38] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.179872
[08/10/2023-11:16:38] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.529038
[08/10/2023-11:16:38] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.113262
[08/10/2023-11:16:38] [V] [TRT] Fastest Tactic: 0x0000000000000000 Time: 0.113262
[08/10/2023-11:16:38] [V] [TRT] *************** Autotuning Reformat: Half(8294400,32400:2,180,1) -> Half(2073600,1:8,11520,64) ***************
[08/10/2023-11:16:38] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(input.184 -> <out>) (Reformat)
[08/10/2023-11:16:38] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.237431
[08/10/2023-11:16:38] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.118539
[08/10/2023-11:16:38] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.114139
[08/10/2023-11:16:38] [V] [TRT] Fastest Tactic: 0x0000000000000000 Time: 0.114139
[08/10/2023-11:16:38] [V] [TRT] *************** Autotuning Reformat: Half(8294400,32400:2,180,1) -> Half(1036800,1:16,5760,32) ***************
[08/10/2023-11:16:38] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(input.184 -> <out>) (Reformat)
[08/10/2023-11:16:38] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.237015
[08/10/2023-11:16:38] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.118517
[08/10/2023-11:16:38] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.237029
[08/10/2023-11:16:38] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 0.118517
[08/10/2023-11:16:38] [V] [TRT] *************** Autotuning Reformat: Half(2073600,1:8,11520,64) -> Float(16588800,32400,180,1) ***************
[08/10/2023-11:16:38] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(input.184 -> <out>) (Reformat)
[08/10/2023-11:16:38] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.401792
[08/10/2023-11:16:38] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.134576
[08/10/2023-11:16:38] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.108809
[08/10/2023-11:16:38] [V] [TRT] Fastest Tactic: 0x0000000000000000 Time: 0.108809
[08/10/2023-11:16:38] [V] [TRT] *************** Autotuning Reformat: Half(2073600,1:8,11520,64) -> Float(16588800,1,92160,512) ***************
[08/10/2023-11:16:38] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(input.184 -> <out>) (Reformat)
[08/10/2023-11:16:38] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.213129
[08/10/2023-11:16:38] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.107506
[08/10/2023-11:16:38] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.212987
[08/10/2023-11:16:38] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 0.107506
[08/10/2023-11:16:38] [V] [TRT] *************** Autotuning Reformat: Half(2073600,1:8,11520,64) -> Float(4147200,1:4,23040,128) ***************
[08/10/2023-11:16:38] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(input.184 -> <out>) (Reformat)
[08/10/2023-11:16:38] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.250889
[08/10/2023-11:16:38] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.107456
[08/10/2023-11:16:38] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.250679
[08/10/2023-11:16:38] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 0.107456
[08/10/2023-11:16:38] [V] [TRT] *************** Autotuning Reformat: Half(2073600,1:8,11520,64) -> Half(16588800,32400,180,1) ***************
[08/10/2023-11:16:38] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(input.184 -> <out>) (Reformat)
[08/10/2023-11:16:38] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.330779
[08/10/2023-11:16:38] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.443561
[08/10/2023-11:16:39] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.0911771
[08/10/2023-11:16:39] [V] [TRT] Fastest Tactic: 0x0000000000000000 Time: 0.0911771
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(2073600,1:8,11520,64) -> Half(8294400,32400:2,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(input.184 -> <out>) (Reformat)
[08/10/2023-11:16:39] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.359707
[08/10/2023-11:16:39] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.163374
[08/10/2023-11:16:39] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.0902149
[08/10/2023-11:16:39] [V] [TRT] Fastest Tactic: 0x0000000000000000 Time: 0.0902149
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(2073600,1:8,11520,64) -> Half(1036800,1:16,5760,32) ***************
[08/10/2023-11:16:39] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(input.184 -> <out>) (Reformat)
[08/10/2023-11:16:39] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.228686
[08/10/2023-11:16:39] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.104249
[08/10/2023-11:16:39] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.228475
[08/10/2023-11:16:39] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 0.104249
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(1036800,1:16,5760,32) -> Float(16588800,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(input.184 -> <out>) (Reformat)
[08/10/2023-11:16:39] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.394807
[08/10/2023-11:16:39] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.134624
[08/10/2023-11:16:39] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.390208
[08/10/2023-11:16:39] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 0.134624
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(1036800,1:16,5760,32) -> Float(16588800,1,92160,512) ***************
[08/10/2023-11:16:39] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(input.184 -> <out>) (Reformat)
[08/10/2023-11:16:39] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.209042
[08/10/2023-11:16:39] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.104576
[08/10/2023-11:16:39] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.208256
[08/10/2023-11:16:39] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 0.104576
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(1036800,1:16,5760,32) -> Float(4147200,1:4,23040,128) ***************
[08/10/2023-11:16:39] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(input.184 -> <out>) (Reformat)
[08/10/2023-11:16:39] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.246112
[08/10/2023-11:16:39] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.104921
[08/10/2023-11:16:39] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.245655
[08/10/2023-11:16:39] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 0.104921
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(1036800,1:16,5760,32) -> Half(16588800,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(input.184 -> <out>) (Reformat)
[08/10/2023-11:16:39] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.311712
[08/10/2023-11:16:39] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.443616
[08/10/2023-11:16:39] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.304466
[08/10/2023-11:16:39] [V] [TRT] Fastest Tactic: 0x0000000000000000 Time: 0.304466
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(1036800,1:16,5760,32) -> Half(8294400,32400:2,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(input.184 -> <out>) (Reformat)
[08/10/2023-11:16:39] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.327758
[08/10/2023-11:16:39] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.162706
[08/10/2023-11:16:39] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.341376
[08/10/2023-11:16:39] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 0.162706
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(1036800,1:16,5760,32) -> Half(2073600,1:8,11520,64) ***************
[08/10/2023-11:16:39] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(input.184 -> <out>) (Reformat)
[08/10/2023-11:16:39] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.227515
[08/10/2023-11:16:39] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.104155
[08/10/2023-11:16:39] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.227648
[08/10/2023-11:16:39] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 0.104155
[08/10/2023-11:16:39] [V] [TRT] =============== Computing reformatting costs
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(64800,32400,180,1) -> Half(64800,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(<in> -> reg_0) (Reformat)
[08/10/2023-11:16:39] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.00786117
[08/10/2023-11:16:39] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.0121989
[08/10/2023-11:16:39] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.0110066
[08/10/2023-11:16:39] [V] [TRT] Fastest Tactic: 0x00000000000003e8 Time: 0.00786117
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(64800,1,360,2) -> Half(64800,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(<in> -> reg_0) (Reformat)
[08/10/2023-11:16:39] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.0105721
[08/10/2023-11:16:39] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.0651154
[08/10/2023-11:16:39] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.0104836
[08/10/2023-11:16:39] [V] [TRT] Fastest Tactic: 0x0000000000000000 Time: 0.0104836
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(32400,1:4,180,1) -> Half(64800,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(<in> -> reg_0) (Reformat)
[08/10/2023-11:16:39] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.0111458
[08/10/2023-11:16:39] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.0650926
[08/10/2023-11:16:39] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.0112327
[08/10/2023-11:16:39] [V] [TRT] Fastest Tactic: 0x00000000000003e8 Time: 0.0111458
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(32400,32400:2,180,1) -> Half(64800,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(<in> -> reg_0) (Reformat)
[08/10/2023-11:16:39] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.010928
[08/10/2023-11:16:39] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.187195
[08/10/2023-11:16:39] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.00799746
[08/10/2023-11:16:39] [V] [TRT] Fastest Tactic: 0x0000000000000000 Time: 0.00799746
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(32400,1:8,180,1) -> Half(64800,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(<in> -> reg_0) (Reformat)
[08/10/2023-11:16:39] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.0110291
[08/10/2023-11:16:39] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.189902
[08/10/2023-11:16:39] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.00856309
[08/10/2023-11:16:39] [V] [TRT] Fastest Tactic: 0x0000000000000000 Time: 0.00856309
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(32400,1:16,180,1) -> Half(64800,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(<in> -> reg_0) (Reformat)
[08/10/2023-11:16:39] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.0111775
[08/10/2023-11:16:39] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.195227
[08/10/2023-11:16:39] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.0111852
[08/10/2023-11:16:39] [V] [TRT] Fastest Tactic: 0x00000000000003e8 Time: 0.0111775
[08/10/2023-11:16:39] [V] [TRT] =============== Computing reformatting costs
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(16588800,32400,180,1) -> Float(16588800,1,92160,512) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(16588800,32400,180,1) -> Float(4147200,1:4,23040,128) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(16588800,32400,180,1) -> Half(16588800,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(16588800,32400,180,1) -> Half(8294400,32400:2,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(16588800,32400,180,1) -> Half(2073600,1:8,11520,64) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(16588800,32400,180,1) -> Half(1036800,1:16,5760,32) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(16588800,1,92160,512) -> Float(16588800,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(16588800,1,92160,512) -> Float(4147200,1:4,23040,128) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(16588800,1,92160,512) -> Half(16588800,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(16588800,1,92160,512) -> Half(8294400,32400:2,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(16588800,1,92160,512) -> Half(2073600,1:8,11520,64) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(16588800,1,92160,512) -> Half(1036800,1:16,5760,32) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(4147200,1:4,23040,128) -> Float(16588800,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(4147200,1:4,23040,128) -> Float(16588800,1,92160,512) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(4147200,1:4,23040,128) -> Half(16588800,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(4147200,1:4,23040,128) -> Half(8294400,32400:2,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(4147200,1:4,23040,128) -> Half(2073600,1:8,11520,64) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(4147200,1:4,23040,128) -> Half(1036800,1:16,5760,32) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(16588800,32400,180,1) -> Float(16588800,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(16588800,32400,180,1) -> Float(16588800,1,92160,512) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(16588800,32400,180,1) -> Float(4147200,1:4,23040,128) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(16588800,32400,180,1) -> Half(8294400,32400:2,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(16588800,32400,180,1) -> Half(2073600,1:8,11520,64) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(16588800,32400,180,1) -> Half(1036800,1:16,5760,32) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(8294400,32400:2,180,1) -> Float(16588800,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(8294400,32400:2,180,1) -> Float(16588800,1,92160,512) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(8294400,32400:2,180,1) -> Float(4147200,1:4,23040,128) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(8294400,32400:2,180,1) -> Half(16588800,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(8294400,32400:2,180,1) -> Half(2073600,1:8,11520,64) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(8294400,32400:2,180,1) -> Half(1036800,1:16,5760,32) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(2073600,1:8,11520,64) -> Float(16588800,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(2073600,1:8,11520,64) -> Float(16588800,1,92160,512) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(2073600,1:8,11520,64) -> Float(4147200,1:4,23040,128) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(2073600,1:8,11520,64) -> Half(16588800,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(2073600,1:8,11520,64) -> Half(8294400,32400:2,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(2073600,1:8,11520,64) -> Half(1036800,1:16,5760,32) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(1036800,1:16,5760,32) -> Float(16588800,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(1036800,1:16,5760,32) -> Float(16588800,1,92160,512) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(1036800,1:16,5760,32) -> Float(4147200,1:4,23040,128) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(1036800,1:16,5760,32) -> Half(16588800,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(1036800,1:16,5760,32) -> Half(8294400,32400:2,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(1036800,1:16,5760,32) -> Half(2073600,1:8,11520,64) ***************
[08/10/2023-11:16:39] [V] [TRT] =============== Computing reformatting costs
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(32400,32400,180,1) -> Half(32400,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(<in> -> height_0) (Reformat)
[08/10/2023-11:16:39] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.00644571
[08/10/2023-11:16:39] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.012109
[08/10/2023-11:16:39] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.00854319
[08/10/2023-11:16:39] [V] [TRT] Fastest Tactic: 0x00000000000003e8 Time: 0.00644571
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(32400,1,180,1) -> Half(32400,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(<in> -> height_0) (Reformat)
[08/10/2023-11:16:39] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.00778634
[08/10/2023-11:16:39] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.0115552
[08/10/2023-11:16:39] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.00784794
[08/10/2023-11:16:39] [V] [TRT] Fastest Tactic: 0x00000000000003e8 Time: 0.00778634
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(32400,1:4,180,1) -> Half(32400,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(<in> -> height_0) (Reformat)
[08/10/2023-11:16:39] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.00871637
[08/10/2023-11:16:39] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.0661059
[08/10/2023-11:16:39] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.00869244
[08/10/2023-11:16:39] [V] [TRT] Fastest Tactic: 0x0000000000000000 Time: 0.00869244
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(32400,32400:2,180,1) -> Half(32400,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(<in> -> height_0) (Reformat)
[08/10/2023-11:16:39] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.00837232
[08/10/2023-11:16:39] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.191557
[08/10/2023-11:16:39] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.00654151
[08/10/2023-11:16:39] [V] [TRT] Fastest Tactic: 0x0000000000000000 Time: 0.00654151
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(32400,1:8,180,1) -> Half(32400,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(<in> -> height_0) (Reformat)
[08/10/2023-11:16:39] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.00843321
[08/10/2023-11:16:39] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.195035
[08/10/2023-11:16:39] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.00859644
[08/10/2023-11:16:39] [V] [TRT] Fastest Tactic: 0x00000000000003e8 Time: 0.00843321
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(32400,1:16,180,1) -> Half(32400,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(<in> -> height_0) (Reformat)
[08/10/2023-11:16:39] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.00867872
[08/10/2023-11:16:39] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.194464
[08/10/2023-11:16:39] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.00873976
[08/10/2023-11:16:39] [V] [TRT] Fastest Tactic: 0x00000000000003e8 Time: 0.00867872
[08/10/2023-11:16:39] [V] [TRT] =============== Computing reformatting costs
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(16588800,32400,180,1) -> Float(16588800,1,92160,512) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(16588800,32400,180,1) -> Float(4147200,1:4,23040,128) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(16588800,32400,180,1) -> Half(16588800,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(16588800,32400,180,1) -> Half(8294400,32400:2,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(16588800,32400,180,1) -> Half(2073600,1:8,11520,64) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(16588800,32400,180,1) -> Half(1036800,1:16,5760,32) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(16588800,1,92160,512) -> Float(16588800,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(16588800,1,92160,512) -> Float(4147200,1:4,23040,128) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(16588800,1,92160,512) -> Half(16588800,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(16588800,1,92160,512) -> Half(8294400,32400:2,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(16588800,1,92160,512) -> Half(2073600,1:8,11520,64) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(16588800,1,92160,512) -> Half(1036800,1:16,5760,32) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(4147200,1:4,23040,128) -> Float(16588800,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(4147200,1:4,23040,128) -> Float(16588800,1,92160,512) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(4147200,1:4,23040,128) -> Half(16588800,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(4147200,1:4,23040,128) -> Half(8294400,32400:2,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(4147200,1:4,23040,128) -> Half(2073600,1:8,11520,64) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(4147200,1:4,23040,128) -> Half(1036800,1:16,5760,32) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(16588800,32400,180,1) -> Float(16588800,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(16588800,32400,180,1) -> Float(16588800,1,92160,512) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(16588800,32400,180,1) -> Float(4147200,1:4,23040,128) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(16588800,32400,180,1) -> Half(8294400,32400:2,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(16588800,32400,180,1) -> Half(2073600,1:8,11520,64) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(16588800,32400,180,1) -> Half(1036800,1:16,5760,32) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(8294400,32400:2,180,1) -> Float(16588800,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(8294400,32400:2,180,1) -> Float(16588800,1,92160,512) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(8294400,32400:2,180,1) -> Float(4147200,1:4,23040,128) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(8294400,32400:2,180,1) -> Half(16588800,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(8294400,32400:2,180,1) -> Half(2073600,1:8,11520,64) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(8294400,32400:2,180,1) -> Half(1036800,1:16,5760,32) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(2073600,1:8,11520,64) -> Float(16588800,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(2073600,1:8,11520,64) -> Float(16588800,1,92160,512) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(2073600,1:8,11520,64) -> Float(4147200,1:4,23040,128) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(2073600,1:8,11520,64) -> Half(16588800,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(2073600,1:8,11520,64) -> Half(8294400,32400:2,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(2073600,1:8,11520,64) -> Half(1036800,1:16,5760,32) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(1036800,1:16,5760,32) -> Float(16588800,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(1036800,1:16,5760,32) -> Float(16588800,1,92160,512) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(1036800,1:16,5760,32) -> Float(4147200,1:4,23040,128) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(1036800,1:16,5760,32) -> Half(16588800,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(1036800,1:16,5760,32) -> Half(8294400,32400:2,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(1036800,1:16,5760,32) -> Half(2073600,1:8,11520,64) ***************
[08/10/2023-11:16:39] [V] [TRT] =============== Computing reformatting costs
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(97200,32400,180,1) -> Half(97200,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(<in> -> dim_0) (Reformat)
[08/10/2023-11:16:39] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.00866393
[08/10/2023-11:16:39] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.013322
[08/10/2023-11:16:39] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.0138888
[08/10/2023-11:16:39] [V] [TRT] Fastest Tactic: 0x00000000000003e8 Time: 0.00866393
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(97200,1,540,3) -> Half(97200,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(<in> -> dim_0) (Reformat)
[08/10/2023-11:16:39] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.0131732
[08/10/2023-11:16:39] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.0668754
[08/10/2023-11:16:39] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.0132177
[08/10/2023-11:16:39] [V] [TRT] Fastest Tactic: 0x00000000000003e8 Time: 0.0131732
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(32400,1:4,180,1) -> Half(97200,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(<in> -> dim_0) (Reformat)
[08/10/2023-11:16:39] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.013839
[08/10/2023-11:16:39] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.0653029
[08/10/2023-11:16:39] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.0138036
[08/10/2023-11:16:39] [V] [TRT] Fastest Tactic: 0x0000000000000000 Time: 0.0138036
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(64800,32400:2,180,1) -> Half(97200,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(<in> -> dim_0) (Reformat)
[08/10/2023-11:16:39] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.0137168
[08/10/2023-11:16:39] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.18901
[08/10/2023-11:16:39] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.00872255
[08/10/2023-11:16:39] [V] [TRT] Fastest Tactic: 0x0000000000000000 Time: 0.00872255
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(32400,1:8,180,1) -> Half(97200,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(<in> -> dim_0) (Reformat)
[08/10/2023-11:16:39] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.0136906
[08/10/2023-11:16:39] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.190158
[08/10/2023-11:16:39] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.00861687
[08/10/2023-11:16:39] [V] [TRT] Fastest Tactic: 0x0000000000000000 Time: 0.00861687
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(32400,1:16,180,1) -> Half(97200,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(<in> -> dim_0) (Reformat)
[08/10/2023-11:16:39] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.0136341
[08/10/2023-11:16:39] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.191383
[08/10/2023-11:16:39] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.0136258
[08/10/2023-11:16:39] [V] [TRT] Fastest Tactic: 0x0000000000000000 Time: 0.0136258
[08/10/2023-11:16:39] [V] [TRT] =============== Computing reformatting costs
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(16588800,32400,180,1) -> Float(16588800,1,92160,512) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(16588800,32400,180,1) -> Float(4147200,1:4,23040,128) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(16588800,32400,180,1) -> Half(16588800,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(16588800,32400,180,1) -> Half(8294400,32400:2,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(16588800,32400,180,1) -> Half(2073600,1:8,11520,64) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(16588800,32400,180,1) -> Half(1036800,1:16,5760,32) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(16588800,1,92160,512) -> Float(16588800,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(16588800,1,92160,512) -> Float(4147200,1:4,23040,128) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(16588800,1,92160,512) -> Half(16588800,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(16588800,1,92160,512) -> Half(8294400,32400:2,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(16588800,1,92160,512) -> Half(2073600,1:8,11520,64) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(16588800,1,92160,512) -> Half(1036800,1:16,5760,32) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(4147200,1:4,23040,128) -> Float(16588800,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(4147200,1:4,23040,128) -> Float(16588800,1,92160,512) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(4147200,1:4,23040,128) -> Half(16588800,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(4147200,1:4,23040,128) -> Half(8294400,32400:2,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(4147200,1:4,23040,128) -> Half(2073600,1:8,11520,64) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(4147200,1:4,23040,128) -> Half(1036800,1:16,5760,32) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(16588800,32400,180,1) -> Float(16588800,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(16588800,32400,180,1) -> Float(16588800,1,92160,512) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(16588800,32400,180,1) -> Float(4147200,1:4,23040,128) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(16588800,32400,180,1) -> Half(8294400,32400:2,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(16588800,32400,180,1) -> Half(2073600,1:8,11520,64) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(16588800,32400,180,1) -> Half(1036800,1:16,5760,32) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(8294400,32400:2,180,1) -> Float(16588800,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(8294400,32400:2,180,1) -> Float(16588800,1,92160,512) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(8294400,32400:2,180,1) -> Float(4147200,1:4,23040,128) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(8294400,32400:2,180,1) -> Half(16588800,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(8294400,32400:2,180,1) -> Half(2073600,1:8,11520,64) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(8294400,32400:2,180,1) -> Half(1036800,1:16,5760,32) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(2073600,1:8,11520,64) -> Float(16588800,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(2073600,1:8,11520,64) -> Float(16588800,1,92160,512) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(2073600,1:8,11520,64) -> Float(4147200,1:4,23040,128) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(2073600,1:8,11520,64) -> Half(16588800,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(2073600,1:8,11520,64) -> Half(8294400,32400:2,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(2073600,1:8,11520,64) -> Half(1036800,1:16,5760,32) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(1036800,1:16,5760,32) -> Float(16588800,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(1036800,1:16,5760,32) -> Float(16588800,1,92160,512) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(1036800,1:16,5760,32) -> Float(4147200,1:4,23040,128) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(1036800,1:16,5760,32) -> Half(16588800,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(1036800,1:16,5760,32) -> Half(8294400,32400:2,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(1036800,1:16,5760,32) -> Half(2073600,1:8,11520,64) ***************
[08/10/2023-11:16:39] [V] [TRT] =============== Computing reformatting costs
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(64800,32400,180,1) -> Half(64800,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(64800,1,360,2) -> Half(64800,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(32400,1:4,180,1) -> Half(64800,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(32400,32400:2,180,1) -> Half(64800,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(32400,1:8,180,1) -> Half(64800,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(32400,1:16,180,1) -> Half(64800,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] =============== Computing reformatting costs
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(16588800,32400,180,1) -> Float(16588800,1,92160,512) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(16588800,32400,180,1) -> Float(4147200,1:4,23040,128) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(16588800,32400,180,1) -> Half(16588800,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(16588800,32400,180,1) -> Half(8294400,32400:2,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(16588800,32400,180,1) -> Half(2073600,1:8,11520,64) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(16588800,32400,180,1) -> Half(1036800,1:16,5760,32) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(16588800,1,92160,512) -> Float(16588800,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(16588800,1,92160,512) -> Float(4147200,1:4,23040,128) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(16588800,1,92160,512) -> Half(16588800,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(16588800,1,92160,512) -> Half(8294400,32400:2,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(16588800,1,92160,512) -> Half(2073600,1:8,11520,64) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(16588800,1,92160,512) -> Half(1036800,1:16,5760,32) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(4147200,1:4,23040,128) -> Float(16588800,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(4147200,1:4,23040,128) -> Float(16588800,1,92160,512) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(4147200,1:4,23040,128) -> Half(16588800,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(4147200,1:4,23040,128) -> Half(8294400,32400:2,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(4147200,1:4,23040,128) -> Half(2073600,1:8,11520,64) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(4147200,1:4,23040,128) -> Half(1036800,1:16,5760,32) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(16588800,32400,180,1) -> Float(16588800,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(16588800,32400,180,1) -> Float(16588800,1,92160,512) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(16588800,32400,180,1) -> Float(4147200,1:4,23040,128) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(16588800,32400,180,1) -> Half(8294400,32400:2,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(16588800,32400,180,1) -> Half(2073600,1:8,11520,64) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(16588800,32400,180,1) -> Half(1036800,1:16,5760,32) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(8294400,32400:2,180,1) -> Float(16588800,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(8294400,32400:2,180,1) -> Float(16588800,1,92160,512) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(8294400,32400:2,180,1) -> Float(4147200,1:4,23040,128) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(8294400,32400:2,180,1) -> Half(16588800,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(8294400,32400:2,180,1) -> Half(2073600,1:8,11520,64) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(8294400,32400:2,180,1) -> Half(1036800,1:16,5760,32) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(2073600,1:8,11520,64) -> Float(16588800,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(2073600,1:8,11520,64) -> Float(16588800,1,92160,512) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(2073600,1:8,11520,64) -> Float(4147200,1:4,23040,128) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(2073600,1:8,11520,64) -> Half(16588800,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(2073600,1:8,11520,64) -> Half(8294400,32400:2,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(2073600,1:8,11520,64) -> Half(1036800,1:16,5760,32) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(1036800,1:16,5760,32) -> Float(16588800,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(1036800,1:16,5760,32) -> Float(16588800,1,92160,512) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(1036800,1:16,5760,32) -> Float(4147200,1:4,23040,128) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(1036800,1:16,5760,32) -> Half(16588800,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(1036800,1:16,5760,32) -> Half(8294400,32400:2,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(1036800,1:16,5760,32) -> Half(2073600,1:8,11520,64) ***************
[08/10/2023-11:16:39] [V] [TRT] =============== Computing reformatting costs
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(64800,32400,180,1) -> Half(64800,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(64800,1,360,2) -> Half(64800,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(32400,1:4,180,1) -> Half(64800,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(32400,32400:2,180,1) -> Half(64800,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(32400,1:8,180,1) -> Half(64800,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(32400,1:16,180,1) -> Half(64800,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] =============== Computing reformatting costs
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(16588800,32400,180,1) -> Float(16588800,1,92160,512) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(16588800,32400,180,1) -> Float(4147200,1:4,23040,128) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(16588800,32400,180,1) -> Half(16588800,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(16588800,32400,180,1) -> Half(8294400,32400:2,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(16588800,32400,180,1) -> Half(2073600,1:8,11520,64) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(16588800,32400,180,1) -> Half(1036800,1:16,5760,32) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(16588800,1,92160,512) -> Float(16588800,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(16588800,1,92160,512) -> Float(4147200,1:4,23040,128) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(16588800,1,92160,512) -> Half(16588800,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(16588800,1,92160,512) -> Half(8294400,32400:2,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(16588800,1,92160,512) -> Half(2073600,1:8,11520,64) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(16588800,1,92160,512) -> Half(1036800,1:16,5760,32) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(4147200,1:4,23040,128) -> Float(16588800,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(4147200,1:4,23040,128) -> Float(16588800,1,92160,512) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(4147200,1:4,23040,128) -> Half(16588800,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(4147200,1:4,23040,128) -> Half(8294400,32400:2,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(4147200,1:4,23040,128) -> Half(2073600,1:8,11520,64) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(4147200,1:4,23040,128) -> Half(1036800,1:16,5760,32) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(16588800,32400,180,1) -> Float(16588800,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(16588800,32400,180,1) -> Float(16588800,1,92160,512) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(16588800,32400,180,1) -> Float(4147200,1:4,23040,128) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(16588800,32400,180,1) -> Half(8294400,32400:2,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(16588800,32400,180,1) -> Half(2073600,1:8,11520,64) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(16588800,32400,180,1) -> Half(1036800,1:16,5760,32) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(8294400,32400:2,180,1) -> Float(16588800,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(8294400,32400:2,180,1) -> Float(16588800,1,92160,512) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(8294400,32400:2,180,1) -> Float(4147200,1:4,23040,128) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(8294400,32400:2,180,1) -> Half(16588800,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(8294400,32400:2,180,1) -> Half(2073600,1:8,11520,64) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(8294400,32400:2,180,1) -> Half(1036800,1:16,5760,32) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(2073600,1:8,11520,64) -> Float(16588800,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(2073600,1:8,11520,64) -> Float(16588800,1,92160,512) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(2073600,1:8,11520,64) -> Float(4147200,1:4,23040,128) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(2073600,1:8,11520,64) -> Half(16588800,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(2073600,1:8,11520,64) -> Half(8294400,32400:2,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(2073600,1:8,11520,64) -> Half(1036800,1:16,5760,32) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(1036800,1:16,5760,32) -> Float(16588800,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(1036800,1:16,5760,32) -> Float(16588800,1,92160,512) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(1036800,1:16,5760,32) -> Float(4147200,1:4,23040,128) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(1036800,1:16,5760,32) -> Half(16588800,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(1036800,1:16,5760,32) -> Half(8294400,32400:2,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(1036800,1:16,5760,32) -> Half(2073600,1:8,11520,64) ***************
[08/10/2023-11:16:39] [V] [TRT] =============== Computing reformatting costs
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(32400,32400,180,1) -> Half(32400,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(32400,1,180,1) -> Half(32400,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(32400,1:4,180,1) -> Half(32400,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(32400,32400:2,180,1) -> Half(32400,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(32400,1:8,180,1) -> Half(32400,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(32400,1:16,180,1) -> Half(32400,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] =============== Computing reformatting costs
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(16588800,32400,180,1) -> Float(16588800,1,92160,512) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(16588800,32400,180,1) -> Float(4147200,1:4,23040,128) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(16588800,32400,180,1) -> Half(16588800,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(16588800,32400,180,1) -> Half(8294400,32400:2,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(16588800,32400,180,1) -> Half(2073600,1:8,11520,64) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(16588800,32400,180,1) -> Half(1036800,1:16,5760,32) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(16588800,1,92160,512) -> Float(16588800,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(16588800,1,92160,512) -> Float(4147200,1:4,23040,128) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(16588800,1,92160,512) -> Half(16588800,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(16588800,1,92160,512) -> Half(8294400,32400:2,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(16588800,1,92160,512) -> Half(2073600,1:8,11520,64) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(16588800,1,92160,512) -> Half(1036800,1:16,5760,32) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(4147200,1:4,23040,128) -> Float(16588800,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(4147200,1:4,23040,128) -> Float(16588800,1,92160,512) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(4147200,1:4,23040,128) -> Half(16588800,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(4147200,1:4,23040,128) -> Half(8294400,32400:2,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(4147200,1:4,23040,128) -> Half(2073600,1:8,11520,64) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(4147200,1:4,23040,128) -> Half(1036800,1:16,5760,32) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(16588800,32400,180,1) -> Float(16588800,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(16588800,32400,180,1) -> Float(16588800,1,92160,512) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(16588800,32400,180,1) -> Float(4147200,1:4,23040,128) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(16588800,32400,180,1) -> Half(8294400,32400:2,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(16588800,32400,180,1) -> Half(2073600,1:8,11520,64) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(16588800,32400,180,1) -> Half(1036800,1:16,5760,32) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(8294400,32400:2,180,1) -> Float(16588800,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(8294400,32400:2,180,1) -> Float(16588800,1,92160,512) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(8294400,32400:2,180,1) -> Float(4147200,1:4,23040,128) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(8294400,32400:2,180,1) -> Half(16588800,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(8294400,32400:2,180,1) -> Half(2073600,1:8,11520,64) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(8294400,32400:2,180,1) -> Half(1036800,1:16,5760,32) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(2073600,1:8,11520,64) -> Float(16588800,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(2073600,1:8,11520,64) -> Float(16588800,1,92160,512) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(2073600,1:8,11520,64) -> Float(4147200,1:4,23040,128) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(2073600,1:8,11520,64) -> Half(16588800,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(2073600,1:8,11520,64) -> Half(8294400,32400:2,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(2073600,1:8,11520,64) -> Half(1036800,1:16,5760,32) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(1036800,1:16,5760,32) -> Float(16588800,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(1036800,1:16,5760,32) -> Float(16588800,1,92160,512) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(1036800,1:16,5760,32) -> Float(4147200,1:4,23040,128) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(1036800,1:16,5760,32) -> Half(16588800,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(1036800,1:16,5760,32) -> Half(8294400,32400:2,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(1036800,1:16,5760,32) -> Half(2073600,1:8,11520,64) ***************
[08/10/2023-11:16:39] [V] [TRT] =============== Computing reformatting costs
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(64800,32400,180,1) -> Half(64800,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(64800,1,360,2) -> Half(64800,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(32400,1:4,180,1) -> Half(64800,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(32400,32400:2,180,1) -> Half(64800,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(32400,1:8,180,1) -> Half(64800,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(32400,1:16,180,1) -> Half(64800,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] =============== Computing reformatting costs
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(16588800,32400,180,1) -> Float(16588800,1,92160,512) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(16588800,32400,180,1) -> Float(4147200,1:4,23040,128) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(16588800,32400,180,1) -> Half(16588800,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(16588800,32400,180,1) -> Half(8294400,32400:2,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(16588800,32400,180,1) -> Half(2073600,1:8,11520,64) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(16588800,32400,180,1) -> Half(1036800,1:16,5760,32) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(16588800,1,92160,512) -> Float(16588800,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(16588800,1,92160,512) -> Float(4147200,1:4,23040,128) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(16588800,1,92160,512) -> Half(16588800,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(16588800,1,92160,512) -> Half(8294400,32400:2,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(16588800,1,92160,512) -> Half(2073600,1:8,11520,64) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(16588800,1,92160,512) -> Half(1036800,1:16,5760,32) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(4147200,1:4,23040,128) -> Float(16588800,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(4147200,1:4,23040,128) -> Float(16588800,1,92160,512) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(4147200,1:4,23040,128) -> Half(16588800,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(4147200,1:4,23040,128) -> Half(8294400,32400:2,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(4147200,1:4,23040,128) -> Half(2073600,1:8,11520,64) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(4147200,1:4,23040,128) -> Half(1036800,1:16,5760,32) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(16588800,32400,180,1) -> Float(16588800,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(16588800,32400,180,1) -> Float(16588800,1,92160,512) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(16588800,32400,180,1) -> Float(4147200,1:4,23040,128) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(16588800,32400,180,1) -> Half(8294400,32400:2,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(16588800,32400,180,1) -> Half(2073600,1:8,11520,64) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(16588800,32400,180,1) -> Half(1036800,1:16,5760,32) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(8294400,32400:2,180,1) -> Float(16588800,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(8294400,32400:2,180,1) -> Float(16588800,1,92160,512) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(8294400,32400:2,180,1) -> Float(4147200,1:4,23040,128) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(8294400,32400:2,180,1) -> Half(16588800,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(8294400,32400:2,180,1) -> Half(2073600,1:8,11520,64) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(8294400,32400:2,180,1) -> Half(1036800,1:16,5760,32) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(2073600,1:8,11520,64) -> Float(16588800,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(2073600,1:8,11520,64) -> Float(16588800,1,92160,512) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(2073600,1:8,11520,64) -> Float(4147200,1:4,23040,128) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(2073600,1:8,11520,64) -> Half(16588800,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(2073600,1:8,11520,64) -> Half(8294400,32400:2,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(2073600,1:8,11520,64) -> Half(1036800,1:16,5760,32) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(1036800,1:16,5760,32) -> Float(16588800,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(1036800,1:16,5760,32) -> Float(16588800,1,92160,512) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(1036800,1:16,5760,32) -> Float(4147200,1:4,23040,128) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(1036800,1:16,5760,32) -> Half(16588800,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(1036800,1:16,5760,32) -> Half(8294400,32400:2,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(1036800,1:16,5760,32) -> Half(2073600,1:8,11520,64) ***************
[08/10/2023-11:16:39] [V] [TRT] =============== Computing reformatting costs
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(32400,32400,180,1) -> Half(32400,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(32400,1,180,1) -> Half(32400,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(32400,1:4,180,1) -> Half(32400,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(32400,32400:2,180,1) -> Half(32400,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(32400,1:8,180,1) -> Half(32400,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(32400,1:16,180,1) -> Half(32400,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] =============== Computing reformatting costs
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(2073600,32400,180,1) -> Float(2073600,1,11520,64) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(2073600,32400,180,1) -> Float(518400,1:4,2880,16) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(2073600,32400,180,1) -> Half(2073600,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(2073600,32400,180,1) -> Half(1036800,32400:2,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(2073600,32400,180,1) -> Half(259200,1:8,1440,8) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(2073600,32400,180,1) -> Half(129600,1:16,720,4) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(2073600,1,11520,64) -> Float(2073600,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(2073600,1,11520,64) -> Float(518400,1:4,2880,16) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(2073600,1,11520,64) -> Half(2073600,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(2073600,1,11520,64) -> Half(1036800,32400:2,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(2073600,1,11520,64) -> Half(259200,1:8,1440,8) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(2073600,1,11520,64) -> Half(129600,1:16,720,4) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(518400,1:4,2880,16) -> Float(2073600,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(518400,1:4,2880,16) -> Float(2073600,1,11520,64) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(518400,1:4,2880,16) -> Half(2073600,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(518400,1:4,2880,16) -> Half(1036800,32400:2,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(518400,1:4,2880,16) -> Half(259200,1:8,1440,8) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(518400,1:4,2880,16) -> Half(129600,1:16,720,4) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(2073600,32400,180,1) -> Float(2073600,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(2073600,32400,180,1) -> Float(2073600,1,11520,64) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(2073600,32400,180,1) -> Float(518400,1:4,2880,16) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(2073600,32400,180,1) -> Half(1036800,32400:2,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(2073600,32400,180,1) -> Half(259200,1:8,1440,8) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(2073600,32400,180,1) -> Half(129600,1:16,720,4) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(1036800,32400:2,180,1) -> Float(2073600,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(1036800,32400:2,180,1) -> Float(2073600,1,11520,64) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(1036800,32400:2,180,1) -> Float(518400,1:4,2880,16) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(1036800,32400:2,180,1) -> Half(2073600,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(1036800,32400:2,180,1) -> Half(259200,1:8,1440,8) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(1036800,32400:2,180,1) -> Half(129600,1:16,720,4) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(259200,1:8,1440,8) -> Float(2073600,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(259200,1:8,1440,8) -> Float(2073600,1,11520,64) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(259200,1:8,1440,8) -> Float(518400,1:4,2880,16) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(259200,1:8,1440,8) -> Half(2073600,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(259200,1:8,1440,8) -> Half(1036800,32400:2,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(259200,1:8,1440,8) -> Half(129600,1:16,720,4) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(129600,1:16,720,4) -> Float(2073600,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(129600,1:16,720,4) -> Float(2073600,1,11520,64) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(129600,1:16,720,4) -> Float(518400,1:4,2880,16) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(129600,1:16,720,4) -> Half(2073600,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(129600,1:16,720,4) -> Half(1036800,32400:2,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(129600,1:16,720,4) -> Half(259200,1:8,1440,8) ***************
[08/10/2023-11:16:39] [V] [TRT] =============== Computing reformatting costs
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(16588800,32400,180,1) -> Float(16588800,1,92160,512) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(16588800,32400,180,1) -> Float(4147200,1:4,23040,128) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(16588800,32400,180,1) -> Half(16588800,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(16588800,32400,180,1) -> Half(8294400,32400:2,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(16588800,32400,180,1) -> Half(2073600,1:8,11520,64) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(16588800,32400,180,1) -> Half(1036800,1:16,5760,32) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(16588800,1,92160,512) -> Float(16588800,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(16588800,1,92160,512) -> Float(4147200,1:4,23040,128) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(16588800,1,92160,512) -> Half(16588800,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(16588800,1,92160,512) -> Half(8294400,32400:2,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(16588800,1,92160,512) -> Half(2073600,1:8,11520,64) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(16588800,1,92160,512) -> Half(1036800,1:16,5760,32) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(4147200,1:4,23040,128) -> Float(16588800,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(4147200,1:4,23040,128) -> Float(16588800,1,92160,512) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(4147200,1:4,23040,128) -> Half(16588800,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(4147200,1:4,23040,128) -> Half(8294400,32400:2,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(4147200,1:4,23040,128) -> Half(2073600,1:8,11520,64) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(4147200,1:4,23040,128) -> Half(1036800,1:16,5760,32) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(16588800,32400,180,1) -> Float(16588800,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(16588800,32400,180,1) -> Float(16588800,1,92160,512) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(16588800,32400,180,1) -> Float(4147200,1:4,23040,128) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(16588800,32400,180,1) -> Half(8294400,32400:2,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(16588800,32400,180,1) -> Half(2073600,1:8,11520,64) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(16588800,32400,180,1) -> Half(1036800,1:16,5760,32) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(8294400,32400:2,180,1) -> Float(16588800,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(8294400,32400:2,180,1) -> Float(16588800,1,92160,512) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(8294400,32400:2,180,1) -> Float(4147200,1:4,23040,128) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(8294400,32400:2,180,1) -> Half(16588800,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(8294400,32400:2,180,1) -> Half(2073600,1:8,11520,64) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(8294400,32400:2,180,1) -> Half(1036800,1:16,5760,32) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(2073600,1:8,11520,64) -> Float(16588800,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(2073600,1:8,11520,64) -> Float(16588800,1,92160,512) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(2073600,1:8,11520,64) -> Float(4147200,1:4,23040,128) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(2073600,1:8,11520,64) -> Half(16588800,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(2073600,1:8,11520,64) -> Half(8294400,32400:2,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(2073600,1:8,11520,64) -> Half(1036800,1:16,5760,32) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(1036800,1:16,5760,32) -> Float(16588800,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(1036800,1:16,5760,32) -> Float(16588800,1,92160,512) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(1036800,1:16,5760,32) -> Float(4147200,1:4,23040,128) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(1036800,1:16,5760,32) -> Half(16588800,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(1036800,1:16,5760,32) -> Half(8294400,32400:2,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(1036800,1:16,5760,32) -> Half(2073600,1:8,11520,64) ***************
[08/10/2023-11:16:39] [V] [TRT] =============== Computing reformatting costs
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(16588800,32400,180,1) -> Float(16588800,1,92160,512) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(16588800,32400,180,1) -> Float(4147200,1:4,23040,128) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(16588800,32400,180,1) -> Half(16588800,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(16588800,32400,180,1) -> Half(8294400,32400:2,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(16588800,32400,180,1) -> Half(2073600,1:8,11520,64) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(16588800,32400,180,1) -> Half(1036800,1:16,5760,32) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(16588800,1,92160,512) -> Float(16588800,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(16588800,1,92160,512) -> Float(4147200,1:4,23040,128) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(16588800,1,92160,512) -> Half(16588800,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(16588800,1,92160,512) -> Half(8294400,32400:2,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(16588800,1,92160,512) -> Half(2073600,1:8,11520,64) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(16588800,1,92160,512) -> Half(1036800,1:16,5760,32) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(4147200,1:4,23040,128) -> Float(16588800,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(4147200,1:4,23040,128) -> Float(16588800,1,92160,512) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(4147200,1:4,23040,128) -> Half(16588800,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(4147200,1:4,23040,128) -> Half(8294400,32400:2,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(4147200,1:4,23040,128) -> Half(2073600,1:8,11520,64) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(4147200,1:4,23040,128) -> Half(1036800,1:16,5760,32) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(16588800,32400,180,1) -> Float(16588800,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(16588800,32400,180,1) -> Float(16588800,1,92160,512) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(16588800,32400,180,1) -> Float(4147200,1:4,23040,128) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(16588800,32400,180,1) -> Half(8294400,32400:2,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(16588800,32400,180,1) -> Half(2073600,1:8,11520,64) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(16588800,32400,180,1) -> Half(1036800,1:16,5760,32) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(8294400,32400:2,180,1) -> Float(16588800,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(8294400,32400:2,180,1) -> Float(16588800,1,92160,512) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(8294400,32400:2,180,1) -> Float(4147200,1:4,23040,128) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(8294400,32400:2,180,1) -> Half(16588800,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(8294400,32400:2,180,1) -> Half(2073600,1:8,11520,64) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(8294400,32400:2,180,1) -> Half(1036800,1:16,5760,32) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(2073600,1:8,11520,64) -> Float(16588800,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(2073600,1:8,11520,64) -> Float(16588800,1,92160,512) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(2073600,1:8,11520,64) -> Float(4147200,1:4,23040,128) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(2073600,1:8,11520,64) -> Half(16588800,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(2073600,1:8,11520,64) -> Half(8294400,32400:2,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(2073600,1:8,11520,64) -> Half(1036800,1:16,5760,32) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(1036800,1:16,5760,32) -> Float(16588800,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(1036800,1:16,5760,32) -> Float(16588800,1,92160,512) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(1036800,1:16,5760,32) -> Float(4147200,1:4,23040,128) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(1036800,1:16,5760,32) -> Half(16588800,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(1036800,1:16,5760,32) -> Half(8294400,32400:2,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(1036800,1:16,5760,32) -> Half(2073600,1:8,11520,64) ***************
[08/10/2023-11:16:39] [V] [TRT] =============== Computing reformatting costs
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(97200,32400,180,1) -> Half(97200,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(97200,1,540,3) -> Half(97200,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(32400,1:4,180,1) -> Half(97200,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(64800,32400:2,180,1) -> Half(97200,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(32400,1:8,180,1) -> Half(97200,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(32400,1:16,180,1) -> Half(97200,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] =============== Computing reformatting costs
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(16588800,32400,180,1) -> Float(16588800,1,92160,512) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(16588800,32400,180,1) -> Float(4147200,1:4,23040,128) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(16588800,32400,180,1) -> Half(16588800,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(16588800,32400,180,1) -> Half(8294400,32400:2,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(16588800,32400,180,1) -> Half(2073600,1:8,11520,64) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(16588800,32400,180,1) -> Half(1036800,1:16,5760,32) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(16588800,1,92160,512) -> Float(16588800,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(16588800,1,92160,512) -> Float(4147200,1:4,23040,128) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(16588800,1,92160,512) -> Half(16588800,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(16588800,1,92160,512) -> Half(8294400,32400:2,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(16588800,1,92160,512) -> Half(2073600,1:8,11520,64) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(16588800,1,92160,512) -> Half(1036800,1:16,5760,32) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(4147200,1:4,23040,128) -> Float(16588800,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(4147200,1:4,23040,128) -> Float(16588800,1,92160,512) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(4147200,1:4,23040,128) -> Half(16588800,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(4147200,1:4,23040,128) -> Half(8294400,32400:2,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(4147200,1:4,23040,128) -> Half(2073600,1:8,11520,64) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(4147200,1:4,23040,128) -> Half(1036800,1:16,5760,32) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(16588800,32400,180,1) -> Float(16588800,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(16588800,32400,180,1) -> Float(16588800,1,92160,512) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(16588800,32400,180,1) -> Float(4147200,1:4,23040,128) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(16588800,32400,180,1) -> Half(8294400,32400:2,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(16588800,32400,180,1) -> Half(2073600,1:8,11520,64) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(16588800,32400,180,1) -> Half(1036800,1:16,5760,32) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(8294400,32400:2,180,1) -> Float(16588800,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(8294400,32400:2,180,1) -> Float(16588800,1,92160,512) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(8294400,32400:2,180,1) -> Float(4147200,1:4,23040,128) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(8294400,32400:2,180,1) -> Half(16588800,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(8294400,32400:2,180,1) -> Half(2073600,1:8,11520,64) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(8294400,32400:2,180,1) -> Half(1036800,1:16,5760,32) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(2073600,1:8,11520,64) -> Float(16588800,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(2073600,1:8,11520,64) -> Float(16588800,1,92160,512) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(2073600,1:8,11520,64) -> Float(4147200,1:4,23040,128) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(2073600,1:8,11520,64) -> Half(16588800,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(2073600,1:8,11520,64) -> Half(8294400,32400:2,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(2073600,1:8,11520,64) -> Half(1036800,1:16,5760,32) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(1036800,1:16,5760,32) -> Float(16588800,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(1036800,1:16,5760,32) -> Float(16588800,1,92160,512) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(1036800,1:16,5760,32) -> Float(4147200,1:4,23040,128) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(1036800,1:16,5760,32) -> Half(16588800,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(1036800,1:16,5760,32) -> Half(8294400,32400:2,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(1036800,1:16,5760,32) -> Half(2073600,1:8,11520,64) ***************
[08/10/2023-11:16:39] [V] [TRT] =============== Computing reformatting costs
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(64800,32400,180,1) -> Half(64800,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(64800,1,360,2) -> Half(64800,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(32400,1:4,180,1) -> Half(64800,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(32400,32400:2,180,1) -> Half(64800,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(32400,1:8,180,1) -> Half(64800,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(32400,1:16,180,1) -> Half(64800,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] =============== Computing reformatting costs
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(16588800,32400,180,1) -> Float(16588800,1,92160,512) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(16588800,32400,180,1) -> Float(4147200,1:4,23040,128) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(16588800,32400,180,1) -> Half(16588800,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(16588800,32400,180,1) -> Half(8294400,32400:2,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(16588800,32400,180,1) -> Half(2073600,1:8,11520,64) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(16588800,32400,180,1) -> Half(1036800,1:16,5760,32) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(16588800,1,92160,512) -> Float(16588800,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(16588800,1,92160,512) -> Float(4147200,1:4,23040,128) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(16588800,1,92160,512) -> Half(16588800,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(16588800,1,92160,512) -> Half(8294400,32400:2,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(16588800,1,92160,512) -> Half(2073600,1:8,11520,64) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(16588800,1,92160,512) -> Half(1036800,1:16,5760,32) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(4147200,1:4,23040,128) -> Float(16588800,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(4147200,1:4,23040,128) -> Float(16588800,1,92160,512) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(4147200,1:4,23040,128) -> Half(16588800,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(4147200,1:4,23040,128) -> Half(8294400,32400:2,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(4147200,1:4,23040,128) -> Half(2073600,1:8,11520,64) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(4147200,1:4,23040,128) -> Half(1036800,1:16,5760,32) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(16588800,32400,180,1) -> Float(16588800,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(16588800,32400,180,1) -> Float(16588800,1,92160,512) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(16588800,32400,180,1) -> Float(4147200,1:4,23040,128) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(16588800,32400,180,1) -> Half(8294400,32400:2,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(16588800,32400,180,1) -> Half(2073600,1:8,11520,64) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(16588800,32400,180,1) -> Half(1036800,1:16,5760,32) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(8294400,32400:2,180,1) -> Float(16588800,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(8294400,32400:2,180,1) -> Float(16588800,1,92160,512) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(8294400,32400:2,180,1) -> Float(4147200,1:4,23040,128) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(8294400,32400:2,180,1) -> Half(16588800,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(8294400,32400:2,180,1) -> Half(2073600,1:8,11520,64) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(8294400,32400:2,180,1) -> Half(1036800,1:16,5760,32) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(2073600,1:8,11520,64) -> Float(16588800,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(2073600,1:8,11520,64) -> Float(16588800,1,92160,512) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(2073600,1:8,11520,64) -> Float(4147200,1:4,23040,128) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(2073600,1:8,11520,64) -> Half(16588800,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(2073600,1:8,11520,64) -> Half(8294400,32400:2,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(2073600,1:8,11520,64) -> Half(1036800,1:16,5760,32) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(1036800,1:16,5760,32) -> Float(16588800,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(1036800,1:16,5760,32) -> Float(16588800,1,92160,512) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(1036800,1:16,5760,32) -> Float(4147200,1:4,23040,128) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(1036800,1:16,5760,32) -> Half(16588800,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(1036800,1:16,5760,32) -> Half(8294400,32400:2,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(1036800,1:16,5760,32) -> Half(2073600,1:8,11520,64) ***************
[08/10/2023-11:16:39] [V] [TRT] =============== Computing reformatting costs
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(64800,32400,180,1) -> Half(64800,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(64800,1,360,2) -> Half(64800,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(32400,1:4,180,1) -> Half(64800,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(32400,32400:2,180,1) -> Half(64800,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(32400,1:8,180,1) -> Half(64800,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(32400,1:16,180,1) -> Half(64800,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] =============== Computing reformatting costs
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(16588800,32400,180,1) -> Float(16588800,1,92160,512) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(16588800,32400,180,1) -> Float(4147200,1:4,23040,128) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(16588800,32400,180,1) -> Half(16588800,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(16588800,32400,180,1) -> Half(8294400,32400:2,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(16588800,32400,180,1) -> Half(2073600,1:8,11520,64) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(16588800,32400,180,1) -> Half(1036800,1:16,5760,32) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(16588800,1,92160,512) -> Float(16588800,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(16588800,1,92160,512) -> Float(4147200,1:4,23040,128) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(16588800,1,92160,512) -> Half(16588800,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(16588800,1,92160,512) -> Half(8294400,32400:2,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(16588800,1,92160,512) -> Half(2073600,1:8,11520,64) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(16588800,1,92160,512) -> Half(1036800,1:16,5760,32) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(4147200,1:4,23040,128) -> Float(16588800,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(4147200,1:4,23040,128) -> Float(16588800,1,92160,512) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(4147200,1:4,23040,128) -> Half(16588800,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(4147200,1:4,23040,128) -> Half(8294400,32400:2,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(4147200,1:4,23040,128) -> Half(2073600,1:8,11520,64) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(4147200,1:4,23040,128) -> Half(1036800,1:16,5760,32) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(16588800,32400,180,1) -> Float(16588800,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(16588800,32400,180,1) -> Float(16588800,1,92160,512) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(16588800,32400,180,1) -> Float(4147200,1:4,23040,128) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(16588800,32400,180,1) -> Half(8294400,32400:2,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(16588800,32400,180,1) -> Half(2073600,1:8,11520,64) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(16588800,32400,180,1) -> Half(1036800,1:16,5760,32) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(8294400,32400:2,180,1) -> Float(16588800,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(8294400,32400:2,180,1) -> Float(16588800,1,92160,512) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(8294400,32400:2,180,1) -> Float(4147200,1:4,23040,128) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(8294400,32400:2,180,1) -> Half(16588800,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(8294400,32400:2,180,1) -> Half(2073600,1:8,11520,64) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(8294400,32400:2,180,1) -> Half(1036800,1:16,5760,32) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(2073600,1:8,11520,64) -> Float(16588800,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(2073600,1:8,11520,64) -> Float(16588800,1,92160,512) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(2073600,1:8,11520,64) -> Float(4147200,1:4,23040,128) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(2073600,1:8,11520,64) -> Half(16588800,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(2073600,1:8,11520,64) -> Half(8294400,32400:2,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(2073600,1:8,11520,64) -> Half(1036800,1:16,5760,32) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(1036800,1:16,5760,32) -> Float(16588800,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(1036800,1:16,5760,32) -> Float(16588800,1,92160,512) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(1036800,1:16,5760,32) -> Float(4147200,1:4,23040,128) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(1036800,1:16,5760,32) -> Half(16588800,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(1036800,1:16,5760,32) -> Half(8294400,32400:2,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(1036800,1:16,5760,32) -> Half(2073600,1:8,11520,64) ***************
[08/10/2023-11:16:39] [V] [TRT] =============== Computing reformatting costs
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(64800,32400,180,1) -> Half(64800,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(64800,1,360,2) -> Half(64800,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(32400,1:4,180,1) -> Half(64800,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(32400,32400:2,180,1) -> Half(64800,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(32400,1:8,180,1) -> Half(64800,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(32400,1:16,180,1) -> Half(64800,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] =============== Computing reformatting costs
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(16588800,32400,180,1) -> Float(16588800,1,92160,512) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(16588800,32400,180,1) -> Float(4147200,1:4,23040,128) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(16588800,32400,180,1) -> Half(16588800,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(16588800,32400,180,1) -> Half(8294400,32400:2,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(16588800,32400,180,1) -> Half(2073600,1:8,11520,64) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(16588800,32400,180,1) -> Half(1036800,1:16,5760,32) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(16588800,1,92160,512) -> Float(16588800,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(16588800,1,92160,512) -> Float(4147200,1:4,23040,128) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(16588800,1,92160,512) -> Half(16588800,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(16588800,1,92160,512) -> Half(8294400,32400:2,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(16588800,1,92160,512) -> Half(2073600,1:8,11520,64) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(16588800,1,92160,512) -> Half(1036800,1:16,5760,32) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(4147200,1:4,23040,128) -> Float(16588800,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(4147200,1:4,23040,128) -> Float(16588800,1,92160,512) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(4147200,1:4,23040,128) -> Half(16588800,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(4147200,1:4,23040,128) -> Half(8294400,32400:2,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(4147200,1:4,23040,128) -> Half(2073600,1:8,11520,64) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(4147200,1:4,23040,128) -> Half(1036800,1:16,5760,32) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(16588800,32400,180,1) -> Float(16588800,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(16588800,32400,180,1) -> Float(16588800,1,92160,512) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(16588800,32400,180,1) -> Float(4147200,1:4,23040,128) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(16588800,32400,180,1) -> Half(8294400,32400:2,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(16588800,32400,180,1) -> Half(2073600,1:8,11520,64) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(16588800,32400,180,1) -> Half(1036800,1:16,5760,32) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(8294400,32400:2,180,1) -> Float(16588800,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(8294400,32400:2,180,1) -> Float(16588800,1,92160,512) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(8294400,32400:2,180,1) -> Float(4147200,1:4,23040,128) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(8294400,32400:2,180,1) -> Half(16588800,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(8294400,32400:2,180,1) -> Half(2073600,1:8,11520,64) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(8294400,32400:2,180,1) -> Half(1036800,1:16,5760,32) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(2073600,1:8,11520,64) -> Float(16588800,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(2073600,1:8,11520,64) -> Float(16588800,1,92160,512) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(2073600,1:8,11520,64) -> Float(4147200,1:4,23040,128) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(2073600,1:8,11520,64) -> Half(16588800,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(2073600,1:8,11520,64) -> Half(8294400,32400:2,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(2073600,1:8,11520,64) -> Half(1036800,1:16,5760,32) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(1036800,1:16,5760,32) -> Float(16588800,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(1036800,1:16,5760,32) -> Float(16588800,1,92160,512) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(1036800,1:16,5760,32) -> Float(4147200,1:4,23040,128) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(1036800,1:16,5760,32) -> Half(16588800,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(1036800,1:16,5760,32) -> Half(8294400,32400:2,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(1036800,1:16,5760,32) -> Half(2073600,1:8,11520,64) ***************
[08/10/2023-11:16:39] [V] [TRT] =============== Computing reformatting costs
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(64800,32400,180,1) -> Half(64800,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(64800,1,360,2) -> Half(64800,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(32400,1:4,180,1) -> Half(64800,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(32400,32400:2,180,1) -> Half(64800,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(32400,1:8,180,1) -> Half(64800,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(32400,1:16,180,1) -> Half(64800,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] =============== Computing reformatting costs
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(16588800,32400,180,1) -> Float(16588800,1,92160,512) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(16588800,32400,180,1) -> Float(4147200,1:4,23040,128) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(16588800,32400,180,1) -> Half(16588800,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(16588800,32400,180,1) -> Half(8294400,32400:2,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(16588800,32400,180,1) -> Half(2073600,1:8,11520,64) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(16588800,32400,180,1) -> Half(1036800,1:16,5760,32) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(16588800,1,92160,512) -> Float(16588800,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(16588800,1,92160,512) -> Float(4147200,1:4,23040,128) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(16588800,1,92160,512) -> Half(16588800,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(16588800,1,92160,512) -> Half(8294400,32400:2,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(16588800,1,92160,512) -> Half(2073600,1:8,11520,64) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(16588800,1,92160,512) -> Half(1036800,1:16,5760,32) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(4147200,1:4,23040,128) -> Float(16588800,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(4147200,1:4,23040,128) -> Float(16588800,1,92160,512) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(4147200,1:4,23040,128) -> Half(16588800,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(4147200,1:4,23040,128) -> Half(8294400,32400:2,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(4147200,1:4,23040,128) -> Half(2073600,1:8,11520,64) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(4147200,1:4,23040,128) -> Half(1036800,1:16,5760,32) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(16588800,32400,180,1) -> Float(16588800,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(16588800,32400,180,1) -> Float(16588800,1,92160,512) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(16588800,32400,180,1) -> Float(4147200,1:4,23040,128) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(16588800,32400,180,1) -> Half(8294400,32400:2,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(16588800,32400,180,1) -> Half(2073600,1:8,11520,64) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(16588800,32400,180,1) -> Half(1036800,1:16,5760,32) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(8294400,32400:2,180,1) -> Float(16588800,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(8294400,32400:2,180,1) -> Float(16588800,1,92160,512) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(8294400,32400:2,180,1) -> Float(4147200,1:4,23040,128) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(8294400,32400:2,180,1) -> Half(16588800,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(8294400,32400:2,180,1) -> Half(2073600,1:8,11520,64) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(8294400,32400:2,180,1) -> Half(1036800,1:16,5760,32) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(2073600,1:8,11520,64) -> Float(16588800,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(2073600,1:8,11520,64) -> Float(16588800,1,92160,512) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(2073600,1:8,11520,64) -> Float(4147200,1:4,23040,128) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(2073600,1:8,11520,64) -> Half(16588800,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(2073600,1:8,11520,64) -> Half(8294400,32400:2,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(2073600,1:8,11520,64) -> Half(1036800,1:16,5760,32) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(1036800,1:16,5760,32) -> Float(16588800,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(1036800,1:16,5760,32) -> Float(16588800,1,92160,512) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(1036800,1:16,5760,32) -> Float(4147200,1:4,23040,128) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(1036800,1:16,5760,32) -> Half(16588800,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(1036800,1:16,5760,32) -> Half(8294400,32400:2,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(1036800,1:16,5760,32) -> Half(2073600,1:8,11520,64) ***************
[08/10/2023-11:16:39] [V] [TRT] =============== Computing reformatting costs
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(32400,32400,180,1) -> Half(32400,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(32400,1,180,1) -> Half(32400,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(32400,1:4,180,1) -> Half(32400,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(32400,32400:2,180,1) -> Half(32400,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(32400,1:8,180,1) -> Half(32400,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(32400,1:16,180,1) -> Half(32400,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] =============== Computing reformatting costs
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(16588800,32400,180,1) -> Float(16588800,1,92160,512) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(16588800,32400,180,1) -> Float(4147200,1:4,23040,128) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(16588800,32400,180,1) -> Half(16588800,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(16588800,32400,180,1) -> Half(8294400,32400:2,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(16588800,32400,180,1) -> Half(2073600,1:8,11520,64) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(16588800,32400,180,1) -> Half(1036800,1:16,5760,32) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(16588800,1,92160,512) -> Float(16588800,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(16588800,1,92160,512) -> Float(4147200,1:4,23040,128) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(16588800,1,92160,512) -> Half(16588800,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(16588800,1,92160,512) -> Half(8294400,32400:2,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(16588800,1,92160,512) -> Half(2073600,1:8,11520,64) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(16588800,1,92160,512) -> Half(1036800,1:16,5760,32) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(4147200,1:4,23040,128) -> Float(16588800,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(4147200,1:4,23040,128) -> Float(16588800,1,92160,512) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(4147200,1:4,23040,128) -> Half(16588800,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(4147200,1:4,23040,128) -> Half(8294400,32400:2,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(4147200,1:4,23040,128) -> Half(2073600,1:8,11520,64) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(4147200,1:4,23040,128) -> Half(1036800,1:16,5760,32) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(16588800,32400,180,1) -> Float(16588800,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(16588800,32400,180,1) -> Float(16588800,1,92160,512) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(16588800,32400,180,1) -> Float(4147200,1:4,23040,128) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(16588800,32400,180,1) -> Half(8294400,32400:2,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(16588800,32400,180,1) -> Half(2073600,1:8,11520,64) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(16588800,32400,180,1) -> Half(1036800,1:16,5760,32) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(8294400,32400:2,180,1) -> Float(16588800,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(8294400,32400:2,180,1) -> Float(16588800,1,92160,512) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(8294400,32400:2,180,1) -> Float(4147200,1:4,23040,128) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(8294400,32400:2,180,1) -> Half(16588800,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(8294400,32400:2,180,1) -> Half(2073600,1:8,11520,64) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(8294400,32400:2,180,1) -> Half(1036800,1:16,5760,32) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(2073600,1:8,11520,64) -> Float(16588800,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(2073600,1:8,11520,64) -> Float(16588800,1,92160,512) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(2073600,1:8,11520,64) -> Float(4147200,1:4,23040,128) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(2073600,1:8,11520,64) -> Half(16588800,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(2073600,1:8,11520,64) -> Half(8294400,32400:2,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(2073600,1:8,11520,64) -> Half(1036800,1:16,5760,32) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(1036800,1:16,5760,32) -> Float(16588800,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(1036800,1:16,5760,32) -> Float(16588800,1,92160,512) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(1036800,1:16,5760,32) -> Float(4147200,1:4,23040,128) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(1036800,1:16,5760,32) -> Half(16588800,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(1036800,1:16,5760,32) -> Half(8294400,32400:2,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(1036800,1:16,5760,32) -> Half(2073600,1:8,11520,64) ***************
[08/10/2023-11:16:39] [V] [TRT] =============== Computing reformatting costs
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(97200,32400,180,1) -> Half(97200,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(97200,1,540,3) -> Half(97200,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(32400,1:4,180,1) -> Half(97200,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(64800,32400:2,180,1) -> Half(97200,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(32400,1:8,180,1) -> Half(97200,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(32400,1:16,180,1) -> Half(97200,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] =============== Computing reformatting costs
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(16588800,32400,180,1) -> Float(16588800,1,92160,512) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(16588800,32400,180,1) -> Float(4147200,1:4,23040,128) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(16588800,32400,180,1) -> Half(16588800,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(16588800,32400,180,1) -> Half(8294400,32400:2,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(16588800,32400,180,1) -> Half(2073600,1:8,11520,64) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(16588800,32400,180,1) -> Half(1036800,1:16,5760,32) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(16588800,1,92160,512) -> Float(16588800,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(16588800,1,92160,512) -> Float(4147200,1:4,23040,128) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(16588800,1,92160,512) -> Half(16588800,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(16588800,1,92160,512) -> Half(8294400,32400:2,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(16588800,1,92160,512) -> Half(2073600,1:8,11520,64) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(16588800,1,92160,512) -> Half(1036800,1:16,5760,32) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(4147200,1:4,23040,128) -> Float(16588800,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(4147200,1:4,23040,128) -> Float(16588800,1,92160,512) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(4147200,1:4,23040,128) -> Half(16588800,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(4147200,1:4,23040,128) -> Half(8294400,32400:2,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(4147200,1:4,23040,128) -> Half(2073600,1:8,11520,64) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(4147200,1:4,23040,128) -> Half(1036800,1:16,5760,32) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(16588800,32400,180,1) -> Float(16588800,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(16588800,32400,180,1) -> Float(16588800,1,92160,512) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(16588800,32400,180,1) -> Float(4147200,1:4,23040,128) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(16588800,32400,180,1) -> Half(8294400,32400:2,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(16588800,32400,180,1) -> Half(2073600,1:8,11520,64) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(16588800,32400,180,1) -> Half(1036800,1:16,5760,32) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(8294400,32400:2,180,1) -> Float(16588800,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(8294400,32400:2,180,1) -> Float(16588800,1,92160,512) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(8294400,32400:2,180,1) -> Float(4147200,1:4,23040,128) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(8294400,32400:2,180,1) -> Half(16588800,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(8294400,32400:2,180,1) -> Half(2073600,1:8,11520,64) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(8294400,32400:2,180,1) -> Half(1036800,1:16,5760,32) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(2073600,1:8,11520,64) -> Float(16588800,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(2073600,1:8,11520,64) -> Float(16588800,1,92160,512) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(2073600,1:8,11520,64) -> Float(4147200,1:4,23040,128) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(2073600,1:8,11520,64) -> Half(16588800,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(2073600,1:8,11520,64) -> Half(8294400,32400:2,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(2073600,1:8,11520,64) -> Half(1036800,1:16,5760,32) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(1036800,1:16,5760,32) -> Float(16588800,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(1036800,1:16,5760,32) -> Float(16588800,1,92160,512) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(1036800,1:16,5760,32) -> Float(4147200,1:4,23040,128) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(1036800,1:16,5760,32) -> Half(16588800,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(1036800,1:16,5760,32) -> Half(8294400,32400:2,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(1036800,1:16,5760,32) -> Half(2073600,1:8,11520,64) ***************
[08/10/2023-11:16:39] [V] [TRT] =============== Computing reformatting costs
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(64800,32400,180,1) -> Half(64800,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(64800,1,360,2) -> Half(64800,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(32400,1:4,180,1) -> Half(64800,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(32400,32400:2,180,1) -> Half(64800,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(32400,1:8,180,1) -> Half(64800,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(32400,1:16,180,1) -> Half(64800,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] =============== Computing reformatting costs
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(2073600,32400,180,1) -> Float(2073600,1,11520,64) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(2073600,32400,180,1) -> Float(518400,1:4,2880,16) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(2073600,32400,180,1) -> Half(2073600,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(2073600,32400,180,1) -> Half(1036800,32400:2,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(2073600,32400,180,1) -> Half(259200,1:8,1440,8) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(2073600,32400,180,1) -> Half(129600,1:16,720,4) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(2073600,1,11520,64) -> Float(2073600,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(2073600,1,11520,64) -> Float(518400,1:4,2880,16) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(2073600,1,11520,64) -> Half(2073600,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(2073600,1,11520,64) -> Half(1036800,32400:2,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(2073600,1,11520,64) -> Half(259200,1:8,1440,8) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(2073600,1,11520,64) -> Half(129600,1:16,720,4) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(518400,1:4,2880,16) -> Float(2073600,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(518400,1:4,2880,16) -> Float(2073600,1,11520,64) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(518400,1:4,2880,16) -> Half(2073600,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(518400,1:4,2880,16) -> Half(1036800,32400:2,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(518400,1:4,2880,16) -> Half(259200,1:8,1440,8) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(518400,1:4,2880,16) -> Half(129600,1:16,720,4) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(2073600,32400,180,1) -> Float(2073600,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(2073600,32400,180,1) -> Float(2073600,1,11520,64) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(2073600,32400,180,1) -> Float(518400,1:4,2880,16) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(2073600,32400,180,1) -> Half(1036800,32400:2,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(2073600,32400,180,1) -> Half(259200,1:8,1440,8) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(2073600,32400,180,1) -> Half(129600,1:16,720,4) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(1036800,32400:2,180,1) -> Float(2073600,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(1036800,32400:2,180,1) -> Float(2073600,1,11520,64) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(1036800,32400:2,180,1) -> Float(518400,1:4,2880,16) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(1036800,32400:2,180,1) -> Half(2073600,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(1036800,32400:2,180,1) -> Half(259200,1:8,1440,8) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(1036800,32400:2,180,1) -> Half(129600,1:16,720,4) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(259200,1:8,1440,8) -> Float(2073600,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(259200,1:8,1440,8) -> Float(2073600,1,11520,64) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(259200,1:8,1440,8) -> Float(518400,1:4,2880,16) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(259200,1:8,1440,8) -> Half(2073600,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(259200,1:8,1440,8) -> Half(1036800,32400:2,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(259200,1:8,1440,8) -> Half(129600,1:16,720,4) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(129600,1:16,720,4) -> Float(2073600,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(129600,1:16,720,4) -> Float(2073600,1,11520,64) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(129600,1:16,720,4) -> Float(518400,1:4,2880,16) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(129600,1:16,720,4) -> Half(2073600,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(129600,1:16,720,4) -> Half(1036800,32400:2,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(129600,1:16,720,4) -> Half(259200,1:8,1440,8) ***************
[08/10/2023-11:16:39] [V] [TRT] =============== Computing reformatting costs
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(16588800,32400,180,1) -> Float(16588800,1,92160,512) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(16588800,32400,180,1) -> Float(4147200,1:4,23040,128) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(16588800,32400,180,1) -> Half(16588800,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(16588800,32400,180,1) -> Half(8294400,32400:2,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(16588800,32400,180,1) -> Half(2073600,1:8,11520,64) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(16588800,32400,180,1) -> Half(1036800,1:16,5760,32) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(16588800,1,92160,512) -> Float(16588800,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(16588800,1,92160,512) -> Float(4147200,1:4,23040,128) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(16588800,1,92160,512) -> Half(16588800,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(16588800,1,92160,512) -> Half(8294400,32400:2,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(16588800,1,92160,512) -> Half(2073600,1:8,11520,64) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(16588800,1,92160,512) -> Half(1036800,1:16,5760,32) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(4147200,1:4,23040,128) -> Float(16588800,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(4147200,1:4,23040,128) -> Float(16588800,1,92160,512) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(4147200,1:4,23040,128) -> Half(16588800,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(4147200,1:4,23040,128) -> Half(8294400,32400:2,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(4147200,1:4,23040,128) -> Half(2073600,1:8,11520,64) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(4147200,1:4,23040,128) -> Half(1036800,1:16,5760,32) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(16588800,32400,180,1) -> Float(16588800,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(16588800,32400,180,1) -> Float(16588800,1,92160,512) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(16588800,32400,180,1) -> Float(4147200,1:4,23040,128) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(16588800,32400,180,1) -> Half(8294400,32400:2,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(16588800,32400,180,1) -> Half(2073600,1:8,11520,64) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(16588800,32400,180,1) -> Half(1036800,1:16,5760,32) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(8294400,32400:2,180,1) -> Float(16588800,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(8294400,32400:2,180,1) -> Float(16588800,1,92160,512) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(8294400,32400:2,180,1) -> Float(4147200,1:4,23040,128) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(8294400,32400:2,180,1) -> Half(16588800,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(8294400,32400:2,180,1) -> Half(2073600,1:8,11520,64) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(8294400,32400:2,180,1) -> Half(1036800,1:16,5760,32) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(2073600,1:8,11520,64) -> Float(16588800,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(2073600,1:8,11520,64) -> Float(16588800,1,92160,512) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(2073600,1:8,11520,64) -> Float(4147200,1:4,23040,128) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(2073600,1:8,11520,64) -> Half(16588800,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(2073600,1:8,11520,64) -> Half(8294400,32400:2,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(2073600,1:8,11520,64) -> Half(1036800,1:16,5760,32) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(1036800,1:16,5760,32) -> Float(16588800,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(1036800,1:16,5760,32) -> Float(16588800,1,92160,512) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(1036800,1:16,5760,32) -> Float(4147200,1:4,23040,128) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(1036800,1:16,5760,32) -> Half(16588800,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(1036800,1:16,5760,32) -> Half(8294400,32400:2,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(1036800,1:16,5760,32) -> Half(2073600,1:8,11520,64) ***************
[08/10/2023-11:16:39] [V] [TRT] =============== Computing reformatting costs
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(16588800,32400,180,1) -> Float(16588800,1,92160,512) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(16588800,32400,180,1) -> Float(4147200,1:4,23040,128) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(16588800,32400,180,1) -> Half(16588800,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(16588800,32400,180,1) -> Half(8294400,32400:2,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(16588800,32400,180,1) -> Half(2073600,1:8,11520,64) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(16588800,32400,180,1) -> Half(1036800,1:16,5760,32) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(16588800,1,92160,512) -> Float(16588800,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(16588800,1,92160,512) -> Float(4147200,1:4,23040,128) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(16588800,1,92160,512) -> Half(16588800,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(16588800,1,92160,512) -> Half(8294400,32400:2,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(16588800,1,92160,512) -> Half(2073600,1:8,11520,64) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(16588800,1,92160,512) -> Half(1036800,1:16,5760,32) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(4147200,1:4,23040,128) -> Float(16588800,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(4147200,1:4,23040,128) -> Float(16588800,1,92160,512) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(4147200,1:4,23040,128) -> Half(16588800,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(4147200,1:4,23040,128) -> Half(8294400,32400:2,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(4147200,1:4,23040,128) -> Half(2073600,1:8,11520,64) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(4147200,1:4,23040,128) -> Half(1036800,1:16,5760,32) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(16588800,32400,180,1) -> Float(16588800,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(16588800,32400,180,1) -> Float(16588800,1,92160,512) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(16588800,32400,180,1) -> Float(4147200,1:4,23040,128) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(16588800,32400,180,1) -> Half(8294400,32400:2,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(16588800,32400,180,1) -> Half(2073600,1:8,11520,64) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(16588800,32400,180,1) -> Half(1036800,1:16,5760,32) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(8294400,32400:2,180,1) -> Float(16588800,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(8294400,32400:2,180,1) -> Float(16588800,1,92160,512) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(8294400,32400:2,180,1) -> Float(4147200,1:4,23040,128) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(8294400,32400:2,180,1) -> Half(16588800,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(8294400,32400:2,180,1) -> Half(2073600,1:8,11520,64) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(8294400,32400:2,180,1) -> Half(1036800,1:16,5760,32) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(2073600,1:8,11520,64) -> Float(16588800,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(2073600,1:8,11520,64) -> Float(16588800,1,92160,512) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(2073600,1:8,11520,64) -> Float(4147200,1:4,23040,128) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(2073600,1:8,11520,64) -> Half(16588800,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(2073600,1:8,11520,64) -> Half(8294400,32400:2,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(2073600,1:8,11520,64) -> Half(1036800,1:16,5760,32) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(1036800,1:16,5760,32) -> Float(16588800,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(1036800,1:16,5760,32) -> Float(16588800,1,92160,512) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(1036800,1:16,5760,32) -> Float(4147200,1:4,23040,128) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(1036800,1:16,5760,32) -> Half(16588800,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(1036800,1:16,5760,32) -> Half(8294400,32400:2,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(1036800,1:16,5760,32) -> Half(2073600,1:8,11520,64) ***************
[08/10/2023-11:16:39] [V] [TRT] =============== Computing reformatting costs
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(64800,32400,180,1) -> Half(64800,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(64800,1,360,2) -> Half(64800,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(32400,1:4,180,1) -> Half(64800,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(32400,32400:2,180,1) -> Half(64800,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(32400,1:8,180,1) -> Half(64800,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(32400,1:16,180,1) -> Half(64800,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] =============== Computing reformatting costs
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(16588800,32400,180,1) -> Float(16588800,1,92160,512) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(16588800,32400,180,1) -> Float(4147200,1:4,23040,128) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(16588800,32400,180,1) -> Half(16588800,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(16588800,32400,180,1) -> Half(8294400,32400:2,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(16588800,32400,180,1) -> Half(2073600,1:8,11520,64) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(16588800,32400,180,1) -> Half(1036800,1:16,5760,32) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(16588800,1,92160,512) -> Float(16588800,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(16588800,1,92160,512) -> Float(4147200,1:4,23040,128) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(16588800,1,92160,512) -> Half(16588800,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(16588800,1,92160,512) -> Half(8294400,32400:2,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(16588800,1,92160,512) -> Half(2073600,1:8,11520,64) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(16588800,1,92160,512) -> Half(1036800,1:16,5760,32) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(4147200,1:4,23040,128) -> Float(16588800,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(4147200,1:4,23040,128) -> Float(16588800,1,92160,512) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(4147200,1:4,23040,128) -> Half(16588800,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(4147200,1:4,23040,128) -> Half(8294400,32400:2,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(4147200,1:4,23040,128) -> Half(2073600,1:8,11520,64) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(4147200,1:4,23040,128) -> Half(1036800,1:16,5760,32) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(16588800,32400,180,1) -> Float(16588800,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(16588800,32400,180,1) -> Float(16588800,1,92160,512) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(16588800,32400,180,1) -> Float(4147200,1:4,23040,128) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(16588800,32400,180,1) -> Half(8294400,32400:2,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(16588800,32400,180,1) -> Half(2073600,1:8,11520,64) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(16588800,32400,180,1) -> Half(1036800,1:16,5760,32) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(8294400,32400:2,180,1) -> Float(16588800,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(8294400,32400:2,180,1) -> Float(16588800,1,92160,512) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(8294400,32400:2,180,1) -> Float(4147200,1:4,23040,128) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(8294400,32400:2,180,1) -> Half(16588800,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(8294400,32400:2,180,1) -> Half(2073600,1:8,11520,64) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(8294400,32400:2,180,1) -> Half(1036800,1:16,5760,32) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(2073600,1:8,11520,64) -> Float(16588800,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(2073600,1:8,11520,64) -> Float(16588800,1,92160,512) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(2073600,1:8,11520,64) -> Float(4147200,1:4,23040,128) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(2073600,1:8,11520,64) -> Half(16588800,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(2073600,1:8,11520,64) -> Half(8294400,32400:2,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(2073600,1:8,11520,64) -> Half(1036800,1:16,5760,32) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(1036800,1:16,5760,32) -> Float(16588800,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(1036800,1:16,5760,32) -> Float(16588800,1,92160,512) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(1036800,1:16,5760,32) -> Float(4147200,1:4,23040,128) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(1036800,1:16,5760,32) -> Half(16588800,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(1036800,1:16,5760,32) -> Half(8294400,32400:2,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(1036800,1:16,5760,32) -> Half(2073600,1:8,11520,64) ***************
[08/10/2023-11:16:39] [V] [TRT] =============== Computing reformatting costs
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(64800,32400,180,1) -> Half(64800,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(64800,1,360,2) -> Half(64800,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(32400,1:4,180,1) -> Half(64800,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(32400,32400:2,180,1) -> Half(64800,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(32400,1:8,180,1) -> Half(64800,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(32400,1:16,180,1) -> Half(64800,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] =============== Computing reformatting costs
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(16588800,32400,180,1) -> Float(16588800,1,92160,512) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(16588800,32400,180,1) -> Float(4147200,1:4,23040,128) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(16588800,32400,180,1) -> Half(16588800,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(16588800,32400,180,1) -> Half(8294400,32400:2,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(16588800,32400,180,1) -> Half(2073600,1:8,11520,64) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(16588800,32400,180,1) -> Half(1036800,1:16,5760,32) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(16588800,1,92160,512) -> Float(16588800,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(16588800,1,92160,512) -> Float(4147200,1:4,23040,128) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(16588800,1,92160,512) -> Half(16588800,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(16588800,1,92160,512) -> Half(8294400,32400:2,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(16588800,1,92160,512) -> Half(2073600,1:8,11520,64) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(16588800,1,92160,512) -> Half(1036800,1:16,5760,32) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(4147200,1:4,23040,128) -> Float(16588800,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(4147200,1:4,23040,128) -> Float(16588800,1,92160,512) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(4147200,1:4,23040,128) -> Half(16588800,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(4147200,1:4,23040,128) -> Half(8294400,32400:2,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(4147200,1:4,23040,128) -> Half(2073600,1:8,11520,64) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(4147200,1:4,23040,128) -> Half(1036800,1:16,5760,32) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(16588800,32400,180,1) -> Float(16588800,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(16588800,32400,180,1) -> Float(16588800,1,92160,512) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(16588800,32400,180,1) -> Float(4147200,1:4,23040,128) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(16588800,32400,180,1) -> Half(8294400,32400:2,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(16588800,32400,180,1) -> Half(2073600,1:8,11520,64) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(16588800,32400,180,1) -> Half(1036800,1:16,5760,32) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(8294400,32400:2,180,1) -> Float(16588800,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(8294400,32400:2,180,1) -> Float(16588800,1,92160,512) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(8294400,32400:2,180,1) -> Float(4147200,1:4,23040,128) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(8294400,32400:2,180,1) -> Half(16588800,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(8294400,32400:2,180,1) -> Half(2073600,1:8,11520,64) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(8294400,32400:2,180,1) -> Half(1036800,1:16,5760,32) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(2073600,1:8,11520,64) -> Float(16588800,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(2073600,1:8,11520,64) -> Float(16588800,1,92160,512) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(2073600,1:8,11520,64) -> Float(4147200,1:4,23040,128) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(2073600,1:8,11520,64) -> Half(16588800,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(2073600,1:8,11520,64) -> Half(8294400,32400:2,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(2073600,1:8,11520,64) -> Half(1036800,1:16,5760,32) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(1036800,1:16,5760,32) -> Float(16588800,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(1036800,1:16,5760,32) -> Float(16588800,1,92160,512) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(1036800,1:16,5760,32) -> Float(4147200,1:4,23040,128) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(1036800,1:16,5760,32) -> Half(16588800,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(1036800,1:16,5760,32) -> Half(8294400,32400:2,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(1036800,1:16,5760,32) -> Half(2073600,1:8,11520,64) ***************
[08/10/2023-11:16:39] [V] [TRT] =============== Computing reformatting costs
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(64800,32400,180,1) -> Half(64800,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(64800,1,360,2) -> Half(64800,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(32400,1:4,180,1) -> Half(64800,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(32400,32400:2,180,1) -> Half(64800,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(32400,1:8,180,1) -> Half(64800,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(32400,1:16,180,1) -> Half(64800,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] =============== Computing reformatting costs
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(16588800,32400,180,1) -> Float(16588800,1,92160,512) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(16588800,32400,180,1) -> Float(4147200,1:4,23040,128) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(16588800,32400,180,1) -> Half(16588800,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(16588800,32400,180,1) -> Half(8294400,32400:2,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(16588800,32400,180,1) -> Half(2073600,1:8,11520,64) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(16588800,32400,180,1) -> Half(1036800,1:16,5760,32) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(16588800,1,92160,512) -> Float(16588800,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(16588800,1,92160,512) -> Float(4147200,1:4,23040,128) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(16588800,1,92160,512) -> Half(16588800,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(16588800,1,92160,512) -> Half(8294400,32400:2,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(16588800,1,92160,512) -> Half(2073600,1:8,11520,64) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(16588800,1,92160,512) -> Half(1036800,1:16,5760,32) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(4147200,1:4,23040,128) -> Float(16588800,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(4147200,1:4,23040,128) -> Float(16588800,1,92160,512) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(4147200,1:4,23040,128) -> Half(16588800,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(4147200,1:4,23040,128) -> Half(8294400,32400:2,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(4147200,1:4,23040,128) -> Half(2073600,1:8,11520,64) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(4147200,1:4,23040,128) -> Half(1036800,1:16,5760,32) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(16588800,32400,180,1) -> Float(16588800,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(16588800,32400,180,1) -> Float(16588800,1,92160,512) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(16588800,32400,180,1) -> Float(4147200,1:4,23040,128) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(16588800,32400,180,1) -> Half(8294400,32400:2,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(16588800,32400,180,1) -> Half(2073600,1:8,11520,64) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(16588800,32400,180,1) -> Half(1036800,1:16,5760,32) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(8294400,32400:2,180,1) -> Float(16588800,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(8294400,32400:2,180,1) -> Float(16588800,1,92160,512) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(8294400,32400:2,180,1) -> Float(4147200,1:4,23040,128) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(8294400,32400:2,180,1) -> Half(16588800,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(8294400,32400:2,180,1) -> Half(2073600,1:8,11520,64) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(8294400,32400:2,180,1) -> Half(1036800,1:16,5760,32) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(2073600,1:8,11520,64) -> Float(16588800,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(2073600,1:8,11520,64) -> Float(16588800,1,92160,512) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(2073600,1:8,11520,64) -> Float(4147200,1:4,23040,128) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(2073600,1:8,11520,64) -> Half(16588800,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(2073600,1:8,11520,64) -> Half(8294400,32400:2,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(2073600,1:8,11520,64) -> Half(1036800,1:16,5760,32) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(1036800,1:16,5760,32) -> Float(16588800,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(1036800,1:16,5760,32) -> Float(16588800,1,92160,512) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(1036800,1:16,5760,32) -> Float(4147200,1:4,23040,128) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(1036800,1:16,5760,32) -> Half(16588800,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(1036800,1:16,5760,32) -> Half(8294400,32400:2,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(1036800,1:16,5760,32) -> Half(2073600,1:8,11520,64) ***************
[08/10/2023-11:16:39] [V] [TRT] =============== Computing reformatting costs
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(32400,32400,180,1) -> Half(32400,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(32400,1,180,1) -> Half(32400,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(32400,1:4,180,1) -> Half(32400,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(32400,32400:2,180,1) -> Half(32400,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(32400,1:8,180,1) -> Half(32400,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(32400,1:16,180,1) -> Half(32400,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] =============== Computing reformatting costs
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(16588800,32400,180,1) -> Float(16588800,1,92160,512) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(16588800,32400,180,1) -> Float(4147200,1:4,23040,128) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(16588800,32400,180,1) -> Half(16588800,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(16588800,32400,180,1) -> Half(8294400,32400:2,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(16588800,32400,180,1) -> Half(2073600,1:8,11520,64) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(16588800,32400,180,1) -> Half(1036800,1:16,5760,32) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(16588800,1,92160,512) -> Float(16588800,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(16588800,1,92160,512) -> Float(4147200,1:4,23040,128) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(16588800,1,92160,512) -> Half(16588800,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(16588800,1,92160,512) -> Half(8294400,32400:2,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(16588800,1,92160,512) -> Half(2073600,1:8,11520,64) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(16588800,1,92160,512) -> Half(1036800,1:16,5760,32) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(4147200,1:4,23040,128) -> Float(16588800,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(4147200,1:4,23040,128) -> Float(16588800,1,92160,512) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(4147200,1:4,23040,128) -> Half(16588800,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(4147200,1:4,23040,128) -> Half(8294400,32400:2,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(4147200,1:4,23040,128) -> Half(2073600,1:8,11520,64) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(4147200,1:4,23040,128) -> Half(1036800,1:16,5760,32) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(16588800,32400,180,1) -> Float(16588800,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(16588800,32400,180,1) -> Float(16588800,1,92160,512) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(16588800,32400,180,1) -> Float(4147200,1:4,23040,128) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(16588800,32400,180,1) -> Half(8294400,32400:2,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(16588800,32400,180,1) -> Half(2073600,1:8,11520,64) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(16588800,32400,180,1) -> Half(1036800,1:16,5760,32) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(8294400,32400:2,180,1) -> Float(16588800,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(8294400,32400:2,180,1) -> Float(16588800,1,92160,512) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(8294400,32400:2,180,1) -> Float(4147200,1:4,23040,128) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(8294400,32400:2,180,1) -> Half(16588800,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(8294400,32400:2,180,1) -> Half(2073600,1:8,11520,64) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(8294400,32400:2,180,1) -> Half(1036800,1:16,5760,32) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(2073600,1:8,11520,64) -> Float(16588800,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(2073600,1:8,11520,64) -> Float(16588800,1,92160,512) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(2073600,1:8,11520,64) -> Float(4147200,1:4,23040,128) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(2073600,1:8,11520,64) -> Half(16588800,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(2073600,1:8,11520,64) -> Half(8294400,32400:2,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(2073600,1:8,11520,64) -> Half(1036800,1:16,5760,32) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(1036800,1:16,5760,32) -> Float(16588800,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(1036800,1:16,5760,32) -> Float(16588800,1,92160,512) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(1036800,1:16,5760,32) -> Float(4147200,1:4,23040,128) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(1036800,1:16,5760,32) -> Half(16588800,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(1036800,1:16,5760,32) -> Half(8294400,32400:2,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(1036800,1:16,5760,32) -> Half(2073600,1:8,11520,64) ***************
[08/10/2023-11:16:39] [V] [TRT] =============== Computing reformatting costs
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(97200,32400,180,1) -> Half(97200,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(97200,1,540,3) -> Half(97200,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(32400,1:4,180,1) -> Half(97200,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(64800,32400:2,180,1) -> Half(97200,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(32400,1:8,180,1) -> Half(97200,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(32400,1:16,180,1) -> Half(97200,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] =============== Computing reformatting costs
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(16588800,32400,180,1) -> Float(16588800,1,92160,512) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(16588800,32400,180,1) -> Float(4147200,1:4,23040,128) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(16588800,32400,180,1) -> Half(16588800,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(16588800,32400,180,1) -> Half(8294400,32400:2,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(16588800,32400,180,1) -> Half(2073600,1:8,11520,64) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(16588800,32400,180,1) -> Half(1036800,1:16,5760,32) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(16588800,1,92160,512) -> Float(16588800,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(16588800,1,92160,512) -> Float(4147200,1:4,23040,128) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(16588800,1,92160,512) -> Half(16588800,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(16588800,1,92160,512) -> Half(8294400,32400:2,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(16588800,1,92160,512) -> Half(2073600,1:8,11520,64) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(16588800,1,92160,512) -> Half(1036800,1:16,5760,32) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(4147200,1:4,23040,128) -> Float(16588800,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(4147200,1:4,23040,128) -> Float(16588800,1,92160,512) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(4147200,1:4,23040,128) -> Half(16588800,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(4147200,1:4,23040,128) -> Half(8294400,32400:2,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(4147200,1:4,23040,128) -> Half(2073600,1:8,11520,64) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(4147200,1:4,23040,128) -> Half(1036800,1:16,5760,32) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(16588800,32400,180,1) -> Float(16588800,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(16588800,32400,180,1) -> Float(16588800,1,92160,512) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(16588800,32400,180,1) -> Float(4147200,1:4,23040,128) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(16588800,32400,180,1) -> Half(8294400,32400:2,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(16588800,32400,180,1) -> Half(2073600,1:8,11520,64) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(16588800,32400,180,1) -> Half(1036800,1:16,5760,32) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(8294400,32400:2,180,1) -> Float(16588800,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(8294400,32400:2,180,1) -> Float(16588800,1,92160,512) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(8294400,32400:2,180,1) -> Float(4147200,1:4,23040,128) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(8294400,32400:2,180,1) -> Half(16588800,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(8294400,32400:2,180,1) -> Half(2073600,1:8,11520,64) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(8294400,32400:2,180,1) -> Half(1036800,1:16,5760,32) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(2073600,1:8,11520,64) -> Float(16588800,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(2073600,1:8,11520,64) -> Float(16588800,1,92160,512) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(2073600,1:8,11520,64) -> Float(4147200,1:4,23040,128) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(2073600,1:8,11520,64) -> Half(16588800,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(2073600,1:8,11520,64) -> Half(8294400,32400:2,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(2073600,1:8,11520,64) -> Half(1036800,1:16,5760,32) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(1036800,1:16,5760,32) -> Float(16588800,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(1036800,1:16,5760,32) -> Float(16588800,1,92160,512) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(1036800,1:16,5760,32) -> Float(4147200,1:4,23040,128) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(1036800,1:16,5760,32) -> Half(16588800,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(1036800,1:16,5760,32) -> Half(8294400,32400:2,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(1036800,1:16,5760,32) -> Half(2073600,1:8,11520,64) ***************
[08/10/2023-11:16:39] [V] [TRT] =============== Computing reformatting costs
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(64800,32400,180,1) -> Half(64800,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(64800,1,360,2) -> Half(64800,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(32400,1:4,180,1) -> Half(64800,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(32400,32400:2,180,1) -> Half(64800,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(32400,1:8,180,1) -> Half(64800,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(32400,1:16,180,1) -> Half(64800,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] =============== Computing reformatting costs
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(16588800,32400,180,1) -> Float(16588800,1,92160,512) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(16588800,32400,180,1) -> Float(4147200,1:4,23040,128) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(16588800,32400,180,1) -> Half(16588800,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(16588800,32400,180,1) -> Half(8294400,32400:2,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(16588800,32400,180,1) -> Half(2073600,1:8,11520,64) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(16588800,32400,180,1) -> Half(1036800,1:16,5760,32) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(16588800,1,92160,512) -> Float(16588800,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(16588800,1,92160,512) -> Float(4147200,1:4,23040,128) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(16588800,1,92160,512) -> Half(16588800,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(16588800,1,92160,512) -> Half(8294400,32400:2,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(16588800,1,92160,512) -> Half(2073600,1:8,11520,64) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(16588800,1,92160,512) -> Half(1036800,1:16,5760,32) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(4147200,1:4,23040,128) -> Float(16588800,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(4147200,1:4,23040,128) -> Float(16588800,1,92160,512) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(4147200,1:4,23040,128) -> Half(16588800,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(4147200,1:4,23040,128) -> Half(8294400,32400:2,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(4147200,1:4,23040,128) -> Half(2073600,1:8,11520,64) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(4147200,1:4,23040,128) -> Half(1036800,1:16,5760,32) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(16588800,32400,180,1) -> Float(16588800,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(16588800,32400,180,1) -> Float(16588800,1,92160,512) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(16588800,32400,180,1) -> Float(4147200,1:4,23040,128) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(16588800,32400,180,1) -> Half(8294400,32400:2,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(16588800,32400,180,1) -> Half(2073600,1:8,11520,64) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(16588800,32400,180,1) -> Half(1036800,1:16,5760,32) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(8294400,32400:2,180,1) -> Float(16588800,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(8294400,32400:2,180,1) -> Float(16588800,1,92160,512) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(8294400,32400:2,180,1) -> Float(4147200,1:4,23040,128) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(8294400,32400:2,180,1) -> Half(16588800,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(8294400,32400:2,180,1) -> Half(2073600,1:8,11520,64) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(8294400,32400:2,180,1) -> Half(1036800,1:16,5760,32) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(2073600,1:8,11520,64) -> Float(16588800,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(2073600,1:8,11520,64) -> Float(16588800,1,92160,512) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(2073600,1:8,11520,64) -> Float(4147200,1:4,23040,128) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(2073600,1:8,11520,64) -> Half(16588800,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(2073600,1:8,11520,64) -> Half(8294400,32400:2,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(2073600,1:8,11520,64) -> Half(1036800,1:16,5760,32) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(1036800,1:16,5760,32) -> Float(16588800,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(1036800,1:16,5760,32) -> Float(16588800,1,92160,512) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(1036800,1:16,5760,32) -> Float(4147200,1:4,23040,128) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(1036800,1:16,5760,32) -> Half(16588800,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(1036800,1:16,5760,32) -> Half(8294400,32400:2,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(1036800,1:16,5760,32) -> Half(2073600,1:8,11520,64) ***************
[08/10/2023-11:16:39] [V] [TRT] =============== Computing reformatting costs
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(64800,32400,180,1) -> Half(64800,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(64800,1,360,2) -> Half(64800,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(32400,1:4,180,1) -> Half(64800,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(32400,32400:2,180,1) -> Half(64800,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(32400,1:8,180,1) -> Half(64800,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(32400,1:16,180,1) -> Half(64800,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] =============== Computing reformatting costs
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(16588800,32400,180,1) -> Float(16588800,1,92160,512) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(16588800,32400,180,1) -> Float(4147200,1:4,23040,128) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(16588800,32400,180,1) -> Half(16588800,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(16588800,32400,180,1) -> Half(8294400,32400:2,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(16588800,32400,180,1) -> Half(2073600,1:8,11520,64) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(16588800,32400,180,1) -> Half(1036800,1:16,5760,32) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(16588800,1,92160,512) -> Float(16588800,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(16588800,1,92160,512) -> Float(4147200,1:4,23040,128) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(16588800,1,92160,512) -> Half(16588800,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(16588800,1,92160,512) -> Half(8294400,32400:2,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(16588800,1,92160,512) -> Half(2073600,1:8,11520,64) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(16588800,1,92160,512) -> Half(1036800,1:16,5760,32) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(4147200,1:4,23040,128) -> Float(16588800,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(4147200,1:4,23040,128) -> Float(16588800,1,92160,512) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(4147200,1:4,23040,128) -> Half(16588800,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(4147200,1:4,23040,128) -> Half(8294400,32400:2,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(4147200,1:4,23040,128) -> Half(2073600,1:8,11520,64) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(4147200,1:4,23040,128) -> Half(1036800,1:16,5760,32) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(16588800,32400,180,1) -> Float(16588800,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(16588800,32400,180,1) -> Float(16588800,1,92160,512) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(16588800,32400,180,1) -> Float(4147200,1:4,23040,128) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(16588800,32400,180,1) -> Half(8294400,32400:2,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(16588800,32400,180,1) -> Half(2073600,1:8,11520,64) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(16588800,32400,180,1) -> Half(1036800,1:16,5760,32) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(8294400,32400:2,180,1) -> Float(16588800,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(8294400,32400:2,180,1) -> Float(16588800,1,92160,512) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(8294400,32400:2,180,1) -> Float(4147200,1:4,23040,128) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(8294400,32400:2,180,1) -> Half(16588800,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(8294400,32400:2,180,1) -> Half(2073600,1:8,11520,64) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(8294400,32400:2,180,1) -> Half(1036800,1:16,5760,32) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(2073600,1:8,11520,64) -> Float(16588800,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(2073600,1:8,11520,64) -> Float(16588800,1,92160,512) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(2073600,1:8,11520,64) -> Float(4147200,1:4,23040,128) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(2073600,1:8,11520,64) -> Half(16588800,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(2073600,1:8,11520,64) -> Half(8294400,32400:2,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(2073600,1:8,11520,64) -> Half(1036800,1:16,5760,32) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(1036800,1:16,5760,32) -> Float(16588800,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(1036800,1:16,5760,32) -> Float(16588800,1,92160,512) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(1036800,1:16,5760,32) -> Float(4147200,1:4,23040,128) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(1036800,1:16,5760,32) -> Half(16588800,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(1036800,1:16,5760,32) -> Half(8294400,32400:2,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(1036800,1:16,5760,32) -> Half(2073600,1:8,11520,64) ***************
[08/10/2023-11:16:39] [V] [TRT] =============== Computing reformatting costs
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(32400,32400,180,1) -> Half(32400,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(32400,1,180,1) -> Half(32400,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(32400,1:4,180,1) -> Half(32400,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(32400,32400:2,180,1) -> Half(32400,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(32400,1:8,180,1) -> Half(32400,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(32400,1:16,180,1) -> Half(32400,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] =============== Computing reformatting costs
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(2073600,32400,180,1) -> Float(2073600,1,11520,64) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(2073600,32400,180,1) -> Float(518400,1:4,2880,16) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(2073600,32400,180,1) -> Half(2073600,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(2073600,32400,180,1) -> Half(1036800,32400:2,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(2073600,32400,180,1) -> Half(259200,1:8,1440,8) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(2073600,32400,180,1) -> Half(129600,1:16,720,4) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(2073600,1,11520,64) -> Float(2073600,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(2073600,1,11520,64) -> Float(518400,1:4,2880,16) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(2073600,1,11520,64) -> Half(2073600,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(2073600,1,11520,64) -> Half(1036800,32400:2,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(2073600,1,11520,64) -> Half(259200,1:8,1440,8) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(2073600,1,11520,64) -> Half(129600,1:16,720,4) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(518400,1:4,2880,16) -> Float(2073600,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(518400,1:4,2880,16) -> Float(2073600,1,11520,64) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(518400,1:4,2880,16) -> Half(2073600,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(518400,1:4,2880,16) -> Half(1036800,32400:2,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(518400,1:4,2880,16) -> Half(259200,1:8,1440,8) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(518400,1:4,2880,16) -> Half(129600,1:16,720,4) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(2073600,32400,180,1) -> Float(2073600,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(2073600,32400,180,1) -> Float(2073600,1,11520,64) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(2073600,32400,180,1) -> Float(518400,1:4,2880,16) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(2073600,32400,180,1) -> Half(1036800,32400:2,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(2073600,32400,180,1) -> Half(259200,1:8,1440,8) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(2073600,32400,180,1) -> Half(129600,1:16,720,4) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(1036800,32400:2,180,1) -> Float(2073600,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(1036800,32400:2,180,1) -> Float(2073600,1,11520,64) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(1036800,32400:2,180,1) -> Float(518400,1:4,2880,16) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(1036800,32400:2,180,1) -> Half(2073600,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(1036800,32400:2,180,1) -> Half(259200,1:8,1440,8) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(1036800,32400:2,180,1) -> Half(129600,1:16,720,4) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(259200,1:8,1440,8) -> Float(2073600,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(259200,1:8,1440,8) -> Float(2073600,1,11520,64) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(259200,1:8,1440,8) -> Float(518400,1:4,2880,16) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(259200,1:8,1440,8) -> Half(2073600,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(259200,1:8,1440,8) -> Half(1036800,32400:2,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(259200,1:8,1440,8) -> Half(129600,1:16,720,4) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(129600,1:16,720,4) -> Float(2073600,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(129600,1:16,720,4) -> Float(2073600,1,11520,64) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(129600,1:16,720,4) -> Float(518400,1:4,2880,16) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(129600,1:16,720,4) -> Half(2073600,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(129600,1:16,720,4) -> Half(1036800,32400:2,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(129600,1:16,720,4) -> Half(259200,1:8,1440,8) ***************
[08/10/2023-11:16:39] [V] [TRT] =============== Computing reformatting costs
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(16588800,32400,180,1) -> Float(16588800,1,92160,512) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(16588800,32400,180,1) -> Float(4147200,1:4,23040,128) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(16588800,32400,180,1) -> Half(16588800,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(16588800,32400,180,1) -> Half(8294400,32400:2,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(16588800,32400,180,1) -> Half(2073600,1:8,11520,64) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(16588800,32400,180,1) -> Half(1036800,1:16,5760,32) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(16588800,1,92160,512) -> Float(16588800,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(16588800,1,92160,512) -> Float(4147200,1:4,23040,128) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(16588800,1,92160,512) -> Half(16588800,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(16588800,1,92160,512) -> Half(8294400,32400:2,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(16588800,1,92160,512) -> Half(2073600,1:8,11520,64) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(16588800,1,92160,512) -> Half(1036800,1:16,5760,32) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(4147200,1:4,23040,128) -> Float(16588800,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(4147200,1:4,23040,128) -> Float(16588800,1,92160,512) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(4147200,1:4,23040,128) -> Half(16588800,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(4147200,1:4,23040,128) -> Half(8294400,32400:2,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(4147200,1:4,23040,128) -> Half(2073600,1:8,11520,64) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(4147200,1:4,23040,128) -> Half(1036800,1:16,5760,32) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(16588800,32400,180,1) -> Float(16588800,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(16588800,32400,180,1) -> Float(16588800,1,92160,512) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(16588800,32400,180,1) -> Float(4147200,1:4,23040,128) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(16588800,32400,180,1) -> Half(8294400,32400:2,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(16588800,32400,180,1) -> Half(2073600,1:8,11520,64) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(16588800,32400,180,1) -> Half(1036800,1:16,5760,32) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(8294400,32400:2,180,1) -> Float(16588800,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(8294400,32400:2,180,1) -> Float(16588800,1,92160,512) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(8294400,32400:2,180,1) -> Float(4147200,1:4,23040,128) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(8294400,32400:2,180,1) -> Half(16588800,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(8294400,32400:2,180,1) -> Half(2073600,1:8,11520,64) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(8294400,32400:2,180,1) -> Half(1036800,1:16,5760,32) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(2073600,1:8,11520,64) -> Float(16588800,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(2073600,1:8,11520,64) -> Float(16588800,1,92160,512) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(2073600,1:8,11520,64) -> Float(4147200,1:4,23040,128) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(2073600,1:8,11520,64) -> Half(16588800,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(2073600,1:8,11520,64) -> Half(8294400,32400:2,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(2073600,1:8,11520,64) -> Half(1036800,1:16,5760,32) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(1036800,1:16,5760,32) -> Float(16588800,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(1036800,1:16,5760,32) -> Float(16588800,1,92160,512) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(1036800,1:16,5760,32) -> Float(4147200,1:4,23040,128) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(1036800,1:16,5760,32) -> Half(16588800,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(1036800,1:16,5760,32) -> Half(8294400,32400:2,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(1036800,1:16,5760,32) -> Half(2073600,1:8,11520,64) ***************
[08/10/2023-11:16:39] [V] [TRT] =============== Computing reformatting costs
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(16588800,32400,180,1) -> Float(16588800,1,92160,512) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(16588800,32400,180,1) -> Float(4147200,1:4,23040,128) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(16588800,32400,180,1) -> Half(16588800,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(16588800,32400,180,1) -> Half(8294400,32400:2,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(16588800,32400,180,1) -> Half(2073600,1:8,11520,64) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(16588800,32400,180,1) -> Half(1036800,1:16,5760,32) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(16588800,1,92160,512) -> Float(16588800,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(16588800,1,92160,512) -> Float(4147200,1:4,23040,128) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(16588800,1,92160,512) -> Half(16588800,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(16588800,1,92160,512) -> Half(8294400,32400:2,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(16588800,1,92160,512) -> Half(2073600,1:8,11520,64) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(16588800,1,92160,512) -> Half(1036800,1:16,5760,32) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(4147200,1:4,23040,128) -> Float(16588800,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(4147200,1:4,23040,128) -> Float(16588800,1,92160,512) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(4147200,1:4,23040,128) -> Half(16588800,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(4147200,1:4,23040,128) -> Half(8294400,32400:2,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(4147200,1:4,23040,128) -> Half(2073600,1:8,11520,64) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(4147200,1:4,23040,128) -> Half(1036800,1:16,5760,32) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(16588800,32400,180,1) -> Float(16588800,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(16588800,32400,180,1) -> Float(16588800,1,92160,512) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(16588800,32400,180,1) -> Float(4147200,1:4,23040,128) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(16588800,32400,180,1) -> Half(8294400,32400:2,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(16588800,32400,180,1) -> Half(2073600,1:8,11520,64) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(16588800,32400,180,1) -> Half(1036800,1:16,5760,32) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(8294400,32400:2,180,1) -> Float(16588800,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(8294400,32400:2,180,1) -> Float(16588800,1,92160,512) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(8294400,32400:2,180,1) -> Float(4147200,1:4,23040,128) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(8294400,32400:2,180,1) -> Half(16588800,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(8294400,32400:2,180,1) -> Half(2073600,1:8,11520,64) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(8294400,32400:2,180,1) -> Half(1036800,1:16,5760,32) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(2073600,1:8,11520,64) -> Float(16588800,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(2073600,1:8,11520,64) -> Float(16588800,1,92160,512) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(2073600,1:8,11520,64) -> Float(4147200,1:4,23040,128) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(2073600,1:8,11520,64) -> Half(16588800,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(2073600,1:8,11520,64) -> Half(8294400,32400:2,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(2073600,1:8,11520,64) -> Half(1036800,1:16,5760,32) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(1036800,1:16,5760,32) -> Float(16588800,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(1036800,1:16,5760,32) -> Float(16588800,1,92160,512) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(1036800,1:16,5760,32) -> Float(4147200,1:4,23040,128) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(1036800,1:16,5760,32) -> Half(16588800,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(1036800,1:16,5760,32) -> Half(8294400,32400:2,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(1036800,1:16,5760,32) -> Half(2073600,1:8,11520,64) ***************
[08/10/2023-11:16:39] [V] [TRT] =============== Computing reformatting costs
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(64800,32400,180,1) -> Half(64800,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(64800,1,360,2) -> Half(64800,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(32400,1:4,180,1) -> Half(64800,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(32400,32400:2,180,1) -> Half(64800,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(32400,1:8,180,1) -> Half(64800,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(32400,1:16,180,1) -> Half(64800,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] =============== Computing reformatting costs
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(16588800,32400,180,1) -> Float(16588800,1,92160,512) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(16588800,32400,180,1) -> Float(4147200,1:4,23040,128) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(16588800,32400,180,1) -> Half(16588800,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(16588800,32400,180,1) -> Half(8294400,32400:2,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(16588800,32400,180,1) -> Half(2073600,1:8,11520,64) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(16588800,32400,180,1) -> Half(1036800,1:16,5760,32) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(16588800,1,92160,512) -> Float(16588800,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(16588800,1,92160,512) -> Float(4147200,1:4,23040,128) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(16588800,1,92160,512) -> Half(16588800,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(16588800,1,92160,512) -> Half(8294400,32400:2,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(16588800,1,92160,512) -> Half(2073600,1:8,11520,64) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(16588800,1,92160,512) -> Half(1036800,1:16,5760,32) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(4147200,1:4,23040,128) -> Float(16588800,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(4147200,1:4,23040,128) -> Float(16588800,1,92160,512) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(4147200,1:4,23040,128) -> Half(16588800,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(4147200,1:4,23040,128) -> Half(8294400,32400:2,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(4147200,1:4,23040,128) -> Half(2073600,1:8,11520,64) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(4147200,1:4,23040,128) -> Half(1036800,1:16,5760,32) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(16588800,32400,180,1) -> Float(16588800,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(16588800,32400,180,1) -> Float(16588800,1,92160,512) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(16588800,32400,180,1) -> Float(4147200,1:4,23040,128) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(16588800,32400,180,1) -> Half(8294400,32400:2,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(16588800,32400,180,1) -> Half(2073600,1:8,11520,64) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(16588800,32400,180,1) -> Half(1036800,1:16,5760,32) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(8294400,32400:2,180,1) -> Float(16588800,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(8294400,32400:2,180,1) -> Float(16588800,1,92160,512) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(8294400,32400:2,180,1) -> Float(4147200,1:4,23040,128) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(8294400,32400:2,180,1) -> Half(16588800,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(8294400,32400:2,180,1) -> Half(2073600,1:8,11520,64) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(8294400,32400:2,180,1) -> Half(1036800,1:16,5760,32) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(2073600,1:8,11520,64) -> Float(16588800,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(2073600,1:8,11520,64) -> Float(16588800,1,92160,512) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(2073600,1:8,11520,64) -> Float(4147200,1:4,23040,128) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(2073600,1:8,11520,64) -> Half(16588800,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(2073600,1:8,11520,64) -> Half(8294400,32400:2,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(2073600,1:8,11520,64) -> Half(1036800,1:16,5760,32) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(1036800,1:16,5760,32) -> Float(16588800,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(1036800,1:16,5760,32) -> Float(16588800,1,92160,512) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(1036800,1:16,5760,32) -> Float(4147200,1:4,23040,128) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(1036800,1:16,5760,32) -> Half(16588800,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(1036800,1:16,5760,32) -> Half(8294400,32400:2,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(1036800,1:16,5760,32) -> Half(2073600,1:8,11520,64) ***************
[08/10/2023-11:16:39] [V] [TRT] =============== Computing reformatting costs
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(32400,32400,180,1) -> Half(32400,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(32400,1,180,1) -> Half(32400,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(32400,1:4,180,1) -> Half(32400,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(32400,32400:2,180,1) -> Half(32400,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(32400,1:8,180,1) -> Half(32400,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(32400,1:16,180,1) -> Half(32400,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] =============== Computing reformatting costs
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(16588800,32400,180,1) -> Float(16588800,1,92160,512) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(16588800,32400,180,1) -> Float(4147200,1:4,23040,128) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(16588800,32400,180,1) -> Half(16588800,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(16588800,32400,180,1) -> Half(8294400,32400:2,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(16588800,32400,180,1) -> Half(2073600,1:8,11520,64) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(16588800,32400,180,1) -> Half(1036800,1:16,5760,32) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(16588800,1,92160,512) -> Float(16588800,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(16588800,1,92160,512) -> Float(4147200,1:4,23040,128) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(16588800,1,92160,512) -> Half(16588800,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(16588800,1,92160,512) -> Half(8294400,32400:2,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(16588800,1,92160,512) -> Half(2073600,1:8,11520,64) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(16588800,1,92160,512) -> Half(1036800,1:16,5760,32) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(4147200,1:4,23040,128) -> Float(16588800,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(4147200,1:4,23040,128) -> Float(16588800,1,92160,512) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(4147200,1:4,23040,128) -> Half(16588800,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(4147200,1:4,23040,128) -> Half(8294400,32400:2,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(4147200,1:4,23040,128) -> Half(2073600,1:8,11520,64) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(4147200,1:4,23040,128) -> Half(1036800,1:16,5760,32) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(16588800,32400,180,1) -> Float(16588800,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(16588800,32400,180,1) -> Float(16588800,1,92160,512) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(16588800,32400,180,1) -> Float(4147200,1:4,23040,128) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(16588800,32400,180,1) -> Half(8294400,32400:2,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(16588800,32400,180,1) -> Half(2073600,1:8,11520,64) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(16588800,32400,180,1) -> Half(1036800,1:16,5760,32) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(8294400,32400:2,180,1) -> Float(16588800,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(8294400,32400:2,180,1) -> Float(16588800,1,92160,512) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(8294400,32400:2,180,1) -> Float(4147200,1:4,23040,128) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(8294400,32400:2,180,1) -> Half(16588800,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(8294400,32400:2,180,1) -> Half(2073600,1:8,11520,64) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(8294400,32400:2,180,1) -> Half(1036800,1:16,5760,32) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(2073600,1:8,11520,64) -> Float(16588800,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(2073600,1:8,11520,64) -> Float(16588800,1,92160,512) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(2073600,1:8,11520,64) -> Float(4147200,1:4,23040,128) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(2073600,1:8,11520,64) -> Half(16588800,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(2073600,1:8,11520,64) -> Half(8294400,32400:2,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(2073600,1:8,11520,64) -> Half(1036800,1:16,5760,32) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(1036800,1:16,5760,32) -> Float(16588800,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(1036800,1:16,5760,32) -> Float(16588800,1,92160,512) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(1036800,1:16,5760,32) -> Float(4147200,1:4,23040,128) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(1036800,1:16,5760,32) -> Half(16588800,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(1036800,1:16,5760,32) -> Half(8294400,32400:2,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(1036800,1:16,5760,32) -> Half(2073600,1:8,11520,64) ***************
[08/10/2023-11:16:39] [V] [TRT] =============== Computing reformatting costs
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(97200,32400,180,1) -> Half(97200,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(97200,1,540,3) -> Half(97200,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(32400,1:4,180,1) -> Half(97200,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(64800,32400:2,180,1) -> Half(97200,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(32400,1:8,180,1) -> Half(97200,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(32400,1:16,180,1) -> Half(97200,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] =============== Computing reformatting costs
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(16588800,32400,180,1) -> Float(16588800,1,92160,512) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(16588800,32400,180,1) -> Float(4147200,1:4,23040,128) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(16588800,32400,180,1) -> Half(16588800,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(16588800,32400,180,1) -> Half(8294400,32400:2,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(16588800,32400,180,1) -> Half(2073600,1:8,11520,64) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(16588800,32400,180,1) -> Half(1036800,1:16,5760,32) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(16588800,1,92160,512) -> Float(16588800,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(16588800,1,92160,512) -> Float(4147200,1:4,23040,128) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(16588800,1,92160,512) -> Half(16588800,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(16588800,1,92160,512) -> Half(8294400,32400:2,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(16588800,1,92160,512) -> Half(2073600,1:8,11520,64) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(16588800,1,92160,512) -> Half(1036800,1:16,5760,32) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(4147200,1:4,23040,128) -> Float(16588800,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(4147200,1:4,23040,128) -> Float(16588800,1,92160,512) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(4147200,1:4,23040,128) -> Half(16588800,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(4147200,1:4,23040,128) -> Half(8294400,32400:2,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(4147200,1:4,23040,128) -> Half(2073600,1:8,11520,64) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(4147200,1:4,23040,128) -> Half(1036800,1:16,5760,32) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(16588800,32400,180,1) -> Float(16588800,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(16588800,32400,180,1) -> Float(16588800,1,92160,512) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(16588800,32400,180,1) -> Float(4147200,1:4,23040,128) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(16588800,32400,180,1) -> Half(8294400,32400:2,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(16588800,32400,180,1) -> Half(2073600,1:8,11520,64) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(16588800,32400,180,1) -> Half(1036800,1:16,5760,32) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(8294400,32400:2,180,1) -> Float(16588800,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(8294400,32400:2,180,1) -> Float(16588800,1,92160,512) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(8294400,32400:2,180,1) -> Float(4147200,1:4,23040,128) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(8294400,32400:2,180,1) -> Half(16588800,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(8294400,32400:2,180,1) -> Half(2073600,1:8,11520,64) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(8294400,32400:2,180,1) -> Half(1036800,1:16,5760,32) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(2073600,1:8,11520,64) -> Float(16588800,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(2073600,1:8,11520,64) -> Float(16588800,1,92160,512) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(2073600,1:8,11520,64) -> Float(4147200,1:4,23040,128) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(2073600,1:8,11520,64) -> Half(16588800,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(2073600,1:8,11520,64) -> Half(8294400,32400:2,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(2073600,1:8,11520,64) -> Half(1036800,1:16,5760,32) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(1036800,1:16,5760,32) -> Float(16588800,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(1036800,1:16,5760,32) -> Float(16588800,1,92160,512) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(1036800,1:16,5760,32) -> Float(4147200,1:4,23040,128) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(1036800,1:16,5760,32) -> Half(16588800,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(1036800,1:16,5760,32) -> Half(8294400,32400:2,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(1036800,1:16,5760,32) -> Half(2073600,1:8,11520,64) ***************
[08/10/2023-11:16:39] [V] [TRT] =============== Computing reformatting costs
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(64800,32400,180,1) -> Half(64800,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(64800,1,360,2) -> Half(64800,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(32400,1:4,180,1) -> Half(64800,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(32400,32400:2,180,1) -> Half(64800,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(32400,1:8,180,1) -> Half(64800,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(32400,1:16,180,1) -> Half(64800,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] =============== Computing reformatting costs
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(16588800,32400,180,1) -> Float(16588800,1,92160,512) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(16588800,32400,180,1) -> Float(4147200,1:4,23040,128) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(16588800,32400,180,1) -> Half(16588800,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(16588800,32400,180,1) -> Half(8294400,32400:2,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(16588800,32400,180,1) -> Half(2073600,1:8,11520,64) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(16588800,32400,180,1) -> Half(1036800,1:16,5760,32) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(16588800,1,92160,512) -> Float(16588800,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(16588800,1,92160,512) -> Float(4147200,1:4,23040,128) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(16588800,1,92160,512) -> Half(16588800,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(16588800,1,92160,512) -> Half(8294400,32400:2,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(16588800,1,92160,512) -> Half(2073600,1:8,11520,64) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(16588800,1,92160,512) -> Half(1036800,1:16,5760,32) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(4147200,1:4,23040,128) -> Float(16588800,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(4147200,1:4,23040,128) -> Float(16588800,1,92160,512) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(4147200,1:4,23040,128) -> Half(16588800,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(4147200,1:4,23040,128) -> Half(8294400,32400:2,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(4147200,1:4,23040,128) -> Half(2073600,1:8,11520,64) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(4147200,1:4,23040,128) -> Half(1036800,1:16,5760,32) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(16588800,32400,180,1) -> Float(16588800,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(16588800,32400,180,1) -> Float(16588800,1,92160,512) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(16588800,32400,180,1) -> Float(4147200,1:4,23040,128) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(16588800,32400,180,1) -> Half(8294400,32400:2,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(16588800,32400,180,1) -> Half(2073600,1:8,11520,64) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(16588800,32400,180,1) -> Half(1036800,1:16,5760,32) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(8294400,32400:2,180,1) -> Float(16588800,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(8294400,32400:2,180,1) -> Float(16588800,1,92160,512) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(8294400,32400:2,180,1) -> Float(4147200,1:4,23040,128) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(8294400,32400:2,180,1) -> Half(16588800,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(8294400,32400:2,180,1) -> Half(2073600,1:8,11520,64) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(8294400,32400:2,180,1) -> Half(1036800,1:16,5760,32) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(2073600,1:8,11520,64) -> Float(16588800,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(2073600,1:8,11520,64) -> Float(16588800,1,92160,512) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(2073600,1:8,11520,64) -> Float(4147200,1:4,23040,128) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(2073600,1:8,11520,64) -> Half(16588800,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(2073600,1:8,11520,64) -> Half(8294400,32400:2,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(2073600,1:8,11520,64) -> Half(1036800,1:16,5760,32) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(1036800,1:16,5760,32) -> Float(16588800,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(1036800,1:16,5760,32) -> Float(16588800,1,92160,512) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(1036800,1:16,5760,32) -> Float(4147200,1:4,23040,128) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(1036800,1:16,5760,32) -> Half(16588800,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(1036800,1:16,5760,32) -> Half(8294400,32400:2,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(1036800,1:16,5760,32) -> Half(2073600,1:8,11520,64) ***************
[08/10/2023-11:16:39] [V] [TRT] =============== Computing reformatting costs
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(64800,32400,180,1) -> Half(64800,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(64800,1,360,2) -> Half(64800,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(32400,1:4,180,1) -> Half(64800,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(32400,32400:2,180,1) -> Half(64800,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(32400,1:8,180,1) -> Half(64800,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(32400,1:16,180,1) -> Half(64800,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] =============== Computing reformatting costs
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(16588800,32400,180,1) -> Float(16588800,1,92160,512) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(16588800,32400,180,1) -> Float(4147200,1:4,23040,128) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(16588800,32400,180,1) -> Half(16588800,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(16588800,32400,180,1) -> Half(8294400,32400:2,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(16588800,32400,180,1) -> Half(2073600,1:8,11520,64) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(16588800,32400,180,1) -> Half(1036800,1:16,5760,32) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(16588800,1,92160,512) -> Float(16588800,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(16588800,1,92160,512) -> Float(4147200,1:4,23040,128) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(16588800,1,92160,512) -> Half(16588800,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(16588800,1,92160,512) -> Half(8294400,32400:2,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(16588800,1,92160,512) -> Half(2073600,1:8,11520,64) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(16588800,1,92160,512) -> Half(1036800,1:16,5760,32) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(4147200,1:4,23040,128) -> Float(16588800,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(4147200,1:4,23040,128) -> Float(16588800,1,92160,512) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(4147200,1:4,23040,128) -> Half(16588800,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(4147200,1:4,23040,128) -> Half(8294400,32400:2,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(4147200,1:4,23040,128) -> Half(2073600,1:8,11520,64) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(4147200,1:4,23040,128) -> Half(1036800,1:16,5760,32) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(16588800,32400,180,1) -> Float(16588800,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(16588800,32400,180,1) -> Float(16588800,1,92160,512) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(16588800,32400,180,1) -> Float(4147200,1:4,23040,128) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(16588800,32400,180,1) -> Half(8294400,32400:2,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(16588800,32400,180,1) -> Half(2073600,1:8,11520,64) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(16588800,32400,180,1) -> Half(1036800,1:16,5760,32) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(8294400,32400:2,180,1) -> Float(16588800,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(8294400,32400:2,180,1) -> Float(16588800,1,92160,512) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(8294400,32400:2,180,1) -> Float(4147200,1:4,23040,128) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(8294400,32400:2,180,1) -> Half(16588800,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(8294400,32400:2,180,1) -> Half(2073600,1:8,11520,64) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(8294400,32400:2,180,1) -> Half(1036800,1:16,5760,32) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(2073600,1:8,11520,64) -> Float(16588800,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(2073600,1:8,11520,64) -> Float(16588800,1,92160,512) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(2073600,1:8,11520,64) -> Float(4147200,1:4,23040,128) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(2073600,1:8,11520,64) -> Half(16588800,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(2073600,1:8,11520,64) -> Half(8294400,32400:2,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(2073600,1:8,11520,64) -> Half(1036800,1:16,5760,32) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(1036800,1:16,5760,32) -> Float(16588800,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(1036800,1:16,5760,32) -> Float(16588800,1,92160,512) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(1036800,1:16,5760,32) -> Float(4147200,1:4,23040,128) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(1036800,1:16,5760,32) -> Half(16588800,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(1036800,1:16,5760,32) -> Half(8294400,32400:2,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(1036800,1:16,5760,32) -> Half(2073600,1:8,11520,64) ***************
[08/10/2023-11:16:39] [V] [TRT] =============== Computing reformatting costs
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(64800,32400,180,1) -> Half(64800,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(64800,1,360,2) -> Half(64800,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(32400,1:4,180,1) -> Half(64800,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(32400,32400:2,180,1) -> Half(64800,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(32400,1:8,180,1) -> Half(64800,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(32400,1:16,180,1) -> Half(64800,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] =============== Computing reformatting costs
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(16588800,32400,180,1) -> Float(16588800,1,92160,512) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(16588800,32400,180,1) -> Float(4147200,1:4,23040,128) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(16588800,32400,180,1) -> Half(16588800,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(16588800,32400,180,1) -> Half(8294400,32400:2,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(16588800,32400,180,1) -> Half(2073600,1:8,11520,64) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(16588800,32400,180,1) -> Half(1036800,1:16,5760,32) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(16588800,1,92160,512) -> Float(16588800,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(16588800,1,92160,512) -> Float(4147200,1:4,23040,128) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(16588800,1,92160,512) -> Half(16588800,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(16588800,1,92160,512) -> Half(8294400,32400:2,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(16588800,1,92160,512) -> Half(2073600,1:8,11520,64) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(16588800,1,92160,512) -> Half(1036800,1:16,5760,32) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(4147200,1:4,23040,128) -> Float(16588800,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(4147200,1:4,23040,128) -> Float(16588800,1,92160,512) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(4147200,1:4,23040,128) -> Half(16588800,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(4147200,1:4,23040,128) -> Half(8294400,32400:2,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(4147200,1:4,23040,128) -> Half(2073600,1:8,11520,64) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(4147200,1:4,23040,128) -> Half(1036800,1:16,5760,32) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(16588800,32400,180,1) -> Float(16588800,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(16588800,32400,180,1) -> Float(16588800,1,92160,512) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(16588800,32400,180,1) -> Float(4147200,1:4,23040,128) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(16588800,32400,180,1) -> Half(8294400,32400:2,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(16588800,32400,180,1) -> Half(2073600,1:8,11520,64) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(16588800,32400,180,1) -> Half(1036800,1:16,5760,32) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(8294400,32400:2,180,1) -> Float(16588800,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(8294400,32400:2,180,1) -> Float(16588800,1,92160,512) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(8294400,32400:2,180,1) -> Float(4147200,1:4,23040,128) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(8294400,32400:2,180,1) -> Half(16588800,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(8294400,32400:2,180,1) -> Half(2073600,1:8,11520,64) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(8294400,32400:2,180,1) -> Half(1036800,1:16,5760,32) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(2073600,1:8,11520,64) -> Float(16588800,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(2073600,1:8,11520,64) -> Float(16588800,1,92160,512) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(2073600,1:8,11520,64) -> Float(4147200,1:4,23040,128) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(2073600,1:8,11520,64) -> Half(16588800,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(2073600,1:8,11520,64) -> Half(8294400,32400:2,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(2073600,1:8,11520,64) -> Half(1036800,1:16,5760,32) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(1036800,1:16,5760,32) -> Float(16588800,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(1036800,1:16,5760,32) -> Float(16588800,1,92160,512) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(1036800,1:16,5760,32) -> Float(4147200,1:4,23040,128) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(1036800,1:16,5760,32) -> Half(16588800,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(1036800,1:16,5760,32) -> Half(8294400,32400:2,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(1036800,1:16,5760,32) -> Half(2073600,1:8,11520,64) ***************
[08/10/2023-11:16:39] [V] [TRT] =============== Computing reformatting costs
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(64800,32400,180,1) -> Half(64800,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(64800,1,360,2) -> Half(64800,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(32400,1:4,180,1) -> Half(64800,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(32400,32400:2,180,1) -> Half(64800,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(32400,1:8,180,1) -> Half(64800,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(32400,1:16,180,1) -> Half(64800,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] =============== Computing reformatting costs
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(16588800,32400,180,1) -> Float(16588800,1,92160,512) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(16588800,32400,180,1) -> Float(4147200,1:4,23040,128) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(16588800,32400,180,1) -> Half(16588800,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(16588800,32400,180,1) -> Half(8294400,32400:2,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(16588800,32400,180,1) -> Half(2073600,1:8,11520,64) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(16588800,32400,180,1) -> Half(1036800,1:16,5760,32) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(16588800,1,92160,512) -> Float(16588800,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(16588800,1,92160,512) -> Float(4147200,1:4,23040,128) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(16588800,1,92160,512) -> Half(16588800,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(16588800,1,92160,512) -> Half(8294400,32400:2,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(16588800,1,92160,512) -> Half(2073600,1:8,11520,64) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(16588800,1,92160,512) -> Half(1036800,1:16,5760,32) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(4147200,1:4,23040,128) -> Float(16588800,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(4147200,1:4,23040,128) -> Float(16588800,1,92160,512) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(4147200,1:4,23040,128) -> Half(16588800,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(4147200,1:4,23040,128) -> Half(8294400,32400:2,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(4147200,1:4,23040,128) -> Half(2073600,1:8,11520,64) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(4147200,1:4,23040,128) -> Half(1036800,1:16,5760,32) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(16588800,32400,180,1) -> Float(16588800,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(16588800,32400,180,1) -> Float(16588800,1,92160,512) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(16588800,32400,180,1) -> Float(4147200,1:4,23040,128) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(16588800,32400,180,1) -> Half(8294400,32400:2,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(16588800,32400,180,1) -> Half(2073600,1:8,11520,64) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(16588800,32400,180,1) -> Half(1036800,1:16,5760,32) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(8294400,32400:2,180,1) -> Float(16588800,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(8294400,32400:2,180,1) -> Float(16588800,1,92160,512) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(8294400,32400:2,180,1) -> Float(4147200,1:4,23040,128) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(8294400,32400:2,180,1) -> Half(16588800,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(8294400,32400:2,180,1) -> Half(2073600,1:8,11520,64) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(8294400,32400:2,180,1) -> Half(1036800,1:16,5760,32) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(2073600,1:8,11520,64) -> Float(16588800,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(2073600,1:8,11520,64) -> Float(16588800,1,92160,512) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(2073600,1:8,11520,64) -> Float(4147200,1:4,23040,128) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(2073600,1:8,11520,64) -> Half(16588800,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(2073600,1:8,11520,64) -> Half(8294400,32400:2,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(2073600,1:8,11520,64) -> Half(1036800,1:16,5760,32) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(1036800,1:16,5760,32) -> Float(16588800,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(1036800,1:16,5760,32) -> Float(16588800,1,92160,512) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(1036800,1:16,5760,32) -> Float(4147200,1:4,23040,128) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(1036800,1:16,5760,32) -> Half(16588800,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(1036800,1:16,5760,32) -> Half(8294400,32400:2,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(1036800,1:16,5760,32) -> Half(2073600,1:8,11520,64) ***************
[08/10/2023-11:16:39] [V] [TRT] =============== Computing reformatting costs
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(32400,32400,180,1) -> Half(32400,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(32400,1,180,1) -> Half(32400,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(32400,1:4,180,1) -> Half(32400,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(32400,32400:2,180,1) -> Half(32400,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(32400,1:8,180,1) -> Half(32400,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(32400,1:16,180,1) -> Half(32400,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] =============== Computing reformatting costs
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(2073600,32400,180,1) -> Float(2073600,1,11520,64) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(2073600,32400,180,1) -> Float(518400,1:4,2880,16) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(2073600,32400,180,1) -> Half(2073600,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(2073600,32400,180,1) -> Half(1036800,32400:2,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(2073600,32400,180,1) -> Half(259200,1:8,1440,8) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(2073600,32400,180,1) -> Half(129600,1:16,720,4) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(2073600,1,11520,64) -> Float(2073600,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(2073600,1,11520,64) -> Float(518400,1:4,2880,16) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(2073600,1,11520,64) -> Half(2073600,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(2073600,1,11520,64) -> Half(1036800,32400:2,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(2073600,1,11520,64) -> Half(259200,1:8,1440,8) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(2073600,1,11520,64) -> Half(129600,1:16,720,4) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(518400,1:4,2880,16) -> Float(2073600,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(518400,1:4,2880,16) -> Float(2073600,1,11520,64) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(518400,1:4,2880,16) -> Half(2073600,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(518400,1:4,2880,16) -> Half(1036800,32400:2,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(518400,1:4,2880,16) -> Half(259200,1:8,1440,8) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(518400,1:4,2880,16) -> Half(129600,1:16,720,4) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(2073600,32400,180,1) -> Float(2073600,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(2073600,32400,180,1) -> Float(2073600,1,11520,64) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(2073600,32400,180,1) -> Float(518400,1:4,2880,16) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(2073600,32400,180,1) -> Half(1036800,32400:2,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(2073600,32400,180,1) -> Half(259200,1:8,1440,8) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(2073600,32400,180,1) -> Half(129600,1:16,720,4) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(1036800,32400:2,180,1) -> Float(2073600,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(1036800,32400:2,180,1) -> Float(2073600,1,11520,64) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(1036800,32400:2,180,1) -> Float(518400,1:4,2880,16) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(1036800,32400:2,180,1) -> Half(2073600,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(1036800,32400:2,180,1) -> Half(259200,1:8,1440,8) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(1036800,32400:2,180,1) -> Half(129600,1:16,720,4) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(259200,1:8,1440,8) -> Float(2073600,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(259200,1:8,1440,8) -> Float(2073600,1,11520,64) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(259200,1:8,1440,8) -> Float(518400,1:4,2880,16) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(259200,1:8,1440,8) -> Half(2073600,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(259200,1:8,1440,8) -> Half(1036800,32400:2,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(259200,1:8,1440,8) -> Half(129600,1:16,720,4) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(129600,1:16,720,4) -> Float(2073600,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(129600,1:16,720,4) -> Float(2073600,1,11520,64) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(129600,1:16,720,4) -> Float(518400,1:4,2880,16) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(129600,1:16,720,4) -> Half(2073600,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(129600,1:16,720,4) -> Half(1036800,32400:2,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(129600,1:16,720,4) -> Half(259200,1:8,1440,8) ***************
[08/10/2023-11:16:39] [V] [TRT] =============== Computing reformatting costs
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(8294400,32400,180,1) -> Float(8294400,1,46080,256) ***************
[08/10/2023-11:16:39] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(<in> -> Conv_158 + Relu_159 || Conv_161 + Relu_162 || Conv_164 + Relu_165 || Conv_167 + Relu_168) (Reformat)
[08/10/2023-11:16:39] [V] [TRT] Tactic: 0x00000000000003e8 Time: 2.62731
[08/10/2023-11:16:39] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.472457
[08/10/2023-11:16:39] [V] [TRT] Tactic: 0x0000000000000000 Time: 2.61484
[08/10/2023-11:16:39] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 0.472457
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(8294400,32400,180,1) -> Float(2073600,1:4,11520,64) ***************
[08/10/2023-11:16:39] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(<in> -> Conv_158 + Relu_159 || Conv_161 + Relu_162 || Conv_164 + Relu_165 || Conv_167 + Relu_168) (Reformat)
[08/10/2023-11:16:39] [V] [TRT] Tactic: 0x00000000000003e8 Time: 2.49237
[08/10/2023-11:16:39] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.48304
[08/10/2023-11:16:39] [V] [TRT] Tactic: 0x0000000000000000 Time: 2.60012
[08/10/2023-11:16:39] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 0.48304
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(8294400,32400,180,1) -> Half(8294400,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(<in> -> Conv_158 + Relu_159 || Conv_161 + Relu_162 || Conv_164 + Relu_165 || Conv_167 + Relu_168) (Reformat)
[08/10/2023-11:16:39] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.507163
[08/10/2023-11:16:39] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.622802
[08/10/2023-11:16:39] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.865371
[08/10/2023-11:16:39] [V] [TRT] Fastest Tactic: 0x00000000000003e8 Time: 0.507163
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(8294400,32400,180,1) -> Half(4147200,32400:2,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(<in> -> Conv_158 + Relu_159 || Conv_161 + Relu_162 || Conv_164 + Relu_165 || Conv_167 + Relu_168) (Reformat)
[08/10/2023-11:16:39] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.768667
[08/10/2023-11:16:39] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.718062
[08/10/2023-11:16:39] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.518528
[08/10/2023-11:16:39] [V] [TRT] Fastest Tactic: 0x0000000000000000 Time: 0.518528
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(8294400,32400,180,1) -> Half(1036800,1:8,5760,32) ***************
[08/10/2023-11:16:39] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(<in> -> Conv_158 + Relu_159 || Conv_161 + Relu_162 || Conv_164 + Relu_165 || Conv_167 + Relu_168) (Reformat)
[08/10/2023-11:16:39] [V] [TRT] Tactic: 0x00000000000003e8 Time: 2.38372
[08/10/2023-11:16:39] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.438583
[08/10/2023-11:16:39] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.523616
[08/10/2023-11:16:39] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 0.438583
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(8294400,32400,180,1) -> Half(518400,1:16,2880,16) ***************
[08/10/2023-11:16:39] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(<in> -> Conv_158 + Relu_159 || Conv_161 + Relu_162 || Conv_164 + Relu_165 || Conv_167 + Relu_168) (Reformat)
[08/10/2023-11:16:39] [V] [TRT] Tactic: 0x00000000000003e8 Time: 2.4145
[08/10/2023-11:16:39] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.43776
[08/10/2023-11:16:39] [V] [TRT] Tactic: 0x0000000000000000 Time: 2.40656
[08/10/2023-11:16:39] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 0.43776
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(8294400,1,46080,256) -> Float(8294400,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(<in> -> Conv_158 + Relu_159 || Conv_161 + Relu_162 || Conv_164 + Relu_165 || Conv_167 + Relu_168) (Reformat)
[08/10/2023-11:16:39] [V] [TRT] Tactic: 0x00000000000003e8 Time: 1.63366
[08/10/2023-11:16:39] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.522391
[08/10/2023-11:16:39] [V] [TRT] Tactic: 0x0000000000000000 Time: 1.73184
[08/10/2023-11:16:39] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 0.522391
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(8294400,1,46080,256) -> Float(2073600,1:4,11520,64) ***************
[08/10/2023-11:16:39] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(<in> -> Conv_158 + Relu_159 || Conv_161 + Relu_162 || Conv_164 + Relu_165 || Conv_167 + Relu_168) (Reformat)
[08/10/2023-11:16:39] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.800005
[08/10/2023-11:16:39] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.406729
[08/10/2023-11:16:39] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.81099
[08/10/2023-11:16:39] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 0.406729
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(8294400,1,46080,256) -> Half(8294400,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(<in> -> Conv_158 + Relu_159 || Conv_161 + Relu_162 || Conv_164 + Relu_165 || Conv_167 + Relu_168) (Reformat)
[08/10/2023-11:16:39] [V] [TRT] Tactic: 0x00000000000003e8 Time: 1.551
[08/10/2023-11:16:39] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.530103
[08/10/2023-11:16:39] [V] [TRT] Tactic: 0x0000000000000000 Time: 1.38939
[08/10/2023-11:16:39] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 0.530103
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(8294400,1,46080,256) -> Half(4147200,32400:2,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(<in> -> Conv_158 + Relu_159 || Conv_161 + Relu_162 || Conv_164 + Relu_165 || Conv_167 + Relu_168) (Reformat)
[08/10/2023-11:16:39] [V] [TRT] Tactic: 0x00000000000003e8 Time: 1.71594
[08/10/2023-11:16:39] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.705454
[08/10/2023-11:16:39] [V] [TRT] Tactic: 0x0000000000000000 Time: 1.45964
[08/10/2023-11:16:39] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 0.705454
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(8294400,1,46080,256) -> Half(1036800,1:8,5760,32) ***************
[08/10/2023-11:16:39] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(<in> -> Conv_158 + Relu_159 || Conv_161 + Relu_162 || Conv_164 + Relu_165 || Conv_167 + Relu_168) (Reformat)
[08/10/2023-11:16:39] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.805691
[08/10/2023-11:16:39] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.386725
[08/10/2023-11:16:39] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.804
[08/10/2023-11:16:39] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 0.386725
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(8294400,1,46080,256) -> Half(518400,1:16,2880,16) ***************
[08/10/2023-11:16:39] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(<in> -> Conv_158 + Relu_159 || Conv_161 + Relu_162 || Conv_164 + Relu_165 || Conv_167 + Relu_168) (Reformat)
[08/10/2023-11:16:39] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.803813
[08/10/2023-11:16:39] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.386235
[08/10/2023-11:16:39] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.803525
[08/10/2023-11:16:39] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 0.386235
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(2073600,1:4,11520,64) -> Float(8294400,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(<in> -> Conv_158 + Relu_159 || Conv_161 + Relu_162 || Conv_164 + Relu_165 || Conv_167 + Relu_168) (Reformat)
[08/10/2023-11:16:39] [V] [TRT] Tactic: 0x00000000000003e8 Time: 1.75901
[08/10/2023-11:16:39] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.523255
[08/10/2023-11:16:39] [V] [TRT] Tactic: 0x0000000000000000 Time: 1.65811
[08/10/2023-11:16:39] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 0.523255
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(2073600,1:4,11520,64) -> Float(8294400,1,46080,256) ***************
[08/10/2023-11:16:39] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(<in> -> Conv_158 + Relu_159 || Conv_161 + Relu_162 || Conv_164 + Relu_165 || Conv_167 + Relu_168) (Reformat)
[08/10/2023-11:16:39] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.796032
[08/10/2023-11:16:39] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.412745
[08/10/2023-11:16:39] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.81424
[08/10/2023-11:16:39] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 0.412745
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(2073600,1:4,11520,64) -> Half(8294400,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(<in> -> Conv_158 + Relu_159 || Conv_161 + Relu_162 || Conv_164 + Relu_165 || Conv_167 + Relu_168) (Reformat)
[08/10/2023-11:16:39] [V] [TRT] Tactic: 0x00000000000003e8 Time: 1.45288
[08/10/2023-11:16:39] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.530702
[08/10/2023-11:16:39] [V] [TRT] Tactic: 0x0000000000000000 Time: 1.37547
[08/10/2023-11:16:39] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 0.530702
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(2073600,1:4,11520,64) -> Half(4147200,32400:2,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(<in> -> Conv_158 + Relu_159 || Conv_161 + Relu_162 || Conv_164 + Relu_165 || Conv_167 + Relu_168) (Reformat)
[08/10/2023-11:16:39] [V] [TRT] Tactic: 0x00000000000003e8 Time: 1.64808
[08/10/2023-11:16:39] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.703383
[08/10/2023-11:16:39] [V] [TRT] Tactic: 0x0000000000000000 Time: 1.53664
[08/10/2023-11:16:39] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 0.703383
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(2073600,1:4,11520,64) -> Half(1036800,1:8,5760,32) ***************
[08/10/2023-11:16:39] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(<in> -> Conv_158 + Relu_159 || Conv_161 + Relu_162 || Conv_164 + Relu_165 || Conv_167 + Relu_168) (Reformat)
[08/10/2023-11:16:39] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.956123
[08/10/2023-11:16:39] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.387854
[08/10/2023-11:16:39] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.955794
[08/10/2023-11:16:39] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 0.387854
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Float(2073600,1:4,11520,64) -> Half(518400,1:16,2880,16) ***************
[08/10/2023-11:16:39] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(<in> -> Conv_158 + Relu_159 || Conv_161 + Relu_162 || Conv_164 + Relu_165 || Conv_167 + Relu_168) (Reformat)
[08/10/2023-11:16:39] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.956329
[08/10/2023-11:16:39] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.387067
[08/10/2023-11:16:39] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.954149
[08/10/2023-11:16:39] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 0.387067
[08/10/2023-11:16:39] [V] [TRT] *************** Autotuning Reformat: Half(8294400,32400,180,1) -> Float(8294400,32400,180,1) ***************
[08/10/2023-11:16:39] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(<in> -> Conv_158 + Relu_159 || Conv_161 + Relu_162 || Conv_164 + Relu_165 || Conv_167 + Relu_168) (Reformat)
[08/10/2023-11:16:40] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.536905
[08/10/2023-11:16:40] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.606249
[08/10/2023-11:16:40] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.781659
[08/10/2023-11:16:40] [V] [TRT] Fastest Tactic: 0x00000000000003e8 Time: 0.536905
[08/10/2023-11:16:40] [V] [TRT] *************** Autotuning Reformat: Half(8294400,32400,180,1) -> Float(8294400,1,46080,256) ***************
[08/10/2023-11:16:40] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(<in> -> Conv_158 + Relu_159 || Conv_161 + Relu_162 || Conv_164 + Relu_165 || Conv_167 + Relu_168) (Reformat)
[08/10/2023-11:16:40] [V] [TRT] Tactic: 0x00000000000003e8 Time: 1.9251
[08/10/2023-11:16:40] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.439506
[08/10/2023-11:16:40] [V] [TRT] Tactic: 0x0000000000000000 Time: 1.92395
[08/10/2023-11:16:40] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 0.439506
[08/10/2023-11:16:40] [V] [TRT] *************** Autotuning Reformat: Half(8294400,32400,180,1) -> Float(2073600,1:4,11520,64) ***************
[08/10/2023-11:16:40] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(<in> -> Conv_158 + Relu_159 || Conv_161 + Relu_162 || Conv_164 + Relu_165 || Conv_167 + Relu_168) (Reformat)
[08/10/2023-11:16:40] [V] [TRT] Tactic: 0x00000000000003e8 Time: 1.93291
[08/10/2023-11:16:40] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.448878
[08/10/2023-11:16:40] [V] [TRT] Tactic: 0x0000000000000000 Time: 1.98602
[08/10/2023-11:16:40] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 0.448878
[08/10/2023-11:16:40] [V] [TRT] *************** Autotuning Reformat: Half(8294400,32400,180,1) -> Half(4147200,32400:2,180,1) ***************
[08/10/2023-11:16:40] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(<in> -> Conv_158 + Relu_159 || Conv_161 + Relu_162 || Conv_164 + Relu_165 || Conv_167 + Relu_168) (Reformat)
[08/10/2023-11:16:40] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.732567
[08/10/2023-11:16:40] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.407589
[08/10/2023-11:16:40] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.513527
[08/10/2023-11:16:40] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 0.407589
[08/10/2023-11:16:40] [V] [TRT] *************** Autotuning Reformat: Half(8294400,32400,180,1) -> Half(1036800,1:8,5760,32) ***************
[08/10/2023-11:16:40] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(<in> -> Conv_158 + Relu_159 || Conv_161 + Relu_162 || Conv_164 + Relu_165 || Conv_167 + Relu_168) (Reformat)
[08/10/2023-11:16:40] [V] [TRT] Tactic: 0x00000000000003e8 Time: 1.95803
[08/10/2023-11:16:40] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.352955
[08/10/2023-11:16:40] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.519854
[08/10/2023-11:16:40] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 0.352955
[08/10/2023-11:16:40] [V] [TRT] *************** Autotuning Reformat: Half(8294400,32400,180,1) -> Half(518400,1:16,2880,16) ***************
[08/10/2023-11:16:40] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(<in> -> Conv_158 + Relu_159 || Conv_161 + Relu_162 || Conv_164 + Relu_165 || Conv_167 + Relu_168) (Reformat)
[08/10/2023-11:16:40] [V] [TRT] Tactic: 0x00000000000003e8 Time: 1.95468
[08/10/2023-11:16:40] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.352507
[08/10/2023-11:16:40] [V] [TRT] Tactic: 0x0000000000000000 Time: 1.9496
[08/10/2023-11:16:40] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 0.352507
[08/10/2023-11:16:40] [V] [TRT] *************** Autotuning Reformat: Half(4147200,32400:2,180,1) -> Float(8294400,32400,180,1) ***************
[08/10/2023-11:16:40] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(<in> -> Conv_158 + Relu_159 || Conv_161 + Relu_162 || Conv_164 + Relu_165 || Conv_167 + Relu_168) (Reformat)
[08/10/2023-11:16:40] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.769952
[08/10/2023-11:16:40] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.539291
[08/10/2023-11:16:40] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.461234
[08/10/2023-11:16:40] [V] [TRT] Fastest Tactic: 0x0000000000000000 Time: 0.461234
[08/10/2023-11:16:40] [V] [TRT] *************** Autotuning Reformat: Half(4147200,32400:2,180,1) -> Float(8294400,1,46080,256) ***************
[08/10/2023-11:16:40] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(<in> -> Conv_158 + Relu_159 || Conv_161 + Relu_162 || Conv_164 + Relu_165 || Conv_167 + Relu_168) (Reformat)
[08/10/2023-11:16:40] [V] [TRT] Tactic: 0x00000000000003e8 Time: 1.4979
[08/10/2023-11:16:40] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.449106
[08/10/2023-11:16:40] [V] [TRT] Tactic: 0x0000000000000000 Time: 1.4977
[08/10/2023-11:16:40] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 0.449106
[08/10/2023-11:16:40] [V] [TRT] *************** Autotuning Reformat: Half(4147200,32400:2,180,1) -> Float(2073600,1:4,11520,64) ***************
[08/10/2023-11:16:40] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(<in> -> Conv_158 + Relu_159 || Conv_161 + Relu_162 || Conv_164 + Relu_165 || Conv_167 + Relu_168) (Reformat)
[08/10/2023-11:16:40] [V] [TRT] Tactic: 0x00000000000003e8 Time: 1.63847
[08/10/2023-11:16:40] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.45867
[08/10/2023-11:16:40] [V] [TRT] Tactic: 0x0000000000000000 Time: 1.67115
[08/10/2023-11:16:40] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 0.45867
[08/10/2023-11:16:40] [V] [TRT] *************** Autotuning Reformat: Half(4147200,32400:2,180,1) -> Half(8294400,32400,180,1) ***************
[08/10/2023-11:16:40] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(<in> -> Conv_158 + Relu_159 || Conv_161 + Relu_162 || Conv_164 + Relu_165 || Conv_167 + Relu_168) (Reformat)
[08/10/2023-11:16:40] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.713522
[08/10/2023-11:16:40] [V] [TRT] Tactic: 0x00000000000003ea Time: 2.78788
[08/10/2023-11:16:40] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.432782
[08/10/2023-11:16:40] [V] [TRT] Fastest Tactic: 0x0000000000000000 Time: 0.432782
[08/10/2023-11:16:40] [V] [TRT] *************** Autotuning Reformat: Half(4147200,32400:2,180,1) -> Half(1036800,1:8,5760,32) ***************
[08/10/2023-11:16:40] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(<in> -> Conv_158 + Relu_159 || Conv_161 + Relu_162 || Conv_164 + Relu_165 || Conv_167 + Relu_168) (Reformat)
[08/10/2023-11:16:40] [V] [TRT] Tactic: 0x00000000000003e8 Time: 1.60216
[08/10/2023-11:16:40] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.446656
[08/10/2023-11:16:40] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.433147
[08/10/2023-11:16:40] [V] [TRT] Fastest Tactic: 0x0000000000000000 Time: 0.433147
[08/10/2023-11:16:40] [V] [TRT] *************** Autotuning Reformat: Half(4147200,32400:2,180,1) -> Half(518400,1:16,2880,16) ***************
[08/10/2023-11:16:40] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(<in> -> Conv_158 + Relu_159 || Conv_161 + Relu_162 || Conv_164 + Relu_165 || Conv_167 + Relu_168) (Reformat)
[08/10/2023-11:16:40] [V] [TRT] Tactic: 0x00000000000003e8 Time: 1.60446
[08/10/2023-11:16:40] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.446789
[08/10/2023-11:16:40] [V] [TRT] Tactic: 0x0000000000000000 Time: 1.60769
[08/10/2023-11:16:40] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 0.446789
[08/10/2023-11:16:40] [V] [TRT] *************** Autotuning Reformat: Half(1036800,1:8,5760,32) -> Float(8294400,32400,180,1) ***************
[08/10/2023-11:16:40] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(<in> -> Conv_158 + Relu_159 || Conv_161 + Relu_162 || Conv_164 + Relu_165 || Conv_167 + Relu_168) (Reformat)
[08/10/2023-11:16:40] [V] [TRT] Tactic: 0x00000000000003e8 Time: 1.38608
[08/10/2023-11:16:40] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.488526
[08/10/2023-11:16:40] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.405856
[08/10/2023-11:16:40] [V] [TRT] Fastest Tactic: 0x0000000000000000 Time: 0.405856
[08/10/2023-11:16:40] [V] [TRT] *************** Autotuning Reformat: Half(1036800,1:8,5760,32) -> Float(8294400,1,46080,256) ***************
[08/10/2023-11:16:40] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(<in> -> Conv_158 + Relu_159 || Conv_161 + Relu_162 || Conv_164 + Relu_165 || Conv_167 + Relu_168) (Reformat)
[08/10/2023-11:16:40] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.809586
[08/10/2023-11:16:40] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.387703
[08/10/2023-11:16:40] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.809239
[08/10/2023-11:16:40] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 0.387703
[08/10/2023-11:16:40] [V] [TRT] *************** Autotuning Reformat: Half(1036800,1:8,5760,32) -> Float(2073600,1:4,11520,64) ***************
[08/10/2023-11:16:40] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(<in> -> Conv_158 + Relu_159 || Conv_161 + Relu_162 || Conv_164 + Relu_165 || Conv_167 + Relu_168) (Reformat)
[08/10/2023-11:16:40] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.957938
[08/10/2023-11:16:40] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.387538
[08/10/2023-11:16:40] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.957481
[08/10/2023-11:16:40] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 0.387538
[08/10/2023-11:16:40] [V] [TRT] *************** Autotuning Reformat: Half(1036800,1:8,5760,32) -> Half(8294400,32400,180,1) ***************
[08/10/2023-11:16:40] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(<in> -> Conv_158 + Relu_159 || Conv_161 + Relu_162 || Conv_164 + Relu_165 || Conv_167 + Relu_168) (Reformat)
[08/10/2023-11:16:40] [V] [TRT] Tactic: 0x00000000000003e8 Time: 1.24134
[08/10/2023-11:16:40] [V] [TRT] Tactic: 0x00000000000003ea Time: 1.91936
[08/10/2023-11:16:40] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.343191
[08/10/2023-11:16:40] [V] [TRT] Fastest Tactic: 0x0000000000000000 Time: 0.343191
[08/10/2023-11:16:40] [V] [TRT] *************** Autotuning Reformat: Half(1036800,1:8,5760,32) -> Half(4147200,32400:2,180,1) ***************
[08/10/2023-11:16:40] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(<in> -> Conv_158 + Relu_159 || Conv_161 + Relu_162 || Conv_164 + Relu_165 || Conv_167 + Relu_168) (Reformat)
[08/10/2023-11:16:40] [V] [TRT] Tactic: 0x00000000000003e8 Time: 1.39456
[08/10/2023-11:16:40] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.645531
[08/10/2023-11:16:40] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.341019
[08/10/2023-11:16:40] [V] [TRT] Fastest Tactic: 0x0000000000000000 Time: 0.341019
[08/10/2023-11:16:40] [V] [TRT] *************** Autotuning Reformat: Half(1036800,1:8,5760,32) -> Half(518400,1:16,2880,16) ***************
[08/10/2023-11:16:40] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(<in> -> Conv_158 + Relu_159 || Conv_161 + Relu_162 || Conv_164 + Relu_165 || Conv_167 + Relu_168) (Reformat)
[08/10/2023-11:16:40] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.907534
[08/10/2023-11:16:40] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.395826
[08/10/2023-11:16:40] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.888919
[08/10/2023-11:16:40] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 0.395826
[08/10/2023-11:16:40] [V] [TRT] *************** Autotuning Reformat: Half(518400,1:16,2880,16) -> Float(8294400,32400,180,1) ***************
[08/10/2023-11:16:40] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(<in> -> Conv_158 + Relu_159 || Conv_161 + Relu_162 || Conv_164 + Relu_165 || Conv_167 + Relu_168) (Reformat)
[08/10/2023-11:16:40] [V] [TRT] Tactic: 0x00000000000003e8 Time: 1.31085
[08/10/2023-11:16:40] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.48869
[08/10/2023-11:16:40] [V] [TRT] Tactic: 0x0000000000000000 Time: 1.33202
[08/10/2023-11:16:40] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 0.48869
[08/10/2023-11:16:40] [V] [TRT] *************** Autotuning Reformat: Half(518400,1:16,2880,16) -> Float(8294400,1,46080,256) ***************
[08/10/2023-11:16:40] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(<in> -> Conv_158 + Relu_159 || Conv_161 + Relu_162 || Conv_164 + Relu_165 || Conv_167 + Relu_168) (Reformat)
[08/10/2023-11:16:40] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.809819
[08/10/2023-11:16:40] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.387886
[08/10/2023-11:16:40] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.810496
[08/10/2023-11:16:40] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 0.387886
[08/10/2023-11:16:40] [V] [TRT] *************** Autotuning Reformat: Half(518400,1:16,2880,16) -> Float(2073600,1:4,11520,64) ***************
[08/10/2023-11:16:40] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(<in> -> Conv_158 + Relu_159 || Conv_161 + Relu_162 || Conv_164 + Relu_165 || Conv_167 + Relu_168) (Reformat)
[08/10/2023-11:16:40] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.957879
[08/10/2023-11:16:40] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.387657
[08/10/2023-11:16:40] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.957262
[08/10/2023-11:16:40] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 0.387657
[08/10/2023-11:16:40] [V] [TRT] *************** Autotuning Reformat: Half(518400,1:16,2880,16) -> Half(8294400,32400,180,1) ***************
[08/10/2023-11:16:40] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(<in> -> Conv_158 + Relu_159 || Conv_161 + Relu_162 || Conv_164 + Relu_165 || Conv_167 + Relu_168) (Reformat)
[08/10/2023-11:16:40] [V] [TRT] Tactic: 0x00000000000003e8 Time: 1.2439
[08/10/2023-11:16:40] [V] [TRT] Tactic: 0x00000000000003ea Time: 1.92034
[08/10/2023-11:16:40] [V] [TRT] Tactic: 0x0000000000000000 Time: 1.2924
[08/10/2023-11:16:40] [V] [TRT] Fastest Tactic: 0x00000000000003e8 Time: 1.2439
[08/10/2023-11:16:40] [V] [TRT] *************** Autotuning Reformat: Half(518400,1:16,2880,16) -> Half(4147200,32400:2,180,1) ***************
[08/10/2023-11:16:40] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(<in> -> Conv_158 + Relu_159 || Conv_161 + Relu_162 || Conv_164 + Relu_165 || Conv_167 + Relu_168) (Reformat)
[08/10/2023-11:16:40] [V] [TRT] Tactic: 0x00000000000003e8 Time: 1.46133
[08/10/2023-11:16:40] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.645541
[08/10/2023-11:16:40] [V] [TRT] Tactic: 0x0000000000000000 Time: 1.72097
[08/10/2023-11:16:40] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 0.645541
[08/10/2023-11:16:40] [V] [TRT] *************** Autotuning Reformat: Half(518400,1:16,2880,16) -> Half(1036800,1:8,5760,32) ***************
[08/10/2023-11:16:40] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(<in> -> Conv_158 + Relu_159 || Conv_161 + Relu_162 || Conv_164 + Relu_165 || Conv_167 + Relu_168) (Reformat)
[08/10/2023-11:16:40] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.907515
[08/10/2023-11:16:40] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.396288
[08/10/2023-11:16:40] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.887817
[08/10/2023-11:16:40] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 0.396288
[08/10/2023-11:16:40] [V] [TRT] =============== Computing reformatting costs
[08/10/2023-11:16:40] [V] [TRT] *************** Autotuning Reformat: Float(8294400,32400,180,1) -> Float(8294400,1,46080,256) ***************
[08/10/2023-11:16:40] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(input.568 -> <out>) (Reformat)
[08/10/2023-11:16:40] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.381394
[08/10/2023-11:16:40] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.128706
[08/10/2023-11:16:40] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.380215
[08/10/2023-11:16:40] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 0.128706
[08/10/2023-11:16:40] [V] [TRT] *************** Autotuning Reformat: Float(8294400,32400,180,1) -> Float(2073600,1:4,11520,64) ***************
[08/10/2023-11:16:40] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(input.568 -> <out>) (Reformat)
[08/10/2023-11:16:40] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.400352
[08/10/2023-11:16:40] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.128937
[08/10/2023-11:16:40] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.40069
[08/10/2023-11:16:40] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 0.128937
[08/10/2023-11:16:40] [V] [TRT] *************** Autotuning Reformat: Float(8294400,32400,180,1) -> Half(8294400,32400,180,1) ***************
[08/10/2023-11:16:40] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(input.568 -> <out>) (Reformat)
[08/10/2023-11:16:40] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.145902
[08/10/2023-11:16:40] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.188841
[08/10/2023-11:16:40] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.14597
[08/10/2023-11:16:40] [V] [TRT] Fastest Tactic: 0x00000000000003e8 Time: 0.145902
[08/10/2023-11:16:40] [V] [TRT] *************** Autotuning Reformat: Float(8294400,32400,180,1) -> Half(4147200,32400:2,180,1) ***************
[08/10/2023-11:16:40] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(input.568 -> <out>) (Reformat)
[08/10/2023-11:16:40] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.198322
[08/10/2023-11:16:40] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.181998
[08/10/2023-11:16:40] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.198825
[08/10/2023-11:16:40] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 0.181998
[08/10/2023-11:16:40] [V] [TRT] *************** Autotuning Reformat: Float(8294400,32400,180,1) -> Half(1036800,1:8,5760,32) ***************
[08/10/2023-11:16:40] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(input.568 -> <out>) (Reformat)
[08/10/2023-11:16:40] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.407342
[08/10/2023-11:16:40] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.117536
[08/10/2023-11:16:40] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.407726
[08/10/2023-11:16:40] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 0.117536
[08/10/2023-11:16:40] [V] [TRT] *************** Autotuning Reformat: Float(8294400,32400,180,1) -> Half(518400,1:16,2880,16) ***************
[08/10/2023-11:16:40] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(input.568 -> <out>) (Reformat)
[08/10/2023-11:16:40] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.407104
[08/10/2023-11:16:40] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.116533
[08/10/2023-11:16:40] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.404215
[08/10/2023-11:16:40] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 0.116533
[08/10/2023-11:16:40] [V] [TRT] *************** Autotuning Reformat: Float(8294400,1,46080,256) -> Float(8294400,32400,180,1) ***************
[08/10/2023-11:16:40] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(input.568 -> <out>) (Reformat)
[08/10/2023-11:16:40] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.44208
[08/10/2023-11:16:40] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.141607
[08/10/2023-11:16:40] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.461655
[08/10/2023-11:16:40] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 0.141607
[08/10/2023-11:16:40] [V] [TRT] *************** Autotuning Reformat: Float(8294400,1,46080,256) -> Float(2073600,1:4,11520,64) ***************
[08/10/2023-11:16:40] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(input.568 -> <out>) (Reformat)
[08/10/2023-11:16:40] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.205303
[08/10/2023-11:16:40] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.113481
[08/10/2023-11:16:40] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.205531
[08/10/2023-11:16:40] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 0.113481
[08/10/2023-11:16:40] [V] [TRT] *************** Autotuning Reformat: Float(8294400,1,46080,256) -> Half(8294400,32400,180,1) ***************
[08/10/2023-11:16:40] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(input.568 -> <out>) (Reformat)
[08/10/2023-11:16:40] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.419003
[08/10/2023-11:16:40] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.137662
[08/10/2023-11:16:40] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.375246
[08/10/2023-11:16:40] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 0.137662
[08/10/2023-11:16:40] [V] [TRT] *************** Autotuning Reformat: Float(8294400,1,46080,256) -> Half(4147200,32400:2,180,1) ***************
[08/10/2023-11:16:40] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(input.568 -> <out>) (Reformat)
[08/10/2023-11:16:40] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.390254
[08/10/2023-11:16:40] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.177947
[08/10/2023-11:16:40] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.400649
[08/10/2023-11:16:40] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 0.177947
[08/10/2023-11:16:40] [V] [TRT] *************** Autotuning Reformat: Float(8294400,1,46080,256) -> Half(1036800,1:8,5760,32) ***************
[08/10/2023-11:16:40] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(input.568 -> <out>) (Reformat)
[08/10/2023-11:16:40] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.206999
[08/10/2023-11:16:40] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.103904
[08/10/2023-11:16:40] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.206423
[08/10/2023-11:16:40] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 0.103904
[08/10/2023-11:16:40] [V] [TRT] *************** Autotuning Reformat: Float(8294400,1,46080,256) -> Half(518400,1:16,2880,16) ***************
[08/10/2023-11:16:40] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(input.568 -> <out>) (Reformat)
[08/10/2023-11:16:40] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.206313
[08/10/2023-11:16:40] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.10683
[08/10/2023-11:16:40] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.212357
[08/10/2023-11:16:41] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 0.10683
[08/10/2023-11:16:41] [V] [TRT] *************** Autotuning Reformat: Float(2073600,1:4,11520,64) -> Float(8294400,32400,180,1) ***************
[08/10/2023-11:16:41] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(input.568 -> <out>) (Reformat)
[08/10/2023-11:16:41] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.465234
[08/10/2023-11:16:41] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.144873
[08/10/2023-11:16:41] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.430706
[08/10/2023-11:16:41] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 0.144873
[08/10/2023-11:16:41] [V] [TRT] *************** Autotuning Reformat: Float(2073600,1:4,11520,64) -> Float(8294400,1,46080,256) ***************
[08/10/2023-11:16:41] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(input.568 -> <out>) (Reformat)
[08/10/2023-11:16:41] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.209934
[08/10/2023-11:16:41] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.116336
[08/10/2023-11:16:41] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.20912
[08/10/2023-11:16:41] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 0.116336
[08/10/2023-11:16:41] [V] [TRT] *************** Autotuning Reformat: Float(2073600,1:4,11520,64) -> Half(8294400,32400,180,1) ***************
[08/10/2023-11:16:41] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(input.568 -> <out>) (Reformat)
[08/10/2023-11:16:41] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.380018
[08/10/2023-11:16:41] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.138891
[08/10/2023-11:16:41] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.373403
[08/10/2023-11:16:41] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 0.138891
[08/10/2023-11:16:41] [V] [TRT] *************** Autotuning Reformat: Float(2073600,1:4,11520,64) -> Half(4147200,32400:2,180,1) ***************
[08/10/2023-11:16:41] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(input.568 -> <out>) (Reformat)
[08/10/2023-11:16:41] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.392174
[08/10/2023-11:16:41] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.180146
[08/10/2023-11:16:41] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.408818
[08/10/2023-11:16:41] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 0.180146
[08/10/2023-11:16:41] [V] [TRT] *************** Autotuning Reformat: Float(2073600,1:4,11520,64) -> Half(1036800,1:8,5760,32) ***************
[08/10/2023-11:16:41] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(input.568 -> <out>) (Reformat)
[08/10/2023-11:16:41] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.245595
[08/10/2023-11:16:41] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.104658
[08/10/2023-11:16:41] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.245262
[08/10/2023-11:16:41] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 0.104658
[08/10/2023-11:16:41] [V] [TRT] *************** Autotuning Reformat: Float(2073600,1:4,11520,64) -> Half(518400,1:16,2880,16) ***************
[08/10/2023-11:16:41] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(input.568 -> <out>) (Reformat)
[08/10/2023-11:16:41] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.245399
[08/10/2023-11:16:41] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.103961
[08/10/2023-11:16:41] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.2448
[08/10/2023-11:16:41] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 0.103961
[08/10/2023-11:16:41] [V] [TRT] *************** Autotuning Reformat: Half(8294400,32400,180,1) -> Float(8294400,32400,180,1) ***************
[08/10/2023-11:16:41] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(input.568 -> <out>) (Reformat)
[08/10/2023-11:16:41] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.152933
[08/10/2023-11:16:41] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.189056
[08/10/2023-11:16:41] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.153376
[08/10/2023-11:16:41] [V] [TRT] Fastest Tactic: 0x00000000000003e8 Time: 0.152933
[08/10/2023-11:16:41] [V] [TRT] *************** Autotuning Reformat: Half(8294400,32400,180,1) -> Float(8294400,1,46080,256) ***************
[08/10/2023-11:16:41] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(input.568 -> <out>) (Reformat)
[08/10/2023-11:16:41] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.319273
[08/10/2023-11:16:41] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.117691
[08/10/2023-11:16:41] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.319113
[08/10/2023-11:16:41] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 0.117691
[08/10/2023-11:16:41] [V] [TRT] *************** Autotuning Reformat: Half(8294400,32400,180,1) -> Float(2073600,1:4,11520,64) ***************
[08/10/2023-11:16:41] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(input.568 -> <out>) (Reformat)
[08/10/2023-11:16:41] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.348649
[08/10/2023-11:16:41] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.11768
[08/10/2023-11:16:41] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.350071
[08/10/2023-11:16:41] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 0.11768
[08/10/2023-11:16:41] [V] [TRT] *************** Autotuning Reformat: Half(8294400,32400,180,1) -> Half(4147200,32400:2,180,1) ***************
[08/10/2023-11:16:41] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(input.568 -> <out>) (Reformat)
[08/10/2023-11:16:41] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.185225
[08/10/2023-11:16:41] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.107431
[08/10/2023-11:16:41] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.184878
[08/10/2023-11:16:41] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 0.107431
[08/10/2023-11:16:41] [V] [TRT] *************** Autotuning Reformat: Half(8294400,32400,180,1) -> Half(1036800,1:8,5760,32) ***************
[08/10/2023-11:16:41] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(input.568 -> <out>) (Reformat)
[08/10/2023-11:16:41] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.348178
[08/10/2023-11:16:41] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.0968389
[08/10/2023-11:16:41] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.349618
[08/10/2023-11:16:41] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 0.0968389
[08/10/2023-11:16:41] [V] [TRT] *************** Autotuning Reformat: Half(8294400,32400,180,1) -> Half(518400,1:16,2880,16) ***************
[08/10/2023-11:16:41] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(input.568 -> <out>) (Reformat)
[08/10/2023-11:16:41] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.34928
[08/10/2023-11:16:41] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.0963429
[08/10/2023-11:16:41] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.349463
[08/10/2023-11:16:41] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 0.0963429
[08/10/2023-11:16:41] [V] [TRT] *************** Autotuning Reformat: Half(4147200,32400:2,180,1) -> Float(8294400,32400,180,1) ***************
[08/10/2023-11:16:41] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(input.568 -> <out>) (Reformat)
[08/10/2023-11:16:41] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.197934
[08/10/2023-11:16:41] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.144302
[08/10/2023-11:16:41] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.119883
[08/10/2023-11:16:41] [V] [TRT] Fastest Tactic: 0x0000000000000000 Time: 0.119883
[08/10/2023-11:16:41] [V] [TRT] *************** Autotuning Reformat: Half(4147200,32400:2,180,1) -> Float(8294400,1,46080,256) ***************
[08/10/2023-11:16:41] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(input.568 -> <out>) (Reformat)
[08/10/2023-11:16:41] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.229307
[08/10/2023-11:16:41] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.121781
[08/10/2023-11:16:41] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.23392
[08/10/2023-11:16:41] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 0.121781
[08/10/2023-11:16:41] [V] [TRT] *************** Autotuning Reformat: Half(4147200,32400:2,180,1) -> Float(2073600,1:4,11520,64) ***************
[08/10/2023-11:16:41] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(input.568 -> <out>) (Reformat)
[08/10/2023-11:16:41] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.25509
[08/10/2023-11:16:41] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.122729
[08/10/2023-11:16:41] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.254391
[08/10/2023-11:16:41] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 0.122729
[08/10/2023-11:16:41] [V] [TRT] *************** Autotuning Reformat: Half(4147200,32400:2,180,1) -> Half(8294400,32400,180,1) ***************
[08/10/2023-11:16:41] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(input.568 -> <out>) (Reformat)
[08/10/2023-11:16:41] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.18496
[08/10/2023-11:16:41] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.542048
[08/10/2023-11:16:41] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.115319
[08/10/2023-11:16:41] [V] [TRT] Fastest Tactic: 0x0000000000000000 Time: 0.115319
[08/10/2023-11:16:41] [V] [TRT] *************** Autotuning Reformat: Half(4147200,32400:2,180,1) -> Half(1036800,1:8,5760,32) ***************
[08/10/2023-11:16:41] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(input.568 -> <out>) (Reformat)
[08/10/2023-11:16:41] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.242779
[08/10/2023-11:16:41] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.12197
[08/10/2023-11:16:41] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.117125
[08/10/2023-11:16:41] [V] [TRT] Fastest Tactic: 0x0000000000000000 Time: 0.117125
[08/10/2023-11:16:41] [V] [TRT] *************** Autotuning Reformat: Half(4147200,32400:2,180,1) -> Half(518400,1:16,2880,16) ***************
[08/10/2023-11:16:41] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(input.568 -> <out>) (Reformat)
[08/10/2023-11:16:41] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.242519
[08/10/2023-11:16:41] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.119099
[08/10/2023-11:16:41] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.237723
[08/10/2023-11:16:41] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 0.119099
[08/10/2023-11:16:41] [V] [TRT] *************** Autotuning Reformat: Half(1036800,1:8,5760,32) -> Float(8294400,32400,180,1) ***************
[08/10/2023-11:16:41] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(input.568 -> <out>) (Reformat)
[08/10/2023-11:16:41] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.326341
[08/10/2023-11:16:41] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.131744
[08/10/2023-11:16:41] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.107817
[08/10/2023-11:16:41] [V] [TRT] Fastest Tactic: 0x0000000000000000 Time: 0.107817
[08/10/2023-11:16:41] [V] [TRT] *************** Autotuning Reformat: Half(1036800,1:8,5760,32) -> Float(8294400,1,46080,256) ***************
[08/10/2023-11:16:41] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(input.568 -> <out>) (Reformat)
[08/10/2023-11:16:41] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.208261
[08/10/2023-11:16:41] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.105147
[08/10/2023-11:16:41] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.208448
[08/10/2023-11:16:41] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 0.105147
[08/10/2023-11:16:41] [V] [TRT] *************** Autotuning Reformat: Half(1036800,1:8,5760,32) -> Float(2073600,1:4,11520,64) ***************
[08/10/2023-11:16:41] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(input.568 -> <out>) (Reformat)
[08/10/2023-11:16:41] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.245486
[08/10/2023-11:16:41] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.104789
[08/10/2023-11:16:41] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.245737
[08/10/2023-11:16:41] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 0.104789
[08/10/2023-11:16:41] [V] [TRT] *************** Autotuning Reformat: Half(1036800,1:8,5760,32) -> Half(8294400,32400,180,1) ***************
[08/10/2023-11:16:41] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(input.568 -> <out>) (Reformat)
[08/10/2023-11:16:41] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.306971
[08/10/2023-11:16:41] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.442834
[08/10/2023-11:16:41] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.0912274
[08/10/2023-11:16:41] [V] [TRT] Fastest Tactic: 0x0000000000000000 Time: 0.0912274
[08/10/2023-11:16:41] [V] [TRT] *************** Autotuning Reformat: Half(1036800,1:8,5760,32) -> Half(4147200,32400:2,180,1) ***************
[08/10/2023-11:16:41] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(input.568 -> <out>) (Reformat)
[08/10/2023-11:16:41] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.351909
[08/10/2023-11:16:41] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.160151
[08/10/2023-11:16:41] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.090752
[08/10/2023-11:16:41] [V] [TRT] Fastest Tactic: 0x0000000000000000 Time: 0.090752
[08/10/2023-11:16:41] [V] [TRT] *************** Autotuning Reformat: Half(1036800,1:8,5760,32) -> Half(518400,1:16,2880,16) ***************
[08/10/2023-11:16:41] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(input.568 -> <out>) (Reformat)
[08/10/2023-11:16:41] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.228361
[08/10/2023-11:16:41] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.10419
[08/10/2023-11:16:41] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.227909
[08/10/2023-11:16:41] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 0.10419
[08/10/2023-11:16:41] [V] [TRT] *************** Autotuning Reformat: Half(518400,1:16,2880,16) -> Float(8294400,32400,180,1) ***************
[08/10/2023-11:16:41] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(input.568 -> <out>) (Reformat)
[08/10/2023-11:16:41] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.359035
[08/10/2023-11:16:41] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.132818
[08/10/2023-11:16:41] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.344878
[08/10/2023-11:16:41] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 0.132818
[08/10/2023-11:16:41] [V] [TRT] *************** Autotuning Reformat: Half(518400,1:16,2880,16) -> Float(8294400,1,46080,256) ***************
[08/10/2023-11:16:41] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(input.568 -> <out>) (Reformat)
[08/10/2023-11:16:41] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.208681
[08/10/2023-11:16:41] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.105175
[08/10/2023-11:16:41] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.208311
[08/10/2023-11:16:41] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 0.105175
[08/10/2023-11:16:41] [V] [TRT] *************** Autotuning Reformat: Half(518400,1:16,2880,16) -> Float(2073600,1:4,11520,64) ***************
[08/10/2023-11:16:41] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(input.568 -> <out>) (Reformat)
[08/10/2023-11:16:41] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.245307
[08/10/2023-11:16:41] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.105189
[08/10/2023-11:16:41] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.245445
[08/10/2023-11:16:41] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 0.105189
[08/10/2023-11:16:41] [V] [TRT] *************** Autotuning Reformat: Half(518400,1:16,2880,16) -> Half(8294400,32400,180,1) ***************
[08/10/2023-11:16:41] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(input.568 -> <out>) (Reformat)
[08/10/2023-11:16:41] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.302226
[08/10/2023-11:16:41] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.44272
[08/10/2023-11:16:41] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.30597
[08/10/2023-11:16:41] [V] [TRT] Fastest Tactic: 0x00000000000003e8 Time: 0.302226
[08/10/2023-11:16:41] [V] [TRT] *************** Autotuning Reformat: Half(518400,1:16,2880,16) -> Half(4147200,32400:2,180,1) ***************
[08/10/2023-11:16:41] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(input.568 -> <out>) (Reformat)
[08/10/2023-11:16:41] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.324137
[08/10/2023-11:16:41] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.159479
[08/10/2023-11:16:41] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.334807
[08/10/2023-11:16:41] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 0.159479
[08/10/2023-11:16:41] [V] [TRT] *************** Autotuning Reformat: Half(518400,1:16,2880,16) -> Half(1036800,1:8,5760,32) ***************
[08/10/2023-11:16:41] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(input.568 -> <out>) (Reformat)
[08/10/2023-11:16:41] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.227136
[08/10/2023-11:16:41] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.103954
[08/10/2023-11:16:41] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.227941
[08/10/2023-11:16:41] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 0.103954
[08/10/2023-11:16:41] [V] [TRT] =============== Computing reformatting costs
[08/10/2023-11:16:41] [V] [TRT] *************** Autotuning Reformat: Float(97200,32400,180,1) -> Half(97200,32400,180,1) ***************
[08/10/2023-11:16:41] [V] [TRT] *************** Autotuning Reformat: Float(97200,1,540,3) -> Half(97200,32400,180,1) ***************
[08/10/2023-11:16:41] [V] [TRT] *************** Autotuning Reformat: Float(32400,1:4,180,1) -> Half(97200,32400,180,1) ***************
[08/10/2023-11:16:41] [V] [TRT] *************** Autotuning Reformat: Half(64800,32400:2,180,1) -> Half(97200,32400,180,1) ***************
[08/10/2023-11:16:41] [V] [TRT] *************** Autotuning Reformat: Half(32400,1:8,180,1) -> Half(97200,32400,180,1) ***************
[08/10/2023-11:16:41] [V] [TRT] *************** Autotuning Reformat: Half(32400,1:16,180,1) -> Half(97200,32400,180,1) ***************
[08/10/2023-11:16:41] [V] [TRT] =============== Computing reformatting costs
[08/10/2023-11:16:41] [V] [TRT] *************** Autotuning Reformat: Float(8294400,32400,180,1) -> Float(8294400,1,46080,256) ***************
[08/10/2023-11:16:41] [V] [TRT] *************** Autotuning Reformat: Float(8294400,32400,180,1) -> Float(2073600,1:4,11520,64) ***************
[08/10/2023-11:16:41] [V] [TRT] *************** Autotuning Reformat: Float(8294400,32400,180,1) -> Half(8294400,32400,180,1) ***************
[08/10/2023-11:16:41] [V] [TRT] *************** Autotuning Reformat: Float(8294400,32400,180,1) -> Half(4147200,32400:2,180,1) ***************
[08/10/2023-11:16:41] [V] [TRT] *************** Autotuning Reformat: Float(8294400,32400,180,1) -> Half(1036800,1:8,5760,32) ***************
[08/10/2023-11:16:41] [V] [TRT] *************** Autotuning Reformat: Float(8294400,32400,180,1) -> Half(518400,1:16,2880,16) ***************
[08/10/2023-11:16:41] [V] [TRT] *************** Autotuning Reformat: Float(8294400,1,46080,256) -> Float(8294400,32400,180,1) ***************
[08/10/2023-11:16:41] [V] [TRT] *************** Autotuning Reformat: Float(8294400,1,46080,256) -> Float(2073600,1:4,11520,64) ***************
[08/10/2023-11:16:41] [V] [TRT] *************** Autotuning Reformat: Float(8294400,1,46080,256) -> Half(8294400,32400,180,1) ***************
[08/10/2023-11:16:41] [V] [TRT] *************** Autotuning Reformat: Float(8294400,1,46080,256) -> Half(4147200,32400:2,180,1) ***************
[08/10/2023-11:16:41] [V] [TRT] *************** Autotuning Reformat: Float(8294400,1,46080,256) -> Half(1036800,1:8,5760,32) ***************
[08/10/2023-11:16:41] [V] [TRT] *************** Autotuning Reformat: Float(8294400,1,46080,256) -> Half(518400,1:16,2880,16) ***************
[08/10/2023-11:16:41] [V] [TRT] *************** Autotuning Reformat: Float(2073600,1:4,11520,64) -> Float(8294400,32400,180,1) ***************
[08/10/2023-11:16:41] [V] [TRT] *************** Autotuning Reformat: Float(2073600,1:4,11520,64) -> Float(8294400,1,46080,256) ***************
[08/10/2023-11:16:41] [V] [TRT] *************** Autotuning Reformat: Float(2073600,1:4,11520,64) -> Half(8294400,32400,180,1) ***************
[08/10/2023-11:16:41] [V] [TRT] *************** Autotuning Reformat: Float(2073600,1:4,11520,64) -> Half(4147200,32400:2,180,1) ***************
[08/10/2023-11:16:41] [V] [TRT] *************** Autotuning Reformat: Float(2073600,1:4,11520,64) -> Half(1036800,1:8,5760,32) ***************
[08/10/2023-11:16:41] [V] [TRT] *************** Autotuning Reformat: Float(2073600,1:4,11520,64) -> Half(518400,1:16,2880,16) ***************
[08/10/2023-11:16:41] [V] [TRT] *************** Autotuning Reformat: Half(8294400,32400,180,1) -> Float(8294400,32400,180,1) ***************
[08/10/2023-11:16:41] [V] [TRT] *************** Autotuning Reformat: Half(8294400,32400,180,1) -> Float(8294400,1,46080,256) ***************
[08/10/2023-11:16:41] [V] [TRT] *************** Autotuning Reformat: Half(8294400,32400,180,1) -> Float(2073600,1:4,11520,64) ***************
[08/10/2023-11:16:41] [V] [TRT] *************** Autotuning Reformat: Half(8294400,32400,180,1) -> Half(4147200,32400:2,180,1) ***************
[08/10/2023-11:16:41] [V] [TRT] *************** Autotuning Reformat: Half(8294400,32400,180,1) -> Half(1036800,1:8,5760,32) ***************
[08/10/2023-11:16:41] [V] [TRT] *************** Autotuning Reformat: Half(8294400,32400,180,1) -> Half(518400,1:16,2880,16) ***************
[08/10/2023-11:16:41] [V] [TRT] *************** Autotuning Reformat: Half(4147200,32400:2,180,1) -> Float(8294400,32400,180,1) ***************
[08/10/2023-11:16:41] [V] [TRT] *************** Autotuning Reformat: Half(4147200,32400:2,180,1) -> Float(8294400,1,46080,256) ***************
[08/10/2023-11:16:41] [V] [TRT] *************** Autotuning Reformat: Half(4147200,32400:2,180,1) -> Float(2073600,1:4,11520,64) ***************
[08/10/2023-11:16:41] [V] [TRT] *************** Autotuning Reformat: Half(4147200,32400:2,180,1) -> Half(8294400,32400,180,1) ***************
[08/10/2023-11:16:41] [V] [TRT] *************** Autotuning Reformat: Half(4147200,32400:2,180,1) -> Half(1036800,1:8,5760,32) ***************
[08/10/2023-11:16:41] [V] [TRT] *************** Autotuning Reformat: Half(4147200,32400:2,180,1) -> Half(518400,1:16,2880,16) ***************
[08/10/2023-11:16:41] [V] [TRT] *************** Autotuning Reformat: Half(1036800,1:8,5760,32) -> Float(8294400,32400,180,1) ***************
[08/10/2023-11:16:41] [V] [TRT] *************** Autotuning Reformat: Half(1036800,1:8,5760,32) -> Float(8294400,1,46080,256) ***************
[08/10/2023-11:16:41] [V] [TRT] *************** Autotuning Reformat: Half(1036800,1:8,5760,32) -> Float(2073600,1:4,11520,64) ***************
[08/10/2023-11:16:41] [V] [TRT] *************** Autotuning Reformat: Half(1036800,1:8,5760,32) -> Half(8294400,32400,180,1) ***************
[08/10/2023-11:16:41] [V] [TRT] *************** Autotuning Reformat: Half(1036800,1:8,5760,32) -> Half(4147200,32400:2,180,1) ***************
[08/10/2023-11:16:41] [V] [TRT] *************** Autotuning Reformat: Half(1036800,1:8,5760,32) -> Half(518400,1:16,2880,16) ***************
[08/10/2023-11:16:41] [V] [TRT] *************** Autotuning Reformat: Half(518400,1:16,2880,16) -> Float(8294400,32400,180,1) ***************
[08/10/2023-11:16:41] [V] [TRT] *************** Autotuning Reformat: Half(518400,1:16,2880,16) -> Float(8294400,1,46080,256) ***************
[08/10/2023-11:16:41] [V] [TRT] *************** Autotuning Reformat: Half(518400,1:16,2880,16) -> Float(2073600,1:4,11520,64) ***************
[08/10/2023-11:16:41] [V] [TRT] *************** Autotuning Reformat: Half(518400,1:16,2880,16) -> Half(8294400,32400,180,1) ***************
[08/10/2023-11:16:41] [V] [TRT] *************** Autotuning Reformat: Half(518400,1:16,2880,16) -> Half(4147200,32400:2,180,1) ***************
[08/10/2023-11:16:41] [V] [TRT] *************** Autotuning Reformat: Half(518400,1:16,2880,16) -> Half(1036800,1:8,5760,32) ***************
[08/10/2023-11:16:41] [V] [TRT] =============== Computing reformatting costs
[08/10/2023-11:16:41] [V] [TRT] *************** Autotuning Reformat: Float(64800,32400,180,1) -> Half(64800,32400,180,1) ***************
[08/10/2023-11:16:41] [V] [TRT] *************** Autotuning Reformat: Float(64800,1,360,2) -> Half(64800,32400,180,1) ***************
[08/10/2023-11:16:41] [V] [TRT] *************** Autotuning Reformat: Float(32400,1:4,180,1) -> Half(64800,32400,180,1) ***************
[08/10/2023-11:16:41] [V] [TRT] *************** Autotuning Reformat: Half(32400,32400:2,180,1) -> Half(64800,32400,180,1) ***************
[08/10/2023-11:16:41] [V] [TRT] *************** Autotuning Reformat: Half(32400,1:8,180,1) -> Half(64800,32400,180,1) ***************
[08/10/2023-11:16:41] [V] [TRT] *************** Autotuning Reformat: Half(32400,1:16,180,1) -> Half(64800,32400,180,1) ***************
[08/10/2023-11:16:41] [V] [TRT] =============== Computing reformatting costs
[08/10/2023-11:16:41] [V] [TRT] *************** Autotuning Reformat: Float(8294400,32400,180,1) -> Float(8294400,1,46080,256) ***************
[08/10/2023-11:16:41] [V] [TRT] *************** Autotuning Reformat: Float(8294400,32400,180,1) -> Float(2073600,1:4,11520,64) ***************
[08/10/2023-11:16:41] [V] [TRT] *************** Autotuning Reformat: Float(8294400,32400,180,1) -> Half(8294400,32400,180,1) ***************
[08/10/2023-11:16:41] [V] [TRT] *************** Autotuning Reformat: Float(8294400,32400,180,1) -> Half(4147200,32400:2,180,1) ***************
[08/10/2023-11:16:41] [V] [TRT] *************** Autotuning Reformat: Float(8294400,32400,180,1) -> Half(1036800,1:8,5760,32) ***************
[08/10/2023-11:16:41] [V] [TRT] *************** Autotuning Reformat: Float(8294400,32400,180,1) -> Half(518400,1:16,2880,16) ***************
[08/10/2023-11:16:41] [V] [TRT] *************** Autotuning Reformat: Float(8294400,1,46080,256) -> Float(8294400,32400,180,1) ***************
[08/10/2023-11:16:41] [V] [TRT] *************** Autotuning Reformat: Float(8294400,1,46080,256) -> Float(2073600,1:4,11520,64) ***************
[08/10/2023-11:16:41] [V] [TRT] *************** Autotuning Reformat: Float(8294400,1,46080,256) -> Half(8294400,32400,180,1) ***************
[08/10/2023-11:16:41] [V] [TRT] *************** Autotuning Reformat: Float(8294400,1,46080,256) -> Half(4147200,32400:2,180,1) ***************
[08/10/2023-11:16:41] [V] [TRT] *************** Autotuning Reformat: Float(8294400,1,46080,256) -> Half(1036800,1:8,5760,32) ***************
[08/10/2023-11:16:41] [V] [TRT] *************** Autotuning Reformat: Float(8294400,1,46080,256) -> Half(518400,1:16,2880,16) ***************
[08/10/2023-11:16:41] [V] [TRT] *************** Autotuning Reformat: Float(2073600,1:4,11520,64) -> Float(8294400,32400,180,1) ***************
[08/10/2023-11:16:41] [V] [TRT] *************** Autotuning Reformat: Float(2073600,1:4,11520,64) -> Float(8294400,1,46080,256) ***************
[08/10/2023-11:16:41] [V] [TRT] *************** Autotuning Reformat: Float(2073600,1:4,11520,64) -> Half(8294400,32400,180,1) ***************
[08/10/2023-11:16:41] [V] [TRT] *************** Autotuning Reformat: Float(2073600,1:4,11520,64) -> Half(4147200,32400:2,180,1) ***************
[08/10/2023-11:16:41] [V] [TRT] *************** Autotuning Reformat: Float(2073600,1:4,11520,64) -> Half(1036800,1:8,5760,32) ***************
[08/10/2023-11:16:41] [V] [TRT] *************** Autotuning Reformat: Float(2073600,1:4,11520,64) -> Half(518400,1:16,2880,16) ***************
[08/10/2023-11:16:41] [V] [TRT] *************** Autotuning Reformat: Half(8294400,32400,180,1) -> Float(8294400,32400,180,1) ***************
[08/10/2023-11:16:41] [V] [TRT] *************** Autotuning Reformat: Half(8294400,32400,180,1) -> Float(8294400,1,46080,256) ***************
[08/10/2023-11:16:41] [V] [TRT] *************** Autotuning Reformat: Half(8294400,32400,180,1) -> Float(2073600,1:4,11520,64) ***************
[08/10/2023-11:16:41] [V] [TRT] *************** Autotuning Reformat: Half(8294400,32400,180,1) -> Half(4147200,32400:2,180,1) ***************
[08/10/2023-11:16:41] [V] [TRT] *************** Autotuning Reformat: Half(8294400,32400,180,1) -> Half(1036800,1:8,5760,32) ***************
[08/10/2023-11:16:41] [V] [TRT] *************** Autotuning Reformat: Half(8294400,32400,180,1) -> Half(518400,1:16,2880,16) ***************
[08/10/2023-11:16:41] [V] [TRT] *************** Autotuning Reformat: Half(4147200,32400:2,180,1) -> Float(8294400,32400,180,1) ***************
[08/10/2023-11:16:41] [V] [TRT] *************** Autotuning Reformat: Half(4147200,32400:2,180,1) -> Float(8294400,1,46080,256) ***************
[08/10/2023-11:16:41] [V] [TRT] *************** Autotuning Reformat: Half(4147200,32400:2,180,1) -> Float(2073600,1:4,11520,64) ***************
[08/10/2023-11:16:41] [V] [TRT] *************** Autotuning Reformat: Half(4147200,32400:2,180,1) -> Half(8294400,32400,180,1) ***************
[08/10/2023-11:16:41] [V] [TRT] *************** Autotuning Reformat: Half(4147200,32400:2,180,1) -> Half(1036800,1:8,5760,32) ***************
[08/10/2023-11:16:41] [V] [TRT] *************** Autotuning Reformat: Half(4147200,32400:2,180,1) -> Half(518400,1:16,2880,16) ***************
[08/10/2023-11:16:41] [V] [TRT] *************** Autotuning Reformat: Half(1036800,1:8,5760,32) -> Float(8294400,32400,180,1) ***************
[08/10/2023-11:16:41] [V] [TRT] *************** Autotuning Reformat: Half(1036800,1:8,5760,32) -> Float(8294400,1,46080,256) ***************
[08/10/2023-11:16:41] [V] [TRT] *************** Autotuning Reformat: Half(1036800,1:8,5760,32) -> Float(2073600,1:4,11520,64) ***************
[08/10/2023-11:16:41] [V] [TRT] *************** Autotuning Reformat: Half(1036800,1:8,5760,32) -> Half(8294400,32400,180,1) ***************
[08/10/2023-11:16:41] [V] [TRT] *************** Autotuning Reformat: Half(1036800,1:8,5760,32) -> Half(4147200,32400:2,180,1) ***************
[08/10/2023-11:16:41] [V] [TRT] *************** Autotuning Reformat: Half(1036800,1:8,5760,32) -> Half(518400,1:16,2880,16) ***************
[08/10/2023-11:16:41] [V] [TRT] *************** Autotuning Reformat: Half(518400,1:16,2880,16) -> Float(8294400,32400,180,1) ***************
[08/10/2023-11:16:41] [V] [TRT] *************** Autotuning Reformat: Half(518400,1:16,2880,16) -> Float(8294400,1,46080,256) ***************
[08/10/2023-11:16:41] [V] [TRT] *************** Autotuning Reformat: Half(518400,1:16,2880,16) -> Float(2073600,1:4,11520,64) ***************
[08/10/2023-11:16:41] [V] [TRT] *************** Autotuning Reformat: Half(518400,1:16,2880,16) -> Half(8294400,32400,180,1) ***************
[08/10/2023-11:16:41] [V] [TRT] *************** Autotuning Reformat: Half(518400,1:16,2880,16) -> Half(4147200,32400:2,180,1) ***************
[08/10/2023-11:16:41] [V] [TRT] *************** Autotuning Reformat: Half(518400,1:16,2880,16) -> Half(1036800,1:8,5760,32) ***************
[08/10/2023-11:16:41] [V] [TRT] =============== Computing reformatting costs
[08/10/2023-11:16:41] [V] [TRT] *************** Autotuning Reformat: Float(64800,32400,180,1) -> Half(64800,32400,180,1) ***************
[08/10/2023-11:16:41] [V] [TRT] *************** Autotuning Reformat: Float(64800,1,360,2) -> Half(64800,32400,180,1) ***************
[08/10/2023-11:16:41] [V] [TRT] *************** Autotuning Reformat: Float(32400,1:4,180,1) -> Half(64800,32400,180,1) ***************
[08/10/2023-11:16:41] [V] [TRT] *************** Autotuning Reformat: Half(32400,32400:2,180,1) -> Half(64800,32400,180,1) ***************
[08/10/2023-11:16:41] [V] [TRT] *************** Autotuning Reformat: Half(32400,1:8,180,1) -> Half(64800,32400,180,1) ***************
[08/10/2023-11:16:41] [V] [TRT] *************** Autotuning Reformat: Half(32400,1:16,180,1) -> Half(64800,32400,180,1) ***************
[08/10/2023-11:16:41] [V] [TRT] =============== Computing reformatting costs
[08/10/2023-11:16:41] [V] [TRT] *************** Autotuning Reformat: Float(8294400,32400,180,1) -> Float(8294400,1,46080,256) ***************
[08/10/2023-11:16:41] [V] [TRT] *************** Autotuning Reformat: Float(8294400,32400,180,1) -> Float(2073600,1:4,11520,64) ***************
[08/10/2023-11:16:41] [V] [TRT] *************** Autotuning Reformat: Float(8294400,32400,180,1) -> Half(8294400,32400,180,1) ***************
[08/10/2023-11:16:41] [V] [TRT] *************** Autotuning Reformat: Float(8294400,32400,180,1) -> Half(4147200,32400:2,180,1) ***************
[08/10/2023-11:16:41] [V] [TRT] *************** Autotuning Reformat: Float(8294400,32400,180,1) -> Half(1036800,1:8,5760,32) ***************
[08/10/2023-11:16:41] [V] [TRT] *************** Autotuning Reformat: Float(8294400,32400,180,1) -> Half(518400,1:16,2880,16) ***************
[08/10/2023-11:16:41] [V] [TRT] *************** Autotuning Reformat: Float(8294400,1,46080,256) -> Float(8294400,32400,180,1) ***************
[08/10/2023-11:16:41] [V] [TRT] *************** Autotuning Reformat: Float(8294400,1,46080,256) -> Float(2073600,1:4,11520,64) ***************
[08/10/2023-11:16:41] [V] [TRT] *************** Autotuning Reformat: Float(8294400,1,46080,256) -> Half(8294400,32400,180,1) ***************
[08/10/2023-11:16:41] [V] [TRT] *************** Autotuning Reformat: Float(8294400,1,46080,256) -> Half(4147200,32400:2,180,1) ***************
[08/10/2023-11:16:41] [V] [TRT] *************** Autotuning Reformat: Float(8294400,1,46080,256) -> Half(1036800,1:8,5760,32) ***************
[08/10/2023-11:16:41] [V] [TRT] *************** Autotuning Reformat: Float(8294400,1,46080,256) -> Half(518400,1:16,2880,16) ***************
[08/10/2023-11:16:41] [V] [TRT] *************** Autotuning Reformat: Float(2073600,1:4,11520,64) -> Float(8294400,32400,180,1) ***************
[08/10/2023-11:16:41] [V] [TRT] *************** Autotuning Reformat: Float(2073600,1:4,11520,64) -> Float(8294400,1,46080,256) ***************
[08/10/2023-11:16:41] [V] [TRT] *************** Autotuning Reformat: Float(2073600,1:4,11520,64) -> Half(8294400,32400,180,1) ***************
[08/10/2023-11:16:41] [V] [TRT] *************** Autotuning Reformat: Float(2073600,1:4,11520,64) -> Half(4147200,32400:2,180,1) ***************
[08/10/2023-11:16:41] [V] [TRT] *************** Autotuning Reformat: Float(2073600,1:4,11520,64) -> Half(1036800,1:8,5760,32) ***************
[08/10/2023-11:16:41] [V] [TRT] *************** Autotuning Reformat: Float(2073600,1:4,11520,64) -> Half(518400,1:16,2880,16) ***************
[08/10/2023-11:16:41] [V] [TRT] *************** Autotuning Reformat: Half(8294400,32400,180,1) -> Float(8294400,32400,180,1) ***************
[08/10/2023-11:16:41] [V] [TRT] *************** Autotuning Reformat: Half(8294400,32400,180,1) -> Float(8294400,1,46080,256) ***************
[08/10/2023-11:16:41] [V] [TRT] *************** Autotuning Reformat: Half(8294400,32400,180,1) -> Float(2073600,1:4,11520,64) ***************
[08/10/2023-11:16:41] [V] [TRT] *************** Autotuning Reformat: Half(8294400,32400,180,1) -> Half(4147200,32400:2,180,1) ***************
[08/10/2023-11:16:41] [V] [TRT] *************** Autotuning Reformat: Half(8294400,32400,180,1) -> Half(1036800,1:8,5760,32) ***************
[08/10/2023-11:16:41] [V] [TRT] *************** Autotuning Reformat: Half(8294400,32400,180,1) -> Half(518400,1:16,2880,16) ***************
[08/10/2023-11:16:41] [V] [TRT] *************** Autotuning Reformat: Half(4147200,32400:2,180,1) -> Float(8294400,32400,180,1) ***************
[08/10/2023-11:16:41] [V] [TRT] *************** Autotuning Reformat: Half(4147200,32400:2,180,1) -> Float(8294400,1,46080,256) ***************
[08/10/2023-11:16:41] [V] [TRT] *************** Autotuning Reformat: Half(4147200,32400:2,180,1) -> Float(2073600,1:4,11520,64) ***************
[08/10/2023-11:16:41] [V] [TRT] *************** Autotuning Reformat: Half(4147200,32400:2,180,1) -> Half(8294400,32400,180,1) ***************
[08/10/2023-11:16:41] [V] [TRT] *************** Autotuning Reformat: Half(4147200,32400:2,180,1) -> Half(1036800,1:8,5760,32) ***************
[08/10/2023-11:16:41] [V] [TRT] *************** Autotuning Reformat: Half(4147200,32400:2,180,1) -> Half(518400,1:16,2880,16) ***************
[08/10/2023-11:16:41] [V] [TRT] *************** Autotuning Reformat: Half(1036800,1:8,5760,32) -> Float(8294400,32400,180,1) ***************
[08/10/2023-11:16:41] [V] [TRT] *************** Autotuning Reformat: Half(1036800,1:8,5760,32) -> Float(8294400,1,46080,256) ***************
[08/10/2023-11:16:41] [V] [TRT] *************** Autotuning Reformat: Half(1036800,1:8,5760,32) -> Float(2073600,1:4,11520,64) ***************
[08/10/2023-11:16:41] [V] [TRT] *************** Autotuning Reformat: Half(1036800,1:8,5760,32) -> Half(8294400,32400,180,1) ***************
[08/10/2023-11:16:41] [V] [TRT] *************** Autotuning Reformat: Half(1036800,1:8,5760,32) -> Half(4147200,32400:2,180,1) ***************
[08/10/2023-11:16:41] [V] [TRT] *************** Autotuning Reformat: Half(1036800,1:8,5760,32) -> Half(518400,1:16,2880,16) ***************
[08/10/2023-11:16:41] [V] [TRT] *************** Autotuning Reformat: Half(518400,1:16,2880,16) -> Float(8294400,32400,180,1) ***************
[08/10/2023-11:16:41] [V] [TRT] *************** Autotuning Reformat: Half(518400,1:16,2880,16) -> Float(8294400,1,46080,256) ***************
[08/10/2023-11:16:41] [V] [TRT] *************** Autotuning Reformat: Half(518400,1:16,2880,16) -> Float(2073600,1:4,11520,64) ***************
[08/10/2023-11:16:41] [V] [TRT] *************** Autotuning Reformat: Half(518400,1:16,2880,16) -> Half(8294400,32400,180,1) ***************
[08/10/2023-11:16:41] [V] [TRT] *************** Autotuning Reformat: Half(518400,1:16,2880,16) -> Half(4147200,32400:2,180,1) ***************
[08/10/2023-11:16:41] [V] [TRT] *************** Autotuning Reformat: Half(518400,1:16,2880,16) -> Half(1036800,1:8,5760,32) ***************
[08/10/2023-11:16:41] [V] [TRT] =============== Computing reformatting costs
[08/10/2023-11:16:41] [V] [TRT] *************** Autotuning Reformat: Float(64800,32400,180,1) -> Half(64800,32400,180,1) ***************
[08/10/2023-11:16:41] [V] [TRT] *************** Autotuning Reformat: Float(64800,1,360,2) -> Half(64800,32400,180,1) ***************
[08/10/2023-11:16:41] [V] [TRT] *************** Autotuning Reformat: Float(32400,1:4,180,1) -> Half(64800,32400,180,1) ***************
[08/10/2023-11:16:41] [V] [TRT] *************** Autotuning Reformat: Half(32400,32400:2,180,1) -> Half(64800,32400,180,1) ***************
[08/10/2023-11:16:41] [V] [TRT] *************** Autotuning Reformat: Half(32400,1:8,180,1) -> Half(64800,32400,180,1) ***************
[08/10/2023-11:16:41] [V] [TRT] *************** Autotuning Reformat: Half(32400,1:16,180,1) -> Half(64800,32400,180,1) ***************
[08/10/2023-11:16:41] [V] [TRT] =============== Computing costs for 
[08/10/2023-11:16:41] [V] [TRT] *************** Autotuning format combination: Float(8294400,32400,180,1) -> Float(4147200,32400,180,1) ***************
[08/10/2023-11:16:41] [V] [TRT] --------------- Timing Runner: Conv_15 + Relu_16 (CudaDepthwiseConvolution)
[08/10/2023-11:16:41] [V] [TRT] CudaDepthwiseConvolution has no valid tactics for this config, skipping
[08/10/2023-11:16:41] [V] [TRT] --------------- Timing Runner: Conv_15 + Relu_16 (FusedConvActConvolution)
[08/10/2023-11:16:41] [V] [TRT] FusedConvActConvolution has no valid tactics for this config, skipping
[08/10/2023-11:16:42] [V] [TRT] --------------- Timing Runner: Conv_15 + Relu_16 (CudnnConvolution)
[08/10/2023-11:16:45] [V] [TRT] Tactic: 0x0000000000000000 Time: 15.2639
[08/10/2023-11:16:45] [V] [TRT] Tactic: 0x0000000000000001 Time: 2.93423
[08/10/2023-11:16:46] [V] [TRT] Tactic: 0x0000000000000002 Time: 18.7443
[08/10/2023-11:16:46] [V] [TRT] Tactic: 0x0000000000000004 skipped. Scratch requested: 17450532864, available: 4294967296
[08/10/2023-11:16:46] [V] [TRT] Tactic: 0x0000000000000005 Time: 46.6908
[08/10/2023-11:16:46] [V] [TRT] Tactic: 0x0000000000000006 Time: 6.17179
[08/10/2023-11:16:46] [V] [TRT] Tactic: 0x0000000000000038 Time: 13.5177
[08/10/2023-11:16:46] [V] [TRT] Tactic: 0x0000000000000039 Time: 2.89076
[08/10/2023-11:16:46] [V] [TRT] Tactic: 0x000000000000003a Time: 17.9887
[08/10/2023-11:16:46] [V] [TRT] Tactic: 0x000000000000003c skipped. Scratch requested: 17450532864, available: 4294967296
[08/10/2023-11:16:47] [V] [TRT] Tactic: 0x000000000000003d Time: 31.7359
[08/10/2023-11:16:47] [V] [TRT] Tactic: 0x000000000000003e Time: 6.17971
[08/10/2023-11:16:47] [V] [TRT] Tactic: 0x0000000000000070 Time: 13.1565
[08/10/2023-11:16:47] [V] [TRT] Tactic: 0x0000000000000071 Time: 10.5322
[08/10/2023-11:16:47] [V] [TRT] Tactic: 0x0000000000000072 Time: 17.6336
[08/10/2023-11:16:47] [V] [TRT] Tactic: 0x0000000000000074 skipped. Scratch requested: 17450532864, available: 4294967296
[08/10/2023-11:16:47] [V] [TRT] Tactic: 0x0000000000000075 Time: 31.7157
[08/10/2023-11:16:48] [V] [TRT] Tactic: 0x0000000000000076 Time: 6.17708
[08/10/2023-11:16:48] [I] [TRT] Some tactics do not have sufficient workspace memory to run. Increasing workspace size will enable more tactics, please check verbose output for requested sizes.
[08/10/2023-11:16:48] [V] [TRT] Fastest Tactic: 0x0000000000000039 Time: 2.89076
[08/10/2023-11:16:48] [V] [TRT] Setting workspace to 17450532864enables more tactics for profiling
[08/10/2023-11:16:48] [V] [TRT] --------------- Timing Runner: Conv_15 + Relu_16 (CaskConvolution)
[08/10/2023-11:16:48] [V] [TRT] Conv_15 + Relu_16 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x128x8_stage3_warpsize1x4x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0x01cf8ce2da913006
[08/10/2023-11:16:48] [V] [TRT] Tactic: 0x01cf8ce2da913006 Time: 10.2606
[08/10/2023-11:16:48] [V] [TRT] Conv_15 + Relu_16 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x64x8_stage3_warpsize2x2x1_g1_ffma_aligna4_alignc4 Tactic: 0x12dbf7d94ee3696d
[08/10/2023-11:16:48] [V] [TRT] Tactic: 0x12dbf7d94ee3696d Time: 10.7016
[08/10/2023-11:16:48] [V] [TRT] Conv_15 + Relu_16 Set Tactic Name: ampere_scudnn_128x64_relu_small_nn_v1 Tactic: 0x3f243c490d502deb
[08/10/2023-11:16:48] [V] [TRT] Tactic: 0x3f243c490d502deb Time: 10.6384
[08/10/2023-11:16:48] [V] [TRT] Conv_15 + Relu_16 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0x4727434768e46395
[08/10/2023-11:16:48] [V] [TRT] Tactic: 0x4727434768e46395 Time: 11.6843
[08/10/2023-11:16:48] [V] [TRT] Conv_15 + Relu_16 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize256x128x8_stage3_warpsize4x2x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0x4efce38acc876f5c
[08/10/2023-11:16:48] [V] [TRT] Tactic: 0x4efce38acc876f5c Time: 10.3699
[08/10/2023-11:16:48] [V] [TRT] Conv_15 + Relu_16 Set Tactic Name: ampere_scudnn_128x128_relu_small_nn_v1 Tactic: 0x503619c69ae500ff
[08/10/2023-11:16:48] [V] [TRT] Tactic: 0x503619c69ae500ff Time: 9.62739
[08/10/2023-11:16:48] [V] [TRT] Conv_15 + Relu_16 Set Tactic Name: ampere_scudnn_128x128_relu_xregs_large_nn_v1 Tactic: 0x5403ad713f811a18
[08/10/2023-11:16:48] [V] [TRT] Tactic: 0x5403ad713f811a18 Time: 9.84016
[08/10/2023-11:16:48] [V] [TRT] Conv_15 + Relu_16 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x128x8_stage3_warpsize1x4x1_g1_ffma_aligna4_alignc4 Tactic: 0x5aa723e0481da855
[08/10/2023-11:16:48] [V] [TRT] Tactic: 0x5aa723e0481da855 Time: 10.24
[08/10/2023-11:16:48] [V] [TRT] Conv_15 + Relu_16 Set Tactic Name: ampere_scudnn_128x64_relu_xregs_large_nn_v1 Tactic: 0x5deb29b7a8e275f7
[08/10/2023-11:16:48] [V] [TRT] Tactic: 0x5deb29b7a8e275f7 Time: 10.0147
[08/10/2023-11:16:48] [V] [TRT] Conv_15 + Relu_16 Set Tactic Name: ampere_scudnn_winograd_128x128_ldg1_ldg4_relu_tile148t_nt_v1 Tactic: 0x94119b4c514b211a
[08/10/2023-11:16:48] [V] [TRT] Tactic: 0x94119b4c514b211a Time: 5.64114
[08/10/2023-11:16:48] [V] [TRT] Conv_15 + Relu_16 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_aligna4_alignc4 Tactic: 0xa31d27de74b895ff
[08/10/2023-11:16:48] [V] [TRT] Tactic: 0xa31d27de74b895ff Time: 11.7093
[08/10/2023-11:16:48] [V] [TRT] Conv_15 + Relu_16 Set Tactic Name: ampere_scudnn_128x32_relu_small_nn_v1 Tactic: 0xa8609adc4e0ceb90
[08/10/2023-11:16:49] [V] [TRT] Tactic: 0xa8609adc4e0ceb90 Time: 11.5655
[08/10/2023-11:16:49] [V] [TRT] Conv_15 + Relu_16 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize256x128x8_stage3_warpsize4x2x1_g1_ffma_aligna4_alignc4 Tactic: 0xbb8c3889c7eacd30
[08/10/2023-11:16:49] [V] [TRT] Tactic: 0xbb8c3889c7eacd30 Time: 10.625
[08/10/2023-11:16:49] [V] [TRT] Conv_15 + Relu_16 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_aligna4_alignc4 Tactic: 0xd828f024626fa982
[08/10/2023-11:16:49] [V] [TRT] Tactic: 0xd828f024626fa982 Time: 16.29
[08/10/2023-11:16:49] [V] [TRT] Conv_15 + Relu_16 Set Tactic Name: ampere_scudnn_128x128_relu_medium_nn_v1 Tactic: 0xf067e6205da31c2e
[08/10/2023-11:16:49] [V] [TRT] Tactic: 0xf067e6205da31c2e Time: 9.91069
[08/10/2023-11:16:49] [V] [TRT] Conv_15 + Relu_16 Set Tactic Name: ampere_scudnn_128x64_relu_medium_nn_v1 Tactic: 0xf64396b97c889179
[08/10/2023-11:16:49] [V] [TRT] Tactic: 0xf64396b97c889179 Time: 10.7136
[08/10/2023-11:16:49] [V] [TRT] Fastest Tactic: 0x94119b4c514b211a Time: 5.64114
[08/10/2023-11:16:49] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CudnnConvolution Tactic: 0x0000000000000039
[08/10/2023-11:16:49] [V] [TRT] *************** Autotuning format combination: Float(8294400,1,46080,256) -> Float(4147200,1,23040,128) ***************
[08/10/2023-11:16:49] [V] [TRT] --------------- Timing Runner: Conv_15 + Relu_16 (CaskConvolution)
[08/10/2023-11:16:49] [V] [TRT] Conv_15 + Relu_16 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize64x128x32_stage5_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0x112d7e4d280f396d
[08/10/2023-11:16:49] [V] [TRT] Tactic: 0x112d7e4d280f396d Time: 2.68712
[08/10/2023-11:16:49] [V] [TRT] Conv_15 + Relu_16 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x64x32_stage5_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0x13cc2ad8dcd32ba2
[08/10/2023-11:16:49] [V] [TRT] Tactic: 0x13cc2ad8dcd32ba2 Time: 2.88032
[08/10/2023-11:16:49] [V] [TRT] Conv_15 + Relu_16 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_aligna4_alignc4 Tactic: 0x19b688348f983aa0
[08/10/2023-11:16:49] [V] [TRT] Tactic: 0x19b688348f983aa0 Time: 10.9366
[08/10/2023-11:16:49] [V] [TRT] Conv_15 + Relu_16 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x64x8_stage3_warpsize2x2x1_g1_ffma_aligna4_alignc4 Tactic: 0x1da91d865428f237
[08/10/2023-11:16:49] [V] [TRT] Tactic: 0x1da91d865428f237 Time: 8.41845
[08/10/2023-11:16:49] [V] [TRT] Conv_15 + Relu_16 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize256x128x32_stage3_warpsize4x2x1_g1_tensor16x8x8_t1r3s3 Tactic: 0x21246c8544eff903
[08/10/2023-11:16:49] [V] [TRT] Tactic: 0x21246c8544eff903 Time: 1.79837
[08/10/2023-11:16:49] [V] [TRT] Conv_15 + Relu_16 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x128x32_stage4_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0x25b2b9d5c9d5ca0d
[08/10/2023-11:16:49] [V] [TRT] Tactic: 0x25b2b9d5c9d5ca0d Time: 1.84285
[08/10/2023-11:16:49] [V] [TRT] Conv_15 + Relu_16 Set Tactic Name: ampere_scudnn_128x64_sliced1x2_ldg4_relu_exp_small_nhwc_tn_v1 Tactic: 0x27b316f52c109002
[08/10/2023-11:16:49] [V] [TRT] Tactic: 0x27b316f52c109002 Time: 10.4771
[08/10/2023-11:16:49] [V] [TRT] Conv_15 + Relu_16 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage4_warpsize2x2x1_g1_tensor16x8x8_t1r3s3 Tactic: 0x2d80d2b384ab13f1
[08/10/2023-11:16:49] [V] [TRT] Tactic: 0x2d80d2b384ab13f1 Time: 2.90084
[08/10/2023-11:16:49] [V] [TRT] Conv_15 + Relu_16 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize256x128x32_stage3_warpsize4x2x1_g1_tensor16x8x8 Tactic: 0x36303f399d3ee4fd
[08/10/2023-11:16:49] [V] [TRT] Tactic: 0x36303f399d3ee4fd Time: 2.25137
[08/10/2023-11:16:49] [V] [TRT] Conv_15 + Relu_16 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage4_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0x385bfab9332b1413
[08/10/2023-11:16:50] [V] [TRT] Tactic: 0x385bfab9332b1413 Time: 3.24408
[08/10/2023-11:16:50] [V] [TRT] Conv_15 + Relu_16 Set Tactic Name: ampere_scudnn_128x32_sliced1x4_ldg4_relu_exp_medium_nhwc_tn_v1 Tactic: 0x3e191488237fab8f
[08/10/2023-11:16:50] [V] [TRT] Tactic: 0x3e191488237fab8f Time: 11.4906
[08/10/2023-11:16:50] [V] [TRT] Conv_15 + Relu_16 Set Tactic Name: ampere_scudnn_128x32_sliced1x4_ldg4_relu_exp_small_nhwc_tn_v1 Tactic: 0x3e2b881168d9689d
[08/10/2023-11:16:50] [V] [TRT] Tactic: 0x3e2b881168d9689d Time: 10.9679
[08/10/2023-11:16:50] [V] [TRT] Conv_15 + Relu_16 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x256x8_stage3_warpsize1x4x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0x3f0c846d6379bc98
[08/10/2023-11:16:50] [V] [TRT] Tactic: 0x3f0c846d6379bc98 Time: 16.0753
[08/10/2023-11:16:50] [V] [TRT] Conv_15 + Relu_16 Set Tactic Name: ampere_scudnn_128x64_sliced1x2_ldg4_relu_exp_medium_nhwc_tn_v1 Tactic: 0x412c44dfeaf9161d
[08/10/2023-11:16:50] [V] [TRT] Tactic: 0x412c44dfeaf9161d Time: 11.3076
[08/10/2023-11:16:50] [V] [TRT] Conv_15 + Relu_16 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x256x32_stage3_warpsize2x4x1_g1_tensor16x8x8 Tactic: 0x482b48242255b8ce
[08/10/2023-11:16:50] [V] [TRT] Tactic: 0x482b48242255b8ce Time: 3.91505
[08/10/2023-11:16:50] [V] [TRT] Conv_15 + Relu_16 Set Tactic Name: ampere_scudnn_128x128_relu_exp_small_nhwc_tn_v1 Tactic: 0x5030121339a48bf3
[08/10/2023-11:16:50] [V] [TRT] Tactic: 0x5030121339a48bf3 Time: 11.1617
[08/10/2023-11:16:50] [V] [TRT] Conv_15 + Relu_16 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x256x32_stage3_warpsize2x4x1_g1_tensor16x8x8_t1r3s3 Tactic: 0x614e89f7852edbc3
[08/10/2023-11:16:50] [V] [TRT] Tactic: 0x614e89f7852edbc3 Time: 5.07564
[08/10/2023-11:16:50] [V] [TRT] Conv_15 + Relu_16 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_aligna4_alignc4 Tactic: 0x62835fce994f06dd
[08/10/2023-11:16:50] [V] [TRT] Tactic: 0x62835fce994f06dd Time: 11.5114
[08/10/2023-11:16:50] [V] [TRT] Conv_15 + Relu_16 Set Tactic Name: ampere_scudnn_128x128_ldg4_relu_exp_large_nhwc_tn_v1 Tactic: 0x634e99502974e4da
[08/10/2023-11:16:50] [V] [TRT] Tactic: 0x634e99502974e4da Time: 11.2612
[08/10/2023-11:16:50] [V] [TRT] Conv_15 + Relu_16 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0x65e41d81f093b482
[08/10/2023-11:16:50] [V] [TRT] Tactic: 0x65e41d81f093b482 Time: 2.63641
[08/10/2023-11:16:50] [V] [TRT] Conv_15 + Relu_16 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize64x128x32_stage5_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0x65f71607d20d5438
[08/10/2023-11:16:50] [V] [TRT] Tactic: 0x65f71607d20d5438 Time: 2.9962
[08/10/2023-11:16:50] [V] [TRT] Conv_15 + Relu_16 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x64x32_stage5_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0x6716429226d146f7
[08/10/2023-11:16:50] [V] [TRT] Tactic: 0x6716429226d146f7 Time: 2.47782
[08/10/2023-11:16:50] [V] [TRT] Conv_15 + Relu_16 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize64x32x64_stage5_warpsize2x2x1_g1_tensor16x8x8_t1r3s3 Tactic: 0x679a281f7660c436
[08/10/2023-11:16:50] [V] [TRT] Tactic: 0x679a281f7660c436 Time: 4.96468
[08/10/2023-11:16:50] [V] [TRT] Conv_15 + Relu_16 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize64x32x64_stage5_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0x6e2a3c7a7fc5e4e1
[08/10/2023-11:16:51] [V] [TRT] Tactic: 0x6e2a3c7a7fc5e4e1 Time: 4.73788
[08/10/2023-11:16:51] [V] [TRT] Conv_15 + Relu_16 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x64x32_stage5_warpsize2x2x1_g1_tensor16x8x8_t1r3s3 Tactic: 0x7aa21282feb15f1a
[08/10/2023-11:16:51] [V] [TRT] Tactic: 0x7aa21282feb15f1a Time: 2.5083
[08/10/2023-11:16:51] [V] [TRT] Conv_15 + Relu_16 Set Tactic Name: ampere_scudnn_128x128_ldg4_relu_exp_medium_nhwc_tn_v1 Tactic: 0x7bc32c782b800c48
[08/10/2023-11:16:51] [V] [TRT] Tactic: 0x7bc32c782b800c48 Time: 9.24919
[08/10/2023-11:16:51] [V] [TRT] Conv_15 + Relu_16 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize256x64x8_stage3_warpsize2x2x1_g1_ffma_aligna4_alignc4 Tactic: 0x8014228ec08b4d49
[08/10/2023-11:16:51] [V] [TRT] Tactic: 0x8014228ec08b4d49 Time: 8.53004
[08/10/2023-11:16:51] [V] [TRT] Conv_15 + Relu_16 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage4_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0x818413b69874bb35
[08/10/2023-11:16:51] [V] [TRT] Tactic: 0x818413b69874bb35 Time: 3.30121
[08/10/2023-11:16:51] [V] [TRT] Conv_15 + Relu_16 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0x94a7db94ba744c45
[08/10/2023-11:16:51] [V] [TRT] Tactic: 0x94a7db94ba744c45 Time: 10.1956
[08/10/2023-11:16:51] [V] [TRT] Conv_15 + Relu_16 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x256x32_stage3_warpsize2x4x1_g1_tensor16x8x8 Tactic: 0x998c97842e775a17
[08/10/2023-11:16:51] [V] [TRT] Tactic: 0x998c97842e775a17 Time: 3.46456
[08/10/2023-11:16:51] [V] [TRT] Conv_15 + Relu_16 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8_t1r3s3 Tactic: 0x999e005e3b016ea6
[08/10/2023-11:16:51] [V] [TRT] Tactic: 0x999e005e3b016ea6 Time: 1.98874
[08/10/2023-11:16:51] [V] [TRT] Conv_15 + Relu_16 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize256x64x32_stage3_warpsize4x2x1_g1_tensor16x8x8_t1r3s3 Tactic: 0xa9a06d0633580c0c
[08/10/2023-11:16:51] [V] [TRT] Tactic: 0xa9a06d0633580c0c Time: 2.13298
[08/10/2023-11:16:51] [V] [TRT] Conv_15 + Relu_16 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0xb443c221fcb1565b
[08/10/2023-11:16:51] [V] [TRT] Tactic: 0xb443c221fcb1565b Time: 1.99274
[08/10/2023-11:16:51] [V] [TRT] Conv_15 + Relu_16 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage3_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0xb47d7d926de02dee
[08/10/2023-11:16:51] [V] [TRT] Tactic: 0xb47d7d926de02dee Time: 3.2111
[08/10/2023-11:16:51] [V] [TRT] Conv_15 + Relu_16 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize256x64x32_stage3_warpsize4x2x1_g1_tensor16x8x8 Tactic: 0xba0185279ccb2b85
[08/10/2023-11:16:51] [V] [TRT] Tactic: 0xba0185279ccb2b85 Time: 2.43133
[08/10/2023-11:16:51] [V] [TRT] Conv_15 + Relu_16 Set Tactic Name: ampere_scudnn_128x64_sliced1x2_ldg4_relu_exp_large_nhwc_tn_v1 Tactic: 0xbdfdef6b84f7ccc9
[08/10/2023-11:16:51] [V] [TRT] Tactic: 0xbdfdef6b84f7ccc9 Time: 11.6878
[08/10/2023-11:16:51] [V] [TRT] Conv_15 + Relu_16 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage3_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0xc0a715d897e240bb
[08/10/2023-11:16:51] [V] [TRT] Tactic: 0xc0a715d897e240bb Time: 3.06999
[08/10/2023-11:16:51] [V] [TRT] Conv_15 + Relu_16 Set Tactic Name: ampere_scudnn_128x128_relu_exp_large_nhwc_tn_v1 Tactic: 0xca7eeb8d9143d738
[08/10/2023-11:16:51] [V] [TRT] Tactic: 0xca7eeb8d9143d738 Time: 10.9264
[08/10/2023-11:16:51] [V] [TRT] Conv_15 + Relu_16 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize256x64x32_stage3_warpsize4x2x1_g1_tensor16x8x8 Tactic: 0xcedbed6d66c946d0
[08/10/2023-11:16:51] [V] [TRT] Tactic: 0xcedbed6d66c946d0 Time: 2.3927
[08/10/2023-11:16:51] [V] [TRT] Conv_15 + Relu_16 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x128x8_stage3_warpsize1x4x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0xd15dd11d64344e83
[08/10/2023-11:16:51] [V] [TRT] Tactic: 0xd15dd11d64344e83 Time: 9.99887
[08/10/2023-11:16:51] [V] [TRT] Conv_15 + Relu_16 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize64x128x32_stage5_warpsize2x2x1_g1_tensor16x8x8_t1r3s3 Tactic: 0xd4428a7355769d7d
[08/10/2023-11:16:51] [V] [TRT] Tactic: 0xd4428a7355769d7d Time: 2.83834
[08/10/2023-11:16:51] [V] [TRT] Conv_15 + Relu_16 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize64x32x64_stage5_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0xd7f5d575d49a4bc7
[08/10/2023-11:16:51] [V] [TRT] Tactic: 0xd7f5d575d49a4bc7 Time: 5.98457
[08/10/2023-11:16:51] [V] [TRT] Conv_15 + Relu_16 Set Tactic Name: ampere_scudnn_128x128_relu_exp_medium_nhwc_tn_v1 Tactic: 0xd9031472c05adf51
[08/10/2023-11:16:52] [V] [TRT] Tactic: 0xd9031472c05adf51 Time: 10.6162
[08/10/2023-11:16:52] [V] [TRT] Conv_15 + Relu_16 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x128x32_stage4_warpsize2x2x1_g1_tensor16x8x8_t1r3s3 Tactic: 0xd920b33c9bd27143
[08/10/2023-11:16:52] [V] [TRT] Tactic: 0xd920b33c9bd27143 Time: 2.21148
[08/10/2023-11:16:52] [V] [TRT] Conv_15 + Relu_16 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize256x128x32_stage3_warpsize4x2x1_g1_tensor16x8x8 Tactic: 0xe797e099911c0624
[08/10/2023-11:16:52] [V] [TRT] Tactic: 0xe797e099911c0624 Time: 2.17776
[08/10/2023-11:16:52] [V] [TRT] Conv_15 + Relu_16 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage3_warpsize2x2x1_g1_tensor16x8x8_t1r3s3 Tactic: 0xeb65d4c1e8fa2294
[08/10/2023-11:16:52] [V] [TRT] Tactic: 0xeb65d4c1e8fa2294 Time: 2.43349
[08/10/2023-11:16:52] [V] [TRT] Conv_15 + Relu_16 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x128x32_stage4_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0xf4156675c5f728d4
[08/10/2023-11:16:52] [V] [TRT] Tactic: 0xf4156675c5f728d4 Time: 2.86392
[08/10/2023-11:16:52] [V] [TRT] Conv_15 + Relu_16 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x64x8_stage3_warpsize2x2x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0xf48db81f02eca9ee
[08/10/2023-11:16:52] [V] [TRT] Tactic: 0xf48db81f02eca9ee Time: 9.64498
[08/10/2023-11:16:52] [V] [TRT] Conv_15 + Relu_16 Set Tactic Name: ampere_scudnn_128x128_ldg4_relu_exp_small_nhwc_tn_v1 Tactic: 0xf90060ce8193b811
[08/10/2023-11:16:52] [V] [TRT] Tactic: 0xf90060ce8193b811 Time: 10.3187
[08/10/2023-11:16:52] [V] [TRT] Fastest Tactic: 0x21246c8544eff903 Time: 1.79837
[08/10/2023-11:16:52] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x21246c8544eff903
[08/10/2023-11:16:52] [V] [TRT] *************** Autotuning format combination: Float(2073600,1:4,11520,64) -> Float(1036800,1:4,5760,32) ***************
[08/10/2023-11:16:52] [V] [TRT] --------------- Timing Runner: Conv_15 + Relu_16 (CaskConvolution)
[08/10/2023-11:16:52] [V] [TRT] Conv_15 + Relu_16 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize64x128x32_stage5_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0x112d7e4d280f396d
[08/10/2023-11:16:52] [V] [TRT] Tactic: 0x112d7e4d280f396d Time: 3.63941
[08/10/2023-11:16:52] [V] [TRT] Conv_15 + Relu_16 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x64x32_stage5_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0x13cc2ad8dcd32ba2
[08/10/2023-11:16:52] [V] [TRT] Tactic: 0x13cc2ad8dcd32ba2 Time: 3.78658
[08/10/2023-11:16:52] [V] [TRT] Conv_15 + Relu_16 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize256x128x32_stage3_warpsize4x2x1_g1_tensor16x8x8_t1r3s3 Tactic: 0x21246c8544eff903
[08/10/2023-11:16:52] [V] [TRT] Tactic: 0x21246c8544eff903 Time: 2.52312
[08/10/2023-11:16:52] [V] [TRT] Conv_15 + Relu_16 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x128x32_stage4_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0x25b2b9d5c9d5ca0d
[08/10/2023-11:16:52] [V] [TRT] Tactic: 0x25b2b9d5c9d5ca0d Time: 2.39305
[08/10/2023-11:16:52] [V] [TRT] Conv_15 + Relu_16 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage4_warpsize2x2x1_g1_tensor16x8x8_t1r3s3 Tactic: 0x2d80d2b384ab13f1
[08/10/2023-11:16:52] [V] [TRT] Tactic: 0x2d80d2b384ab13f1 Time: 3.75294
[08/10/2023-11:16:52] [V] [TRT] Conv_15 + Relu_16 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize256x128x32_stage3_warpsize4x2x1_g1_tensor16x8x8 Tactic: 0x36303f399d3ee4fd
[08/10/2023-11:16:52] [V] [TRT] Tactic: 0x36303f399d3ee4fd Time: 3.12908
[08/10/2023-11:16:52] [V] [TRT] Conv_15 + Relu_16 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage4_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0x385bfab9332b1413
[08/10/2023-11:16:52] [V] [TRT] Tactic: 0x385bfab9332b1413 Time: 4.7439
[08/10/2023-11:16:52] [V] [TRT] Conv_15 + Relu_16 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x256x32_stage3_warpsize2x4x1_g1_tensor16x8x8 Tactic: 0x482b48242255b8ce
[08/10/2023-11:16:52] [V] [TRT] Tactic: 0x482b48242255b8ce Time: 4.64285
[08/10/2023-11:16:52] [V] [TRT] Conv_15 + Relu_16 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x256x32_stage3_warpsize2x4x1_g1_tensor16x8x8_t1r3s3 Tactic: 0x614e89f7852edbc3
[08/10/2023-11:16:52] [V] [TRT] Tactic: 0x614e89f7852edbc3 Time: 3.55398
[08/10/2023-11:16:52] [V] [TRT] Conv_15 + Relu_16 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0x65e41d81f093b482
[08/10/2023-11:16:52] [V] [TRT] Tactic: 0x65e41d81f093b482 Time: 2.30771
[08/10/2023-11:16:52] [V] [TRT] Conv_15 + Relu_16 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize64x128x32_stage5_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0x65f71607d20d5438
[08/10/2023-11:16:52] [V] [TRT] Tactic: 0x65f71607d20d5438 Time: 2.35845
[08/10/2023-11:16:52] [V] [TRT] Conv_15 + Relu_16 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x64x32_stage5_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0x6716429226d146f7
[08/10/2023-11:16:52] [V] [TRT] Tactic: 0x6716429226d146f7 Time: 2.50945
[08/10/2023-11:16:52] [V] [TRT] Conv_15 + Relu_16 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize64x32x64_stage5_warpsize2x2x1_g1_tensor16x8x8_t1r3s3 Tactic: 0x679a281f7660c436
[08/10/2023-11:16:52] [V] [TRT] Tactic: 0x679a281f7660c436 Time: 4.65148
[08/10/2023-11:16:52] [V] [TRT] Conv_15 + Relu_16 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize64x32x64_stage5_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0x6e2a3c7a7fc5e4e1
[08/10/2023-11:16:52] [V] [TRT] Tactic: 0x6e2a3c7a7fc5e4e1 Time: 5.06369
[08/10/2023-11:16:52] [V] [TRT] Conv_15 + Relu_16 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x64x32_stage5_warpsize2x2x1_g1_tensor16x8x8_t1r3s3 Tactic: 0x7aa21282feb15f1a
[08/10/2023-11:16:52] [V] [TRT] Tactic: 0x7aa21282feb15f1a Time: 3.17735
[08/10/2023-11:16:52] [V] [TRT] Conv_15 + Relu_16 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage4_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0x818413b69874bb35
[08/10/2023-11:16:52] [V] [TRT] Tactic: 0x818413b69874bb35 Time: 3.95216
[08/10/2023-11:16:52] [V] [TRT] Conv_15 + Relu_16 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x256x32_stage3_warpsize2x4x1_g1_tensor16x8x8 Tactic: 0x998c97842e775a17
[08/10/2023-11:16:52] [V] [TRT] Tactic: 0x998c97842e775a17 Time: 4.23091
[08/10/2023-11:16:52] [V] [TRT] Conv_15 + Relu_16 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8_t1r3s3 Tactic: 0x999e005e3b016ea6
[08/10/2023-11:16:52] [V] [TRT] Tactic: 0x999e005e3b016ea6 Time: 2.47788
[08/10/2023-11:16:52] [V] [TRT] Conv_15 + Relu_16 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize256x64x32_stage3_warpsize4x2x1_g1_tensor16x8x8_t1r3s3 Tactic: 0xa9a06d0633580c0c
[08/10/2023-11:16:53] [V] [TRT] Tactic: 0xa9a06d0633580c0c Time: 2.67649
[08/10/2023-11:16:53] [V] [TRT] Conv_15 + Relu_16 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0xb443c221fcb1565b
[08/10/2023-11:16:53] [V] [TRT] Tactic: 0xb443c221fcb1565b Time: 2.42127
[08/10/2023-11:16:53] [V] [TRT] Conv_15 + Relu_16 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage3_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0xb47d7d926de02dee
[08/10/2023-11:16:53] [V] [TRT] Tactic: 0xb47d7d926de02dee Time: 2.78837
[08/10/2023-11:16:53] [V] [TRT] Conv_15 + Relu_16 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize256x64x32_stage3_warpsize4x2x1_g1_tensor16x8x8 Tactic: 0xba0185279ccb2b85
[08/10/2023-11:16:53] [V] [TRT] Tactic: 0xba0185279ccb2b85 Time: 2.42589
[08/10/2023-11:16:53] [V] [TRT] Conv_15 + Relu_16 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage3_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0xc0a715d897e240bb
[08/10/2023-11:16:53] [V] [TRT] Tactic: 0xc0a715d897e240bb Time: 3.00764
[08/10/2023-11:16:53] [V] [TRT] Conv_15 + Relu_16 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize256x64x32_stage3_warpsize4x2x1_g1_tensor16x8x8 Tactic: 0xcedbed6d66c946d0
[08/10/2023-11:16:53] [V] [TRT] Tactic: 0xcedbed6d66c946d0 Time: 2.18773
[08/10/2023-11:16:53] [V] [TRT] Conv_15 + Relu_16 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize64x128x32_stage5_warpsize2x2x1_g1_tensor16x8x8_t1r3s3 Tactic: 0xd4428a7355769d7d
[08/10/2023-11:16:53] [V] [TRT] Tactic: 0xd4428a7355769d7d Time: 2.2747
[08/10/2023-11:16:53] [V] [TRT] Conv_15 + Relu_16 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize64x32x64_stage5_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0xd7f5d575d49a4bc7
[08/10/2023-11:16:53] [V] [TRT] Tactic: 0xd7f5d575d49a4bc7 Time: 5.47344
[08/10/2023-11:16:53] [V] [TRT] Conv_15 + Relu_16 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x128x32_stage4_warpsize2x2x1_g1_tensor16x8x8_t1r3s3 Tactic: 0xd920b33c9bd27143
[08/10/2023-11:16:53] [V] [TRT] Tactic: 0xd920b33c9bd27143 Time: 1.78849
[08/10/2023-11:16:53] [V] [TRT] Conv_15 + Relu_16 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize256x128x32_stage3_warpsize4x2x1_g1_tensor16x8x8 Tactic: 0xe797e099911c0624
[08/10/2023-11:16:53] [V] [TRT] Tactic: 0xe797e099911c0624 Time: 2.2472
[08/10/2023-11:16:53] [V] [TRT] Conv_15 + Relu_16 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage3_warpsize2x2x1_g1_tensor16x8x8_t1r3s3 Tactic: 0xeb65d4c1e8fa2294
[08/10/2023-11:16:53] [V] [TRT] Tactic: 0xeb65d4c1e8fa2294 Time: 3.27911
[08/10/2023-11:16:53] [V] [TRT] Conv_15 + Relu_16 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x128x32_stage4_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0xf4156675c5f728d4
[08/10/2023-11:16:53] [V] [TRT] Tactic: 0xf4156675c5f728d4 Time: 3.80834
[08/10/2023-11:16:53] [V] [TRT] Fastest Tactic: 0xd920b33c9bd27143 Time: 1.78849
[08/10/2023-11:16:53] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0xd920b33c9bd27143
[08/10/2023-11:16:53] [V] [TRT] *************** Autotuning format combination: Half(8294400,32400,180,1) -> Half(4147200,32400,180,1) ***************
[08/10/2023-11:16:53] [W] [TRT] Weights [name=Conv_15 + Relu_16.weight] had the following issues when converted to FP16:
[08/10/2023-11:16:53] [W] [TRT]  - Subnormal FP16 values detected. 
[08/10/2023-11:16:53] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.
[08/10/2023-11:16:53] [V] [TRT] --------------- Timing Runner: Conv_15 + Relu_16 (CudnnConvolution)
[08/10/2023-11:16:53] [V] [TRT] Tactic: 0x0000000000000000 Time: 15.1635
[08/10/2023-11:16:53] [V] [TRT] Tactic: 0x0000000000000001 Time: 14.3618
[08/10/2023-11:16:53] [V] [TRT] Tactic: 0x0000000000000002 Time: 16.497
[08/10/2023-11:16:53] [V] [TRT] Tactic: 0x0000000000000004 skipped. Scratch requested: 17450532864, available: 4294967296
[08/10/2023-11:16:54] [V] [TRT] Tactic: 0x0000000000000005 Time: 55.3544
[08/10/2023-11:16:54] [V] [TRT] Tactic: 0x0000000000000006 Time: 6.94828
[08/10/2023-11:16:54] [V] [TRT] Tactic: 0x0000000000000038 Time: 14.1122
[08/10/2023-11:16:54] [V] [TRT] Tactic: 0x000000000000003a Time: 18.049
[08/10/2023-11:16:54] [V] [TRT] Tactic: 0x000000000000003c skipped. Scratch requested: 17450532864, available: 4294967296
[08/10/2023-11:16:54] [V] [TRT] Tactic: 0x000000000000003d Time: 38.8943
[08/10/2023-11:16:55] [V] [TRT] Tactic: 0x000000000000003e Time: 7.71411
[08/10/2023-11:16:55] [V] [TRT] Fastest Tactic: 0x0000000000000006 Time: 6.94828
[08/10/2023-11:16:55] [V] [TRT] Setting workspace to 17450532864enables more tactics for profiling
[08/10/2023-11:16:55] [W] [TRT] Weights [name=Conv_15 + Relu_16.weight] had the following issues when converted to FP16:
[08/10/2023-11:16:55] [W] [TRT]  - Subnormal FP16 values detected. 
[08/10/2023-11:16:55] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.
[08/10/2023-11:16:55] [V] [TRT] --------------- Timing Runner: Conv_15 + Relu_16 (CaskConvolution)
[08/10/2023-11:16:55] [V] [TRT] CaskConvolution has no valid tactics for this config, skipping
[08/10/2023-11:16:55] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CudnnConvolution Tactic: 0x0000000000000006
[08/10/2023-11:16:55] [V] [TRT] *************** Autotuning format combination: Half(4147200,32400:2,180,1) -> Half(2073600,32400:2,180,1) ***************
[08/10/2023-11:16:55] [W] [TRT] Weights [name=Conv_15 + Relu_16.weight] had the following issues when converted to FP16:
[08/10/2023-11:16:55] [W] [TRT]  - Subnormal FP16 values detected. 
[08/10/2023-11:16:55] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.
[08/10/2023-11:16:55] [V] [TRT] --------------- Timing Runner: Conv_15 + Relu_16 (FusedConvActConvolution)
[08/10/2023-11:16:55] [V] [TRT] FusedConvActConvolution has no valid tactics for this config, skipping
[08/10/2023-11:16:55] [W] [TRT] Weights [name=Conv_15 + Relu_16.weight] had the following issues when converted to FP16:
[08/10/2023-11:16:55] [W] [TRT]  - Subnormal FP16 values detected. 
[08/10/2023-11:16:55] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.
[08/10/2023-11:16:55] [V] [TRT] --------------- Timing Runner: Conv_15 + Relu_16 (CaskConvolution)
[08/10/2023-11:16:55] [V] [TRT] CaskConvolution has no valid tactics for this config, skipping
[08/10/2023-11:16:55] [V] [TRT] *************** Autotuning format combination: Half(1036800,1:8,5760,32) -> Float(4147200,32400,180,1) ***************
[08/10/2023-11:16:55] [W] [TRT] Weights [name=Conv_15 + Relu_16.weight] had the following issues when converted to FP16:
[08/10/2023-11:16:55] [W] [TRT]  - Subnormal FP16 values detected. 
[08/10/2023-11:16:55] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.
[08/10/2023-11:16:55] [V] [TRT] --------------- Timing Runner: Conv_15 + Relu_16 (CaskConvolution)
[08/10/2023-11:16:55] [V] [TRT] CaskConvolution has no valid tactics for this config, skipping
[08/10/2023-11:16:55] [V] [TRT] *************** Autotuning format combination: Half(1036800,1:8,5760,32) -> Half(518400,1:8,2880,16) ***************
[08/10/2023-11:16:55] [W] [TRT] Weights [name=Conv_15 + Relu_16.weight] had the following issues when converted to FP16:
[08/10/2023-11:16:55] [W] [TRT]  - Subnormal FP16 values detected. 
[08/10/2023-11:16:55] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.
[08/10/2023-11:16:55] [V] [TRT] --------------- Timing Runner: Conv_15 + Relu_16 (CudaDepthwiseConvolution)
[08/10/2023-11:16:55] [V] [TRT] CudaDepthwiseConvolution has no valid tactics for this config, skipping
[08/10/2023-11:16:55] [W] [TRT] Weights [name=Conv_15 + Relu_16.weight] had the following issues when converted to FP16:
[08/10/2023-11:16:55] [W] [TRT]  - Subnormal FP16 values detected. 
[08/10/2023-11:16:55] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.
[08/10/2023-11:16:55] [V] [TRT] --------------- Timing Runner: Conv_15 + Relu_16 (CaskConvolution)
[08/10/2023-11:16:55] [V] [TRT] Conv_15 + Relu_16 Set Tactic Name: ampere_h16816cudnn_256x128_ldg8_relu_exp_medium_nhwc_tn_v1 Tactic: 0x03896956a39a1203
[08/10/2023-11:16:55] [V] [TRT] Tactic: 0x03896956a39a1203 Time: 1.36134
[08/10/2023-11:16:55] [V] [TRT] Conv_15 + Relu_16 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x128x32_stage3_warpsize4x2x1_g1_tensor16x8x16_t1r3s3 Tactic: 0x048d6d0400f33439
[08/10/2023-11:16:55] [V] [TRT] Tactic: 0x048d6d0400f33439 Time: 1.35301
[08/10/2023-11:16:55] [V] [TRT] Conv_15 + Relu_16 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x64x64_stage3_warpsize2x2x1_g1_tensor16x8x16_t1r3s3 Tactic: 0x05b6220f243edacd
[08/10/2023-11:16:55] [V] [TRT] Tactic: 0x05b6220f243edacd Time: 1.36174
[08/10/2023-11:16:55] [V] [TRT] Conv_15 + Relu_16 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x64x32_stage3_warpsize4x1x1_g1_tensor16x8x16_t1r3s3_aACCESS Tactic: 0x0866ddee325d07a6
[08/10/2023-11:16:55] [V] [TRT] Tactic: 0x0866ddee325d07a6 Time: 1.35775
[08/10/2023-11:16:55] [V] [TRT] Conv_15 + Relu_16 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x128x32_stage4_warpsize2x2x1_g1_tensor16x8x16 Tactic: 0x0e07ff4f4f7c1ac9
[08/10/2023-11:16:55] [V] [TRT] Tactic: 0x0e07ff4f4f7c1ac9 Time: 1.06251
[08/10/2023-11:16:55] [V] [TRT] Conv_15 + Relu_16 Set Tactic Name: ampere_h16816cudnn_256x128_ldg8_relu_exp_stages_64x3_small_nhwc_tn_v1 Tactic: 0x0e199750c30c6928
[08/10/2023-11:16:55] [V] [TRT] Tactic: 0x0e199750c30c6928 Time: 0.917536
[08/10/2023-11:16:55] [V] [TRT] Conv_15 + Relu_16 Set Tactic Name: ampere_h1688cudnn_128x128_ldg8_relu_exp_medium_nhwc_tn_v1 Tactic: 0x100c1ad308e08d35
[08/10/2023-11:16:55] [V] [TRT] Tactic: 0x100c1ad308e08d35 Time: 1.14977
[08/10/2023-11:16:55] [V] [TRT] Conv_15 + Relu_16 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x128x32_stage5_warpsize2x2x1_g1_tensor16x8x16_t1r3s3 Tactic: 0x17ebf0c9f418f10a
[08/10/2023-11:16:55] [V] [TRT] Tactic: 0x17ebf0c9f418f10a Time: 1.0466
[08/10/2023-11:16:55] [V] [TRT] Conv_15 + Relu_16 Set Tactic Name: ampere_h16816cudnn_128x64_ldg8_relu_exp_stages_64x4_medium_nhwc_tn_v1 Tactic: 0x196cbc423a9bb69e
[08/10/2023-11:16:55] [V] [TRT] Tactic: 0x196cbc423a9bb69e Time: 1.17956
[08/10/2023-11:16:55] [V] [TRT] Conv_15 + Relu_16 Set Tactic Name: ampere_h16816cudnn_128x64_sliced1x2_ldg8_relu_exp_stages_64x3_small_nhwc_tn_v1 Tactic: 0x19822de884f42a6a
[08/10/2023-11:16:55] [V] [TRT] Tactic: 0x19822de884f42a6a Time: 0.993874
[08/10/2023-11:16:55] [V] [TRT] Conv_15 + Relu_16 Set Tactic Name: ampere_h16816cudnn_64x64_sliced1x2_ldg8_relu_exp_stages_64x5_small_nhwc_tn_v1 Tactic: 0x1a5ba808ad4cc5a8
[08/10/2023-11:16:55] [V] [TRT] Tactic: 0x1a5ba808ad4cc5a8 Time: 1.44518
[08/10/2023-11:16:55] [V] [TRT] Conv_15 + Relu_16 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x64x64_stage3_warpsize2x2x1_g1_tensor16x8x16 Tactic: 0x21b295c0c8f6c95a
[08/10/2023-11:16:55] [V] [TRT] Tactic: 0x21b295c0c8f6c95a Time: 1.31526
[08/10/2023-11:16:55] [V] [TRT] Conv_15 + Relu_16 Set Tactic Name: ampere_h16816cudnn_128x128_ldg8_relu_exp_medium_nhwc_tn_v1 Tactic: 0x234580e8a194335c
[08/10/2023-11:16:55] [V] [TRT] Tactic: 0x234580e8a194335c Time: 0.896521
[08/10/2023-11:16:55] [V] [TRT] Conv_15 + Relu_16 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x128x32_stage4_warpsize2x2x1_g1_tensor16x8x16_t1r3s3_aACCESS Tactic: 0x245530c34bd6090f
[08/10/2023-11:16:55] [V] [TRT] Tactic: 0x245530c34bd6090f Time: 0.850917
[08/10/2023-11:16:55] [V] [TRT] Conv_15 + Relu_16 Set Tactic Name: ampere_h16816cudnn_64x64_sliced1x2_ldg8_relu_exp_stages_64x6_small_nhwc_tn_v1 Tactic: 0x24e01e7405cfdfe9
[08/10/2023-11:16:55] [V] [TRT] Tactic: 0x24e01e7405cfdfe9 Time: 1.48246
[08/10/2023-11:16:55] [V] [TRT] Conv_15 + Relu_16 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x64x64_stage3_warpsize2x2x1_g1_tensor16x8x16_t1r3s3_aACCESS Tactic: 0x263a38afd75e3a43
[08/10/2023-11:16:55] [V] [TRT] Tactic: 0x263a38afd75e3a43 Time: 0.967337
[08/10/2023-11:16:55] [V] [TRT] Conv_15 + Relu_16 Set Tactic Name: ampere_h16816cudnn_64x128_sliced1x2_ldg8_relu_exp_stages_64x4_large_nhwc_tn_v1 Tactic: 0x2721a7f18c2700db
[08/10/2023-11:16:55] [V] [TRT] Tactic: 0x2721a7f18c2700db Time: 1.14078
[08/10/2023-11:16:55] [V] [TRT] Conv_15 + Relu_16 Set Tactic Name: ampere_h16816cudnn_128x64_sliced1x2_ldg8_relu_exp_stages_64x3_medium_nhwc_tn_v1 Tactic: 0x29329b5741ea05f2
[08/10/2023-11:16:55] [V] [TRT] Tactic: 0x29329b5741ea05f2 Time: 1.00277
[08/10/2023-11:16:55] [V] [TRT] Conv_15 + Relu_16 Set Tactic Name: ampere_h16816cudnn_128x128_ldg8_relu_exp_large_nhwc_tn_v1 Tactic: 0x2970ae65966d569d
[08/10/2023-11:16:55] [V] [TRT] Tactic: 0x2970ae65966d569d Time: 0.878299
[08/10/2023-11:16:55] [V] [TRT] Conv_15 + Relu_16 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x256x64_stage3_warpsize1x4x1_g1_tensor16x8x16 Tactic: 0x29dc14520e0e4eb6
[08/10/2023-11:16:55] [V] [TRT] Tactic: 0x29dc14520e0e4eb6 Time: 1.75872
[08/10/2023-11:16:55] [V] [TRT] Conv_15 + Relu_16 Set Tactic Name: ampere_h16816cudnn_256x64_ldg8_relu_exp_stages_64x3_large_nhwc_tn_v1 Tactic: 0x2cb4a662694e89d3
[08/10/2023-11:16:55] [V] [TRT] Tactic: 0x2cb4a662694e89d3 Time: 0.963904
[08/10/2023-11:16:55] [V] [TRT] Conv_15 + Relu_16 Set Tactic Name: ampere_h16816cudnn_128x128_ldg8_relu_exp_stages_64x3_medium_nhwc_tn_v1 Tactic: 0x2eaa2202de9404d6
[08/10/2023-11:16:55] [V] [TRT] Tactic: 0x2eaa2202de9404d6 Time: 0.951918
[08/10/2023-11:16:55] [V] [TRT] Conv_15 + Relu_16 Set Tactic Name: ampere_h1688cudnn_256x128_ldg8_relu_exp_medium_nhwc_tn_v1 Tactic: 0x30c0f36d0aeeac6a
[08/10/2023-11:16:55] [V] [TRT] Tactic: 0x30c0f36d0aeeac6a Time: 1.19026
[08/10/2023-11:16:55] [V] [TRT] Conv_15 + Relu_16 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x64x64_stage4_warpsize2x2x1_g1_tensor16x8x16_t1r3s3_aACCESS Tactic: 0x30e8a8d7a953e5e9
[08/10/2023-11:16:55] [V] [TRT] Tactic: 0x30e8a8d7a953e5e9 Time: 1.14741
[08/10/2023-11:16:55] [V] [TRT] Conv_15 + Relu_16 Set Tactic Name: ampere_h16816cudnn_128x64_ldg8_relu_exp_stages_64x4_large_nhwc_tn_v1 Tactic: 0x3197d5044d71e98e
[08/10/2023-11:16:55] [V] [TRT] Tactic: 0x3197d5044d71e98e Time: 1.21453
[08/10/2023-11:16:55] [V] [TRT] Conv_15 + Relu_16 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x64x64_stage4_warpsize2x2x1_g1_tensor16x8x16_t1r3s3 Tactic: 0x31d93dc22d2af081
[08/10/2023-11:16:55] [V] [TRT] Tactic: 0x31d93dc22d2af081 Time: 1.39144
[08/10/2023-11:16:55] [V] [TRT] Conv_15 + Relu_16 Set Tactic Name: ampere_h1688cudnn_128x128_ldg8_relu_exp_small_nhwc_tn_v1 Tactic: 0x3369260c04f9ad73
[08/10/2023-11:16:55] [V] [TRT] Tactic: 0x3369260c04f9ad73 Time: 1.12469
[08/10/2023-11:16:55] [V] [TRT] Conv_15 + Relu_16 Set Tactic Name: ampere_h16816cudnn_64x128_ldg8_relu_exp_stages_64x4_small_nhwc_tn_v1 Tactic: 0x3b5fa0f2a8fc2410
[08/10/2023-11:16:55] [V] [TRT] Tactic: 0x3b5fa0f2a8fc2410 Time: 1.15173
[08/10/2023-11:16:55] [V] [TRT] Conv_15 + Relu_16 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x32x32_stage4_warpsize4x1x1_g1_tensor16x8x16_t1r3s3 Tactic: 0x3e7eb35b91b9fa63
[08/10/2023-11:16:55] [V] [TRT] Tactic: 0x3e7eb35b91b9fa63 Time: 1.93454
[08/10/2023-11:16:55] [V] [TRT] Conv_15 + Relu_16 Set Tactic Name: ampere_h16816cudnn_256x64_ldg8_relu_exp_stages_64x3_medium_nhwc_tn_v1 Tactic: 0x3f031df0e579d52c
[08/10/2023-11:16:55] [V] [TRT] Tactic: 0x3f031df0e579d52c Time: 1.2663
[08/10/2023-11:16:55] [V] [TRT] Conv_15 + Relu_16 Set Tactic Name: ampere_h16816cudnn_256x64_sliced1x2_ldg8_relu_exp_small_nhwc_tn_v1 Tactic: 0x42a5b8709d0039be
[08/10/2023-11:16:55] [V] [TRT] Tactic: 0x42a5b8709d0039be Time: 1.3771
[08/10/2023-11:16:55] [V] [TRT] Conv_15 + Relu_16 Set Tactic Name: ampere_h16816cudnn_256x128_ldg8_relu_exp_small_nhwc_tn_v1 Tactic: 0x463794ee4acffd1f
[08/10/2023-11:16:55] [V] [TRT] Tactic: 0x463794ee4acffd1f Time: 1.16478
[08/10/2023-11:16:55] [V] [TRT] Conv_15 + Relu_16 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x64x64_stage4_warpsize2x2x1_g1_tensor16x8x16 Tactic: 0x4a81ea1e51436a30
[08/10/2023-11:16:55] [V] [TRT] Tactic: 0x4a81ea1e51436a30 Time: 1.37033
[08/10/2023-11:16:55] [V] [TRT] Conv_15 + Relu_16 Set Tactic Name: ampere_h1688cudnn_256x64_sliced1x2_ldg8_relu_exp_small_nhwc_tn_v1 Tactic: 0x4aed1956bd10a795
[08/10/2023-11:16:55] [V] [TRT] Tactic: 0x4aed1956bd10a795 Time: 1.29051
[08/10/2023-11:16:55] [V] [TRT] Conv_15 + Relu_16 Set Tactic Name: ampere_h16816cudnn_64x128_sliced1x2_ldg8_relu_exp_stages_64x3_large_nhwc_tn_v1 Tactic: 0x4fc05923455fc266
[08/10/2023-11:16:55] [V] [TRT] Tactic: 0x4fc05923455fc266 Time: 1.02946
[08/10/2023-11:16:55] [V] [TRT] Conv_15 + Relu_16 Set Tactic Name: ampere_h16816cudnn_256x64_ldg8_relu_exp_large_nhwc_tn_v1 Tactic: 0x5013c38f55afa9ba
[08/10/2023-11:16:55] [V] [TRT] Tactic: 0x5013c38f55afa9ba Time: 1.04944
[08/10/2023-11:16:55] [V] [TRT] Conv_15 + Relu_16 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x64x32_stage3_warpsize4x1x1_g1_tensor16x8x16_t1r3s3 Tactic: 0x529f4431bdae94f5
[08/10/2023-11:16:55] [V] [TRT] Tactic: 0x529f4431bdae94f5 Time: 0.891867
[08/10/2023-11:16:55] [V] [TRT] Conv_15 + Relu_16 Set Tactic Name: ampere_h16816cudnn_64x128_ldg8_relu_exp_stages_64x3_small_nhwc_tn_v1 Tactic: 0x53be5e206184e6ad
[08/10/2023-11:16:55] [V] [TRT] Tactic: 0x53be5e206184e6ad Time: 1.07253
[08/10/2023-11:16:55] [V] [TRT] Conv_15 + Relu_16 Set Tactic Name: ampere_h16816cudnn_64x64_ldg8_relu_exp_stages_64x5_large_nhwc_tn_v1 Tactic: 0x544bff7ff9c5d908
[08/10/2023-11:16:55] [V] [TRT] Tactic: 0x544bff7ff9c5d908 Time: 1.43499
[08/10/2023-11:16:55] [V] [TRT] Conv_15 + Relu_16 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x64x64_stage3_warpsize2x2x1_g1_tensor16x8x16 Tactic: 0x5568fd8a32f4a40f
[08/10/2023-11:16:55] [V] [TRT] Tactic: 0x5568fd8a32f4a40f Time: 1.04996
[08/10/2023-11:16:55] [V] [TRT] Conv_15 + Relu_16 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x128x64_stage3_warpsize2x2x1_g1_tensor16x8x16 Tactic: 0x57c9a5ff682354a6
[08/10/2023-11:16:55] [V] [TRT] Tactic: 0x57c9a5ff682354a6 Time: 0.90965
[08/10/2023-11:16:55] [V] [TRT] Conv_15 + Relu_16 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x32x64_stage5_warpsize2x2x1_g1_tensor16x8x16 Tactic: 0x5820b3dda403c4d0
[08/10/2023-11:16:55] [V] [TRT] Tactic: 0x5820b3dda403c4d0 Time: 2.00197
[08/10/2023-11:16:55] [V] [TRT] Conv_15 + Relu_16 Set Tactic Name: ampere_h16816cudnn_128x128_ldg8_relu_exp_stages_64x3_small_nhwc_tn_v1 Tactic: 0x58816bf2e9c36afb
[08/10/2023-11:16:55] [V] [TRT] Tactic: 0x58816bf2e9c36afb Time: 0.976699
[08/10/2023-11:16:55] [V] [TRT] Conv_15 + Relu_16 Set Tactic Name: ampere_h16816cudnn_128x64_ldg8_relu_exp_stages_64x3_large_nhwc_tn_v1 Tactic: 0x59762bd684092b33
[08/10/2023-11:16:55] [V] [TRT] Tactic: 0x59762bd684092b33 Time: 1.09745
[08/10/2023-11:16:55] [V] [TRT] Conv_15 + Relu_16 Set Tactic Name: ampere_h16816cudnn_128x64_sliced1x2_ldg8_relu_exp_stages_64x4_medium_nhwc_tn_v1 Tactic: 0x59c5c647b4c76593
[08/10/2023-11:16:55] [V] [TRT] Tactic: 0x59c5c647b4c76593 Time: 1.16391
[08/10/2023-11:16:55] [V] [TRT] Conv_15 + Relu_16 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x128x64_stage3_warpsize4x2x1_g1_tensor16x8x16 Tactic: 0x5a55274960724ba8
[08/10/2023-11:16:55] [V] [TRT] Tactic: 0x5a55274960724ba8 Time: 0.928791
[08/10/2023-11:16:55] [V] [TRT] Conv_15 + Relu_16 Set Tactic Name: ampere_h1688cudnn_256x128_ldg8_relu_exp_large_nhwc_tn_v1 Tactic: 0x5c2e1c87d85b06f1
[08/10/2023-11:16:55] [V] [TRT] Tactic: 0x5c2e1c87d85b06f1 Time: 1.20228
[08/10/2023-11:16:55] [V] [TRT] Conv_15 + Relu_16 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x256x64_stage3_warpsize1x4x1_g1_tensor16x8x16 Tactic: 0x5d067c18f40c23e3
[08/10/2023-11:16:55] [V] [TRT] Tactic: 0x5d067c18f40c23e3 Time: 1.82407
[08/10/2023-11:16:55] [V] [TRT] Conv_15 + Relu_16 Set Tactic Name: ampere_h16816cudnn_64x64_ldg8_relu_exp_stages_64x5_medium_nhwc_tn_v1 Tactic: 0x5f31c22ec167f384
[08/10/2023-11:16:55] [V] [TRT] Tactic: 0x5f31c22ec167f384 Time: 1.93486
[08/10/2023-11:16:55] [V] [TRT] Conv_15 + Relu_16 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x128x32_stage4_warpsize2x2x1_g1_tensor16x8x16_t1r3s3 Tactic: 0x60c3421152ef8e10
[08/10/2023-11:16:55] [V] [TRT] Tactic: 0x60c3421152ef8e10 Time: 0.828914
[08/10/2023-11:16:55] [V] [TRT] Conv_15 + Relu_16 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x128x32_stage3_warpsize4x2x1_g1_tensor16x8x16 Tactic: 0x60da8c7151d91e47
[08/10/2023-11:16:55] [V] [TRT] Tactic: 0x60da8c7151d91e47 Time: 1.26101
[08/10/2023-11:16:55] [V] [TRT] Conv_15 + Relu_16 Set Tactic Name: ampere_h1688cudnn_256x64_ldg8_relu_exp_medium_nhwc_tn_v1 Tactic: 0x6426696f872a3b13
[08/10/2023-11:16:55] [V] [TRT] Tactic: 0x6426696f872a3b13 Time: 1.71111
[08/10/2023-11:16:55] [V] [TRT] Conv_15 + Relu_16 Set Tactic Name: ampere_h16816cudnn_128x64_ldg8_relu_exp_stages_64x3_medium_nhwc_tn_v1 Tactic: 0x699be152cfb6d6ff
[08/10/2023-11:16:55] [V] [TRT] Tactic: 0x699be152cfb6d6ff Time: 1.06882
[08/10/2023-11:16:55] [V] [TRT] Conv_15 + Relu_16 Set Tactic Name: ampere_h16816cudnn_64x64_ldg8_relu_exp_stages_64x6_large_nhwc_tn_v1 Tactic: 0x6af049035146c349
[08/10/2023-11:16:55] [V] [TRT] Tactic: 0x6af049035146c349 Time: 2.59974
[08/10/2023-11:16:55] [V] [TRT] Conv_15 + Relu_16 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x256x32_stage3_warpsize2x4x1_g1_tensor16x8x16 Tactic: 0x7005d10718f6c22d
[08/10/2023-11:16:55] [V] [TRT] Tactic: 0x7005d10718f6c22d Time: 1.72004
[08/10/2023-11:16:55] [V] [TRT] Conv_15 + Relu_16 Set Tactic Name: ampere_h16816cudnn_64x64_sliced1x2_ldg8_relu_exp_stages_64x5_medium_nhwc_tn_v1 Tactic: 0x706f08da35c795a5
[08/10/2023-11:16:55] [V] [TRT] Tactic: 0x706f08da35c795a5 Time: 1.39452
[08/10/2023-11:16:55] [V] [TRT] Conv_15 + Relu_16 Set Tactic Name: ampere_h16816cudnn_128x64_sliced1x2_ldg8_relu_exp_stages_64x4_small_nhwc_tn_v1 Tactic: 0x7163d33a4d8ce8d7
[08/10/2023-11:16:55] [V] [TRT] Tactic: 0x7163d33a4d8ce8d7 Time: 1.12187
[08/10/2023-11:16:55] [V] [TRT] Conv_15 + Relu_16 Set Tactic Name: ampere_h1688cudnn_256x64_ldg8_relu_exp_small_nhwc_tn_v1 Tactic: 0x7aad3976677d7155
[08/10/2023-11:16:55] [V] [TRT] Tactic: 0x7aad3976677d7155 Time: 1.34036
[08/10/2023-11:16:55] [V] [TRT] Conv_15 + Relu_16 Set Tactic Name: ampere_h16816cudnn_64x64_sliced1x2_ldg8_relu_exp_stages_64x5_large_nhwc_tn_v1 Tactic: 0x8015519605ab9963
[08/10/2023-11:16:55] [V] [TRT] Tactic: 0x8015519605ab9963 Time: 1.36117
[08/10/2023-11:16:55] [V] [TRT] Conv_15 + Relu_16 Set Tactic Name: ampere_h16816cudnn_256x64_sliced1x2_ldg8_relu_exp_medium_nhwc_tn_v1 Tactic: 0x82f8a2214b0b4178
[08/10/2023-11:16:55] [V] [TRT] Tactic: 0x82f8a2214b0b4178 Time: 1.34682
[08/10/2023-11:16:55] [V] [TRT] Conv_15 + Relu_16 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x32x32_stage4_warpsize4x1x1_g1_tensor16x8x16 Tactic: 0x834e11ecd4ab9454
[08/10/2023-11:16:55] [V] [TRT] Tactic: 0x834e11ecd4ab9454 Time: 1.68503
[08/10/2023-11:16:55] [V] [TRT] Conv_15 + Relu_16 Set Tactic Name: ampere_h16816cudnn_128x64_sliced1x2_ldg8_relu_exp_stages_64x3_large_nhwc_tn_v1 Tactic: 0x83ccd4762c1376a1
[08/10/2023-11:16:55] [V] [TRT] Tactic: 0x83ccd4762c1376a1 Time: 1.02092
[08/10/2023-11:16:55] [V] [TRT] Conv_15 + Relu_16 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x64x64_stage3_warpsize4x1x1_g1_tensor16x8x16_t1r3s3 Tactic: 0x84ee897dd1f4d2aa
[08/10/2023-11:16:55] [V] [TRT] Tactic: 0x84ee897dd1f4d2aa Time: 1.00504
[08/10/2023-11:16:55] [V] [TRT] Conv_15 + Relu_16 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x128x64_stage3_warpsize2x2x1_g1_tensor16x8x16 Tactic: 0x866e7a5f6401b67f
[08/10/2023-11:16:55] [V] [TRT] Tactic: 0x866e7a5f6401b67f Time: 0.909888
[08/10/2023-11:16:55] [V] [TRT] Conv_15 + Relu_16 Set Tactic Name: ampere_h16816cudnn_64x64_ldg8_relu_exp_stages_64x6_medium_nhwc_tn_v1 Tactic: 0x88971eec55aba850
[08/10/2023-11:16:55] [V] [TRT] Tactic: 0x88971eec55aba850 Time: 1.76895
[08/10/2023-11:16:55] [V] [TRT] Conv_15 + Relu_16 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x128x64_stage3_warpsize4x2x1_g1_tensor16x8x16 Tactic: 0x8bf2f8e96c50a971
[08/10/2023-11:16:56] [V] [TRT] Tactic: 0x8bf2f8e96c50a971 Time: 1.10359
[08/10/2023-11:16:56] [V] [TRT] Conv_15 + Relu_16 Set Tactic Name: ampere_h16816cudnn_256x128_ldg8_relu_exp_stages_64x3_large_nhwc_tn_v1 Tactic: 0x94576ece6beb35e3
[08/10/2023-11:16:56] [V] [TRT] Tactic: 0x94576ece6beb35e3 Time: 0.936562
[08/10/2023-11:16:56] [V] [TRT] Conv_15 + Relu_16 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x64x64_stage3_warpsize4x1x1_g1_tensor16x8x16 Tactic: 0x9fff6e65019cc310
[08/10/2023-11:16:56] [V] [TRT] Tactic: 0x9fff6e65019cc310 Time: 1.01094
[08/10/2023-11:16:56] [V] [TRT] Conv_15 + Relu_16 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x32x64_stage5_warpsize2x2x1_g1_tensor16x8x16_t1r3s3 Tactic: 0xa033e20ae9f412b2
[08/10/2023-11:16:56] [V] [TRT] Tactic: 0xa033e20ae9f412b2 Time: 2.00509
[08/10/2023-11:16:56] [V] [TRT] Conv_15 + Relu_16 Set Tactic Name: ampere_h16816cudnn_64x128_ldg8_relu_exp_stages_64x4_large_nhwc_tn_v1 Tactic: 0xa111596c001b78db
[08/10/2023-11:16:56] [V] [TRT] Tactic: 0xa111596c001b78db Time: 1.15022
[08/10/2023-11:16:56] [V] [TRT] Conv_15 + Relu_16 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x256x32_stage3_warpsize2x4x1_g1_tensor16x8x16 Tactic: 0xa1a20ea714d420f4
[08/10/2023-11:16:56] [V] [TRT] Tactic: 0xa1a20ea714d420f4 Time: 1.67741
[08/10/2023-11:16:56] [V] [TRT] Conv_15 + Relu_16 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x32x32_stage4_warpsize4x1x1_g1_tensor16x8x16_t1r3s3 Tactic: 0xa40cb43c296a36a8
[08/10/2023-11:16:56] [V] [TRT] Tactic: 0xa40cb43c296a36a8 Time: 2.43666
[08/10/2023-11:16:56] [V] [TRT] Conv_15 + Relu_16 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x64x32_stage3_warpsize4x1x1_g1_tensor16x8x16 Tactic: 0xa570c55d303796ff
[08/10/2023-11:16:56] [V] [TRT] Tactic: 0xa570c55d303796ff Time: 1.16974
[08/10/2023-11:16:56] [V] [TRT] Conv_15 + Relu_16 Set Tactic Name: ampere_h16816cudnn_64x64_sliced1x2_ldg8_relu_exp_stages_64x6_medium_nhwc_tn_v1 Tactic: 0xa7c9d418a10bce71
[08/10/2023-11:16:56] [V] [TRT] Tactic: 0xa7c9d418a10bce71 Time: 2.07884
[08/10/2023-11:16:56] [V] [TRT] Conv_15 + Relu_16 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x32x32_stage4_warpsize4x1x1_g1_tensor16x8x16 Tactic: 0xa83b68f30462f971
[08/10/2023-11:16:56] [V] [TRT] Tactic: 0xa83b68f30462f971 Time: 1.73257
[08/10/2023-11:16:56] [V] [TRT] Conv_15 + Relu_16 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x256x32_stage3_warpsize2x4x1_g1_tensor16x8x16_t1r3s3 Tactic: 0xa9177bbe4e767df8
[08/10/2023-11:16:56] [V] [TRT] Tactic: 0xa9177bbe4e767df8 Time: 2.2119
[08/10/2023-11:16:56] [V] [TRT] Conv_15 + Relu_16 Set Tactic Name: ampere_h1688cudnn_128x128_ldg8_relu_exp_large_nhwc_tn_v1 Tactic: 0xa927df92ac1ef1b8
[08/10/2023-11:16:56] [V] [TRT] Tactic: 0xa927df92ac1ef1b8 Time: 1.16035
[08/10/2023-11:16:56] [V] [TRT] Conv_15 + Relu_16 Set Tactic Name: ampere_h16816cudnn_128x64_ldg8_relu_exp_stages_64x4_small_nhwc_tn_v1 Tactic: 0xabd92c9ae596b545
[08/10/2023-11:16:56] [V] [TRT] Tactic: 0xabd92c9ae596b545 Time: 1.35963
[08/10/2023-11:16:56] [V] [TRT] Conv_15 + Relu_16 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x128x32_stage5_warpsize2x2x1_g1_tensor16x8x16 Tactic: 0xafd1e8bf6bd3d638
[08/10/2023-11:16:56] [V] [TRT] Tactic: 0xafd1e8bf6bd3d638 Time: 1.24094
[08/10/2023-11:16:56] [V] [TRT] Conv_15 + Relu_16 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x128x32_stage3_warpsize4x2x1_g1_tensor16x8x16 Tactic: 0xb17d53d15dfbfc9e
[08/10/2023-11:16:56] [V] [TRT] Tactic: 0xb17d53d15dfbfc9e Time: 1.26885
[08/10/2023-11:16:56] [V] [TRT] Conv_15 + Relu_16 Set Tactic Name: ampere_h16816cudnn_128x128_ldg8_relu_exp_small_nhwc_tn_v1 Tactic: 0xb33e57fb3e8a0a56
[08/10/2023-11:16:56] [V] [TRT] Tactic: 0xb33e57fb3e8a0a56 Time: 1.35956
[08/10/2023-11:16:56] [V] [TRT] Conv_15 + Relu_16 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x64x32_stage5_warpsize2x2x1_g1_tensor16x8x16_t1r3s3 Tactic: 0xb4bec086187edcfc
[08/10/2023-11:16:56] [V] [TRT] Tactic: 0xb4bec086187edcfc Time: 1.34733
[08/10/2023-11:16:56] [V] [TRT] Conv_15 + Relu_16 Set Tactic Name: ampere_h16816cudnn_256x64_ldg8_relu_exp_stages_64x3_small_nhwc_tn_v1 Tactic: 0xb6fa5ffcc1a9d518
[08/10/2023-11:16:56] [V] [TRT] Tactic: 0xb6fa5ffcc1a9d518 Time: 1.17618
[08/10/2023-11:16:56] [V] [TRT] Conv_15 + Relu_16 Set Tactic Name: ampere_h16816cudnn_64x128_sliced1x2_ldg8_relu_exp_stages_64x3_medium_nhwc_tn_v1 Tactic: 0xb85e52e87caf60a2
[08/10/2023-11:16:56] [V] [TRT] Tactic: 0xb85e52e87caf60a2 Time: 1.26526
[08/10/2023-11:16:56] [V] [TRT] Conv_15 + Relu_16 Set Tactic Name: ampere_h16816cudnn_64x128_ldg8_relu_exp_stages_64x3_medium_nhwc_tn_v1 Tactic: 0xb8d86216e1235cda
[08/10/2023-11:16:56] [V] [TRT] Tactic: 0xb8d86216e1235cda Time: 1.11318
[08/10/2023-11:16:56] [V] [TRT] Conv_15 + Relu_16 Set Tactic Name: ampere_h16816cudnn_256x128_ldg8_relu_exp_stages_64x3_medium_nhwc_tn_v1 Tactic: 0xb9171d70ba2eda4b
[08/10/2023-11:16:56] [V] [TRT] Tactic: 0xb9171d70ba2eda4b Time: 0.947941
[08/10/2023-11:16:56] [V] [TRT] Conv_15 + Relu_16 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x64x32_stage5_warpsize2x2x1_g1_tensor16x8x16 Tactic: 0xbd08239a9317f2fd
[08/10/2023-11:16:56] [V] [TRT] Tactic: 0xbd08239a9317f2fd Time: 1.3474
[08/10/2023-11:16:56] [V] [TRT] Conv_15 + Relu_16 Set Tactic Name: ampere_h16816cudnn_64x128_sliced1x2_ldg8_relu_exp_stages_64x4_small_nhwc_tn_v1 Tactic: 0xbd6f5e6f24c05c10
[08/10/2023-11:16:56] [V] [TRT] Tactic: 0xbd6f5e6f24c05c10 Time: 1.30019
[08/10/2023-11:16:56] [V] [TRT] Conv_15 + Relu_16 Set Tactic Name: ampere_h16816cudnn_64x64_sliced1x2_ldg8_relu_exp_stages_64x6_large_nhwc_tn_v1 Tactic: 0xbeaee7eaad288322
[08/10/2023-11:16:56] [V] [TRT] Tactic: 0xbeaee7eaad288322 Time: 2.15348
[08/10/2023-11:16:56] [V] [TRT] Conv_15 + Relu_16 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x128x64_stage3_warpsize2x2x1_g1_tensor16x8x16_t1r3s3 Tactic: 0xc0a02dc6095497cc
[08/10/2023-11:16:56] [V] [TRT] Tactic: 0xc0a02dc6095497cc Time: 1.228
[08/10/2023-11:16:56] [V] [TRT] Conv_15 + Relu_16 Set Tactic Name: ampere_h16816cudnn_128x128_ldg8_relu_exp_stages_64x3_large_nhwc_tn_v1 Tactic: 0xc2cf926c41243630
[08/10/2023-11:16:56] [V] [TRT] Tactic: 0xc2cf926c41243630 Time: 1.38307
[08/10/2023-11:16:56] [V] [TRT] Conv_15 + Relu_16 Set Tactic Name: ampere_h16816cudnn_128x64_ldg8_relu_exp_stages_64x3_small_nhwc_tn_v1 Tactic: 0xc338d2482cee77f8
[08/10/2023-11:16:56] [V] [TRT] Tactic: 0xc338d2482cee77f8 Time: 1.51003
[08/10/2023-11:16:56] [V] [TRT] Conv_15 + Relu_16 Set Tactic Name: ampere_h1688cudnn_256x128_ldg8_relu_exp_small_nhwc_tn_v1 Tactic: 0xc660e51970bc5a3a
[08/10/2023-11:16:56] [V] [TRT] Tactic: 0xc660e51970bc5a3a Time: 1.32439
[08/10/2023-11:16:56] [V] [TRT] Conv_15 + Relu_16 Set Tactic Name: ampere_h16816cudnn_64x128_ldg8_relu_exp_stages_64x4_medium_nhwc_tn_v1 Tactic: 0xc82f3f06140e3cbb
[08/10/2023-11:16:56] [V] [TRT] Tactic: 0xc82f3f06140e3cbb Time: 1.33495
[08/10/2023-11:16:56] [V] [TRT] Conv_15 + Relu_16 Set Tactic Name: ampere_h16816cudnn_64x128_sliced1x2_ldg8_relu_exp_stages_64x4_medium_nhwc_tn_v1 Tactic: 0xc8a90ff8898200c3
[08/10/2023-11:16:56] [V] [TRT] Tactic: 0xc8a90ff8898200c3 Time: 1.28056
[08/10/2023-11:16:56] [V] [TRT] Conv_15 + Relu_16 Set Tactic Name: ampere_h1688cudnn_256x64_sliced1x2_ldg8_relu_exp_medium_nhwc_tn_v1 Tactic: 0xc9cc55109bb4de26
[08/10/2023-11:16:56] [V] [TRT] Tactic: 0xc9cc55109bb4de26 Time: 1.73953
[08/10/2023-11:16:56] [V] [TRT] Conv_15 + Relu_16 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x64x32_stage5_warpsize2x2x1_g1_tensor16x8x16 Tactic: 0xc9d24bd069159fa8
[08/10/2023-11:16:56] [V] [TRT] Tactic: 0xc9d24bd069159fa8 Time: 1.26296
[08/10/2023-11:16:56] [V] [TRT] Conv_15 + Relu_16 Set Tactic Name: ampere_h16816cudnn_64x128_ldg8_relu_exp_stages_64x3_large_nhwc_tn_v1 Tactic: 0xc9f0a7bec963ba66
[08/10/2023-11:16:56] [V] [TRT] Tactic: 0xc9f0a7bec963ba66 Time: 1.28622
[08/10/2023-11:16:56] [V] [TRT] Conv_15 + Relu_16 Set Tactic Name: ampere_h16816cudnn_256x64_ldg8_relu_exp_small_nhwc_tn_v1 Tactic: 0xca5d3a11fd48f571
[08/10/2023-11:16:56] [V] [TRT] Tactic: 0xca5d3a11fd48f571 Time: 1.2228
[08/10/2023-11:16:56] [V] [TRT] Conv_15 + Relu_16 Set Tactic Name: ampere_h16816cudnn_64x64_ldg8_relu_exp_stages_64x5_small_nhwc_tn_v1 Tactic: 0xce0506e1512285c3
[08/10/2023-11:16:56] [V] [TRT] Tactic: 0xce0506e1512285c3 Time: 1.6677
[08/10/2023-11:16:56] [V] [TRT] Conv_15 + Relu_16 Set Tactic Name: ampere_h1688cudnn_256x64_sliced1x2_ldg8_relu_exp_large_nhwc_tn_v1 Tactic: 0xd0a3e0c815f7fb5e
[08/10/2023-11:16:56] [V] [TRT] Tactic: 0xd0a3e0c815f7fb5e Time: 1.90294
[08/10/2023-11:16:56] [V] [TRT] Conv_15 + Relu_16 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x64x32_stage3_warpsize4x1x1_g1_tensor16x8x16 Tactic: 0xd1aaad17ca35fbaa
[08/10/2023-11:16:56] [V] [TRT] Tactic: 0xd1aaad17ca35fbaa Time: 1.10801
[08/10/2023-11:16:56] [V] [TRT] Conv_15 + Relu_16 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x128x64_stage3_warpsize4x2x1_g1_tensor16x8x16_t1r3s3 Tactic: 0xd2fca0486ca97266
[08/10/2023-11:16:56] [V] [TRT] Tactic: 0xd2fca0486ca97266 Time: 0.916302
[08/10/2023-11:16:56] [V] [TRT] Conv_15 + Relu_16 Set Tactic Name: ampere_h16816cudnn_64x128_sliced1x2_ldg8_relu_exp_stages_64x3_small_nhwc_tn_v1 Tactic: 0xd58ea0bdedb89ead
[08/10/2023-11:16:56] [V] [TRT] Tactic: 0xd58ea0bdedb89ead Time: 1.2251
[08/10/2023-11:16:56] [V] [TRT] Conv_15 + Relu_16 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x256x64_stage3_warpsize1x4x1_g1_tensor16x8x16_t1r3s3 Tactic: 0xd7c261fa01db2af9
[08/10/2023-11:16:56] [V] [TRT] Tactic: 0xd7c261fa01db2af9 Time: 2.13746
[08/10/2023-11:16:56] [V] [TRT] Conv_15 + Relu_16 Set Tactic Name: ampere_h16816cudnn_256x64_sliced1x2_ldg8_relu_exp_large_nhwc_tn_v1 Tactic: 0xd8eb41ee35e76575
[08/10/2023-11:16:56] [V] [TRT] Tactic: 0xd8eb41ee35e76575 Time: 1.95419
[08/10/2023-11:16:56] [V] [TRT] Conv_15 + Relu_16 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x128x32_stage5_warpsize2x2x1_g1_tensor16x8x16 Tactic: 0xdb0b80f591d1bb6d
[08/10/2023-11:16:56] [V] [TRT] Tactic: 0xdb0b80f591d1bb6d Time: 1.28725
[08/10/2023-11:16:56] [V] [TRT] Conv_15 + Relu_16 Set Tactic Name: ampere_h16816cudnn_256x128_ldg8_relu_exp_large_nhwc_tn_v1 Tactic: 0xdc796d70e228a1d4
[08/10/2023-11:16:56] [V] [TRT] Tactic: 0xdc796d70e228a1d4 Time: 1.10596
[08/10/2023-11:16:56] [V] [TRT] Conv_15 + Relu_16 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x32x32_stage4_warpsize4x1x1_g1_tensor16x8x16 Tactic: 0xdce100b9fe609424
[08/10/2023-11:16:56] [V] [TRT] Tactic: 0xdce100b9fe609424 Time: 1.53297
[08/10/2023-11:16:56] [V] [TRT] Conv_15 + Relu_16 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x128x32_stage4_warpsize2x2x1_g1_tensor16x8x16 Tactic: 0xdfa020ef435ef810
[08/10/2023-11:16:56] [V] [TRT] Tactic: 0xdfa020ef435ef810 Time: 0.828736
[08/10/2023-11:16:56] [V] [TRT] Conv_15 + Relu_16 Set Tactic Name: ampere_h1688cudnn_256x64_ldg8_relu_exp_large_nhwc_tn_v1 Tactic: 0xe0e3c0e8cf9a2d9e
[08/10/2023-11:16:56] [V] [TRT] Tactic: 0xe0e3c0e8cf9a2d9e Time: 1.48148
[08/10/2023-11:16:56] [V] [TRT] Conv_15 + Relu_16 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x32x64_stage5_warpsize2x2x1_g1_tensor16x8x16 Tactic: 0xe1ff5ad20f5c6bf6
[08/10/2023-11:16:56] [V] [TRT] Tactic: 0xe1ff5ad20f5c6bf6 Time: 2.62645
[08/10/2023-11:16:56] [V] [TRT] Conv_15 + Relu_16 Set Tactic Name: ampere_h16816cudnn_256x64_ldg8_relu_exp_medium_nhwc_tn_v1 Tactic: 0xe4711898bd599c36
[08/10/2023-11:16:56] [V] [TRT] Tactic: 0xe4711898bd599c36 Time: 1.40846
[08/10/2023-11:16:56] [V] [TRT] Conv_15 + Relu_16 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x64x64_stage3_warpsize4x1x1_g1_tensor16x8x16 Tactic: 0xeb25062ffb9eae45
[08/10/2023-11:16:56] [V] [TRT] Tactic: 0xeb25062ffb9eae45 Time: 1.51466
[08/10/2023-11:16:56] [V] [TRT] Conv_15 + Relu_16 Set Tactic Name: ampere_h16816cudnn_128x64_sliced1x2_ldg8_relu_exp_stages_64x4_large_nhwc_tn_v1 Tactic: 0xeb2d2aa4e56bb41c
[08/10/2023-11:16:56] [V] [TRT] Tactic: 0xeb2d2aa4e56bb41c Time: 1.67911
[08/10/2023-11:16:56] [V] [TRT] Conv_15 + Relu_16 Set Tactic Name: ampere_h16816cudnn_64x64_ldg8_relu_exp_stages_64x6_small_nhwc_tn_v1 Tactic: 0xf0beb09df9a19f82
[08/10/2023-11:16:56] [V] [TRT] Tactic: 0xf0beb09df9a19f82 Time: 2.54956
[08/10/2023-11:16:56] [V] [TRT] Conv_15 + Relu_16 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x64x64_stage4_warpsize2x2x1_g1_tensor16x8x16 Tactic: 0xf35e0311fa1cc516
[08/10/2023-11:16:56] [V] [TRT] Tactic: 0xf35e0311fa1cc516 Time: 1.82325
[08/10/2023-11:16:56] [V] [TRT] Conv_15 + Relu_16 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x32x32_stage4_warpsize4x1x1_g1_tensor16x8x16 Tactic: 0xf79479a62ea9f901
[08/10/2023-11:16:56] [V] [TRT] Tactic: 0xf79479a62ea9f901 Time: 2.29044
[08/10/2023-11:16:56] [V] [TRT] Fastest Tactic: 0xdfa020ef435ef810 Time: 0.828736
[08/10/2023-11:16:56] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0xdfa020ef435ef810
[08/10/2023-11:16:56] [V] [TRT] *************** Autotuning format combination: Half(518400,1:16,2880,16) -> Half(259200,1:16,1440,8) ***************
[08/10/2023-11:16:56] [W] [TRT] Weights [name=Conv_15 + Relu_16.weight] had the following issues when converted to FP16:
[08/10/2023-11:16:56] [W] [TRT]  - Subnormal FP16 values detected. 
[08/10/2023-11:16:56] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.
[08/10/2023-11:16:56] [V] [TRT] --------------- Timing Runner: Conv_15 + Relu_16 (CaskConvolution)
[08/10/2023-11:16:56] [V] [TRT] Conv_15 + Relu_16 Set Tactic Name: ampere_h16816cudnn_256x128_ldg8_relu_exp_medium_nhwc_tn_v1 Tactic: 0x03896956a39a1203
[08/10/2023-11:16:56] [V] [TRT] Tactic: 0x03896956a39a1203 Time: 1.69327
[08/10/2023-11:16:56] [V] [TRT] Conv_15 + Relu_16 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x128x32_stage3_warpsize4x2x1_g1_tensor16x8x16_t1r3s3 Tactic: 0x048d6d0400f33439
[08/10/2023-11:16:56] [V] [TRT] Tactic: 0x048d6d0400f33439 Time: 1.1609
[08/10/2023-11:16:56] [V] [TRT] Conv_15 + Relu_16 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x64x64_stage3_warpsize2x2x1_g1_tensor16x8x16_t1r3s3 Tactic: 0x05b6220f243edacd
[08/10/2023-11:16:56] [V] [TRT] Tactic: 0x05b6220f243edacd Time: 1.6144
[08/10/2023-11:16:56] [V] [TRT] Conv_15 + Relu_16 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x64x32_stage3_warpsize4x1x1_g1_tensor16x8x16_t1r3s3_aACCESS Tactic: 0x0866ddee325d07a6
[08/10/2023-11:16:56] [V] [TRT] Tactic: 0x0866ddee325d07a6 Time: 1.23717
[08/10/2023-11:16:56] [V] [TRT] Conv_15 + Relu_16 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x128x32_stage4_warpsize2x2x1_g1_tensor16x8x16 Tactic: 0x0e07ff4f4f7c1ac9
[08/10/2023-11:16:57] [V] [TRT] Tactic: 0x0e07ff4f4f7c1ac9 Time: 1.23446
[08/10/2023-11:16:57] [V] [TRT] Conv_15 + Relu_16 Set Tactic Name: ampere_h16816cudnn_256x128_ldg8_relu_exp_stages_64x3_small_nhwc_tn_v1 Tactic: 0x0e199750c30c6928
[08/10/2023-11:16:57] [V] [TRT] Tactic: 0x0e199750c30c6928 Time: 1.14779
[08/10/2023-11:16:57] [V] [TRT] Conv_15 + Relu_16 Set Tactic Name: ampere_h1688cudnn_128x128_ldg8_relu_exp_medium_nhwc_tn_v1 Tactic: 0x100c1ad308e08d35
[08/10/2023-11:16:57] [V] [TRT] Tactic: 0x100c1ad308e08d35 Time: 1.38006
[08/10/2023-11:16:57] [V] [TRT] Conv_15 + Relu_16 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x128x32_stage5_warpsize2x2x1_g1_tensor16x8x16_t1r3s3 Tactic: 0x17ebf0c9f418f10a
[08/10/2023-11:16:57] [V] [TRT] Tactic: 0x17ebf0c9f418f10a Time: 1.24739
[08/10/2023-11:16:57] [V] [TRT] Conv_15 + Relu_16 Set Tactic Name: ampere_h16816cudnn_128x64_ldg8_relu_exp_stages_64x4_medium_nhwc_tn_v1 Tactic: 0x196cbc423a9bb69e
[08/10/2023-11:16:57] [V] [TRT] Tactic: 0x196cbc423a9bb69e Time: 1.38229
[08/10/2023-11:16:57] [V] [TRT] Conv_15 + Relu_16 Set Tactic Name: ampere_h16816cudnn_128x64_sliced1x2_ldg8_relu_exp_stages_64x3_small_nhwc_tn_v1 Tactic: 0x19822de884f42a6a
[08/10/2023-11:16:57] [V] [TRT] Tactic: 0x19822de884f42a6a Time: 1.00357
[08/10/2023-11:16:57] [V] [TRT] Conv_15 + Relu_16 Set Tactic Name: ampere_h16816cudnn_64x64_sliced1x2_ldg8_relu_exp_stages_64x5_small_nhwc_tn_v1 Tactic: 0x1a5ba808ad4cc5a8
[08/10/2023-11:16:57] [V] [TRT] Tactic: 0x1a5ba808ad4cc5a8 Time: 1.54495
[08/10/2023-11:16:57] [V] [TRT] Conv_15 + Relu_16 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x64x64_stage3_warpsize2x2x1_g1_tensor16x8x16 Tactic: 0x21b295c0c8f6c95a
[08/10/2023-11:16:57] [V] [TRT] Tactic: 0x21b295c0c8f6c95a Time: 1.25925
[08/10/2023-11:16:57] [V] [TRT] Conv_15 + Relu_16 Set Tactic Name: ampere_h16816cudnn_128x128_ldg8_relu_exp_medium_nhwc_tn_v1 Tactic: 0x234580e8a194335c
[08/10/2023-11:16:57] [V] [TRT] Tactic: 0x234580e8a194335c Time: 0.882533
[08/10/2023-11:16:57] [V] [TRT] Conv_15 + Relu_16 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x128x32_stage4_warpsize2x2x1_g1_tensor16x8x16_t1r3s3_aACCESS Tactic: 0x245530c34bd6090f
[08/10/2023-11:16:57] [V] [TRT] Tactic: 0x245530c34bd6090f Time: 0.839374
[08/10/2023-11:16:57] [V] [TRT] Conv_15 + Relu_16 Set Tactic Name: ampere_h16816cudnn_64x64_sliced1x2_ldg8_relu_exp_stages_64x6_small_nhwc_tn_v1 Tactic: 0x24e01e7405cfdfe9
[08/10/2023-11:16:57] [V] [TRT] Tactic: 0x24e01e7405cfdfe9 Time: 1.49357
[08/10/2023-11:16:57] [V] [TRT] Conv_15 + Relu_16 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x64x64_stage3_warpsize2x2x1_g1_tensor16x8x16_t1r3s3_aACCESS Tactic: 0x263a38afd75e3a43
[08/10/2023-11:16:57] [V] [TRT] Tactic: 0x263a38afd75e3a43 Time: 0.964946
[08/10/2023-11:16:57] [V] [TRT] Conv_15 + Relu_16 Set Tactic Name: ampere_h16816cudnn_64x128_sliced1x2_ldg8_relu_exp_stages_64x4_large_nhwc_tn_v1 Tactic: 0x2721a7f18c2700db
[08/10/2023-11:16:57] [V] [TRT] Tactic: 0x2721a7f18c2700db Time: 1.16203
[08/10/2023-11:16:57] [V] [TRT] Conv_15 + Relu_16 Set Tactic Name: ampere_h16816cudnn_128x64_sliced1x2_ldg8_relu_exp_stages_64x3_medium_nhwc_tn_v1 Tactic: 0x29329b5741ea05f2
[08/10/2023-11:16:57] [V] [TRT] Tactic: 0x29329b5741ea05f2 Time: 1.04127
[08/10/2023-11:16:57] [V] [TRT] Conv_15 + Relu_16 Set Tactic Name: ampere_h16816cudnn_128x128_ldg8_relu_exp_large_nhwc_tn_v1 Tactic: 0x2970ae65966d569d
[08/10/2023-11:16:57] [V] [TRT] Tactic: 0x2970ae65966d569d Time: 0.908366
[08/10/2023-11:16:57] [V] [TRT] Conv_15 + Relu_16 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x256x64_stage3_warpsize1x4x1_g1_tensor16x8x16 Tactic: 0x29dc14520e0e4eb6
[08/10/2023-11:16:57] [V] [TRT] Tactic: 0x29dc14520e0e4eb6 Time: 1.79652
[08/10/2023-11:16:57] [V] [TRT] Conv_15 + Relu_16 Set Tactic Name: ampere_h16816cudnn_256x64_ldg8_relu_exp_stages_64x3_large_nhwc_tn_v1 Tactic: 0x2cb4a662694e89d3
[08/10/2023-11:16:57] [V] [TRT] Tactic: 0x2cb4a662694e89d3 Time: 0.974619
[08/10/2023-11:16:57] [V] [TRT] Conv_15 + Relu_16 Set Tactic Name: ampere_h16816cudnn_128x128_ldg8_relu_exp_stages_64x3_medium_nhwc_tn_v1 Tactic: 0x2eaa2202de9404d6
[08/10/2023-11:16:57] [V] [TRT] Tactic: 0x2eaa2202de9404d6 Time: 0.959927
[08/10/2023-11:16:57] [V] [TRT] Conv_15 + Relu_16 Set Tactic Name: ampere_h1688cudnn_256x128_ldg8_relu_exp_medium_nhwc_tn_v1 Tactic: 0x30c0f36d0aeeac6a
[08/10/2023-11:16:57] [V] [TRT] Tactic: 0x30c0f36d0aeeac6a Time: 1.20889
[08/10/2023-11:16:57] [V] [TRT] Conv_15 + Relu_16 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x64x64_stage4_warpsize2x2x1_g1_tensor16x8x16_t1r3s3_aACCESS Tactic: 0x30e8a8d7a953e5e9
[08/10/2023-11:16:57] [V] [TRT] Tactic: 0x30e8a8d7a953e5e9 Time: 1.152
[08/10/2023-11:16:57] [V] [TRT] Conv_15 + Relu_16 Set Tactic Name: ampere_h16816cudnn_128x64_ldg8_relu_exp_stages_64x4_large_nhwc_tn_v1 Tactic: 0x3197d5044d71e98e
[08/10/2023-11:16:57] [V] [TRT] Tactic: 0x3197d5044d71e98e Time: 1.22865
[08/10/2023-11:16:57] [V] [TRT] Conv_15 + Relu_16 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x64x64_stage4_warpsize2x2x1_g1_tensor16x8x16_t1r3s3 Tactic: 0x31d93dc22d2af081
[08/10/2023-11:16:57] [V] [TRT] Tactic: 0x31d93dc22d2af081 Time: 1.5646
[08/10/2023-11:16:57] [V] [TRT] Conv_15 + Relu_16 Set Tactic Name: ampere_h1688cudnn_128x128_ldg8_relu_exp_small_nhwc_tn_v1 Tactic: 0x3369260c04f9ad73
[08/10/2023-11:16:57] [V] [TRT] Tactic: 0x3369260c04f9ad73 Time: 1.44498
[08/10/2023-11:16:57] [V] [TRT] Conv_15 + Relu_16 Set Tactic Name: ampere_h16816cudnn_64x128_ldg8_relu_exp_stages_64x4_small_nhwc_tn_v1 Tactic: 0x3b5fa0f2a8fc2410
[08/10/2023-11:16:57] [V] [TRT] Tactic: 0x3b5fa0f2a8fc2410 Time: 1.1332
[08/10/2023-11:16:57] [V] [TRT] Conv_15 + Relu_16 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x32x32_stage4_warpsize4x1x1_g1_tensor16x8x16_t1r3s3 Tactic: 0x3e7eb35b91b9fa63
[08/10/2023-11:16:57] [V] [TRT] Tactic: 0x3e7eb35b91b9fa63 Time: 1.6832
[08/10/2023-11:16:57] [V] [TRT] Conv_15 + Relu_16 Set Tactic Name: ampere_h16816cudnn_256x64_ldg8_relu_exp_stages_64x3_medium_nhwc_tn_v1 Tactic: 0x3f031df0e579d52c
[08/10/2023-11:16:57] [V] [TRT] Tactic: 0x3f031df0e579d52c Time: 0.97093
[08/10/2023-11:16:57] [V] [TRT] Conv_15 + Relu_16 Set Tactic Name: ampere_h16816cudnn_256x64_sliced1x2_ldg8_relu_exp_small_nhwc_tn_v1 Tactic: 0x42a5b8709d0039be
[08/10/2023-11:16:57] [V] [TRT] Tactic: 0x42a5b8709d0039be Time: 1.28446
[08/10/2023-11:16:57] [V] [TRT] Conv_15 + Relu_16 Set Tactic Name: ampere_h16816cudnn_256x128_ldg8_relu_exp_small_nhwc_tn_v1 Tactic: 0x463794ee4acffd1f
[08/10/2023-11:16:57] [V] [TRT] Tactic: 0x463794ee4acffd1f Time: 1.05922
[08/10/2023-11:16:57] [V] [TRT] Conv_15 + Relu_16 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x64x64_stage4_warpsize2x2x1_g1_tensor16x8x16 Tactic: 0x4a81ea1e51436a30
[08/10/2023-11:16:57] [V] [TRT] Tactic: 0x4a81ea1e51436a30 Time: 1.40441
[08/10/2023-11:16:57] [V] [TRT] Conv_15 + Relu_16 Set Tactic Name: ampere_h1688cudnn_256x64_sliced1x2_ldg8_relu_exp_small_nhwc_tn_v1 Tactic: 0x4aed1956bd10a795
[08/10/2023-11:16:57] [V] [TRT] Tactic: 0x4aed1956bd10a795 Time: 1.39209
[08/10/2023-11:16:57] [V] [TRT] Conv_15 + Relu_16 Set Tactic Name: ampere_h16816cudnn_64x128_sliced1x2_ldg8_relu_exp_stages_64x3_large_nhwc_tn_v1 Tactic: 0x4fc05923455fc266
[08/10/2023-11:16:57] [V] [TRT] Tactic: 0x4fc05923455fc266 Time: 1.05813
[08/10/2023-11:16:57] [V] [TRT] Conv_15 + Relu_16 Set Tactic Name: ampere_h16816cudnn_256x64_ldg8_relu_exp_large_nhwc_tn_v1 Tactic: 0x5013c38f55afa9ba
[08/10/2023-11:16:57] [V] [TRT] Tactic: 0x5013c38f55afa9ba Time: 1.06149
[08/10/2023-11:16:57] [V] [TRT] Conv_15 + Relu_16 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x64x32_stage3_warpsize4x1x1_g1_tensor16x8x16_t1r3s3 Tactic: 0x529f4431bdae94f5
[08/10/2023-11:16:57] [V] [TRT] Tactic: 0x529f4431bdae94f5 Time: 0.897682
[08/10/2023-11:16:57] [V] [TRT] Conv_15 + Relu_16 Set Tactic Name: ampere_h16816cudnn_64x128_ldg8_relu_exp_stages_64x3_small_nhwc_tn_v1 Tactic: 0x53be5e206184e6ad
[08/10/2023-11:16:57] [V] [TRT] Tactic: 0x53be5e206184e6ad Time: 1.09061
[08/10/2023-11:16:57] [V] [TRT] Conv_15 + Relu_16 Set Tactic Name: ampere_h16816cudnn_64x64_ldg8_relu_exp_stages_64x5_large_nhwc_tn_v1 Tactic: 0x544bff7ff9c5d908
[08/10/2023-11:16:57] [V] [TRT] Tactic: 0x544bff7ff9c5d908 Time: 1.449
[08/10/2023-11:16:57] [V] [TRT] Conv_15 + Relu_16 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x64x64_stage3_warpsize2x2x1_g1_tensor16x8x16 Tactic: 0x5568fd8a32f4a40f
[08/10/2023-11:16:57] [V] [TRT] Tactic: 0x5568fd8a32f4a40f Time: 1.05431
[08/10/2023-11:16:57] [V] [TRT] Conv_15 + Relu_16 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x128x64_stage3_warpsize2x2x1_g1_tensor16x8x16 Tactic: 0x57c9a5ff682354a6
[08/10/2023-11:16:57] [V] [TRT] Tactic: 0x57c9a5ff682354a6 Time: 0.913806
[08/10/2023-11:16:57] [V] [TRT] Conv_15 + Relu_16 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x32x64_stage5_warpsize2x2x1_g1_tensor16x8x16 Tactic: 0x5820b3dda403c4d0
[08/10/2023-11:16:57] [V] [TRT] Tactic: 0x5820b3dda403c4d0 Time: 2.0098
[08/10/2023-11:16:57] [V] [TRT] Conv_15 + Relu_16 Set Tactic Name: ampere_h16816cudnn_128x128_ldg8_relu_exp_stages_64x3_small_nhwc_tn_v1 Tactic: 0x58816bf2e9c36afb
[08/10/2023-11:16:57] [V] [TRT] Tactic: 0x58816bf2e9c36afb Time: 0.957733
[08/10/2023-11:16:57] [V] [TRT] Conv_15 + Relu_16 Set Tactic Name: ampere_h16816cudnn_128x64_ldg8_relu_exp_stages_64x3_large_nhwc_tn_v1 Tactic: 0x59762bd684092b33
[08/10/2023-11:16:57] [V] [TRT] Tactic: 0x59762bd684092b33 Time: 1.08117
[08/10/2023-11:16:57] [V] [TRT] Conv_15 + Relu_16 Set Tactic Name: ampere_h16816cudnn_128x64_sliced1x2_ldg8_relu_exp_stages_64x4_medium_nhwc_tn_v1 Tactic: 0x59c5c647b4c76593
[08/10/2023-11:16:57] [V] [TRT] Tactic: 0x59c5c647b4c76593 Time: 1.14663
[08/10/2023-11:16:57] [V] [TRT] Conv_15 + Relu_16 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x128x64_stage3_warpsize4x2x1_g1_tensor16x8x16 Tactic: 0x5a55274960724ba8
[08/10/2023-11:16:57] [V] [TRT] Tactic: 0x5a55274960724ba8 Time: 0.934761
[08/10/2023-11:16:57] [V] [TRT] Conv_15 + Relu_16 Set Tactic Name: ampere_h1688cudnn_256x128_ldg8_relu_exp_large_nhwc_tn_v1 Tactic: 0x5c2e1c87d85b06f1
[08/10/2023-11:16:57] [V] [TRT] Tactic: 0x5c2e1c87d85b06f1 Time: 1.23856
[08/10/2023-11:16:57] [V] [TRT] Conv_15 + Relu_16 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x256x64_stage3_warpsize1x4x1_g1_tensor16x8x16 Tactic: 0x5d067c18f40c23e3
[08/10/2023-11:16:57] [V] [TRT] Tactic: 0x5d067c18f40c23e3 Time: 1.87666
[08/10/2023-11:16:57] [V] [TRT] Conv_15 + Relu_16 Set Tactic Name: ampere_h16816cudnn_64x64_ldg8_relu_exp_stages_64x5_medium_nhwc_tn_v1 Tactic: 0x5f31c22ec167f384
[08/10/2023-11:16:57] [V] [TRT] Tactic: 0x5f31c22ec167f384 Time: 1.4469
[08/10/2023-11:16:57] [V] [TRT] Conv_15 + Relu_16 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x128x32_stage4_warpsize2x2x1_g1_tensor16x8x16_t1r3s3 Tactic: 0x60c3421152ef8e10
[08/10/2023-11:16:57] [V] [TRT] Tactic: 0x60c3421152ef8e10 Time: 0.833056
[08/10/2023-11:16:57] [V] [TRT] Conv_15 + Relu_16 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x128x32_stage3_warpsize4x2x1_g1_tensor16x8x16 Tactic: 0x60da8c7151d91e47
[08/10/2023-11:16:57] [V] [TRT] Tactic: 0x60da8c7151d91e47 Time: 0.960416
[08/10/2023-11:16:57] [V] [TRT] Conv_15 + Relu_16 Set Tactic Name: ampere_h1688cudnn_256x64_ldg8_relu_exp_medium_nhwc_tn_v1 Tactic: 0x6426696f872a3b13
[08/10/2023-11:16:57] [V] [TRT] Tactic: 0x6426696f872a3b13 Time: 1.43964
[08/10/2023-11:16:57] [V] [TRT] Conv_15 + Relu_16 Set Tactic Name: ampere_h16816cudnn_128x64_ldg8_relu_exp_stages_64x3_medium_nhwc_tn_v1 Tactic: 0x699be152cfb6d6ff
[08/10/2023-11:16:57] [V] [TRT] Tactic: 0x699be152cfb6d6ff Time: 1.08348
[08/10/2023-11:16:57] [V] [TRT] Conv_15 + Relu_16 Set Tactic Name: ampere_h16816cudnn_64x64_ldg8_relu_exp_stages_64x6_large_nhwc_tn_v1 Tactic: 0x6af049035146c349
[08/10/2023-11:16:57] [V] [TRT] Tactic: 0x6af049035146c349 Time: 1.73035
[08/10/2023-11:16:57] [V] [TRT] Conv_15 + Relu_16 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x256x32_stage3_warpsize2x4x1_g1_tensor16x8x16 Tactic: 0x7005d10718f6c22d
[08/10/2023-11:16:57] [V] [TRT] Tactic: 0x7005d10718f6c22d Time: 1.71243
[08/10/2023-11:16:57] [V] [TRT] Conv_15 + Relu_16 Set Tactic Name: ampere_h16816cudnn_64x64_sliced1x2_ldg8_relu_exp_stages_64x5_medium_nhwc_tn_v1 Tactic: 0x706f08da35c795a5
[08/10/2023-11:16:57] [V] [TRT] Tactic: 0x706f08da35c795a5 Time: 1.38085
[08/10/2023-11:16:57] [V] [TRT] Conv_15 + Relu_16 Set Tactic Name: ampere_h16816cudnn_128x64_sliced1x2_ldg8_relu_exp_stages_64x4_small_nhwc_tn_v1 Tactic: 0x7163d33a4d8ce8d7
[08/10/2023-11:16:57] [V] [TRT] Tactic: 0x7163d33a4d8ce8d7 Time: 1.10254
[08/10/2023-11:16:57] [V] [TRT] Conv_15 + Relu_16 Set Tactic Name: ampere_h1688cudnn_256x64_ldg8_relu_exp_small_nhwc_tn_v1 Tactic: 0x7aad3976677d7155
[08/10/2023-11:16:57] [V] [TRT] Tactic: 0x7aad3976677d7155 Time: 1.32357
[08/10/2023-11:16:57] [V] [TRT] Conv_15 + Relu_16 Set Tactic Name: ampere_h16816cudnn_64x64_sliced1x2_ldg8_relu_exp_stages_64x5_large_nhwc_tn_v1 Tactic: 0x8015519605ab9963
[08/10/2023-11:16:57] [V] [TRT] Tactic: 0x8015519605ab9963 Time: 1.37547
[08/10/2023-11:16:57] [V] [TRT] Conv_15 + Relu_16 Set Tactic Name: ampere_h16816cudnn_256x64_sliced1x2_ldg8_relu_exp_medium_nhwc_tn_v1 Tactic: 0x82f8a2214b0b4178
[08/10/2023-11:16:57] [V] [TRT] Tactic: 0x82f8a2214b0b4178 Time: 1.43769
[08/10/2023-11:16:57] [V] [TRT] Conv_15 + Relu_16 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x32x32_stage4_warpsize4x1x1_g1_tensor16x8x16 Tactic: 0x834e11ecd4ab9454
[08/10/2023-11:16:57] [V] [TRT] Tactic: 0x834e11ecd4ab9454 Time: 1.68459
[08/10/2023-11:16:57] [V] [TRT] Conv_15 + Relu_16 Set Tactic Name: ampere_h16816cudnn_128x64_sliced1x2_ldg8_relu_exp_stages_64x3_large_nhwc_tn_v1 Tactic: 0x83ccd4762c1376a1
[08/10/2023-11:16:57] [V] [TRT] Tactic: 0x83ccd4762c1376a1 Time: 1.04544
[08/10/2023-11:16:57] [V] [TRT] Conv_15 + Relu_16 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x64x64_stage3_warpsize4x1x1_g1_tensor16x8x16_t1r3s3 Tactic: 0x84ee897dd1f4d2aa
[08/10/2023-11:16:57] [V] [TRT] Tactic: 0x84ee897dd1f4d2aa Time: 1.02928
[08/10/2023-11:16:57] [V] [TRT] Conv_15 + Relu_16 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x128x64_stage3_warpsize2x2x1_g1_tensor16x8x16 Tactic: 0x866e7a5f6401b67f
[08/10/2023-11:16:57] [V] [TRT] Tactic: 0x866e7a5f6401b67f Time: 0.932667
[08/10/2023-11:16:57] [V] [TRT] Conv_15 + Relu_16 Set Tactic Name: ampere_h16816cudnn_64x64_ldg8_relu_exp_stages_64x6_medium_nhwc_tn_v1 Tactic: 0x88971eec55aba850
[08/10/2023-11:16:57] [V] [TRT] Tactic: 0x88971eec55aba850 Time: 1.7182
[08/10/2023-11:16:57] [V] [TRT] Conv_15 + Relu_16 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x128x64_stage3_warpsize4x2x1_g1_tensor16x8x16 Tactic: 0x8bf2f8e96c50a971
[08/10/2023-11:16:57] [V] [TRT] Tactic: 0x8bf2f8e96c50a971 Time: 1.21741
[08/10/2023-11:16:57] [V] [TRT] Conv_15 + Relu_16 Set Tactic Name: ampere_h16816cudnn_256x128_ldg8_relu_exp_stages_64x3_large_nhwc_tn_v1 Tactic: 0x94576ece6beb35e3
[08/10/2023-11:16:57] [V] [TRT] Tactic: 0x94576ece6beb35e3 Time: 0.935109
[08/10/2023-11:16:57] [V] [TRT] Conv_15 + Relu_16 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x64x64_stage3_warpsize4x1x1_g1_tensor16x8x16 Tactic: 0x9fff6e65019cc310
[08/10/2023-11:16:57] [V] [TRT] Tactic: 0x9fff6e65019cc310 Time: 1.01487
[08/10/2023-11:16:57] [V] [TRT] Conv_15 + Relu_16 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x32x64_stage5_warpsize2x2x1_g1_tensor16x8x16_t1r3s3 Tactic: 0xa033e20ae9f412b2
[08/10/2023-11:16:58] [V] [TRT] Tactic: 0xa033e20ae9f412b2 Time: 2.00155
[08/10/2023-11:16:58] [V] [TRT] Conv_15 + Relu_16 Set Tactic Name: ampere_h16816cudnn_64x128_ldg8_relu_exp_stages_64x4_large_nhwc_tn_v1 Tactic: 0xa111596c001b78db
[08/10/2023-11:16:58] [V] [TRT] Tactic: 0xa111596c001b78db Time: 1.15457
[08/10/2023-11:16:58] [V] [TRT] Conv_15 + Relu_16 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x256x32_stage3_warpsize2x4x1_g1_tensor16x8x16 Tactic: 0xa1a20ea714d420f4
[08/10/2023-11:16:58] [V] [TRT] Tactic: 0xa1a20ea714d420f4 Time: 1.67702
[08/10/2023-11:16:58] [V] [TRT] Conv_15 + Relu_16 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x32x32_stage4_warpsize4x1x1_g1_tensor16x8x16_t1r3s3 Tactic: 0xa40cb43c296a36a8
[08/10/2023-11:16:58] [V] [TRT] Tactic: 0xa40cb43c296a36a8 Time: 1.52496
[08/10/2023-11:16:58] [V] [TRT] Conv_15 + Relu_16 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x64x32_stage3_warpsize4x1x1_g1_tensor16x8x16 Tactic: 0xa570c55d303796ff
[08/10/2023-11:16:58] [V] [TRT] Tactic: 0xa570c55d303796ff Time: 0.94347
[08/10/2023-11:16:58] [V] [TRT] Conv_15 + Relu_16 Set Tactic Name: ampere_h16816cudnn_64x64_sliced1x2_ldg8_relu_exp_stages_64x6_medium_nhwc_tn_v1 Tactic: 0xa7c9d418a10bce71
[08/10/2023-11:16:58] [V] [TRT] Tactic: 0xa7c9d418a10bce71 Time: 1.64061
[08/10/2023-11:16:58] [V] [TRT] Conv_15 + Relu_16 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x32x32_stage4_warpsize4x1x1_g1_tensor16x8x16 Tactic: 0xa83b68f30462f971
[08/10/2023-11:16:58] [V] [TRT] Tactic: 0xa83b68f30462f971 Time: 1.58318
[08/10/2023-11:16:58] [V] [TRT] Conv_15 + Relu_16 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x256x32_stage3_warpsize2x4x1_g1_tensor16x8x16_t1r3s3 Tactic: 0xa9177bbe4e767df8
[08/10/2023-11:16:58] [V] [TRT] Tactic: 0xa9177bbe4e767df8 Time: 1.72301
[08/10/2023-11:16:58] [V] [TRT] Conv_15 + Relu_16 Set Tactic Name: ampere_h1688cudnn_128x128_ldg8_relu_exp_large_nhwc_tn_v1 Tactic: 0xa927df92ac1ef1b8
[08/10/2023-11:16:58] [V] [TRT] Tactic: 0xa927df92ac1ef1b8 Time: 1.19216
[08/10/2023-11:16:58] [V] [TRT] Conv_15 + Relu_16 Set Tactic Name: ampere_h16816cudnn_128x64_ldg8_relu_exp_stages_64x4_small_nhwc_tn_v1 Tactic: 0xabd92c9ae596b545
[08/10/2023-11:16:58] [V] [TRT] Tactic: 0xabd92c9ae596b545 Time: 1.18833
[08/10/2023-11:16:58] [V] [TRT] Conv_15 + Relu_16 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x128x32_stage5_warpsize2x2x1_g1_tensor16x8x16 Tactic: 0xafd1e8bf6bd3d638
[08/10/2023-11:16:58] [V] [TRT] Tactic: 0xafd1e8bf6bd3d638 Time: 1.05509
[08/10/2023-11:16:58] [V] [TRT] Conv_15 + Relu_16 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x128x32_stage3_warpsize4x2x1_g1_tensor16x8x16 Tactic: 0xb17d53d15dfbfc9e
[08/10/2023-11:16:58] [V] [TRT] Tactic: 0xb17d53d15dfbfc9e Time: 1.00113
[08/10/2023-11:16:58] [V] [TRT] Conv_15 + Relu_16 Set Tactic Name: ampere_h16816cudnn_128x128_ldg8_relu_exp_small_nhwc_tn_v1 Tactic: 0xb33e57fb3e8a0a56
[08/10/2023-11:16:58] [V] [TRT] Tactic: 0xb33e57fb3e8a0a56 Time: 0.871858
[08/10/2023-11:16:58] [V] [TRT] Conv_15 + Relu_16 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x64x32_stage5_warpsize2x2x1_g1_tensor16x8x16_t1r3s3 Tactic: 0xb4bec086187edcfc
[08/10/2023-11:16:58] [V] [TRT] Tactic: 0xb4bec086187edcfc Time: 1.05665
[08/10/2023-11:16:58] [V] [TRT] Conv_15 + Relu_16 Set Tactic Name: ampere_h16816cudnn_256x64_ldg8_relu_exp_stages_64x3_small_nhwc_tn_v1 Tactic: 0xb6fa5ffcc1a9d518
[08/10/2023-11:16:58] [V] [TRT] Tactic: 0xb6fa5ffcc1a9d518 Time: 0.964224
[08/10/2023-11:16:58] [V] [TRT] Conv_15 + Relu_16 Set Tactic Name: ampere_h16816cudnn_64x128_sliced1x2_ldg8_relu_exp_stages_64x3_medium_nhwc_tn_v1 Tactic: 0xb85e52e87caf60a2
[08/10/2023-11:16:58] [V] [TRT] Tactic: 0xb85e52e87caf60a2 Time: 1.02706
[08/10/2023-11:16:58] [V] [TRT] Conv_15 + Relu_16 Set Tactic Name: ampere_h16816cudnn_64x128_ldg8_relu_exp_stages_64x3_medium_nhwc_tn_v1 Tactic: 0xb8d86216e1235cda
[08/10/2023-11:16:58] [V] [TRT] Tactic: 0xb8d86216e1235cda Time: 1.08603
[08/10/2023-11:16:58] [V] [TRT] Conv_15 + Relu_16 Set Tactic Name: ampere_h16816cudnn_256x128_ldg8_relu_exp_stages_64x3_medium_nhwc_tn_v1 Tactic: 0xb9171d70ba2eda4b
[08/10/2023-11:16:58] [V] [TRT] Tactic: 0xb9171d70ba2eda4b Time: 0.926853
[08/10/2023-11:16:58] [V] [TRT] Conv_15 + Relu_16 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x64x32_stage5_warpsize2x2x1_g1_tensor16x8x16 Tactic: 0xbd08239a9317f2fd
[08/10/2023-11:16:58] [V] [TRT] Tactic: 0xbd08239a9317f2fd Time: 1.07317
[08/10/2023-11:16:58] [V] [TRT] Conv_15 + Relu_16 Set Tactic Name: ampere_h16816cudnn_64x128_sliced1x2_ldg8_relu_exp_stages_64x4_small_nhwc_tn_v1 Tactic: 0xbd6f5e6f24c05c10
[08/10/2023-11:16:58] [V] [TRT] Tactic: 0xbd6f5e6f24c05c10 Time: 1.09936
[08/10/2023-11:16:58] [V] [TRT] Conv_15 + Relu_16 Set Tactic Name: ampere_h16816cudnn_64x64_sliced1x2_ldg8_relu_exp_stages_64x6_large_nhwc_tn_v1 Tactic: 0xbeaee7eaad288322
[08/10/2023-11:16:58] [V] [TRT] Tactic: 0xbeaee7eaad288322 Time: 1.63013
[08/10/2023-11:16:58] [V] [TRT] Conv_15 + Relu_16 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x128x64_stage3_warpsize2x2x1_g1_tensor16x8x16_t1r3s3 Tactic: 0xc0a02dc6095497cc
[08/10/2023-11:16:58] [V] [TRT] Tactic: 0xc0a02dc6095497cc Time: 0.901125
[08/10/2023-11:16:58] [V] [TRT] Conv_15 + Relu_16 Set Tactic Name: ampere_h16816cudnn_128x128_ldg8_relu_exp_stages_64x3_large_nhwc_tn_v1 Tactic: 0xc2cf926c41243630
[08/10/2023-11:16:58] [V] [TRT] Tactic: 0xc2cf926c41243630 Time: 0.955493
[08/10/2023-11:16:58] [V] [TRT] Conv_15 + Relu_16 Set Tactic Name: ampere_h16816cudnn_128x64_ldg8_relu_exp_stages_64x3_small_nhwc_tn_v1 Tactic: 0xc338d2482cee77f8
[08/10/2023-11:16:58] [V] [TRT] Tactic: 0xc338d2482cee77f8 Time: 1.0863
[08/10/2023-11:16:58] [V] [TRT] Conv_15 + Relu_16 Set Tactic Name: ampere_h1688cudnn_256x128_ldg8_relu_exp_small_nhwc_tn_v1 Tactic: 0xc660e51970bc5a3a
[08/10/2023-11:16:58] [V] [TRT] Tactic: 0xc660e51970bc5a3a Time: 1.14299
[08/10/2023-11:16:58] [V] [TRT] Conv_15 + Relu_16 Set Tactic Name: ampere_h16816cudnn_64x128_ldg8_relu_exp_stages_64x4_medium_nhwc_tn_v1 Tactic: 0xc82f3f06140e3cbb
[08/10/2023-11:16:58] [V] [TRT] Tactic: 0xc82f3f06140e3cbb Time: 1.165
[08/10/2023-11:16:58] [V] [TRT] Conv_15 + Relu_16 Set Tactic Name: ampere_h16816cudnn_64x128_sliced1x2_ldg8_relu_exp_stages_64x4_medium_nhwc_tn_v1 Tactic: 0xc8a90ff8898200c3
[08/10/2023-11:16:58] [V] [TRT] Tactic: 0xc8a90ff8898200c3 Time: 1.116
[08/10/2023-11:16:58] [V] [TRT] Conv_15 + Relu_16 Set Tactic Name: ampere_h1688cudnn_256x64_sliced1x2_ldg8_relu_exp_medium_nhwc_tn_v1 Tactic: 0xc9cc55109bb4de26
[08/10/2023-11:16:58] [V] [TRT] Tactic: 0xc9cc55109bb4de26 Time: 1.46859
[08/10/2023-11:16:58] [V] [TRT] Conv_15 + Relu_16 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x64x32_stage5_warpsize2x2x1_g1_tensor16x8x16 Tactic: 0xc9d24bd069159fa8
[08/10/2023-11:16:58] [V] [TRT] Tactic: 0xc9d24bd069159fa8 Time: 1.0722
[08/10/2023-11:16:58] [V] [TRT] Conv_15 + Relu_16 Set Tactic Name: ampere_h16816cudnn_64x128_ldg8_relu_exp_stages_64x3_large_nhwc_tn_v1 Tactic: 0xc9f0a7bec963ba66
[08/10/2023-11:16:58] [V] [TRT] Tactic: 0xc9f0a7bec963ba66 Time: 1.08932
[08/10/2023-11:16:58] [V] [TRT] Conv_15 + Relu_16 Set Tactic Name: ampere_h16816cudnn_256x64_ldg8_relu_exp_small_nhwc_tn_v1 Tactic: 0xca5d3a11fd48f571
[08/10/2023-11:16:58] [V] [TRT] Tactic: 0xca5d3a11fd48f571 Time: 1.02335
[08/10/2023-11:16:58] [V] [TRT] Conv_15 + Relu_16 Set Tactic Name: ampere_h16816cudnn_64x64_ldg8_relu_exp_stages_64x5_small_nhwc_tn_v1 Tactic: 0xce0506e1512285c3
[08/10/2023-11:16:58] [V] [TRT] Tactic: 0xce0506e1512285c3 Time: 1.43983
[08/10/2023-11:16:58] [V] [TRT] Conv_15 + Relu_16 Set Tactic Name: ampere_h1688cudnn_256x64_sliced1x2_ldg8_relu_exp_large_nhwc_tn_v1 Tactic: 0xd0a3e0c815f7fb5e
[08/10/2023-11:16:58] [V] [TRT] Tactic: 0xd0a3e0c815f7fb5e Time: 1.47467
[08/10/2023-11:16:58] [V] [TRT] Conv_15 + Relu_16 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x64x32_stage3_warpsize4x1x1_g1_tensor16x8x16 Tactic: 0xd1aaad17ca35fbaa
[08/10/2023-11:16:58] [V] [TRT] Tactic: 0xd1aaad17ca35fbaa Time: 0.894834
[08/10/2023-11:16:58] [V] [TRT] Conv_15 + Relu_16 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x128x64_stage3_warpsize4x2x1_g1_tensor16x8x16_t1r3s3 Tactic: 0xd2fca0486ca97266
[08/10/2023-11:16:58] [V] [TRT] Tactic: 0xd2fca0486ca97266 Time: 0.914139
[08/10/2023-11:16:58] [V] [TRT] Conv_15 + Relu_16 Set Tactic Name: ampere_h16816cudnn_64x128_sliced1x2_ldg8_relu_exp_stages_64x3_small_nhwc_tn_v1 Tactic: 0xd58ea0bdedb89ead
[08/10/2023-11:16:58] [V] [TRT] Tactic: 0xd58ea0bdedb89ead Time: 1.02543
[08/10/2023-11:16:58] [V] [TRT] Conv_15 + Relu_16 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x256x64_stage3_warpsize1x4x1_g1_tensor16x8x16_t1r3s3 Tactic: 0xd7c261fa01db2af9
[08/10/2023-11:16:58] [V] [TRT] Tactic: 0xd7c261fa01db2af9 Time: 1.85998
[08/10/2023-11:16:58] [V] [TRT] Conv_15 + Relu_16 Set Tactic Name: ampere_h16816cudnn_256x64_sliced1x2_ldg8_relu_exp_large_nhwc_tn_v1 Tactic: 0xd8eb41ee35e76575
[08/10/2023-11:16:58] [V] [TRT] Tactic: 0xd8eb41ee35e76575 Time: 1.76203
[08/10/2023-11:16:58] [V] [TRT] Conv_15 + Relu_16 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x128x32_stage5_warpsize2x2x1_g1_tensor16x8x16 Tactic: 0xdb0b80f591d1bb6d
[08/10/2023-11:16:58] [V] [TRT] Tactic: 0xdb0b80f591d1bb6d Time: 1.07656
[08/10/2023-11:16:58] [V] [TRT] Conv_15 + Relu_16 Set Tactic Name: ampere_h16816cudnn_256x128_ldg8_relu_exp_large_nhwc_tn_v1 Tactic: 0xdc796d70e228a1d4
[08/10/2023-11:16:58] [V] [TRT] Tactic: 0xdc796d70e228a1d4 Time: 1.1084
[08/10/2023-11:16:58] [V] [TRT] Conv_15 + Relu_16 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x32x32_stage4_warpsize4x1x1_g1_tensor16x8x16 Tactic: 0xdce100b9fe609424
[08/10/2023-11:16:58] [V] [TRT] Tactic: 0xdce100b9fe609424 Time: 1.53656
[08/10/2023-11:16:58] [V] [TRT] Conv_15 + Relu_16 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x128x32_stage4_warpsize2x2x1_g1_tensor16x8x16 Tactic: 0xdfa020ef435ef810
[08/10/2023-11:16:58] [V] [TRT] Tactic: 0xdfa020ef435ef810 Time: 0.831598
[08/10/2023-11:16:58] [V] [TRT] Conv_15 + Relu_16 Set Tactic Name: ampere_h1688cudnn_256x64_ldg8_relu_exp_large_nhwc_tn_v1 Tactic: 0xe0e3c0e8cf9a2d9e
[08/10/2023-11:16:58] [V] [TRT] Tactic: 0xe0e3c0e8cf9a2d9e Time: 1.44413
[08/10/2023-11:16:58] [V] [TRT] Conv_15 + Relu_16 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x32x64_stage5_warpsize2x2x1_g1_tensor16x8x16 Tactic: 0xe1ff5ad20f5c6bf6
[08/10/2023-11:16:58] [V] [TRT] Tactic: 0xe1ff5ad20f5c6bf6 Time: 2.00733
[08/10/2023-11:16:58] [V] [TRT] Conv_15 + Relu_16 Set Tactic Name: ampere_h16816cudnn_256x64_ldg8_relu_exp_medium_nhwc_tn_v1 Tactic: 0xe4711898bd599c36
[08/10/2023-11:16:58] [V] [TRT] Tactic: 0xe4711898bd599c36 Time: 1.04784
[08/10/2023-11:16:58] [V] [TRT] Conv_15 + Relu_16 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x64x64_stage3_warpsize4x1x1_g1_tensor16x8x16 Tactic: 0xeb25062ffb9eae45
[08/10/2023-11:16:58] [V] [TRT] Tactic: 0xeb25062ffb9eae45 Time: 1.01013
[08/10/2023-11:16:58] [V] [TRT] Conv_15 + Relu_16 Set Tactic Name: ampere_h16816cudnn_128x64_sliced1x2_ldg8_relu_exp_stages_64x4_large_nhwc_tn_v1 Tactic: 0xeb2d2aa4e56bb41c
[08/10/2023-11:16:58] [V] [TRT] Tactic: 0xeb2d2aa4e56bb41c Time: 1.19526
[08/10/2023-11:16:58] [V] [TRT] Conv_15 + Relu_16 Set Tactic Name: ampere_h16816cudnn_64x64_ldg8_relu_exp_stages_64x6_small_nhwc_tn_v1 Tactic: 0xf0beb09df9a19f82
[08/10/2023-11:16:58] [V] [TRT] Tactic: 0xf0beb09df9a19f82 Time: 1.59184
[08/10/2023-11:16:58] [V] [TRT] Conv_15 + Relu_16 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x64x64_stage4_warpsize2x2x1_g1_tensor16x8x16 Tactic: 0xf35e0311fa1cc516
[08/10/2023-11:16:58] [V] [TRT] Tactic: 0xf35e0311fa1cc516 Time: 1.37051
[08/10/2023-11:16:58] [V] [TRT] Conv_15 + Relu_16 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x32x32_stage4_warpsize4x1x1_g1_tensor16x8x16 Tactic: 0xf79479a62ea9f901
[08/10/2023-11:16:58] [V] [TRT] Tactic: 0xf79479a62ea9f901 Time: 1.68951
[08/10/2023-11:16:58] [V] [TRT] Fastest Tactic: 0xdfa020ef435ef810 Time: 0.831598
[08/10/2023-11:16:58] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0xdfa020ef435ef810
[08/10/2023-11:16:58] [V] [TRT] =============== Computing costs for 
[08/10/2023-11:16:58] [V] [TRT] *************** Autotuning format combination: Float(4147200,32400,180,1) -> Float(4147200,32400,180,1) ***************
[08/10/2023-11:16:58] [V] [TRT] --------------- Timing Runner: Conv_17 + Relu_18 (CudaDepthwiseConvolution)
[08/10/2023-11:16:58] [V] [TRT] CudaDepthwiseConvolution has no valid tactics for this config, skipping
[08/10/2023-11:16:58] [V] [TRT] --------------- Timing Runner: Conv_17 + Relu_18 (FusedConvActConvolution)
[08/10/2023-11:16:58] [V] [TRT] FusedConvActConvolution has no valid tactics for this config, skipping
[08/10/2023-11:16:58] [V] [TRT] --------------- Timing Runner: Conv_17 + Relu_18 (CudnnConvolution)
[08/10/2023-11:16:58] [V] [TRT] Tactic: 0x0000000000000000 Time: 6.95558
[08/10/2023-11:16:58] [V] [TRT] Tactic: 0x0000000000000001 Time: 1.83614
[08/10/2023-11:16:58] [V] [TRT] Tactic: 0x0000000000000002 Time: 9.68033
[08/10/2023-11:16:58] [V] [TRT] Tactic: 0x0000000000000004 skipped. Scratch requested: 8725266432, available: 4294967296
[08/10/2023-11:16:59] [V] [TRT] Tactic: 0x0000000000000005 Time: 23.3266
[08/10/2023-11:16:59] [V] [TRT] Tactic: 0x0000000000000006 Time: 3.47691
[08/10/2023-11:16:59] [V] [TRT] Tactic: 0x0000000000000038 Time: 7.10283
[08/10/2023-11:16:59] [V] [TRT] Tactic: 0x0000000000000039 Time: 1.64775
[08/10/2023-11:16:59] [V] [TRT] Tactic: 0x000000000000003a Time: 9.51884
[08/10/2023-11:16:59] [V] [TRT] Tactic: 0x000000000000003c skipped. Scratch requested: 8725266432, available: 4294967296
[08/10/2023-11:16:59] [V] [TRT] Tactic: 0x000000000000003d Time: 16.4821
[08/10/2023-11:16:59] [V] [TRT] Tactic: 0x000000000000003e Time: 3.47576
[08/10/2023-11:16:59] [V] [TRT] Tactic: 0x0000000000000070 Time: 6.81208
[08/10/2023-11:16:59] [V] [TRT] Tactic: 0x0000000000000071 Time: 5.56262
[08/10/2023-11:16:59] [V] [TRT] Tactic: 0x0000000000000072 Time: 9
[08/10/2023-11:16:59] [V] [TRT] Tactic: 0x0000000000000074 skipped. Scratch requested: 8725266432, available: 4294967296
[08/10/2023-11:16:59] [V] [TRT] Tactic: 0x0000000000000075 Time: 16.5263
[08/10/2023-11:16:59] [V] [TRT] Tactic: 0x0000000000000076 Time: 3.46763
[08/10/2023-11:16:59] [V] [TRT] Fastest Tactic: 0x0000000000000039 Time: 1.64775
[08/10/2023-11:16:59] [V] [TRT] Setting workspace to 8725266432enables more tactics for profiling
[08/10/2023-11:16:59] [V] [TRT] --------------- Timing Runner: Conv_17 + Relu_18 (CaskConvolution)
[08/10/2023-11:16:59] [V] [TRT] Conv_17 + Relu_18 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x128x8_stage3_warpsize1x4x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0x01cf8ce2da913006
[08/10/2023-11:16:59] [V] [TRT] Tactic: 0x01cf8ce2da913006 Time: 5.16522
[08/10/2023-11:16:59] [V] [TRT] Conv_17 + Relu_18 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x64x8_stage3_warpsize2x2x1_g1_ffma_aligna4_alignc4 Tactic: 0x12dbf7d94ee3696d
[08/10/2023-11:17:00] [V] [TRT] Tactic: 0x12dbf7d94ee3696d Time: 5.68053
[08/10/2023-11:17:00] [V] [TRT] Conv_17 + Relu_18 Set Tactic Name: ampere_scudnn_128x64_relu_small_nn_v1 Tactic: 0x3f243c490d502deb
[08/10/2023-11:17:00] [V] [TRT] Tactic: 0x3f243c490d502deb Time: 5.48695
[08/10/2023-11:17:00] [V] [TRT] Conv_17 + Relu_18 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0x4727434768e46395
[08/10/2023-11:17:00] [V] [TRT] Tactic: 0x4727434768e46395 Time: 5.93419
[08/10/2023-11:17:00] [V] [TRT] Conv_17 + Relu_18 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize256x128x8_stage3_warpsize4x2x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0x4efce38acc876f5c
[08/10/2023-11:17:00] [V] [TRT] Tactic: 0x4efce38acc876f5c Time: 5.33958
[08/10/2023-11:17:00] [V] [TRT] Conv_17 + Relu_18 Set Tactic Name: ampere_scudnn_128x128_relu_small_nn_v1 Tactic: 0x503619c69ae500ff
[08/10/2023-11:17:00] [V] [TRT] Tactic: 0x503619c69ae500ff Time: 4.8805
[08/10/2023-11:17:00] [V] [TRT] Conv_17 + Relu_18 Set Tactic Name: ampere_scudnn_128x128_relu_xregs_large_nn_v1 Tactic: 0x5403ad713f811a18
[08/10/2023-11:17:00] [V] [TRT] Tactic: 0x5403ad713f811a18 Time: 5.03695
[08/10/2023-11:17:00] [V] [TRT] Conv_17 + Relu_18 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x128x8_stage3_warpsize1x4x1_g1_ffma_aligna4_alignc4 Tactic: 0x5aa723e0481da855
[08/10/2023-11:17:00] [V] [TRT] Tactic: 0x5aa723e0481da855 Time: 5.16565
[08/10/2023-11:17:00] [V] [TRT] Conv_17 + Relu_18 Set Tactic Name: ampere_scudnn_128x64_relu_xregs_large_nn_v1 Tactic: 0x5deb29b7a8e275f7
[08/10/2023-11:17:00] [V] [TRT] Tactic: 0x5deb29b7a8e275f7 Time: 5.35839
[08/10/2023-11:17:00] [V] [TRT] Conv_17 + Relu_18 Set Tactic Name: ampere_scudnn_winograd_128x128_ldg1_ldg4_relu_tile148t_nt_v1 Tactic: 0x94119b4c514b211a
[08/10/2023-11:17:00] [V] [TRT] Tactic: 0x94119b4c514b211a Time: 2.94673
[08/10/2023-11:17:00] [V] [TRT] Conv_17 + Relu_18 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_aligna4_alignc4 Tactic: 0xa31d27de74b895ff
[08/10/2023-11:17:00] [V] [TRT] Tactic: 0xa31d27de74b895ff Time: 5.92589
[08/10/2023-11:17:00] [V] [TRT] Conv_17 + Relu_18 Set Tactic Name: ampere_scudnn_128x32_relu_small_nn_v1 Tactic: 0xa8609adc4e0ceb90
[08/10/2023-11:17:00] [V] [TRT] Tactic: 0xa8609adc4e0ceb90 Time: 5.90184
[08/10/2023-11:17:00] [V] [TRT] Conv_17 + Relu_18 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize256x128x8_stage3_warpsize4x2x1_g1_ffma_aligna4_alignc4 Tactic: 0xbb8c3889c7eacd30
[08/10/2023-11:17:00] [V] [TRT] Tactic: 0xbb8c3889c7eacd30 Time: 5.40574
[08/10/2023-11:17:00] [V] [TRT] Conv_17 + Relu_18 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_aligna4_alignc4 Tactic: 0xd828f024626fa982
[08/10/2023-11:17:00] [V] [TRT] Tactic: 0xd828f024626fa982 Time: 8.24389
[08/10/2023-11:17:00] [V] [TRT] Conv_17 + Relu_18 Set Tactic Name: ampere_scudnn_128x128_relu_medium_nn_v1 Tactic: 0xf067e6205da31c2e
[08/10/2023-11:17:00] [V] [TRT] Tactic: 0xf067e6205da31c2e Time: 4.94758
[08/10/2023-11:17:00] [V] [TRT] Conv_17 + Relu_18 Set Tactic Name: ampere_scudnn_128x64_relu_medium_nn_v1 Tactic: 0xf64396b97c889179
[08/10/2023-11:17:00] [V] [TRT] Tactic: 0xf64396b97c889179 Time: 5.42243
[08/10/2023-11:17:00] [V] [TRT] Fastest Tactic: 0x94119b4c514b211a Time: 2.94673
[08/10/2023-11:17:00] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CudnnConvolution Tactic: 0x0000000000000039
[08/10/2023-11:17:00] [V] [TRT] *************** Autotuning format combination: Float(4147200,1,23040,128) -> Float(4147200,1,23040,128) ***************
[08/10/2023-11:17:00] [V] [TRT] --------------- Timing Runner: Conv_17 + Relu_18 (CaskConvolution)
[08/10/2023-11:17:00] [V] [TRT] Conv_17 + Relu_18 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize64x128x32_stage5_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0x112d7e4d280f396d
[08/10/2023-11:17:00] [V] [TRT] Tactic: 0x112d7e4d280f396d Time: 1.45312
[08/10/2023-11:17:00] [V] [TRT] Conv_17 + Relu_18 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x64x32_stage5_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0x13cc2ad8dcd32ba2
[08/10/2023-11:17:00] [V] [TRT] Tactic: 0x13cc2ad8dcd32ba2 Time: 1.52789
[08/10/2023-11:17:00] [V] [TRT] Conv_17 + Relu_18 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_aligna4_alignc4 Tactic: 0x19b688348f983aa0
[08/10/2023-11:17:00] [V] [TRT] Tactic: 0x19b688348f983aa0 Time: 5.51462
[08/10/2023-11:17:00] [V] [TRT] Conv_17 + Relu_18 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x64x8_stage3_warpsize2x2x1_g1_ffma_aligna4_alignc4 Tactic: 0x1da91d865428f237
[08/10/2023-11:17:00] [V] [TRT] Tactic: 0x1da91d865428f237 Time: 4.31019
[08/10/2023-11:17:00] [V] [TRT] Conv_17 + Relu_18 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize256x128x32_stage3_warpsize4x2x1_g1_tensor16x8x8_t1r3s3 Tactic: 0x21246c8544eff903
[08/10/2023-11:17:00] [V] [TRT] Tactic: 0x21246c8544eff903 Time: 0.991941
[08/10/2023-11:17:00] [V] [TRT] Conv_17 + Relu_18 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x128x32_stage4_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0x25b2b9d5c9d5ca0d
[08/10/2023-11:17:00] [V] [TRT] Tactic: 0x25b2b9d5c9d5ca0d Time: 0.992805
[08/10/2023-11:17:00] [V] [TRT] Conv_17 + Relu_18 Set Tactic Name: ampere_scudnn_128x64_sliced1x2_ldg4_relu_exp_small_nhwc_tn_v1 Tactic: 0x27b316f52c109002
[08/10/2023-11:17:00] [V] [TRT] Tactic: 0x27b316f52c109002 Time: 5.33145
[08/10/2023-11:17:00] [V] [TRT] Conv_17 + Relu_18 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage4_warpsize2x2x1_g1_tensor16x8x8_t1r3s3 Tactic: 0x2d80d2b384ab13f1
[08/10/2023-11:17:00] [V] [TRT] Tactic: 0x2d80d2b384ab13f1 Time: 1.63762
[08/10/2023-11:17:00] [V] [TRT] Conv_17 + Relu_18 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize256x128x32_stage3_warpsize4x2x1_g1_tensor16x8x8 Tactic: 0x36303f399d3ee4fd
[08/10/2023-11:17:00] [V] [TRT] Tactic: 0x36303f399d3ee4fd Time: 1.55017
[08/10/2023-11:17:00] [V] [TRT] Conv_17 + Relu_18 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage4_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0x385bfab9332b1413
[08/10/2023-11:17:00] [V] [TRT] Tactic: 0x385bfab9332b1413 Time: 1.96519
[08/10/2023-11:17:00] [V] [TRT] Conv_17 + Relu_18 Set Tactic Name: ampere_scudnn_128x32_sliced1x4_ldg4_relu_exp_medium_nhwc_tn_v1 Tactic: 0x3e191488237fab8f
[08/10/2023-11:17:01] [V] [TRT] Tactic: 0x3e191488237fab8f Time: 5.38074
[08/10/2023-11:17:01] [V] [TRT] Conv_17 + Relu_18 Set Tactic Name: ampere_scudnn_128x32_sliced1x4_ldg4_relu_exp_small_nhwc_tn_v1 Tactic: 0x3e2b881168d9689d
[08/10/2023-11:17:01] [V] [TRT] Tactic: 0x3e2b881168d9689d Time: 5.35814
[08/10/2023-11:17:01] [V] [TRT] Conv_17 + Relu_18 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x256x8_stage3_warpsize1x4x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0x3f0c846d6379bc98
[08/10/2023-11:17:01] [V] [TRT] Tactic: 0x3f0c846d6379bc98 Time: 8.39813
[08/10/2023-11:17:01] [V] [TRT] Conv_17 + Relu_18 Set Tactic Name: ampere_scudnn_128x64_sliced1x2_ldg4_relu_exp_medium_nhwc_tn_v1 Tactic: 0x412c44dfeaf9161d
[08/10/2023-11:17:01] [V] [TRT] Tactic: 0x412c44dfeaf9161d Time: 5.25313
[08/10/2023-11:17:01] [V] [TRT] Conv_17 + Relu_18 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x256x32_stage3_warpsize2x4x1_g1_tensor16x8x8 Tactic: 0x482b48242255b8ce
[08/10/2023-11:17:01] [V] [TRT] Tactic: 0x482b48242255b8ce Time: 1.82869
[08/10/2023-11:17:01] [V] [TRT] Conv_17 + Relu_18 Set Tactic Name: ampere_scudnn_128x128_relu_exp_small_nhwc_tn_v1 Tactic: 0x5030121339a48bf3
[08/10/2023-11:17:01] [V] [TRT] Tactic: 0x5030121339a48bf3 Time: 4.77788
[08/10/2023-11:17:01] [V] [TRT] Conv_17 + Relu_18 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x256x32_stage3_warpsize2x4x1_g1_tensor16x8x8_t1r3s3 Tactic: 0x614e89f7852edbc3
[08/10/2023-11:17:01] [V] [TRT] Tactic: 0x614e89f7852edbc3 Time: 1.75329
[08/10/2023-11:17:01] [V] [TRT] Conv_17 + Relu_18 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_aligna4_alignc4 Tactic: 0x62835fce994f06dd
[08/10/2023-11:17:01] [V] [TRT] Tactic: 0x62835fce994f06dd Time: 4.73551
[08/10/2023-11:17:01] [V] [TRT] Conv_17 + Relu_18 Set Tactic Name: ampere_scudnn_128x128_ldg4_relu_exp_large_nhwc_tn_v1 Tactic: 0x634e99502974e4da
[08/10/2023-11:17:01] [V] [TRT] Tactic: 0x634e99502974e4da Time: 4.79174
[08/10/2023-11:17:01] [V] [TRT] Conv_17 + Relu_18 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0x65e41d81f093b482
[08/10/2023-11:17:01] [V] [TRT] Tactic: 0x65e41d81f093b482 Time: 1.13321
[08/10/2023-11:17:01] [V] [TRT] Conv_17 + Relu_18 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize64x128x32_stage5_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0x65f71607d20d5438
[08/10/2023-11:17:01] [V] [TRT] Tactic: 0x65f71607d20d5438 Time: 1.29005
[08/10/2023-11:17:01] [V] [TRT] Conv_17 + Relu_18 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x64x32_stage5_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0x6716429226d146f7
[08/10/2023-11:17:01] [V] [TRT] Tactic: 0x6716429226d146f7 Time: 1.35319
[08/10/2023-11:17:01] [V] [TRT] Conv_17 + Relu_18 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize64x32x64_stage5_warpsize2x2x1_g1_tensor16x8x8_t1r3s3 Tactic: 0x679a281f7660c436
[08/10/2023-11:17:01] [V] [TRT] Tactic: 0x679a281f7660c436 Time: 2.51152
[08/10/2023-11:17:01] [V] [TRT] Conv_17 + Relu_18 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize64x32x64_stage5_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0x6e2a3c7a7fc5e4e1
[08/10/2023-11:17:01] [V] [TRT] Tactic: 0x6e2a3c7a7fc5e4e1 Time: 2.61658
[08/10/2023-11:17:01] [V] [TRT] Conv_17 + Relu_18 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x64x32_stage5_warpsize2x2x1_g1_tensor16x8x8_t1r3s3 Tactic: 0x7aa21282feb15f1a
[08/10/2023-11:17:01] [V] [TRT] Tactic: 0x7aa21282feb15f1a Time: 1.35861
[08/10/2023-11:17:01] [V] [TRT] Conv_17 + Relu_18 Set Tactic Name: ampere_scudnn_128x128_ldg4_relu_exp_medium_nhwc_tn_v1 Tactic: 0x7bc32c782b800c48
[08/10/2023-11:17:01] [V] [TRT] Tactic: 0x7bc32c782b800c48 Time: 4.66525
[08/10/2023-11:17:01] [V] [TRT] Conv_17 + Relu_18 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize256x64x8_stage3_warpsize2x2x1_g1_ffma_aligna4_alignc4 Tactic: 0x8014228ec08b4d49
[08/10/2023-11:17:01] [V] [TRT] Tactic: 0x8014228ec08b4d49 Time: 4.75258
[08/10/2023-11:17:01] [V] [TRT] Conv_17 + Relu_18 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage4_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0x818413b69874bb35
[08/10/2023-11:17:01] [V] [TRT] Tactic: 0x818413b69874bb35 Time: 1.7398
[08/10/2023-11:17:01] [V] [TRT] Conv_17 + Relu_18 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0x94a7db94ba744c45
[08/10/2023-11:17:01] [V] [TRT] Tactic: 0x94a7db94ba744c45 Time: 4.84223
[08/10/2023-11:17:01] [V] [TRT] Conv_17 + Relu_18 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x256x32_stage3_warpsize2x4x1_g1_tensor16x8x8 Tactic: 0x998c97842e775a17
[08/10/2023-11:17:01] [V] [TRT] Tactic: 0x998c97842e775a17 Time: 1.82866
[08/10/2023-11:17:01] [V] [TRT] Conv_17 + Relu_18 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8_t1r3s3 Tactic: 0x999e005e3b016ea6
[08/10/2023-11:17:01] [V] [TRT] Tactic: 0x999e005e3b016ea6 Time: 1.02089
[08/10/2023-11:17:01] [V] [TRT] Conv_17 + Relu_18 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize256x64x32_stage3_warpsize4x2x1_g1_tensor16x8x8_t1r3s3 Tactic: 0xa9a06d0633580c0c
[08/10/2023-11:17:01] [V] [TRT] Tactic: 0xa9a06d0633580c0c Time: 1.12727
[08/10/2023-11:17:01] [V] [TRT] Conv_17 + Relu_18 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0xb443c221fcb1565b
[08/10/2023-11:17:01] [V] [TRT] Tactic: 0xb443c221fcb1565b Time: 1.04764
[08/10/2023-11:17:01] [V] [TRT] Conv_17 + Relu_18 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage3_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0xb47d7d926de02dee
[08/10/2023-11:17:01] [V] [TRT] Tactic: 0xb47d7d926de02dee Time: 1.40953
[08/10/2023-11:17:01] [V] [TRT] Conv_17 + Relu_18 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize256x64x32_stage3_warpsize4x2x1_g1_tensor16x8x8 Tactic: 0xba0185279ccb2b85
[08/10/2023-11:17:01] [V] [TRT] Tactic: 0xba0185279ccb2b85 Time: 1.28244
[08/10/2023-11:17:01] [V] [TRT] Conv_17 + Relu_18 Set Tactic Name: ampere_scudnn_128x64_sliced1x2_ldg4_relu_exp_large_nhwc_tn_v1 Tactic: 0xbdfdef6b84f7ccc9
[08/10/2023-11:17:01] [V] [TRT] Tactic: 0xbdfdef6b84f7ccc9 Time: 5.17237
[08/10/2023-11:17:01] [V] [TRT] Conv_17 + Relu_18 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage3_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0xc0a715d897e240bb
[08/10/2023-11:17:01] [V] [TRT] Tactic: 0xc0a715d897e240bb Time: 1.37657
[08/10/2023-11:17:01] [V] [TRT] Conv_17 + Relu_18 Set Tactic Name: ampere_scudnn_128x128_relu_exp_large_nhwc_tn_v1 Tactic: 0xca7eeb8d9143d738
[08/10/2023-11:17:01] [V] [TRT] Tactic: 0xca7eeb8d9143d738 Time: 4.90332
[08/10/2023-11:17:01] [V] [TRT] Conv_17 + Relu_18 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize256x64x32_stage3_warpsize4x2x1_g1_tensor16x8x8 Tactic: 0xcedbed6d66c946d0
[08/10/2023-11:17:01] [V] [TRT] Tactic: 0xcedbed6d66c946d0 Time: 1.20197
[08/10/2023-11:17:01] [V] [TRT] Conv_17 + Relu_18 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x128x8_stage3_warpsize1x4x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0xd15dd11d64344e83
[08/10/2023-11:17:01] [V] [TRT] Tactic: 0xd15dd11d64344e83 Time: 4.32112
[08/10/2023-11:17:01] [V] [TRT] Conv_17 + Relu_18 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize64x128x32_stage5_warpsize2x2x1_g1_tensor16x8x8_t1r3s3 Tactic: 0xd4428a7355769d7d
[08/10/2023-11:17:01] [V] [TRT] Tactic: 0xd4428a7355769d7d Time: 1.23667
[08/10/2023-11:17:01] [V] [TRT] Conv_17 + Relu_18 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize64x32x64_stage5_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0xd7f5d575d49a4bc7
[08/10/2023-11:17:01] [V] [TRT] Tactic: 0xd7f5d575d49a4bc7 Time: 2.96659
[08/10/2023-11:17:01] [V] [TRT] Conv_17 + Relu_18 Set Tactic Name: ampere_scudnn_128x128_relu_exp_medium_nhwc_tn_v1 Tactic: 0xd9031472c05adf51
[08/10/2023-11:17:02] [V] [TRT] Tactic: 0xd9031472c05adf51 Time: 4.84287
[08/10/2023-11:17:02] [V] [TRT] Conv_17 + Relu_18 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x128x32_stage4_warpsize2x2x1_g1_tensor16x8x8_t1r3s3 Tactic: 0xd920b33c9bd27143
[08/10/2023-11:17:02] [V] [TRT] Tactic: 0xd920b33c9bd27143 Time: 0.962085
[08/10/2023-11:17:02] [V] [TRT] Conv_17 + Relu_18 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize256x128x32_stage3_warpsize4x2x1_g1_tensor16x8x8 Tactic: 0xe797e099911c0624
[08/10/2023-11:17:02] [V] [TRT] Tactic: 0xe797e099911c0624 Time: 1.04128
[08/10/2023-11:17:02] [V] [TRT] Conv_17 + Relu_18 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage3_warpsize2x2x1_g1_tensor16x8x8_t1r3s3 Tactic: 0xeb65d4c1e8fa2294
[08/10/2023-11:17:02] [V] [TRT] Tactic: 0xeb65d4c1e8fa2294 Time: 1.32436
[08/10/2023-11:17:02] [V] [TRT] Conv_17 + Relu_18 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x128x32_stage4_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0xf4156675c5f728d4
[08/10/2023-11:17:02] [V] [TRT] Tactic: 0xf4156675c5f728d4 Time: 1.49885
[08/10/2023-11:17:02] [V] [TRT] Conv_17 + Relu_18 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x64x8_stage3_warpsize2x2x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0xf48db81f02eca9ee
[08/10/2023-11:17:02] [V] [TRT] Tactic: 0xf48db81f02eca9ee Time: 4.4118
[08/10/2023-11:17:02] [V] [TRT] Conv_17 + Relu_18 Set Tactic Name: ampere_scudnn_128x128_ldg4_relu_exp_small_nhwc_tn_v1 Tactic: 0xf90060ce8193b811
[08/10/2023-11:17:02] [V] [TRT] Tactic: 0xf90060ce8193b811 Time: 5.12927
[08/10/2023-11:17:02] [V] [TRT] Fastest Tactic: 0xd920b33c9bd27143 Time: 0.962085
[08/10/2023-11:17:02] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0xd920b33c9bd27143
[08/10/2023-11:17:02] [V] [TRT] *************** Autotuning format combination: Float(1036800,1:4,5760,32) -> Float(1036800,1:4,5760,32) ***************
[08/10/2023-11:17:02] [V] [TRT] --------------- Timing Runner: Conv_17 + Relu_18 (CaskConvolution)
[08/10/2023-11:17:02] [V] [TRT] Conv_17 + Relu_18 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize64x128x32_stage5_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0x112d7e4d280f396d
[08/10/2023-11:17:02] [V] [TRT] Tactic: 0x112d7e4d280f396d Time: 1.44937
[08/10/2023-11:17:02] [V] [TRT] Conv_17 + Relu_18 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x64x32_stage5_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0x13cc2ad8dcd32ba2
[08/10/2023-11:17:02] [V] [TRT] Tactic: 0x13cc2ad8dcd32ba2 Time: 1.52353
[08/10/2023-11:17:02] [V] [TRT] Conv_17 + Relu_18 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize256x128x32_stage3_warpsize4x2x1_g1_tensor16x8x8_t1r3s3 Tactic: 0x21246c8544eff903
[08/10/2023-11:17:02] [V] [TRT] Tactic: 0x21246c8544eff903 Time: 0.991909
[08/10/2023-11:17:02] [V] [TRT] Conv_17 + Relu_18 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x128x32_stage4_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0x25b2b9d5c9d5ca0d
[08/10/2023-11:17:02] [V] [TRT] Tactic: 0x25b2b9d5c9d5ca0d Time: 1.00272
[08/10/2023-11:17:02] [V] [TRT] Conv_17 + Relu_18 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage4_warpsize2x2x1_g1_tensor16x8x8_t1r3s3 Tactic: 0x2d80d2b384ab13f1
[08/10/2023-11:17:02] [V] [TRT] Tactic: 0x2d80d2b384ab13f1 Time: 1.62316
[08/10/2023-11:17:02] [V] [TRT] Conv_17 + Relu_18 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize256x128x32_stage3_warpsize4x2x1_g1_tensor16x8x8 Tactic: 0x36303f399d3ee4fd
[08/10/2023-11:17:02] [V] [TRT] Tactic: 0x36303f399d3ee4fd Time: 1.21136
[08/10/2023-11:17:02] [V] [TRT] Conv_17 + Relu_18 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage4_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0x385bfab9332b1413
[08/10/2023-11:17:02] [V] [TRT] Tactic: 0x385bfab9332b1413 Time: 1.82549
[08/10/2023-11:17:02] [V] [TRT] Conv_17 + Relu_18 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x256x32_stage3_warpsize2x4x1_g1_tensor16x8x8 Tactic: 0x482b48242255b8ce
[08/10/2023-11:17:02] [V] [TRT] Tactic: 0x482b48242255b8ce Time: 1.86799
[08/10/2023-11:17:02] [V] [TRT] Conv_17 + Relu_18 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x256x32_stage3_warpsize2x4x1_g1_tensor16x8x8_t1r3s3 Tactic: 0x614e89f7852edbc3
[08/10/2023-11:17:02] [V] [TRT] Tactic: 0x614e89f7852edbc3 Time: 1.76804
[08/10/2023-11:17:02] [V] [TRT] Conv_17 + Relu_18 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0x65e41d81f093b482
[08/10/2023-11:17:02] [V] [TRT] Tactic: 0x65e41d81f093b482 Time: 1.11853
[08/10/2023-11:17:02] [V] [TRT] Conv_17 + Relu_18 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize64x128x32_stage5_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0x65f71607d20d5438
[08/10/2023-11:17:02] [V] [TRT] Tactic: 0x65f71607d20d5438 Time: 1.29114
[08/10/2023-11:17:02] [V] [TRT] Conv_17 + Relu_18 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x64x32_stage5_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0x6716429226d146f7
[08/10/2023-11:17:02] [V] [TRT] Tactic: 0x6716429226d146f7 Time: 1.35483
[08/10/2023-11:17:02] [V] [TRT] Conv_17 + Relu_18 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize64x32x64_stage5_warpsize2x2x1_g1_tensor16x8x8_t1r3s3 Tactic: 0x679a281f7660c436
[08/10/2023-11:17:02] [V] [TRT] Tactic: 0x679a281f7660c436 Time: 2.51605
[08/10/2023-11:17:02] [V] [TRT] Conv_17 + Relu_18 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize64x32x64_stage5_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0x6e2a3c7a7fc5e4e1
[08/10/2023-11:17:02] [V] [TRT] Tactic: 0x6e2a3c7a7fc5e4e1 Time: 2.61723
[08/10/2023-11:17:02] [V] [TRT] Conv_17 + Relu_18 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x64x32_stage5_warpsize2x2x1_g1_tensor16x8x8_t1r3s3 Tactic: 0x7aa21282feb15f1a
[08/10/2023-11:17:02] [V] [TRT] Tactic: 0x7aa21282feb15f1a Time: 1.35719
[08/10/2023-11:17:02] [V] [TRT] Conv_17 + Relu_18 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage4_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0x818413b69874bb35
[08/10/2023-11:17:02] [V] [TRT] Tactic: 0x818413b69874bb35 Time: 1.73903
[08/10/2023-11:17:02] [V] [TRT] Conv_17 + Relu_18 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x256x32_stage3_warpsize2x4x1_g1_tensor16x8x8 Tactic: 0x998c97842e775a17
[08/10/2023-11:17:02] [V] [TRT] Tactic: 0x998c97842e775a17 Time: 1.82747
[08/10/2023-11:17:02] [V] [TRT] Conv_17 + Relu_18 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8_t1r3s3 Tactic: 0x999e005e3b016ea6
[08/10/2023-11:17:02] [V] [TRT] Tactic: 0x999e005e3b016ea6 Time: 1.01731
[08/10/2023-11:17:02] [V] [TRT] Conv_17 + Relu_18 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize256x64x32_stage3_warpsize4x2x1_g1_tensor16x8x8_t1r3s3 Tactic: 0xa9a06d0633580c0c
[08/10/2023-11:17:02] [V] [TRT] Tactic: 0xa9a06d0633580c0c Time: 1.15088
[08/10/2023-11:17:02] [V] [TRT] Conv_17 + Relu_18 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0xb443c221fcb1565b
[08/10/2023-11:17:02] [V] [TRT] Tactic: 0xb443c221fcb1565b Time: 1.07311
[08/10/2023-11:17:02] [V] [TRT] Conv_17 + Relu_18 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage3_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0xb47d7d926de02dee
[08/10/2023-11:17:02] [V] [TRT] Tactic: 0xb47d7d926de02dee Time: 1.43499
[08/10/2023-11:17:02] [V] [TRT] Conv_17 + Relu_18 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize256x64x32_stage3_warpsize4x2x1_g1_tensor16x8x8 Tactic: 0xba0185279ccb2b85
[08/10/2023-11:17:02] [V] [TRT] Tactic: 0xba0185279ccb2b85 Time: 1.2853
[08/10/2023-11:17:02] [V] [TRT] Conv_17 + Relu_18 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage3_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0xc0a715d897e240bb
[08/10/2023-11:17:02] [V] [TRT] Tactic: 0xc0a715d897e240bb Time: 1.38258
[08/10/2023-11:17:02] [V] [TRT] Conv_17 + Relu_18 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize256x64x32_stage3_warpsize4x2x1_g1_tensor16x8x8 Tactic: 0xcedbed6d66c946d0
[08/10/2023-11:17:02] [V] [TRT] Tactic: 0xcedbed6d66c946d0 Time: 1.17401
[08/10/2023-11:17:02] [V] [TRT] Conv_17 + Relu_18 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize64x128x32_stage5_warpsize2x2x1_g1_tensor16x8x8_t1r3s3 Tactic: 0xd4428a7355769d7d
[08/10/2023-11:17:02] [V] [TRT] Tactic: 0xd4428a7355769d7d Time: 1.23532
[08/10/2023-11:17:02] [V] [TRT] Conv_17 + Relu_18 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize64x32x64_stage5_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0xd7f5d575d49a4bc7
[08/10/2023-11:17:02] [V] [TRT] Tactic: 0xd7f5d575d49a4bc7 Time: 2.95865
[08/10/2023-11:17:02] [V] [TRT] Conv_17 + Relu_18 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x128x32_stage4_warpsize2x2x1_g1_tensor16x8x8_t1r3s3 Tactic: 0xd920b33c9bd27143
[08/10/2023-11:17:02] [V] [TRT] Tactic: 0xd920b33c9bd27143 Time: 0.963118
[08/10/2023-11:17:02] [V] [TRT] Conv_17 + Relu_18 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize256x128x32_stage3_warpsize4x2x1_g1_tensor16x8x8 Tactic: 0xe797e099911c0624
[08/10/2023-11:17:02] [V] [TRT] Tactic: 0xe797e099911c0624 Time: 1.04059
[08/10/2023-11:17:02] [V] [TRT] Conv_17 + Relu_18 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage3_warpsize2x2x1_g1_tensor16x8x8_t1r3s3 Tactic: 0xeb65d4c1e8fa2294
[08/10/2023-11:17:02] [V] [TRT] Tactic: 0xeb65d4c1e8fa2294 Time: 1.32412
[08/10/2023-11:17:02] [V] [TRT] Conv_17 + Relu_18 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x128x32_stage4_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0xf4156675c5f728d4
[08/10/2023-11:17:02] [V] [TRT] Tactic: 0xf4156675c5f728d4 Time: 2.07505
[08/10/2023-11:17:02] [V] [TRT] Fastest Tactic: 0xd920b33c9bd27143 Time: 0.963118
[08/10/2023-11:17:02] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0xd920b33c9bd27143
[08/10/2023-11:17:02] [V] [TRT] *************** Autotuning format combination: Half(4147200,32400,180,1) -> Half(4147200,32400,180,1) ***************
[08/10/2023-11:17:02] [W] [TRT] Weights [name=Conv_17 + Relu_18.weight] had the following issues when converted to FP16:
[08/10/2023-11:17:02] [W] [TRT]  - Subnormal FP16 values detected. 
[08/10/2023-11:17:02] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.
[08/10/2023-11:17:02] [V] [TRT] --------------- Timing Runner: Conv_17 + Relu_18 (CudnnConvolution)
[08/10/2023-11:17:02] [V] [TRT] Tactic: 0x0000000000000000 Time: 7.0001
[08/10/2023-11:17:02] [V] [TRT] Tactic: 0x0000000000000001 Time: 5.47245
[08/10/2023-11:17:02] [V] [TRT] Tactic: 0x0000000000000002 Time: 7.98045
[08/10/2023-11:17:02] [V] [TRT] Tactic: 0x0000000000000004 skipped. Scratch requested: 8725266432, available: 4294967296
[08/10/2023-11:17:03] [V] [TRT] Tactic: 0x0000000000000005 Time: 24.082
[08/10/2023-11:17:03] [V] [TRT] Tactic: 0x0000000000000006 Time: 4.18782
[08/10/2023-11:17:03] [V] [TRT] Tactic: 0x0000000000000038 Time: 7.06605
[08/10/2023-11:17:03] [V] [TRT] Tactic: 0x000000000000003a Time: 7.99824
[08/10/2023-11:17:03] [V] [TRT] Tactic: 0x000000000000003c skipped. Scratch requested: 8725266432, available: 4294967296
[08/10/2023-11:17:03] [V] [TRT] Tactic: 0x000000000000003d Time: 23.0695
[08/10/2023-11:17:03] [V] [TRT] Tactic: 0x000000000000003e Time: 3.9542
[08/10/2023-11:17:03] [V] [TRT] Fastest Tactic: 0x000000000000003e Time: 3.9542
[08/10/2023-11:17:03] [V] [TRT] Setting workspace to 8725266432enables more tactics for profiling
[08/10/2023-11:17:03] [W] [TRT] Weights [name=Conv_17 + Relu_18.weight] had the following issues when converted to FP16:
[08/10/2023-11:17:03] [W] [TRT]  - Subnormal FP16 values detected. 
[08/10/2023-11:17:03] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.
[08/10/2023-11:17:03] [V] [TRT] --------------- Timing Runner: Conv_17 + Relu_18 (CaskConvolution)
[08/10/2023-11:17:03] [V] [TRT] CaskConvolution has no valid tactics for this config, skipping
[08/10/2023-11:17:03] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CudnnConvolution Tactic: 0x000000000000003e
[08/10/2023-11:17:03] [V] [TRT] *************** Autotuning format combination: Half(2073600,32400:2,180,1) -> Half(2073600,32400:2,180,1) ***************
[08/10/2023-11:17:03] [W] [TRT] Weights [name=Conv_17 + Relu_18.weight] had the following issues when converted to FP16:
[08/10/2023-11:17:03] [W] [TRT]  - Subnormal FP16 values detected. 
[08/10/2023-11:17:03] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.
[08/10/2023-11:17:03] [V] [TRT] --------------- Timing Runner: Conv_17 + Relu_18 (FusedConvActConvolution)
[08/10/2023-11:17:03] [V] [TRT] FusedConvActConvolution has no valid tactics for this config, skipping
[08/10/2023-11:17:03] [W] [TRT] Weights [name=Conv_17 + Relu_18.weight] had the following issues when converted to FP16:
[08/10/2023-11:17:03] [W] [TRT]  - Subnormal FP16 values detected. 
[08/10/2023-11:17:03] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.
[08/10/2023-11:17:03] [V] [TRT] --------------- Timing Runner: Conv_17 + Relu_18 (CaskConvolution)
[08/10/2023-11:17:03] [V] [TRT] CaskConvolution has no valid tactics for this config, skipping
[08/10/2023-11:17:03] [V] [TRT] *************** Autotuning format combination: Half(518400,1:8,2880,16) -> Float(4147200,32400,180,1) ***************
[08/10/2023-11:17:03] [W] [TRT] Weights [name=Conv_17 + Relu_18.weight] had the following issues when converted to FP16:
[08/10/2023-11:17:03] [W] [TRT]  - Subnormal FP16 values detected. 
[08/10/2023-11:17:03] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.
[08/10/2023-11:17:03] [V] [TRT] --------------- Timing Runner: Conv_17 + Relu_18 (CaskConvolution)
[08/10/2023-11:17:03] [V] [TRT] CaskConvolution has no valid tactics for this config, skipping
[08/10/2023-11:17:03] [V] [TRT] *************** Autotuning format combination: Half(518400,1:8,2880,16) -> Half(518400,1:8,2880,16) ***************
[08/10/2023-11:17:03] [W] [TRT] Weights [name=Conv_17 + Relu_18.weight] had the following issues when converted to FP16:
[08/10/2023-11:17:03] [W] [TRT]  - Subnormal FP16 values detected. 
[08/10/2023-11:17:03] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.
[08/10/2023-11:17:03] [V] [TRT] --------------- Timing Runner: Conv_17 + Relu_18 (CudaDepthwiseConvolution)
[08/10/2023-11:17:03] [V] [TRT] CudaDepthwiseConvolution has no valid tactics for this config, skipping
[08/10/2023-11:17:03] [W] [TRT] Weights [name=Conv_17 + Relu_18.weight] had the following issues when converted to FP16:
[08/10/2023-11:17:03] [W] [TRT]  - Subnormal FP16 values detected. 
[08/10/2023-11:17:03] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.
[08/10/2023-11:17:03] [V] [TRT] --------------- Timing Runner: Conv_17 + Relu_18 (CaskConvolution)
[08/10/2023-11:17:03] [V] [TRT] Conv_17 + Relu_18 Set Tactic Name: ampere_h16816cudnn_256x128_ldg8_relu_exp_medium_nhwc_tn_v1 Tactic: 0x03896956a39a1203
[08/10/2023-11:17:03] [V] [TRT] Tactic: 0x03896956a39a1203 Time: 0.527913
[08/10/2023-11:17:03] [V] [TRT] Conv_17 + Relu_18 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x128x32_stage3_warpsize4x2x1_g1_tensor16x8x16_t1r3s3 Tactic: 0x048d6d0400f33439
[08/10/2023-11:17:03] [V] [TRT] Tactic: 0x048d6d0400f33439 Time: 0.490519
[08/10/2023-11:17:03] [V] [TRT] Conv_17 + Relu_18 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x64x64_stage3_warpsize2x2x1_g1_tensor16x8x16_t1r3s3 Tactic: 0x05b6220f243edacd
[08/10/2023-11:17:03] [V] [TRT] Tactic: 0x05b6220f243edacd Time: 0.561929
[08/10/2023-11:17:03] [V] [TRT] Conv_17 + Relu_18 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x64x32_stage3_warpsize4x1x1_g1_tensor16x8x16_t1r3s3_aACCESS Tactic: 0x0866ddee325d07a6
[08/10/2023-11:17:03] [V] [TRT] Tactic: 0x0866ddee325d07a6 Time: 0.504631
[08/10/2023-11:17:03] [V] [TRT] Conv_17 + Relu_18 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x128x32_stage4_warpsize2x2x1_g1_tensor16x8x16 Tactic: 0x0e07ff4f4f7c1ac9
[08/10/2023-11:17:03] [V] [TRT] Tactic: 0x0e07ff4f4f7c1ac9 Time: 0.480347
[08/10/2023-11:17:03] [V] [TRT] Conv_17 + Relu_18 Set Tactic Name: ampere_h16816cudnn_256x128_ldg8_relu_exp_stages_64x3_small_nhwc_tn_v1 Tactic: 0x0e199750c30c6928
[08/10/2023-11:17:03] [V] [TRT] Tactic: 0x0e199750c30c6928 Time: 0.511223
[08/10/2023-11:17:03] [V] [TRT] Conv_17 + Relu_18 Set Tactic Name: ampere_h1688cudnn_128x128_ldg8_relu_exp_medium_nhwc_tn_v1 Tactic: 0x100c1ad308e08d35
[08/10/2023-11:17:03] [V] [TRT] Tactic: 0x100c1ad308e08d35 Time: 0.625362
[08/10/2023-11:17:03] [V] [TRT] Conv_17 + Relu_18 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x128x32_stage5_warpsize2x2x1_g1_tensor16x8x16_t1r3s3 Tactic: 0x17ebf0c9f418f10a
[08/10/2023-11:17:03] [V] [TRT] Tactic: 0x17ebf0c9f418f10a Time: 0.562514
[08/10/2023-11:17:03] [V] [TRT] Conv_17 + Relu_18 Set Tactic Name: ampere_h16816cudnn_128x64_ldg8_relu_exp_stages_64x4_medium_nhwc_tn_v1 Tactic: 0x196cbc423a9bb69e
[08/10/2023-11:17:03] [V] [TRT] Tactic: 0x196cbc423a9bb69e Time: 0.72411
[08/10/2023-11:17:03] [V] [TRT] Conv_17 + Relu_18 Set Tactic Name: ampere_h16816cudnn_128x64_sliced1x2_ldg8_relu_exp_stages_64x3_small_nhwc_tn_v1 Tactic: 0x19822de884f42a6a
[08/10/2023-11:17:03] [V] [TRT] Tactic: 0x19822de884f42a6a Time: 0.558121
[08/10/2023-11:17:03] [V] [TRT] Conv_17 + Relu_18 Set Tactic Name: ampere_h16816cudnn_64x64_sliced1x2_ldg8_relu_exp_stages_64x5_small_nhwc_tn_v1 Tactic: 0x1a5ba808ad4cc5a8
[08/10/2023-11:17:03] [V] [TRT] Tactic: 0x1a5ba808ad4cc5a8 Time: 0.749577
[08/10/2023-11:17:03] [V] [TRT] Conv_17 + Relu_18 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x64x64_stage3_warpsize2x2x1_g1_tensor16x8x16 Tactic: 0x21b295c0c8f6c95a
[08/10/2023-11:17:03] [V] [TRT] Tactic: 0x21b295c0c8f6c95a Time: 0.575863
[08/10/2023-11:17:03] [V] [TRT] Conv_17 + Relu_18 Set Tactic Name: ampere_h16816cudnn_128x128_ldg8_relu_exp_medium_nhwc_tn_v1 Tactic: 0x234580e8a194335c
[08/10/2023-11:17:03] [V] [TRT] Tactic: 0x234580e8a194335c Time: 0.476489
[08/10/2023-11:17:03] [V] [TRT] Conv_17 + Relu_18 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x128x32_stage4_warpsize2x2x1_g1_tensor16x8x16_t1r3s3_aACCESS Tactic: 0x245530c34bd6090f
[08/10/2023-11:17:03] [V] [TRT] Tactic: 0x245530c34bd6090f Time: 0.452827
[08/10/2023-11:17:03] [V] [TRT] Conv_17 + Relu_18 Set Tactic Name: ampere_h16816cudnn_64x64_sliced1x2_ldg8_relu_exp_stages_64x6_small_nhwc_tn_v1 Tactic: 0x24e01e7405cfdfe9
[08/10/2023-11:17:03] [V] [TRT] Tactic: 0x24e01e7405cfdfe9 Time: 0.963831
[08/10/2023-11:17:03] [V] [TRT] Conv_17 + Relu_18 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x64x64_stage3_warpsize2x2x1_g1_tensor16x8x16_t1r3s3_aACCESS Tactic: 0x263a38afd75e3a43
[08/10/2023-11:17:03] [V] [TRT] Tactic: 0x263a38afd75e3a43 Time: 0.527045
[08/10/2023-11:17:03] [V] [TRT] Conv_17 + Relu_18 Set Tactic Name: ampere_h16816cudnn_64x128_sliced1x2_ldg8_relu_exp_stages_64x4_large_nhwc_tn_v1 Tactic: 0x2721a7f18c2700db
[08/10/2023-11:17:03] [V] [TRT] Tactic: 0x2721a7f18c2700db Time: 0.709673
[08/10/2023-11:17:03] [V] [TRT] Conv_17 + Relu_18 Set Tactic Name: ampere_h16816cudnn_128x64_sliced1x2_ldg8_relu_exp_stages_64x3_medium_nhwc_tn_v1 Tactic: 0x29329b5741ea05f2
[08/10/2023-11:17:03] [V] [TRT] Tactic: 0x29329b5741ea05f2 Time: 0.550779
[08/10/2023-11:17:03] [V] [TRT] Conv_17 + Relu_18 Set Tactic Name: ampere_h16816cudnn_128x128_ldg8_relu_exp_large_nhwc_tn_v1 Tactic: 0x2970ae65966d569d
[08/10/2023-11:17:03] [V] [TRT] Tactic: 0x2970ae65966d569d Time: 0.473938
[08/10/2023-11:17:03] [V] [TRT] Conv_17 + Relu_18 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x256x64_stage3_warpsize1x4x1_g1_tensor16x8x16 Tactic: 0x29dc14520e0e4eb6
[08/10/2023-11:17:03] [V] [TRT] Tactic: 0x29dc14520e0e4eb6 Time: 1.00168
[08/10/2023-11:17:03] [V] [TRT] Conv_17 + Relu_18 Set Tactic Name: ampere_h16816cudnn_256x64_ldg8_relu_exp_stages_64x3_large_nhwc_tn_v1 Tactic: 0x2cb4a662694e89d3
[08/10/2023-11:17:03] [V] [TRT] Tactic: 0x2cb4a662694e89d3 Time: 0.567616
[08/10/2023-11:17:03] [V] [TRT] Conv_17 + Relu_18 Set Tactic Name: ampere_h16816cudnn_128x128_ldg8_relu_exp_stages_64x3_medium_nhwc_tn_v1 Tactic: 0x2eaa2202de9404d6
[08/10/2023-11:17:03] [V] [TRT] Tactic: 0x2eaa2202de9404d6 Time: 0.551237
[08/10/2023-11:17:03] [V] [TRT] Conv_17 + Relu_18 Set Tactic Name: ampere_h1688cudnn_256x128_ldg8_relu_exp_medium_nhwc_tn_v1 Tactic: 0x30c0f36d0aeeac6a
[08/10/2023-11:17:03] [V] [TRT] Tactic: 0x30c0f36d0aeeac6a Time: 0.645413
[08/10/2023-11:17:03] [V] [TRT] Conv_17 + Relu_18 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x64x64_stage4_warpsize2x2x1_g1_tensor16x8x16_t1r3s3_aACCESS Tactic: 0x30e8a8d7a953e5e9
[08/10/2023-11:17:03] [V] [TRT] Tactic: 0x30e8a8d7a953e5e9 Time: 0.633435
[08/10/2023-11:17:03] [V] [TRT] Conv_17 + Relu_18 Set Tactic Name: ampere_h16816cudnn_128x64_ldg8_relu_exp_stages_64x4_large_nhwc_tn_v1 Tactic: 0x3197d5044d71e98e
[08/10/2023-11:17:03] [V] [TRT] Tactic: 0x3197d5044d71e98e Time: 0.730811
[08/10/2023-11:17:03] [V] [TRT] Conv_17 + Relu_18 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x64x64_stage4_warpsize2x2x1_g1_tensor16x8x16_t1r3s3 Tactic: 0x31d93dc22d2af081
[08/10/2023-11:17:03] [V] [TRT] Tactic: 0x31d93dc22d2af081 Time: 0.710624
[08/10/2023-11:17:03] [V] [TRT] Conv_17 + Relu_18 Set Tactic Name: ampere_h1688cudnn_128x128_ldg8_relu_exp_small_nhwc_tn_v1 Tactic: 0x3369260c04f9ad73
[08/10/2023-11:17:03] [V] [TRT] Tactic: 0x3369260c04f9ad73 Time: 0.589586
[08/10/2023-11:17:03] [V] [TRT] Conv_17 + Relu_18 Set Tactic Name: ampere_h16816cudnn_64x128_ldg8_relu_exp_stages_64x4_small_nhwc_tn_v1 Tactic: 0x3b5fa0f2a8fc2410
[08/10/2023-11:17:03] [V] [TRT] Tactic: 0x3b5fa0f2a8fc2410 Time: 0.66661
[08/10/2023-11:17:03] [V] [TRT] Conv_17 + Relu_18 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x32x32_stage4_warpsize4x1x1_g1_tensor16x8x16_t1r3s3 Tactic: 0x3e7eb35b91b9fa63
[08/10/2023-11:17:03] [V] [TRT] Tactic: 0x3e7eb35b91b9fa63 Time: 0.864901
[08/10/2023-11:17:03] [V] [TRT] Conv_17 + Relu_18 Set Tactic Name: ampere_h16816cudnn_256x64_ldg8_relu_exp_stages_64x3_medium_nhwc_tn_v1 Tactic: 0x3f031df0e579d52c
[08/10/2023-11:17:03] [V] [TRT] Tactic: 0x3f031df0e579d52c Time: 0.562601
[08/10/2023-11:17:03] [V] [TRT] Conv_17 + Relu_18 Set Tactic Name: ampere_h16816cudnn_256x64_sliced1x2_ldg8_relu_exp_small_nhwc_tn_v1 Tactic: 0x42a5b8709d0039be
[08/10/2023-11:17:03] [V] [TRT] Tactic: 0x42a5b8709d0039be Time: 0.661326
[08/10/2023-11:17:03] [V] [TRT] Conv_17 + Relu_18 Set Tactic Name: ampere_h16816cudnn_256x128_ldg8_relu_exp_small_nhwc_tn_v1 Tactic: 0x463794ee4acffd1f
[08/10/2023-11:17:03] [V] [TRT] Tactic: 0x463794ee4acffd1f Time: 0.515392
[08/10/2023-11:17:03] [V] [TRT] Conv_17 + Relu_18 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x64x64_stage4_warpsize2x2x1_g1_tensor16x8x16 Tactic: 0x4a81ea1e51436a30
[08/10/2023-11:17:03] [V] [TRT] Tactic: 0x4a81ea1e51436a30 Time: 0.718021
[08/10/2023-11:17:03] [V] [TRT] Conv_17 + Relu_18 Set Tactic Name: ampere_h1688cudnn_256x64_sliced1x2_ldg8_relu_exp_small_nhwc_tn_v1 Tactic: 0x4aed1956bd10a795
[08/10/2023-11:17:03] [V] [TRT] Tactic: 0x4aed1956bd10a795 Time: 0.720594
[08/10/2023-11:17:03] [V] [TRT] Conv_17 + Relu_18 Set Tactic Name: ampere_h16816cudnn_64x128_sliced1x2_ldg8_relu_exp_stages_64x3_large_nhwc_tn_v1 Tactic: 0x4fc05923455fc266
[08/10/2023-11:17:03] [V] [TRT] Tactic: 0x4fc05923455fc266 Time: 0.567328
[08/10/2023-11:17:03] [V] [TRT] Conv_17 + Relu_18 Set Tactic Name: ampere_h16816cudnn_256x64_ldg8_relu_exp_large_nhwc_tn_v1 Tactic: 0x5013c38f55afa9ba
[08/10/2023-11:17:03] [V] [TRT] Tactic: 0x5013c38f55afa9ba Time: 0.592005
[08/10/2023-11:17:03] [V] [TRT] Conv_17 + Relu_18 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x64x32_stage3_warpsize4x1x1_g1_tensor16x8x16_t1r3s3 Tactic: 0x529f4431bdae94f5
[08/10/2023-11:17:03] [V] [TRT] Tactic: 0x529f4431bdae94f5 Time: 0.491671
[08/10/2023-11:17:03] [V] [TRT] Conv_17 + Relu_18 Set Tactic Name: ampere_h16816cudnn_64x128_ldg8_relu_exp_stages_64x3_small_nhwc_tn_v1 Tactic: 0x53be5e206184e6ad
[08/10/2023-11:17:03] [V] [TRT] Tactic: 0x53be5e206184e6ad Time: 0.612901
[08/10/2023-11:17:03] [V] [TRT] Conv_17 + Relu_18 Set Tactic Name: ampere_h16816cudnn_64x64_ldg8_relu_exp_stages_64x5_large_nhwc_tn_v1 Tactic: 0x544bff7ff9c5d908
[08/10/2023-11:17:03] [V] [TRT] Tactic: 0x544bff7ff9c5d908 Time: 0.846222
[08/10/2023-11:17:03] [V] [TRT] Conv_17 + Relu_18 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x64x64_stage3_warpsize2x2x1_g1_tensor16x8x16 Tactic: 0x5568fd8a32f4a40f
[08/10/2023-11:17:03] [V] [TRT] Tactic: 0x5568fd8a32f4a40f Time: 0.575173
[08/10/2023-11:17:03] [V] [TRT] Conv_17 + Relu_18 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x128x64_stage3_warpsize2x2x1_g1_tensor16x8x16 Tactic: 0x57c9a5ff682354a6
[08/10/2023-11:17:03] [V] [TRT] Tactic: 0x57c9a5ff682354a6 Time: 0.533952
[08/10/2023-11:17:03] [V] [TRT] Conv_17 + Relu_18 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x32x64_stage5_warpsize2x2x1_g1_tensor16x8x16 Tactic: 0x5820b3dda403c4d0
[08/10/2023-11:17:03] [V] [TRT] Tactic: 0x5820b3dda403c4d0 Time: 1.05502
[08/10/2023-11:17:03] [V] [TRT] Conv_17 + Relu_18 Set Tactic Name: ampere_h16816cudnn_128x128_ldg8_relu_exp_stages_64x3_small_nhwc_tn_v1 Tactic: 0x58816bf2e9c36afb
[08/10/2023-11:17:03] [V] [TRT] Tactic: 0x58816bf2e9c36afb Time: 0.801682
[08/10/2023-11:17:03] [V] [TRT] Conv_17 + Relu_18 Set Tactic Name: ampere_h16816cudnn_128x64_ldg8_relu_exp_stages_64x3_large_nhwc_tn_v1 Tactic: 0x59762bd684092b33
[08/10/2023-11:17:03] [V] [TRT] Tactic: 0x59762bd684092b33 Time: 0.796315
[08/10/2023-11:17:03] [V] [TRT] Conv_17 + Relu_18 Set Tactic Name: ampere_h16816cudnn_128x64_sliced1x2_ldg8_relu_exp_stages_64x4_medium_nhwc_tn_v1 Tactic: 0x59c5c647b4c76593
[08/10/2023-11:17:03] [V] [TRT] Tactic: 0x59c5c647b4c76593 Time: 0.717211
[08/10/2023-11:17:03] [V] [TRT] Conv_17 + Relu_18 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x128x64_stage3_warpsize4x2x1_g1_tensor16x8x16 Tactic: 0x5a55274960724ba8
[08/10/2023-11:17:03] [V] [TRT] Tactic: 0x5a55274960724ba8 Time: 0.50507
[08/10/2023-11:17:03] [V] [TRT] Conv_17 + Relu_18 Set Tactic Name: ampere_h1688cudnn_256x128_ldg8_relu_exp_large_nhwc_tn_v1 Tactic: 0x5c2e1c87d85b06f1
[08/10/2023-11:17:03] [V] [TRT] Tactic: 0x5c2e1c87d85b06f1 Time: 0.657806
[08/10/2023-11:17:04] [V] [TRT] Conv_17 + Relu_18 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x256x64_stage3_warpsize1x4x1_g1_tensor16x8x16 Tactic: 0x5d067c18f40c23e3
[08/10/2023-11:17:04] [V] [TRT] Tactic: 0x5d067c18f40c23e3 Time: 1.0243
[08/10/2023-11:17:04] [V] [TRT] Conv_17 + Relu_18 Set Tactic Name: ampere_h16816cudnn_64x64_ldg8_relu_exp_stages_64x5_medium_nhwc_tn_v1 Tactic: 0x5f31c22ec167f384
[08/10/2023-11:17:04] [V] [TRT] Tactic: 0x5f31c22ec167f384 Time: 0.842971
[08/10/2023-11:17:04] [V] [TRT] Conv_17 + Relu_18 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x128x32_stage4_warpsize2x2x1_g1_tensor16x8x16_t1r3s3 Tactic: 0x60c3421152ef8e10
[08/10/2023-11:17:04] [V] [TRT] Tactic: 0x60c3421152ef8e10 Time: 0.44613
[08/10/2023-11:17:04] [V] [TRT] Conv_17 + Relu_18 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x128x32_stage3_warpsize4x2x1_g1_tensor16x8x16 Tactic: 0x60da8c7151d91e47
[08/10/2023-11:17:04] [V] [TRT] Tactic: 0x60da8c7151d91e47 Time: 0.494853
[08/10/2023-11:17:04] [V] [TRT] Conv_17 + Relu_18 Set Tactic Name: ampere_h1688cudnn_256x64_ldg8_relu_exp_medium_nhwc_tn_v1 Tactic: 0x6426696f872a3b13
[08/10/2023-11:17:04] [V] [TRT] Tactic: 0x6426696f872a3b13 Time: 0.770043
[08/10/2023-11:17:04] [V] [TRT] Conv_17 + Relu_18 Set Tactic Name: ampere_h16816cudnn_128x64_ldg8_relu_exp_stages_64x3_medium_nhwc_tn_v1 Tactic: 0x699be152cfb6d6ff
[08/10/2023-11:17:04] [V] [TRT] Tactic: 0x699be152cfb6d6ff Time: 0.596672
[08/10/2023-11:17:04] [V] [TRT] Conv_17 + Relu_18 Set Tactic Name: ampere_h16816cudnn_64x64_ldg8_relu_exp_stages_64x6_large_nhwc_tn_v1 Tactic: 0x6af049035146c349
[08/10/2023-11:17:04] [V] [TRT] Tactic: 0x6af049035146c349 Time: 1.07009
[08/10/2023-11:17:04] [V] [TRT] Conv_17 + Relu_18 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x256x32_stage3_warpsize2x4x1_g1_tensor16x8x16 Tactic: 0x7005d10718f6c22d
[08/10/2023-11:17:04] [V] [TRT] Tactic: 0x7005d10718f6c22d Time: 0.921678
[08/10/2023-11:17:04] [V] [TRT] Conv_17 + Relu_18 Set Tactic Name: ampere_h16816cudnn_64x64_sliced1x2_ldg8_relu_exp_stages_64x5_medium_nhwc_tn_v1 Tactic: 0x706f08da35c795a5
[08/10/2023-11:17:04] [V] [TRT] Tactic: 0x706f08da35c795a5 Time: 0.769701
[08/10/2023-11:17:04] [V] [TRT] Conv_17 + Relu_18 Set Tactic Name: ampere_h16816cudnn_128x64_sliced1x2_ldg8_relu_exp_stages_64x4_small_nhwc_tn_v1 Tactic: 0x7163d33a4d8ce8d7
[08/10/2023-11:17:04] [V] [TRT] Tactic: 0x7163d33a4d8ce8d7 Time: 0.706062
[08/10/2023-11:17:04] [V] [TRT] Conv_17 + Relu_18 Set Tactic Name: ampere_h1688cudnn_256x64_ldg8_relu_exp_small_nhwc_tn_v1 Tactic: 0x7aad3976677d7155
[08/10/2023-11:17:04] [V] [TRT] Tactic: 0x7aad3976677d7155 Time: 0.735127
[08/10/2023-11:17:04] [V] [TRT] Conv_17 + Relu_18 Set Tactic Name: ampere_h16816cudnn_64x64_sliced1x2_ldg8_relu_exp_stages_64x5_large_nhwc_tn_v1 Tactic: 0x8015519605ab9963
[08/10/2023-11:17:04] [V] [TRT] Tactic: 0x8015519605ab9963 Time: 0.785431
[08/10/2023-11:17:04] [V] [TRT] Conv_17 + Relu_18 Set Tactic Name: ampere_h16816cudnn_256x64_sliced1x2_ldg8_relu_exp_medium_nhwc_tn_v1 Tactic: 0x82f8a2214b0b4178
[08/10/2023-11:17:04] [V] [TRT] Tactic: 0x82f8a2214b0b4178 Time: 0.798747
[08/10/2023-11:17:04] [V] [TRT] Conv_17 + Relu_18 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x32x32_stage4_warpsize4x1x1_g1_tensor16x8x16 Tactic: 0x834e11ecd4ab9454
[08/10/2023-11:17:04] [V] [TRT] Tactic: 0x834e11ecd4ab9454 Time: 0.888978
[08/10/2023-11:17:04] [V] [TRT] Conv_17 + Relu_18 Set Tactic Name: ampere_h16816cudnn_128x64_sliced1x2_ldg8_relu_exp_stages_64x3_large_nhwc_tn_v1 Tactic: 0x83ccd4762c1376a1
[08/10/2023-11:17:04] [V] [TRT] Tactic: 0x83ccd4762c1376a1 Time: 0.574048
[08/10/2023-11:17:04] [V] [TRT] Conv_17 + Relu_18 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x64x64_stage3_warpsize4x1x1_g1_tensor16x8x16_t1r3s3 Tactic: 0x84ee897dd1f4d2aa
[08/10/2023-11:17:04] [V] [TRT] Tactic: 0x84ee897dd1f4d2aa Time: 0.562743
[08/10/2023-11:17:04] [V] [TRT] Conv_17 + Relu_18 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x128x64_stage3_warpsize2x2x1_g1_tensor16x8x16 Tactic: 0x866e7a5f6401b67f
[08/10/2023-11:17:04] [V] [TRT] Tactic: 0x866e7a5f6401b67f Time: 0.515525
[08/10/2023-11:17:04] [V] [TRT] Conv_17 + Relu_18 Set Tactic Name: ampere_h16816cudnn_64x64_ldg8_relu_exp_stages_64x6_medium_nhwc_tn_v1 Tactic: 0x88971eec55aba850
[08/10/2023-11:17:04] [V] [TRT] Tactic: 0x88971eec55aba850 Time: 1.05095
[08/10/2023-11:17:04] [V] [TRT] Conv_17 + Relu_18 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x128x64_stage3_warpsize4x2x1_g1_tensor16x8x16 Tactic: 0x8bf2f8e96c50a971
[08/10/2023-11:17:04] [V] [TRT] Tactic: 0x8bf2f8e96c50a971 Time: 0.501417
[08/10/2023-11:17:04] [V] [TRT] Conv_17 + Relu_18 Set Tactic Name: ampere_h16816cudnn_256x128_ldg8_relu_exp_stages_64x3_large_nhwc_tn_v1 Tactic: 0x94576ece6beb35e3
[08/10/2023-11:17:04] [V] [TRT] Tactic: 0x94576ece6beb35e3 Time: 0.524398
[08/10/2023-11:17:04] [V] [TRT] Conv_17 + Relu_18 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x64x64_stage3_warpsize4x1x1_g1_tensor16x8x16 Tactic: 0x9fff6e65019cc310
[08/10/2023-11:17:04] [V] [TRT] Tactic: 0x9fff6e65019cc310 Time: 0.563017
[08/10/2023-11:17:04] [V] [TRT] Conv_17 + Relu_18 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x32x64_stage5_warpsize2x2x1_g1_tensor16x8x16_t1r3s3 Tactic: 0xa033e20ae9f412b2
[08/10/2023-11:17:04] [V] [TRT] Tactic: 0xa033e20ae9f412b2 Time: 1.04215
[08/10/2023-11:17:04] [V] [TRT] Conv_17 + Relu_18 Set Tactic Name: ampere_h16816cudnn_64x128_ldg8_relu_exp_stages_64x4_large_nhwc_tn_v1 Tactic: 0xa111596c001b78db
[08/10/2023-11:17:04] [V] [TRT] Tactic: 0xa111596c001b78db Time: 0.697888
[08/10/2023-11:17:04] [V] [TRT] Conv_17 + Relu_18 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x256x32_stage3_warpsize2x4x1_g1_tensor16x8x16 Tactic: 0xa1a20ea714d420f4
[08/10/2023-11:17:04] [V] [TRT] Tactic: 0xa1a20ea714d420f4 Time: 0.91093
[08/10/2023-11:17:04] [V] [TRT] Conv_17 + Relu_18 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x32x32_stage4_warpsize4x1x1_g1_tensor16x8x16_t1r3s3 Tactic: 0xa40cb43c296a36a8
[08/10/2023-11:17:04] [V] [TRT] Tactic: 0xa40cb43c296a36a8 Time: 0.791118
[08/10/2023-11:17:04] [V] [TRT] Conv_17 + Relu_18 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x64x32_stage3_warpsize4x1x1_g1_tensor16x8x16 Tactic: 0xa570c55d303796ff
[08/10/2023-11:17:04] [V] [TRT] Tactic: 0xa570c55d303796ff Time: 0.504795
[08/10/2023-11:17:04] [V] [TRT] Conv_17 + Relu_18 Set Tactic Name: ampere_h16816cudnn_64x64_sliced1x2_ldg8_relu_exp_stages_64x6_medium_nhwc_tn_v1 Tactic: 0xa7c9d418a10bce71
[08/10/2023-11:17:04] [V] [TRT] Tactic: 0xa7c9d418a10bce71 Time: 1.06473
[08/10/2023-11:17:04] [V] [TRT] Conv_17 + Relu_18 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x32x32_stage4_warpsize4x1x1_g1_tensor16x8x16 Tactic: 0xa83b68f30462f971
[08/10/2023-11:17:04] [V] [TRT] Tactic: 0xa83b68f30462f971 Time: 0.794158
[08/10/2023-11:17:04] [V] [TRT] Conv_17 + Relu_18 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x256x32_stage3_warpsize2x4x1_g1_tensor16x8x16_t1r3s3 Tactic: 0xa9177bbe4e767df8
[08/10/2023-11:17:04] [V] [TRT] Tactic: 0xa9177bbe4e767df8 Time: 0.909184
[08/10/2023-11:17:04] [V] [TRT] Conv_17 + Relu_18 Set Tactic Name: ampere_h1688cudnn_128x128_ldg8_relu_exp_large_nhwc_tn_v1 Tactic: 0xa927df92ac1ef1b8
[08/10/2023-11:17:04] [V] [TRT] Tactic: 0xa927df92ac1ef1b8 Time: 0.623145
[08/10/2023-11:17:04] [V] [TRT] Conv_17 + Relu_18 Set Tactic Name: ampere_h16816cudnn_128x64_ldg8_relu_exp_stages_64x4_small_nhwc_tn_v1 Tactic: 0xabd92c9ae596b545
[08/10/2023-11:17:04] [V] [TRT] Tactic: 0xabd92c9ae596b545 Time: 0.712407
[08/10/2023-11:17:04] [V] [TRT] Conv_17 + Relu_18 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x128x32_stage5_warpsize2x2x1_g1_tensor16x8x16 Tactic: 0xafd1e8bf6bd3d638
[08/10/2023-11:17:04] [V] [TRT] Tactic: 0xafd1e8bf6bd3d638 Time: 0.552809
[08/10/2023-11:17:04] [V] [TRT] Conv_17 + Relu_18 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x128x32_stage3_warpsize4x2x1_g1_tensor16x8x16 Tactic: 0xb17d53d15dfbfc9e
[08/10/2023-11:17:04] [V] [TRT] Tactic: 0xb17d53d15dfbfc9e Time: 0.511287
[08/10/2023-11:17:04] [V] [TRT] Conv_17 + Relu_18 Set Tactic Name: ampere_h16816cudnn_128x128_ldg8_relu_exp_small_nhwc_tn_v1 Tactic: 0xb33e57fb3e8a0a56
[08/10/2023-11:17:04] [V] [TRT] Tactic: 0xb33e57fb3e8a0a56 Time: 0.469522
[08/10/2023-11:17:04] [V] [TRT] Conv_17 + Relu_18 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x64x32_stage5_warpsize2x2x1_g1_tensor16x8x16_t1r3s3 Tactic: 0xb4bec086187edcfc
[08/10/2023-11:17:04] [V] [TRT] Tactic: 0xb4bec086187edcfc Time: 0.55376
[08/10/2023-11:17:04] [V] [TRT] Conv_17 + Relu_18 Set Tactic Name: ampere_h16816cudnn_256x64_ldg8_relu_exp_stages_64x3_small_nhwc_tn_v1 Tactic: 0xb6fa5ffcc1a9d518
[08/10/2023-11:17:04] [V] [TRT] Tactic: 0xb6fa5ffcc1a9d518 Time: 0.563511
[08/10/2023-11:17:04] [V] [TRT] Conv_17 + Relu_18 Set Tactic Name: ampere_h16816cudnn_64x128_sliced1x2_ldg8_relu_exp_stages_64x3_medium_nhwc_tn_v1 Tactic: 0xb85e52e87caf60a2
[08/10/2023-11:17:04] [V] [TRT] Tactic: 0xb85e52e87caf60a2 Time: 0.579369
[08/10/2023-11:17:04] [V] [TRT] Conv_17 + Relu_18 Set Tactic Name: ampere_h16816cudnn_64x128_ldg8_relu_exp_stages_64x3_medium_nhwc_tn_v1 Tactic: 0xb8d86216e1235cda
[08/10/2023-11:17:04] [V] [TRT] Tactic: 0xb8d86216e1235cda Time: 0.613902
[08/10/2023-11:17:04] [V] [TRT] Conv_17 + Relu_18 Set Tactic Name: ampere_h16816cudnn_256x128_ldg8_relu_exp_stages_64x3_medium_nhwc_tn_v1 Tactic: 0xb9171d70ba2eda4b
[08/10/2023-11:17:04] [V] [TRT] Tactic: 0xb9171d70ba2eda4b Time: 0.535104
[08/10/2023-11:17:04] [V] [TRT] Conv_17 + Relu_18 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x64x32_stage5_warpsize2x2x1_g1_tensor16x8x16 Tactic: 0xbd08239a9317f2fd
[08/10/2023-11:17:04] [V] [TRT] Tactic: 0xbd08239a9317f2fd Time: 0.575826
[08/10/2023-11:17:04] [V] [TRT] Conv_17 + Relu_18 Set Tactic Name: ampere_h16816cudnn_64x128_sliced1x2_ldg8_relu_exp_stages_64x4_small_nhwc_tn_v1 Tactic: 0xbd6f5e6f24c05c10
[08/10/2023-11:17:04] [V] [TRT] Tactic: 0xbd6f5e6f24c05c10 Time: 0.698679
[08/10/2023-11:17:04] [V] [TRT] Conv_17 + Relu_18 Set Tactic Name: ampere_h16816cudnn_64x64_sliced1x2_ldg8_relu_exp_stages_64x6_large_nhwc_tn_v1 Tactic: 0xbeaee7eaad288322
[08/10/2023-11:17:04] [V] [TRT] Tactic: 0xbeaee7eaad288322 Time: 1.07789
[08/10/2023-11:17:04] [V] [TRT] Conv_17 + Relu_18 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x128x64_stage3_warpsize2x2x1_g1_tensor16x8x16_t1r3s3 Tactic: 0xc0a02dc6095497cc
[08/10/2023-11:17:04] [V] [TRT] Tactic: 0xc0a02dc6095497cc Time: 0.520389
[08/10/2023-11:17:04] [V] [TRT] Conv_17 + Relu_18 Set Tactic Name: ampere_h16816cudnn_128x128_ldg8_relu_exp_stages_64x3_large_nhwc_tn_v1 Tactic: 0xc2cf926c41243630
[08/10/2023-11:17:04] [V] [TRT] Tactic: 0xc2cf926c41243630 Time: 0.556832
[08/10/2023-11:17:04] [V] [TRT] Conv_17 + Relu_18 Set Tactic Name: ampere_h16816cudnn_128x64_ldg8_relu_exp_stages_64x3_small_nhwc_tn_v1 Tactic: 0xc338d2482cee77f8
[08/10/2023-11:17:04] [V] [TRT] Tactic: 0xc338d2482cee77f8 Time: 0.597577
[08/10/2023-11:17:04] [V] [TRT] Conv_17 + Relu_18 Set Tactic Name: ampere_h1688cudnn_256x128_ldg8_relu_exp_small_nhwc_tn_v1 Tactic: 0xc660e51970bc5a3a
[08/10/2023-11:17:04] [V] [TRT] Tactic: 0xc660e51970bc5a3a Time: 0.614368
[08/10/2023-11:17:04] [V] [TRT] Conv_17 + Relu_18 Set Tactic Name: ampere_h16816cudnn_64x128_ldg8_relu_exp_stages_64x4_medium_nhwc_tn_v1 Tactic: 0xc82f3f06140e3cbb
[08/10/2023-11:17:04] [V] [TRT] Tactic: 0xc82f3f06140e3cbb Time: 0.688064
[08/10/2023-11:17:04] [V] [TRT] Conv_17 + Relu_18 Set Tactic Name: ampere_h16816cudnn_64x128_sliced1x2_ldg8_relu_exp_stages_64x4_medium_nhwc_tn_v1 Tactic: 0xc8a90ff8898200c3
[08/10/2023-11:17:04] [V] [TRT] Tactic: 0xc8a90ff8898200c3 Time: 0.685047
[08/10/2023-11:17:04] [V] [TRT] Conv_17 + Relu_18 Set Tactic Name: ampere_h1688cudnn_256x64_sliced1x2_ldg8_relu_exp_medium_nhwc_tn_v1 Tactic: 0xc9cc55109bb4de26
[08/10/2023-11:17:04] [V] [TRT] Tactic: 0xc9cc55109bb4de26 Time: 0.811035
[08/10/2023-11:17:04] [V] [TRT] Conv_17 + Relu_18 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x64x32_stage5_warpsize2x2x1_g1_tensor16x8x16 Tactic: 0xc9d24bd069159fa8
[08/10/2023-11:17:04] [V] [TRT] Tactic: 0xc9d24bd069159fa8 Time: 0.58853
[08/10/2023-11:17:04] [V] [TRT] Conv_17 + Relu_18 Set Tactic Name: ampere_h16816cudnn_64x128_ldg8_relu_exp_stages_64x3_large_nhwc_tn_v1 Tactic: 0xc9f0a7bec963ba66
[08/10/2023-11:17:04] [V] [TRT] Tactic: 0xc9f0a7bec963ba66 Time: 0.598697
[08/10/2023-11:17:04] [V] [TRT] Conv_17 + Relu_18 Set Tactic Name: ampere_h16816cudnn_256x64_ldg8_relu_exp_small_nhwc_tn_v1 Tactic: 0xca5d3a11fd48f571
[08/10/2023-11:17:04] [V] [TRT] Tactic: 0xca5d3a11fd48f571 Time: 0.557879
[08/10/2023-11:17:04] [V] [TRT] Conv_17 + Relu_18 Set Tactic Name: ampere_h16816cudnn_64x64_ldg8_relu_exp_stages_64x5_small_nhwc_tn_v1 Tactic: 0xce0506e1512285c3
[08/10/2023-11:17:04] [V] [TRT] Tactic: 0xce0506e1512285c3 Time: 0.838816
[08/10/2023-11:17:04] [V] [TRT] Conv_17 + Relu_18 Set Tactic Name: ampere_h1688cudnn_256x64_sliced1x2_ldg8_relu_exp_large_nhwc_tn_v1 Tactic: 0xd0a3e0c815f7fb5e
[08/10/2023-11:17:04] [V] [TRT] Tactic: 0xd0a3e0c815f7fb5e Time: 0.816133
[08/10/2023-11:17:04] [V] [TRT] Conv_17 + Relu_18 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x64x32_stage3_warpsize4x1x1_g1_tensor16x8x16 Tactic: 0xd1aaad17ca35fbaa
[08/10/2023-11:17:04] [V] [TRT] Tactic: 0xd1aaad17ca35fbaa Time: 0.478715
[08/10/2023-11:17:04] [V] [TRT] Conv_17 + Relu_18 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x128x64_stage3_warpsize4x2x1_g1_tensor16x8x16_t1r3s3 Tactic: 0xd2fca0486ca97266
[08/10/2023-11:17:04] [V] [TRT] Tactic: 0xd2fca0486ca97266 Time: 0.505797
[08/10/2023-11:17:04] [V] [TRT] Conv_17 + Relu_18 Set Tactic Name: ampere_h16816cudnn_64x128_sliced1x2_ldg8_relu_exp_stages_64x3_small_nhwc_tn_v1 Tactic: 0xd58ea0bdedb89ead
[08/10/2023-11:17:04] [V] [TRT] Tactic: 0xd58ea0bdedb89ead Time: 0.566231
[08/10/2023-11:17:04] [V] [TRT] Conv_17 + Relu_18 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x256x64_stage3_warpsize1x4x1_g1_tensor16x8x16_t1r3s3 Tactic: 0xd7c261fa01db2af9
[08/10/2023-11:17:04] [V] [TRT] Tactic: 0xd7c261fa01db2af9 Time: 1.01243
[08/10/2023-11:17:04] [V] [TRT] Conv_17 + Relu_18 Set Tactic Name: ampere_h16816cudnn_256x64_sliced1x2_ldg8_relu_exp_large_nhwc_tn_v1 Tactic: 0xd8eb41ee35e76575
[08/10/2023-11:17:04] [V] [TRT] Tactic: 0xd8eb41ee35e76575 Time: 1.25663
[08/10/2023-11:17:04] [V] [TRT] Conv_17 + Relu_18 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x128x32_stage5_warpsize2x2x1_g1_tensor16x8x16 Tactic: 0xdb0b80f591d1bb6d
[08/10/2023-11:17:04] [V] [TRT] Tactic: 0xdb0b80f591d1bb6d Time: 0.566295
[08/10/2023-11:17:04] [V] [TRT] Conv_17 + Relu_18 Set Tactic Name: ampere_h16816cudnn_256x128_ldg8_relu_exp_large_nhwc_tn_v1 Tactic: 0xdc796d70e228a1d4
[08/10/2023-11:17:04] [V] [TRT] Tactic: 0xdc796d70e228a1d4 Time: 0.553774
[08/10/2023-11:17:04] [V] [TRT] Conv_17 + Relu_18 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x32x32_stage4_warpsize4x1x1_g1_tensor16x8x16 Tactic: 0xdce100b9fe609424
[08/10/2023-11:17:04] [V] [TRT] Tactic: 0xdce100b9fe609424 Time: 0.816635
[08/10/2023-11:17:04] [V] [TRT] Conv_17 + Relu_18 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x128x32_stage4_warpsize2x2x1_g1_tensor16x8x16 Tactic: 0xdfa020ef435ef810
[08/10/2023-11:17:04] [V] [TRT] Tactic: 0xdfa020ef435ef810 Time: 0.448709
[08/10/2023-11:17:04] [V] [TRT] Conv_17 + Relu_18 Set Tactic Name: ampere_h1688cudnn_256x64_ldg8_relu_exp_large_nhwc_tn_v1 Tactic: 0xe0e3c0e8cf9a2d9e
[08/10/2023-11:17:04] [V] [TRT] Tactic: 0xe0e3c0e8cf9a2d9e Time: 0.767392
[08/10/2023-11:17:04] [V] [TRT] Conv_17 + Relu_18 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x32x64_stage5_warpsize2x2x1_g1_tensor16x8x16 Tactic: 0xe1ff5ad20f5c6bf6
[08/10/2023-11:17:04] [V] [TRT] Tactic: 0xe1ff5ad20f5c6bf6 Time: 1.0853
[08/10/2023-11:17:04] [V] [TRT] Conv_17 + Relu_18 Set Tactic Name: ampere_h16816cudnn_256x64_ldg8_relu_exp_medium_nhwc_tn_v1 Tactic: 0xe4711898bd599c36
[08/10/2023-11:17:04] [V] [TRT] Tactic: 0xe4711898bd599c36 Time: 0.571589
[08/10/2023-11:17:04] [V] [TRT] Conv_17 + Relu_18 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x64x64_stage3_warpsize4x1x1_g1_tensor16x8x16 Tactic: 0xeb25062ffb9eae45
[08/10/2023-11:17:04] [V] [TRT] Tactic: 0xeb25062ffb9eae45 Time: 0.573787
[08/10/2023-11:17:04] [V] [TRT] Conv_17 + Relu_18 Set Tactic Name: ampere_h16816cudnn_128x64_sliced1x2_ldg8_relu_exp_stages_64x4_large_nhwc_tn_v1 Tactic: 0xeb2d2aa4e56bb41c
[08/10/2023-11:17:04] [V] [TRT] Tactic: 0xeb2d2aa4e56bb41c Time: 0.741417
[08/10/2023-11:17:04] [V] [TRT] Conv_17 + Relu_18 Set Tactic Name: ampere_h16816cudnn_64x64_ldg8_relu_exp_stages_64x6_small_nhwc_tn_v1 Tactic: 0xf0beb09df9a19f82
[08/10/2023-11:17:04] [V] [TRT] Tactic: 0xf0beb09df9a19f82 Time: 0.989851
[08/10/2023-11:17:04] [V] [TRT] Conv_17 + Relu_18 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x64x64_stage4_warpsize2x2x1_g1_tensor16x8x16 Tactic: 0xf35e0311fa1cc516
[08/10/2023-11:17:04] [V] [TRT] Tactic: 0xf35e0311fa1cc516 Time: 0.720229
[08/10/2023-11:17:04] [V] [TRT] Conv_17 + Relu_18 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x32x32_stage4_warpsize4x1x1_g1_tensor16x8x16 Tactic: 0xf79479a62ea9f901
[08/10/2023-11:17:04] [V] [TRT] Tactic: 0xf79479a62ea9f901 Time: 0.872841
[08/10/2023-11:17:04] [V] [TRT] Fastest Tactic: 0x60c3421152ef8e10 Time: 0.44613
[08/10/2023-11:17:04] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x60c3421152ef8e10
[08/10/2023-11:17:04] [V] [TRT] *************** Autotuning format combination: Half(259200,1:16,1440,8) -> Half(259200,1:16,1440,8) ***************
[08/10/2023-11:17:04] [W] [TRT] Weights [name=Conv_17 + Relu_18.weight] had the following issues when converted to FP16:
[08/10/2023-11:17:04] [W] [TRT]  - Subnormal FP16 values detected. 
[08/10/2023-11:17:04] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.
[08/10/2023-11:17:04] [V] [TRT] --------------- Timing Runner: Conv_17 + Relu_18 (CaskConvolution)
[08/10/2023-11:17:04] [V] [TRT] Conv_17 + Relu_18 Set Tactic Name: ampere_h16816cudnn_256x128_ldg8_relu_exp_medium_nhwc_tn_v1 Tactic: 0x03896956a39a1203
[08/10/2023-11:17:04] [V] [TRT] Tactic: 0x03896956a39a1203 Time: 0.541582
[08/10/2023-11:17:04] [V] [TRT] Conv_17 + Relu_18 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x128x32_stage3_warpsize4x2x1_g1_tensor16x8x16_t1r3s3 Tactic: 0x048d6d0400f33439
[08/10/2023-11:17:04] [V] [TRT] Tactic: 0x048d6d0400f33439 Time: 0.497778
[08/10/2023-11:17:04] [V] [TRT] Conv_17 + Relu_18 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x64x64_stage3_warpsize2x2x1_g1_tensor16x8x16_t1r3s3 Tactic: 0x05b6220f243edacd
[08/10/2023-11:17:04] [V] [TRT] Tactic: 0x05b6220f243edacd Time: 0.561239
[08/10/2023-11:17:04] [V] [TRT] Conv_17 + Relu_18 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x64x32_stage3_warpsize4x1x1_g1_tensor16x8x16_t1r3s3_aACCESS Tactic: 0x0866ddee325d07a6
[08/10/2023-11:17:04] [V] [TRT] Tactic: 0x0866ddee325d07a6 Time: 0.509051
[08/10/2023-11:17:04] [V] [TRT] Conv_17 + Relu_18 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x128x32_stage4_warpsize2x2x1_g1_tensor16x8x16 Tactic: 0x0e07ff4f4f7c1ac9
[08/10/2023-11:17:04] [V] [TRT] Tactic: 0x0e07ff4f4f7c1ac9 Time: 0.482578
[08/10/2023-11:17:04] [V] [TRT] Conv_17 + Relu_18 Set Tactic Name: ampere_h16816cudnn_256x128_ldg8_relu_exp_stages_64x3_small_nhwc_tn_v1 Tactic: 0x0e199750c30c6928
[08/10/2023-11:17:04] [V] [TRT] Tactic: 0x0e199750c30c6928 Time: 0.517422
[08/10/2023-11:17:04] [V] [TRT] Conv_17 + Relu_18 Set Tactic Name: ampere_h1688cudnn_128x128_ldg8_relu_exp_medium_nhwc_tn_v1 Tactic: 0x100c1ad308e08d35
[08/10/2023-11:17:04] [V] [TRT] Tactic: 0x100c1ad308e08d35 Time: 0.621961
[08/10/2023-11:17:04] [V] [TRT] Conv_17 + Relu_18 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x128x32_stage5_warpsize2x2x1_g1_tensor16x8x16_t1r3s3 Tactic: 0x17ebf0c9f418f10a
[08/10/2023-11:17:04] [V] [TRT] Tactic: 0x17ebf0c9f418f10a Time: 0.551442
[08/10/2023-11:17:04] [V] [TRT] Conv_17 + Relu_18 Set Tactic Name: ampere_h16816cudnn_128x64_ldg8_relu_exp_stages_64x4_medium_nhwc_tn_v1 Tactic: 0x196cbc423a9bb69e
[08/10/2023-11:17:04] [V] [TRT] Tactic: 0x196cbc423a9bb69e Time: 0.720827
[08/10/2023-11:17:04] [V] [TRT] Conv_17 + Relu_18 Set Tactic Name: ampere_h16816cudnn_128x64_sliced1x2_ldg8_relu_exp_stages_64x3_small_nhwc_tn_v1 Tactic: 0x19822de884f42a6a
[08/10/2023-11:17:04] [V] [TRT] Tactic: 0x19822de884f42a6a Time: 0.566048
[08/10/2023-11:17:04] [V] [TRT] Conv_17 + Relu_18 Set Tactic Name: ampere_h16816cudnn_64x64_sliced1x2_ldg8_relu_exp_stages_64x5_small_nhwc_tn_v1 Tactic: 0x1a5ba808ad4cc5a8
[08/10/2023-11:17:04] [V] [TRT] Tactic: 0x1a5ba808ad4cc5a8 Time: 0.759854
[08/10/2023-11:17:04] [V] [TRT] Conv_17 + Relu_18 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x64x64_stage3_warpsize2x2x1_g1_tensor16x8x16 Tactic: 0x21b295c0c8f6c95a
[08/10/2023-11:17:04] [V] [TRT] Tactic: 0x21b295c0c8f6c95a Time: 0.578418
[08/10/2023-11:17:04] [V] [TRT] Conv_17 + Relu_18 Set Tactic Name: ampere_h16816cudnn_128x128_ldg8_relu_exp_medium_nhwc_tn_v1 Tactic: 0x234580e8a194335c
[08/10/2023-11:17:04] [V] [TRT] Tactic: 0x234580e8a194335c Time: 0.485422
[08/10/2023-11:17:04] [V] [TRT] Conv_17 + Relu_18 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x128x32_stage4_warpsize2x2x1_g1_tensor16x8x16_t1r3s3_aACCESS Tactic: 0x245530c34bd6090f
[08/10/2023-11:17:04] [V] [TRT] Tactic: 0x245530c34bd6090f Time: 0.465632
[08/10/2023-11:17:04] [V] [TRT] Conv_17 + Relu_18 Set Tactic Name: ampere_h16816cudnn_64x64_sliced1x2_ldg8_relu_exp_stages_64x6_small_nhwc_tn_v1 Tactic: 0x24e01e7405cfdfe9
[08/10/2023-11:17:04] [V] [TRT] Tactic: 0x24e01e7405cfdfe9 Time: 0.995141
[08/10/2023-11:17:04] [V] [TRT] Conv_17 + Relu_18 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x64x64_stage3_warpsize2x2x1_g1_tensor16x8x16_t1r3s3_aACCESS Tactic: 0x263a38afd75e3a43
[08/10/2023-11:17:04] [V] [TRT] Tactic: 0x263a38afd75e3a43 Time: 0.543387
[08/10/2023-11:17:04] [V] [TRT] Conv_17 + Relu_18 Set Tactic Name: ampere_h16816cudnn_64x128_sliced1x2_ldg8_relu_exp_stages_64x4_large_nhwc_tn_v1 Tactic: 0x2721a7f18c2700db
[08/10/2023-11:17:04] [V] [TRT] Tactic: 0x2721a7f18c2700db Time: 0.717376
[08/10/2023-11:17:04] [V] [TRT] Conv_17 + Relu_18 Set Tactic Name: ampere_h16816cudnn_128x64_sliced1x2_ldg8_relu_exp_stages_64x3_medium_nhwc_tn_v1 Tactic: 0x29329b5741ea05f2
[08/10/2023-11:17:04] [V] [TRT] Tactic: 0x29329b5741ea05f2 Time: 0.570423
[08/10/2023-11:17:04] [V] [TRT] Conv_17 + Relu_18 Set Tactic Name: ampere_h16816cudnn_128x128_ldg8_relu_exp_large_nhwc_tn_v1 Tactic: 0x2970ae65966d569d
[08/10/2023-11:17:04] [V] [TRT] Tactic: 0x2970ae65966d569d Time: 0.47808
[08/10/2023-11:17:04] [V] [TRT] Conv_17 + Relu_18 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x256x64_stage3_warpsize1x4x1_g1_tensor16x8x16 Tactic: 0x29dc14520e0e4eb6
[08/10/2023-11:17:04] [V] [TRT] Tactic: 0x29dc14520e0e4eb6 Time: 1.00342
[08/10/2023-11:17:04] [V] [TRT] Conv_17 + Relu_18 Set Tactic Name: ampere_h16816cudnn_256x64_ldg8_relu_exp_stages_64x3_large_nhwc_tn_v1 Tactic: 0x2cb4a662694e89d3
[08/10/2023-11:17:04] [V] [TRT] Tactic: 0x2cb4a662694e89d3 Time: 0.575163
[08/10/2023-11:17:04] [V] [TRT] Conv_17 + Relu_18 Set Tactic Name: ampere_h16816cudnn_128x128_ldg8_relu_exp_stages_64x3_medium_nhwc_tn_v1 Tactic: 0x2eaa2202de9404d6
[08/10/2023-11:17:04] [V] [TRT] Tactic: 0x2eaa2202de9404d6 Time: 0.555822
[08/10/2023-11:17:04] [V] [TRT] Conv_17 + Relu_18 Set Tactic Name: ampere_h1688cudnn_256x128_ldg8_relu_exp_medium_nhwc_tn_v1 Tactic: 0x30c0f36d0aeeac6a
[08/10/2023-11:17:04] [V] [TRT] Tactic: 0x30c0f36d0aeeac6a Time: 0.649559
[08/10/2023-11:17:04] [V] [TRT] Conv_17 + Relu_18 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x64x64_stage4_warpsize2x2x1_g1_tensor16x8x16_t1r3s3_aACCESS Tactic: 0x30e8a8d7a953e5e9
[08/10/2023-11:17:04] [V] [TRT] Tactic: 0x30e8a8d7a953e5e9 Time: 0.633481
[08/10/2023-11:17:04] [V] [TRT] Conv_17 + Relu_18 Set Tactic Name: ampere_h16816cudnn_128x64_ldg8_relu_exp_stages_64x4_large_nhwc_tn_v1 Tactic: 0x3197d5044d71e98e
[08/10/2023-11:17:04] [V] [TRT] Tactic: 0x3197d5044d71e98e Time: 0.740517
[08/10/2023-11:17:04] [V] [TRT] Conv_17 + Relu_18 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x64x64_stage4_warpsize2x2x1_g1_tensor16x8x16_t1r3s3 Tactic: 0x31d93dc22d2af081
[08/10/2023-11:17:04] [V] [TRT] Tactic: 0x31d93dc22d2af081 Time: 0.714368
[08/10/2023-11:17:04] [V] [TRT] Conv_17 + Relu_18 Set Tactic Name: ampere_h1688cudnn_128x128_ldg8_relu_exp_small_nhwc_tn_v1 Tactic: 0x3369260c04f9ad73
[08/10/2023-11:17:04] [V] [TRT] Tactic: 0x3369260c04f9ad73 Time: 0.592763
[08/10/2023-11:17:04] [V] [TRT] Conv_17 + Relu_18 Set Tactic Name: ampere_h16816cudnn_64x128_ldg8_relu_exp_stages_64x4_small_nhwc_tn_v1 Tactic: 0x3b5fa0f2a8fc2410
[08/10/2023-11:17:04] [V] [TRT] Tactic: 0x3b5fa0f2a8fc2410 Time: 0.676677
[08/10/2023-11:17:04] [V] [TRT] Conv_17 + Relu_18 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x32x32_stage4_warpsize4x1x1_g1_tensor16x8x16_t1r3s3 Tactic: 0x3e7eb35b91b9fa63
[08/10/2023-11:17:04] [V] [TRT] Tactic: 0x3e7eb35b91b9fa63 Time: 0.867008
[08/10/2023-11:17:04] [V] [TRT] Conv_17 + Relu_18 Set Tactic Name: ampere_h16816cudnn_256x64_ldg8_relu_exp_stages_64x3_medium_nhwc_tn_v1 Tactic: 0x3f031df0e579d52c
[08/10/2023-11:17:04] [V] [TRT] Tactic: 0x3f031df0e579d52c Time: 0.572005
[08/10/2023-11:17:04] [V] [TRT] Conv_17 + Relu_18 Set Tactic Name: ampere_h16816cudnn_256x64_sliced1x2_ldg8_relu_exp_small_nhwc_tn_v1 Tactic: 0x42a5b8709d0039be
[08/10/2023-11:17:04] [V] [TRT] Tactic: 0x42a5b8709d0039be Time: 0.708421
[08/10/2023-11:17:04] [V] [TRT] Conv_17 + Relu_18 Set Tactic Name: ampere_h16816cudnn_256x128_ldg8_relu_exp_small_nhwc_tn_v1 Tactic: 0x463794ee4acffd1f
[08/10/2023-11:17:04] [V] [TRT] Tactic: 0x463794ee4acffd1f Time: 0.538962
[08/10/2023-11:17:04] [V] [TRT] Conv_17 + Relu_18 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x64x64_stage4_warpsize2x2x1_g1_tensor16x8x16 Tactic: 0x4a81ea1e51436a30
[08/10/2023-11:17:04] [V] [TRT] Tactic: 0x4a81ea1e51436a30 Time: 0.73397
[08/10/2023-11:17:04] [V] [TRT] Conv_17 + Relu_18 Set Tactic Name: ampere_h1688cudnn_256x64_sliced1x2_ldg8_relu_exp_small_nhwc_tn_v1 Tactic: 0x4aed1956bd10a795
[08/10/2023-11:17:05] [V] [TRT] Tactic: 0x4aed1956bd10a795 Time: 0.772901
[08/10/2023-11:17:05] [V] [TRT] Conv_17 + Relu_18 Set Tactic Name: ampere_h16816cudnn_64x128_sliced1x2_ldg8_relu_exp_stages_64x3_large_nhwc_tn_v1 Tactic: 0x4fc05923455fc266
[08/10/2023-11:17:05] [V] [TRT] Tactic: 0x4fc05923455fc266 Time: 0.581198
[08/10/2023-11:17:05] [V] [TRT] Conv_17 + Relu_18 Set Tactic Name: ampere_h16816cudnn_256x64_ldg8_relu_exp_large_nhwc_tn_v1 Tactic: 0x5013c38f55afa9ba
[08/10/2023-11:17:05] [V] [TRT] Tactic: 0x5013c38f55afa9ba Time: 0.591195
[08/10/2023-11:17:05] [V] [TRT] Conv_17 + Relu_18 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x64x32_stage3_warpsize4x1x1_g1_tensor16x8x16_t1r3s3 Tactic: 0x529f4431bdae94f5
[08/10/2023-11:17:05] [V] [TRT] Tactic: 0x529f4431bdae94f5 Time: 0.491511
[08/10/2023-11:17:05] [V] [TRT] Conv_17 + Relu_18 Set Tactic Name: ampere_h16816cudnn_64x128_ldg8_relu_exp_stages_64x3_small_nhwc_tn_v1 Tactic: 0x53be5e206184e6ad
[08/10/2023-11:17:05] [V] [TRT] Tactic: 0x53be5e206184e6ad Time: 0.613179
[08/10/2023-11:17:05] [V] [TRT] Conv_17 + Relu_18 Set Tactic Name: ampere_h16816cudnn_64x64_ldg8_relu_exp_stages_64x5_large_nhwc_tn_v1 Tactic: 0x544bff7ff9c5d908
[08/10/2023-11:17:05] [V] [TRT] Tactic: 0x544bff7ff9c5d908 Time: 0.82949
[08/10/2023-11:17:05] [V] [TRT] Conv_17 + Relu_18 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x64x64_stage3_warpsize2x2x1_g1_tensor16x8x16 Tactic: 0x5568fd8a32f4a40f
[08/10/2023-11:17:05] [V] [TRT] Tactic: 0x5568fd8a32f4a40f Time: 0.564768
[08/10/2023-11:17:05] [V] [TRT] Conv_17 + Relu_18 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x128x64_stage3_warpsize2x2x1_g1_tensor16x8x16 Tactic: 0x57c9a5ff682354a6
[08/10/2023-11:17:05] [V] [TRT] Tactic: 0x57c9a5ff682354a6 Time: 0.528782
[08/10/2023-11:17:05] [V] [TRT] Conv_17 + Relu_18 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x32x64_stage5_warpsize2x2x1_g1_tensor16x8x16 Tactic: 0x5820b3dda403c4d0
[08/10/2023-11:17:05] [V] [TRT] Tactic: 0x5820b3dda403c4d0 Time: 1.05172
[08/10/2023-11:17:05] [V] [TRT] Conv_17 + Relu_18 Set Tactic Name: ampere_h16816cudnn_128x128_ldg8_relu_exp_stages_64x3_small_nhwc_tn_v1 Tactic: 0x58816bf2e9c36afb
[08/10/2023-11:17:05] [V] [TRT] Tactic: 0x58816bf2e9c36afb Time: 0.553687
[08/10/2023-11:17:05] [V] [TRT] Conv_17 + Relu_18 Set Tactic Name: ampere_h16816cudnn_128x64_ldg8_relu_exp_stages_64x3_large_nhwc_tn_v1 Tactic: 0x59762bd684092b33
[08/10/2023-11:17:05] [V] [TRT] Tactic: 0x59762bd684092b33 Time: 0.600864
[08/10/2023-11:17:05] [V] [TRT] Conv_17 + Relu_18 Set Tactic Name: ampere_h16816cudnn_128x64_sliced1x2_ldg8_relu_exp_stages_64x4_medium_nhwc_tn_v1 Tactic: 0x59c5c647b4c76593
[08/10/2023-11:17:05] [V] [TRT] Tactic: 0x59c5c647b4c76593 Time: 0.71659
[08/10/2023-11:17:05] [V] [TRT] Conv_17 + Relu_18 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x128x64_stage3_warpsize4x2x1_g1_tensor16x8x16 Tactic: 0x5a55274960724ba8
[08/10/2023-11:17:05] [V] [TRT] Tactic: 0x5a55274960724ba8 Time: 0.51109
[08/10/2023-11:17:05] [V] [TRT] Conv_17 + Relu_18 Set Tactic Name: ampere_h1688cudnn_256x128_ldg8_relu_exp_large_nhwc_tn_v1 Tactic: 0x5c2e1c87d85b06f1
[08/10/2023-11:17:05] [V] [TRT] Tactic: 0x5c2e1c87d85b06f1 Time: 0.660059
[08/10/2023-11:17:05] [V] [TRT] Conv_17 + Relu_18 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x256x64_stage3_warpsize1x4x1_g1_tensor16x8x16 Tactic: 0x5d067c18f40c23e3
[08/10/2023-11:17:05] [V] [TRT] Tactic: 0x5d067c18f40c23e3 Time: 1.02899
[08/10/2023-11:17:05] [V] [TRT] Conv_17 + Relu_18 Set Tactic Name: ampere_h16816cudnn_64x64_ldg8_relu_exp_stages_64x5_medium_nhwc_tn_v1 Tactic: 0x5f31c22ec167f384
[08/10/2023-11:17:05] [V] [TRT] Tactic: 0x5f31c22ec167f384 Time: 0.846542
[08/10/2023-11:17:05] [V] [TRT] Conv_17 + Relu_18 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x128x32_stage4_warpsize2x2x1_g1_tensor16x8x16_t1r3s3 Tactic: 0x60c3421152ef8e10
[08/10/2023-11:17:05] [V] [TRT] Tactic: 0x60c3421152ef8e10 Time: 0.446537
[08/10/2023-11:17:05] [V] [TRT] Conv_17 + Relu_18 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x128x32_stage3_warpsize4x2x1_g1_tensor16x8x16 Tactic: 0x60da8c7151d91e47
[08/10/2023-11:17:05] [V] [TRT] Tactic: 0x60da8c7151d91e47 Time: 0.497513
[08/10/2023-11:17:05] [V] [TRT] Conv_17 + Relu_18 Set Tactic Name: ampere_h1688cudnn_256x64_ldg8_relu_exp_medium_nhwc_tn_v1 Tactic: 0x6426696f872a3b13
[08/10/2023-11:17:05] [V] [TRT] Tactic: 0x6426696f872a3b13 Time: 0.764663
[08/10/2023-11:17:05] [V] [TRT] Conv_17 + Relu_18 Set Tactic Name: ampere_h16816cudnn_128x64_ldg8_relu_exp_stages_64x3_medium_nhwc_tn_v1 Tactic: 0x699be152cfb6d6ff
[08/10/2023-11:17:05] [V] [TRT] Tactic: 0x699be152cfb6d6ff Time: 0.596878
[08/10/2023-11:17:05] [V] [TRT] Conv_17 + Relu_18 Set Tactic Name: ampere_h16816cudnn_64x64_ldg8_relu_exp_stages_64x6_large_nhwc_tn_v1 Tactic: 0x6af049035146c349
[08/10/2023-11:17:05] [V] [TRT] Tactic: 0x6af049035146c349 Time: 1.07219
[08/10/2023-11:17:05] [V] [TRT] Conv_17 + Relu_18 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x256x32_stage3_warpsize2x4x1_g1_tensor16x8x16 Tactic: 0x7005d10718f6c22d
[08/10/2023-11:17:05] [V] [TRT] Tactic: 0x7005d10718f6c22d Time: 0.921179
[08/10/2023-11:17:05] [V] [TRT] Conv_17 + Relu_18 Set Tactic Name: ampere_h16816cudnn_64x64_sliced1x2_ldg8_relu_exp_stages_64x5_medium_nhwc_tn_v1 Tactic: 0x706f08da35c795a5
[08/10/2023-11:17:05] [V] [TRT] Tactic: 0x706f08da35c795a5 Time: 0.772306
[08/10/2023-11:17:05] [V] [TRT] Conv_17 + Relu_18 Set Tactic Name: ampere_h16816cudnn_128x64_sliced1x2_ldg8_relu_exp_stages_64x4_small_nhwc_tn_v1 Tactic: 0x7163d33a4d8ce8d7
[08/10/2023-11:17:05] [V] [TRT] Tactic: 0x7163d33a4d8ce8d7 Time: 0.688155
[08/10/2023-11:17:05] [V] [TRT] Conv_17 + Relu_18 Set Tactic Name: ampere_h1688cudnn_256x64_ldg8_relu_exp_small_nhwc_tn_v1 Tactic: 0x7aad3976677d7155
[08/10/2023-11:17:05] [V] [TRT] Tactic: 0x7aad3976677d7155 Time: 0.722039
[08/10/2023-11:17:05] [V] [TRT] Conv_17 + Relu_18 Set Tactic Name: ampere_h16816cudnn_64x64_sliced1x2_ldg8_relu_exp_stages_64x5_large_nhwc_tn_v1 Tactic: 0x8015519605ab9963
[08/10/2023-11:17:05] [V] [TRT] Tactic: 0x8015519605ab9963 Time: 0.770277
[08/10/2023-11:17:05] [V] [TRT] Conv_17 + Relu_18 Set Tactic Name: ampere_h16816cudnn_256x64_sliced1x2_ldg8_relu_exp_medium_nhwc_tn_v1 Tactic: 0x82f8a2214b0b4178
[08/10/2023-11:17:05] [V] [TRT] Tactic: 0x82f8a2214b0b4178 Time: 0.803685
[08/10/2023-11:17:05] [V] [TRT] Conv_17 + Relu_18 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x32x32_stage4_warpsize4x1x1_g1_tensor16x8x16 Tactic: 0x834e11ecd4ab9454
[08/10/2023-11:17:05] [V] [TRT] Tactic: 0x834e11ecd4ab9454 Time: 0.889335
[08/10/2023-11:17:05] [V] [TRT] Conv_17 + Relu_18 Set Tactic Name: ampere_h16816cudnn_128x64_sliced1x2_ldg8_relu_exp_stages_64x3_large_nhwc_tn_v1 Tactic: 0x83ccd4762c1376a1
[08/10/2023-11:17:05] [V] [TRT] Tactic: 0x83ccd4762c1376a1 Time: 0.586066
[08/10/2023-11:17:05] [V] [TRT] Conv_17 + Relu_18 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x64x64_stage3_warpsize4x1x1_g1_tensor16x8x16_t1r3s3 Tactic: 0x84ee897dd1f4d2aa
[08/10/2023-11:17:05] [V] [TRT] Tactic: 0x84ee897dd1f4d2aa Time: 0.574848
[08/10/2023-11:17:05] [V] [TRT] Conv_17 + Relu_18 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x128x64_stage3_warpsize2x2x1_g1_tensor16x8x16 Tactic: 0x866e7a5f6401b67f
[08/10/2023-11:17:05] [V] [TRT] Tactic: 0x866e7a5f6401b67f Time: 0.526149
[08/10/2023-11:17:05] [V] [TRT] Conv_17 + Relu_18 Set Tactic Name: ampere_h16816cudnn_64x64_ldg8_relu_exp_stages_64x6_medium_nhwc_tn_v1 Tactic: 0x88971eec55aba850
[08/10/2023-11:17:05] [V] [TRT] Tactic: 0x88971eec55aba850 Time: 1.05906
[08/10/2023-11:17:05] [V] [TRT] Conv_17 + Relu_18 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x128x64_stage3_warpsize4x2x1_g1_tensor16x8x16 Tactic: 0x8bf2f8e96c50a971
[08/10/2023-11:17:05] [V] [TRT] Tactic: 0x8bf2f8e96c50a971 Time: 0.503634
[08/10/2023-11:17:05] [V] [TRT] Conv_17 + Relu_18 Set Tactic Name: ampere_h16816cudnn_256x128_ldg8_relu_exp_stages_64x3_large_nhwc_tn_v1 Tactic: 0x94576ece6beb35e3
[08/10/2023-11:17:05] [V] [TRT] Tactic: 0x94576ece6beb35e3 Time: 0.524251
[08/10/2023-11:17:05] [V] [TRT] Conv_17 + Relu_18 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x64x64_stage3_warpsize4x1x1_g1_tensor16x8x16 Tactic: 0x9fff6e65019cc310
[08/10/2023-11:17:05] [V] [TRT] Tactic: 0x9fff6e65019cc310 Time: 0.564626
[08/10/2023-11:17:05] [V] [TRT] Conv_17 + Relu_18 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x32x64_stage5_warpsize2x2x1_g1_tensor16x8x16_t1r3s3 Tactic: 0xa033e20ae9f412b2
[08/10/2023-11:17:05] [V] [TRT] Tactic: 0xa033e20ae9f412b2 Time: 1.04196
[08/10/2023-11:17:05] [V] [TRT] Conv_17 + Relu_18 Set Tactic Name: ampere_h16816cudnn_64x128_ldg8_relu_exp_stages_64x4_large_nhwc_tn_v1 Tactic: 0xa111596c001b78db
[08/10/2023-11:17:05] [V] [TRT] Tactic: 0xa111596c001b78db Time: 0.693463
[08/10/2023-11:17:05] [V] [TRT] Conv_17 + Relu_18 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x256x32_stage3_warpsize2x4x1_g1_tensor16x8x16 Tactic: 0xa1a20ea714d420f4
[08/10/2023-11:17:05] [V] [TRT] Tactic: 0xa1a20ea714d420f4 Time: 0.909499
[08/10/2023-11:17:05] [V] [TRT] Conv_17 + Relu_18 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x32x32_stage4_warpsize4x1x1_g1_tensor16x8x16_t1r3s3 Tactic: 0xa40cb43c296a36a8
[08/10/2023-11:17:05] [V] [TRT] Tactic: 0xa40cb43c296a36a8 Time: 0.788123
[08/10/2023-11:17:05] [V] [TRT] Conv_17 + Relu_18 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x64x32_stage3_warpsize4x1x1_g1_tensor16x8x16 Tactic: 0xa570c55d303796ff
[08/10/2023-11:17:05] [V] [TRT] Tactic: 0xa570c55d303796ff Time: 0.506002
[08/10/2023-11:17:05] [V] [TRT] Conv_17 + Relu_18 Set Tactic Name: ampere_h16816cudnn_64x64_sliced1x2_ldg8_relu_exp_stages_64x6_medium_nhwc_tn_v1 Tactic: 0xa7c9d418a10bce71
[08/10/2023-11:17:05] [V] [TRT] Tactic: 0xa7c9d418a10bce71 Time: 1.05936
[08/10/2023-11:17:05] [V] [TRT] Conv_17 + Relu_18 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x32x32_stage4_warpsize4x1x1_g1_tensor16x8x16 Tactic: 0xa83b68f30462f971
[08/10/2023-11:17:05] [V] [TRT] Tactic: 0xa83b68f30462f971 Time: 0.796466
[08/10/2023-11:17:05] [V] [TRT] Conv_17 + Relu_18 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x256x32_stage3_warpsize2x4x1_g1_tensor16x8x16_t1r3s3 Tactic: 0xa9177bbe4e767df8
[08/10/2023-11:17:05] [V] [TRT] Tactic: 0xa9177bbe4e767df8 Time: 0.910331
[08/10/2023-11:17:05] [V] [TRT] Conv_17 + Relu_18 Set Tactic Name: ampere_h1688cudnn_128x128_ldg8_relu_exp_large_nhwc_tn_v1 Tactic: 0xa927df92ac1ef1b8
[08/10/2023-11:17:05] [V] [TRT] Tactic: 0xa927df92ac1ef1b8 Time: 0.622985
[08/10/2023-11:17:05] [V] [TRT] Conv_17 + Relu_18 Set Tactic Name: ampere_h16816cudnn_128x64_ldg8_relu_exp_stages_64x4_small_nhwc_tn_v1 Tactic: 0xabd92c9ae596b545
[08/10/2023-11:17:05] [V] [TRT] Tactic: 0xabd92c9ae596b545 Time: 0.712448
[08/10/2023-11:17:05] [V] [TRT] Conv_17 + Relu_18 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x128x32_stage5_warpsize2x2x1_g1_tensor16x8x16 Tactic: 0xafd1e8bf6bd3d638
[08/10/2023-11:17:05] [V] [TRT] Tactic: 0xafd1e8bf6bd3d638 Time: 0.552087
[08/10/2023-11:17:05] [V] [TRT] Conv_17 + Relu_18 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x128x32_stage3_warpsize4x2x1_g1_tensor16x8x16 Tactic: 0xb17d53d15dfbfc9e
[08/10/2023-11:17:05] [V] [TRT] Tactic: 0xb17d53d15dfbfc9e Time: 0.522482
[08/10/2023-11:17:05] [V] [TRT] Conv_17 + Relu_18 Set Tactic Name: ampere_h16816cudnn_128x128_ldg8_relu_exp_small_nhwc_tn_v1 Tactic: 0xb33e57fb3e8a0a56
[08/10/2023-11:17:05] [V] [TRT] Tactic: 0xb33e57fb3e8a0a56 Time: 0.482491
[08/10/2023-11:17:05] [V] [TRT] Conv_17 + Relu_18 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x64x32_stage5_warpsize2x2x1_g1_tensor16x8x16_t1r3s3 Tactic: 0xb4bec086187edcfc
[08/10/2023-11:17:05] [V] [TRT] Tactic: 0xb4bec086187edcfc Time: 0.567511
[08/10/2023-11:17:05] [V] [TRT] Conv_17 + Relu_18 Set Tactic Name: ampere_h16816cudnn_256x64_ldg8_relu_exp_stages_64x3_small_nhwc_tn_v1 Tactic: 0xb6fa5ffcc1a9d518
[08/10/2023-11:17:05] [V] [TRT] Tactic: 0xb6fa5ffcc1a9d518 Time: 0.577728
[08/10/2023-11:17:05] [V] [TRT] Conv_17 + Relu_18 Set Tactic Name: ampere_h16816cudnn_64x128_sliced1x2_ldg8_relu_exp_stages_64x3_medium_nhwc_tn_v1 Tactic: 0xb85e52e87caf60a2
[08/10/2023-11:17:05] [V] [TRT] Tactic: 0xb85e52e87caf60a2 Time: 0.581385
[08/10/2023-11:17:05] [V] [TRT] Conv_17 + Relu_18 Set Tactic Name: ampere_h16816cudnn_64x128_ldg8_relu_exp_stages_64x3_medium_nhwc_tn_v1 Tactic: 0xb8d86216e1235cda
[08/10/2023-11:17:05] [V] [TRT] Tactic: 0xb8d86216e1235cda Time: 0.611822
[08/10/2023-11:17:05] [V] [TRT] Conv_17 + Relu_18 Set Tactic Name: ampere_h16816cudnn_256x128_ldg8_relu_exp_stages_64x3_medium_nhwc_tn_v1 Tactic: 0xb9171d70ba2eda4b
[08/10/2023-11:17:05] [V] [TRT] Tactic: 0xb9171d70ba2eda4b Time: 0.527223
[08/10/2023-11:17:05] [V] [TRT] Conv_17 + Relu_18 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x64x32_stage5_warpsize2x2x1_g1_tensor16x8x16 Tactic: 0xbd08239a9317f2fd
[08/10/2023-11:17:05] [V] [TRT] Tactic: 0xbd08239a9317f2fd Time: 0.563611
[08/10/2023-11:17:05] [V] [TRT] Conv_17 + Relu_18 Set Tactic Name: ampere_h16816cudnn_64x128_sliced1x2_ldg8_relu_exp_stages_64x4_small_nhwc_tn_v1 Tactic: 0xbd6f5e6f24c05c10
[08/10/2023-11:17:05] [V] [TRT] Tactic: 0xbd6f5e6f24c05c10 Time: 0.687968
[08/10/2023-11:17:05] [V] [TRT] Conv_17 + Relu_18 Set Tactic Name: ampere_h16816cudnn_64x64_sliced1x2_ldg8_relu_exp_stages_64x6_large_nhwc_tn_v1 Tactic: 0xbeaee7eaad288322
[08/10/2023-11:17:05] [V] [TRT] Tactic: 0xbeaee7eaad288322 Time: 1.0686
[08/10/2023-11:17:05] [V] [TRT] Conv_17 + Relu_18 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x128x64_stage3_warpsize2x2x1_g1_tensor16x8x16_t1r3s3 Tactic: 0xc0a02dc6095497cc
[08/10/2023-11:17:05] [V] [TRT] Tactic: 0xc0a02dc6095497cc Time: 0.519675
[08/10/2023-11:17:05] [V] [TRT] Conv_17 + Relu_18 Set Tactic Name: ampere_h16816cudnn_128x128_ldg8_relu_exp_stages_64x3_large_nhwc_tn_v1 Tactic: 0xc2cf926c41243630
[08/10/2023-11:17:05] [V] [TRT] Tactic: 0xc2cf926c41243630 Time: 0.556887
[08/10/2023-11:17:05] [V] [TRT] Conv_17 + Relu_18 Set Tactic Name: ampere_h16816cudnn_128x64_ldg8_relu_exp_stages_64x3_small_nhwc_tn_v1 Tactic: 0xc338d2482cee77f8
[08/10/2023-11:17:05] [V] [TRT] Tactic: 0xc338d2482cee77f8 Time: 0.597467
[08/10/2023-11:17:05] [V] [TRT] Conv_17 + Relu_18 Set Tactic Name: ampere_h1688cudnn_256x128_ldg8_relu_exp_small_nhwc_tn_v1 Tactic: 0xc660e51970bc5a3a
[08/10/2023-11:17:05] [V] [TRT] Tactic: 0xc660e51970bc5a3a Time: 0.609408
[08/10/2023-11:17:05] [V] [TRT] Conv_17 + Relu_18 Set Tactic Name: ampere_h16816cudnn_64x128_ldg8_relu_exp_stages_64x4_medium_nhwc_tn_v1 Tactic: 0xc82f3f06140e3cbb
[08/10/2023-11:17:05] [V] [TRT] Tactic: 0xc82f3f06140e3cbb Time: 0.684992
[08/10/2023-11:17:05] [V] [TRT] Conv_17 + Relu_18 Set Tactic Name: ampere_h16816cudnn_64x128_sliced1x2_ldg8_relu_exp_stages_64x4_medium_nhwc_tn_v1 Tactic: 0xc8a90ff8898200c3
[08/10/2023-11:17:05] [V] [TRT] Tactic: 0xc8a90ff8898200c3 Time: 0.686464
[08/10/2023-11:17:05] [V] [TRT] Conv_17 + Relu_18 Set Tactic Name: ampere_h1688cudnn_256x64_sliced1x2_ldg8_relu_exp_medium_nhwc_tn_v1 Tactic: 0xc9cc55109bb4de26
[08/10/2023-11:17:05] [V] [TRT] Tactic: 0xc9cc55109bb4de26 Time: 0.81312
[08/10/2023-11:17:05] [V] [TRT] Conv_17 + Relu_18 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x64x32_stage5_warpsize2x2x1_g1_tensor16x8x16 Tactic: 0xc9d24bd069159fa8
[08/10/2023-11:17:05] [V] [TRT] Tactic: 0xc9d24bd069159fa8 Time: 0.590798
[08/10/2023-11:17:05] [V] [TRT] Conv_17 + Relu_18 Set Tactic Name: ampere_h16816cudnn_64x128_ldg8_relu_exp_stages_64x3_large_nhwc_tn_v1 Tactic: 0xc9f0a7bec963ba66
[08/10/2023-11:17:05] [V] [TRT] Tactic: 0xc9f0a7bec963ba66 Time: 0.599104
[08/10/2023-11:17:05] [V] [TRT] Conv_17 + Relu_18 Set Tactic Name: ampere_h16816cudnn_256x64_ldg8_relu_exp_small_nhwc_tn_v1 Tactic: 0xca5d3a11fd48f571
[08/10/2023-11:17:05] [V] [TRT] Tactic: 0xca5d3a11fd48f571 Time: 0.557591
[08/10/2023-11:17:05] [V] [TRT] Conv_17 + Relu_18 Set Tactic Name: ampere_h16816cudnn_64x64_ldg8_relu_exp_stages_64x5_small_nhwc_tn_v1 Tactic: 0xce0506e1512285c3
[08/10/2023-11:17:05] [V] [TRT] Tactic: 0xce0506e1512285c3 Time: 0.840987
[08/10/2023-11:17:05] [V] [TRT] Conv_17 + Relu_18 Set Tactic Name: ampere_h1688cudnn_256x64_sliced1x2_ldg8_relu_exp_large_nhwc_tn_v1 Tactic: 0xd0a3e0c815f7fb5e
[08/10/2023-11:17:05] [V] [TRT] Tactic: 0xd0a3e0c815f7fb5e Time: 0.816709
[08/10/2023-11:17:05] [V] [TRT] Conv_17 + Relu_18 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x64x32_stage3_warpsize4x1x1_g1_tensor16x8x16 Tactic: 0xd1aaad17ca35fbaa
[08/10/2023-11:17:05] [V] [TRT] Tactic: 0xd1aaad17ca35fbaa Time: 0.478798
[08/10/2023-11:17:05] [V] [TRT] Conv_17 + Relu_18 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x128x64_stage3_warpsize4x2x1_g1_tensor16x8x16_t1r3s3 Tactic: 0xd2fca0486ca97266
[08/10/2023-11:17:05] [V] [TRT] Tactic: 0xd2fca0486ca97266 Time: 0.505847
[08/10/2023-11:17:05] [V] [TRT] Conv_17 + Relu_18 Set Tactic Name: ampere_h16816cudnn_64x128_sliced1x2_ldg8_relu_exp_stages_64x3_small_nhwc_tn_v1 Tactic: 0xd58ea0bdedb89ead
[08/10/2023-11:17:05] [V] [TRT] Tactic: 0xd58ea0bdedb89ead Time: 0.566482
[08/10/2023-11:17:05] [V] [TRT] Conv_17 + Relu_18 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x256x64_stage3_warpsize1x4x1_g1_tensor16x8x16_t1r3s3 Tactic: 0xd7c261fa01db2af9
[08/10/2023-11:17:05] [V] [TRT] Tactic: 0xd7c261fa01db2af9 Time: 1.0018
[08/10/2023-11:17:05] [V] [TRT] Conv_17 + Relu_18 Set Tactic Name: ampere_h16816cudnn_256x64_sliced1x2_ldg8_relu_exp_large_nhwc_tn_v1 Tactic: 0xd8eb41ee35e76575
[08/10/2023-11:17:05] [V] [TRT] Tactic: 0xd8eb41ee35e76575 Time: 0.788165
[08/10/2023-11:17:05] [V] [TRT] Conv_17 + Relu_18 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x128x32_stage5_warpsize2x2x1_g1_tensor16x8x16 Tactic: 0xdb0b80f591d1bb6d
[08/10/2023-11:17:05] [V] [TRT] Tactic: 0xdb0b80f591d1bb6d Time: 0.552315
[08/10/2023-11:17:05] [V] [TRT] Conv_17 + Relu_18 Set Tactic Name: ampere_h16816cudnn_256x128_ldg8_relu_exp_large_nhwc_tn_v1 Tactic: 0xdc796d70e228a1d4
[08/10/2023-11:17:05] [V] [TRT] Tactic: 0xdc796d70e228a1d4 Time: 0.541298
[08/10/2023-11:17:05] [V] [TRT] Conv_17 + Relu_18 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x32x32_stage4_warpsize4x1x1_g1_tensor16x8x16 Tactic: 0xdce100b9fe609424
[08/10/2023-11:17:05] [V] [TRT] Tactic: 0xdce100b9fe609424 Time: 0.817038
[08/10/2023-11:17:05] [V] [TRT] Conv_17 + Relu_18 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x128x32_stage4_warpsize2x2x1_g1_tensor16x8x16 Tactic: 0xdfa020ef435ef810
[08/10/2023-11:17:05] [V] [TRT] Tactic: 0xdfa020ef435ef810 Time: 0.456969
[08/10/2023-11:17:05] [V] [TRT] Conv_17 + Relu_18 Set Tactic Name: ampere_h1688cudnn_256x64_ldg8_relu_exp_large_nhwc_tn_v1 Tactic: 0xe0e3c0e8cf9a2d9e
[08/10/2023-11:17:05] [V] [TRT] Tactic: 0xe0e3c0e8cf9a2d9e Time: 0.781467
[08/10/2023-11:17:05] [V] [TRT] Conv_17 + Relu_18 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x32x64_stage5_warpsize2x2x1_g1_tensor16x8x16 Tactic: 0xe1ff5ad20f5c6bf6
[08/10/2023-11:17:05] [V] [TRT] Tactic: 0xe1ff5ad20f5c6bf6 Time: 1.10755
[08/10/2023-11:17:05] [V] [TRT] Conv_17 + Relu_18 Set Tactic Name: ampere_h16816cudnn_256x64_ldg8_relu_exp_medium_nhwc_tn_v1 Tactic: 0xe4711898bd599c36
[08/10/2023-11:17:05] [V] [TRT] Tactic: 0xe4711898bd599c36 Time: 0.583767
[08/10/2023-11:17:05] [V] [TRT] Conv_17 + Relu_18 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x64x64_stage3_warpsize4x1x1_g1_tensor16x8x16 Tactic: 0xeb25062ffb9eae45
[08/10/2023-11:17:05] [V] [TRT] Tactic: 0xeb25062ffb9eae45 Time: 0.571255
[08/10/2023-11:17:05] [V] [TRT] Conv_17 + Relu_18 Set Tactic Name: ampere_h16816cudnn_128x64_sliced1x2_ldg8_relu_exp_stages_64x4_large_nhwc_tn_v1 Tactic: 0xeb2d2aa4e56bb41c
[08/10/2023-11:17:05] [V] [TRT] Tactic: 0xeb2d2aa4e56bb41c Time: 0.741376
[08/10/2023-11:17:05] [V] [TRT] Conv_17 + Relu_18 Set Tactic Name: ampere_h16816cudnn_64x64_ldg8_relu_exp_stages_64x6_small_nhwc_tn_v1 Tactic: 0xf0beb09df9a19f82
[08/10/2023-11:17:05] [V] [TRT] Tactic: 0xf0beb09df9a19f82 Time: 1.07946
[08/10/2023-11:17:05] [V] [TRT] Conv_17 + Relu_18 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x64x64_stage4_warpsize2x2x1_g1_tensor16x8x16 Tactic: 0xf35e0311fa1cc516
[08/10/2023-11:17:05] [V] [TRT] Tactic: 0xf35e0311fa1cc516 Time: 0.720777
[08/10/2023-11:17:05] [V] [TRT] Conv_17 + Relu_18 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x32x32_stage4_warpsize4x1x1_g1_tensor16x8x16 Tactic: 0xf79479a62ea9f901
[08/10/2023-11:17:05] [V] [TRT] Tactic: 0xf79479a62ea9f901 Time: 0.871529
[08/10/2023-11:17:05] [V] [TRT] Fastest Tactic: 0x60c3421152ef8e10 Time: 0.446537
[08/10/2023-11:17:05] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x60c3421152ef8e10
[08/10/2023-11:17:05] [V] [TRT] =============== Computing costs for 
[08/10/2023-11:17:05] [V] [TRT] *************** Autotuning format combination: Float(4147200,32400,180,1) -> Float(4147200,32400,180,1) ***************
[08/10/2023-11:17:05] [V] [TRT] *************** Autotuning format combination: Float(4147200,1,23040,128) -> Float(4147200,1,23040,128) ***************
[08/10/2023-11:17:05] [V] [TRT] *************** Autotuning format combination: Float(1036800,1:4,5760,32) -> Float(1036800,1:4,5760,32) ***************
[08/10/2023-11:17:05] [V] [TRT] *************** Autotuning format combination: Half(4147200,32400,180,1) -> Half(4147200,32400,180,1) ***************
[08/10/2023-11:17:05] [W] [TRT] Weights [name=Conv_19 + Relu_20.weight] had the following issues when converted to FP16:
[08/10/2023-11:17:05] [W] [TRT]  - Subnormal FP16 values detected. 
[08/10/2023-11:17:05] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.
[08/10/2023-11:17:05] [V] [TRT] *************** Autotuning format combination: Half(2073600,32400:2,180,1) -> Half(2073600,32400:2,180,1) ***************
[08/10/2023-11:17:05] [W] [TRT] Weights [name=Conv_19 + Relu_20.weight] had the following issues when converted to FP16:
[08/10/2023-11:17:05] [W] [TRT]  - Subnormal FP16 values detected. 
[08/10/2023-11:17:05] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.
[08/10/2023-11:17:05] [V] [TRT] --------------- Timing Runner: Conv_19 + Relu_20 (FusedConvActConvolution)
[08/10/2023-11:17:05] [V] [TRT] FusedConvActConvolution has no valid tactics for this config, skipping
[08/10/2023-11:17:05] [W] [TRT] Weights [name=Conv_19 + Relu_20.weight] had the following issues when converted to FP16:
[08/10/2023-11:17:05] [W] [TRT]  - Subnormal FP16 values detected. 
[08/10/2023-11:17:05] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.
[08/10/2023-11:17:05] [V] [TRT] --------------- Timing Runner: Conv_19 + Relu_20 (CaskConvolution)
[08/10/2023-11:17:05] [V] [TRT] CaskConvolution has no valid tactics for this config, skipping
[08/10/2023-11:17:05] [V] [TRT] *************** Autotuning format combination: Half(518400,1:8,2880,16) -> Float(4147200,32400,180,1) ***************
[08/10/2023-11:17:05] [W] [TRT] Weights [name=Conv_19 + Relu_20.weight] had the following issues when converted to FP16:
[08/10/2023-11:17:05] [W] [TRT]  - Subnormal FP16 values detected. 
[08/10/2023-11:17:05] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.
[08/10/2023-11:17:05] [V] [TRT] --------------- Timing Runner: Conv_19 + Relu_20 (CaskConvolution)
[08/10/2023-11:17:05] [V] [TRT] CaskConvolution has no valid tactics for this config, skipping
[08/10/2023-11:17:05] [V] [TRT] *************** Autotuning format combination: Half(518400,1:8,2880,16) -> Half(518400,1:8,2880,16) ***************
[08/10/2023-11:17:05] [W] [TRT] Weights [name=Conv_19 + Relu_20.weight] had the following issues when converted to FP16:
[08/10/2023-11:17:05] [W] [TRT]  - Subnormal FP16 values detected. 
[08/10/2023-11:17:05] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.
[08/10/2023-11:17:05] [V] [TRT] *************** Autotuning format combination: Half(259200,1:16,1440,8) -> Half(259200,1:16,1440,8) ***************
[08/10/2023-11:17:05] [W] [TRT] Weights [name=Conv_19 + Relu_20.weight] had the following issues when converted to FP16:
[08/10/2023-11:17:05] [W] [TRT]  - Subnormal FP16 values detected. 
[08/10/2023-11:17:05] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.
[08/10/2023-11:17:05] [V] [TRT] =============== Computing costs for 
[08/10/2023-11:17:05] [V] [TRT] *************** Autotuning format combination: Float(4147200,32400,180,1) -> Float(4147200,32400,180,1) ***************
[08/10/2023-11:17:05] [V] [TRT] *************** Autotuning format combination: Float(4147200,1,23040,128) -> Float(4147200,1,23040,128) ***************
[08/10/2023-11:17:05] [V] [TRT] *************** Autotuning format combination: Float(1036800,1:4,5760,32) -> Float(1036800,1:4,5760,32) ***************
[08/10/2023-11:17:05] [V] [TRT] *************** Autotuning format combination: Half(4147200,32400,180,1) -> Half(4147200,32400,180,1) ***************
[08/10/2023-11:17:05] [W] [TRT] Weights [name=Conv_21 + Relu_22.weight] had the following issues when converted to FP16:
[08/10/2023-11:17:05] [W] [TRT]  - Subnormal FP16 values detected. 
[08/10/2023-11:17:05] [W] [TRT]  - Values less than smallest positive FP16 Subnormal value detected. Converting to FP16 minimum subnormalized value. 
[08/10/2023-11:17:05] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.
[08/10/2023-11:17:05] [V] [TRT] *************** Autotuning format combination: Half(2073600,32400:2,180,1) -> Half(2073600,32400:2,180,1) ***************
[08/10/2023-11:17:05] [W] [TRT] Weights [name=Conv_21 + Relu_22.weight] had the following issues when converted to FP16:
[08/10/2023-11:17:05] [W] [TRT]  - Subnormal FP16 values detected. 
[08/10/2023-11:17:05] [W] [TRT]  - Values less than smallest positive FP16 Subnormal value detected. Converting to FP16 minimum subnormalized value. 
[08/10/2023-11:17:05] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.
[08/10/2023-11:17:05] [V] [TRT] --------------- Timing Runner: Conv_21 + Relu_22 (FusedConvActConvolution)
[08/10/2023-11:17:05] [V] [TRT] FusedConvActConvolution has no valid tactics for this config, skipping
[08/10/2023-11:17:05] [W] [TRT] Weights [name=Conv_21 + Relu_22.weight] had the following issues when converted to FP16:
[08/10/2023-11:17:05] [W] [TRT]  - Subnormal FP16 values detected. 
[08/10/2023-11:17:05] [W] [TRT]  - Values less than smallest positive FP16 Subnormal value detected. Converting to FP16 minimum subnormalized value. 
[08/10/2023-11:17:05] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.
[08/10/2023-11:17:05] [V] [TRT] --------------- Timing Runner: Conv_21 + Relu_22 (CaskConvolution)
[08/10/2023-11:17:05] [V] [TRT] CaskConvolution has no valid tactics for this config, skipping
[08/10/2023-11:17:05] [V] [TRT] *************** Autotuning format combination: Half(518400,1:8,2880,16) -> Float(4147200,32400,180,1) ***************
[08/10/2023-11:17:05] [W] [TRT] Weights [name=Conv_21 + Relu_22.weight] had the following issues when converted to FP16:
[08/10/2023-11:17:05] [W] [TRT]  - Subnormal FP16 values detected. 
[08/10/2023-11:17:05] [W] [TRT]  - Values less than smallest positive FP16 Subnormal value detected. Converting to FP16 minimum subnormalized value. 
[08/10/2023-11:17:05] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.
[08/10/2023-11:17:05] [V] [TRT] --------------- Timing Runner: Conv_21 + Relu_22 (CaskConvolution)
[08/10/2023-11:17:05] [V] [TRT] CaskConvolution has no valid tactics for this config, skipping
[08/10/2023-11:17:05] [V] [TRT] *************** Autotuning format combination: Half(518400,1:8,2880,16) -> Half(518400,1:8,2880,16) ***************
[08/10/2023-11:17:05] [W] [TRT] Weights [name=Conv_21 + Relu_22.weight] had the following issues when converted to FP16:
[08/10/2023-11:17:05] [W] [TRT]  - Subnormal FP16 values detected. 
[08/10/2023-11:17:05] [W] [TRT]  - Values less than smallest positive FP16 Subnormal value detected. Converting to FP16 minimum subnormalized value. 
[08/10/2023-11:17:05] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.
[08/10/2023-11:17:05] [V] [TRT] *************** Autotuning format combination: Half(259200,1:16,1440,8) -> Half(259200,1:16,1440,8) ***************
[08/10/2023-11:17:05] [W] [TRT] Weights [name=Conv_21 + Relu_22.weight] had the following issues when converted to FP16:
[08/10/2023-11:17:05] [W] [TRT]  - Subnormal FP16 values detected. 
[08/10/2023-11:17:05] [W] [TRT]  - Values less than smallest positive FP16 Subnormal value detected. Converting to FP16 minimum subnormalized value. 
[08/10/2023-11:17:05] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.
[08/10/2023-11:17:05] [V] [TRT] =============== Computing costs for 
[08/10/2023-11:17:05] [V] [TRT] *************** Autotuning format combination: Float(4147200,32400,180,1) -> Float(4147200,32400,180,1) ***************
[08/10/2023-11:17:05] [V] [TRT] *************** Autotuning format combination: Float(4147200,1,23040,128) -> Float(4147200,1,23040,128) ***************
[08/10/2023-11:17:05] [V] [TRT] *************** Autotuning format combination: Float(1036800,1:4,5760,32) -> Float(1036800,1:4,5760,32) ***************
[08/10/2023-11:17:05] [V] [TRT] *************** Autotuning format combination: Half(4147200,32400,180,1) -> Half(4147200,32400,180,1) ***************
[08/10/2023-11:17:05] [W] [TRT] Weights [name=Conv_23 + Relu_24.weight] had the following issues when converted to FP16:
[08/10/2023-11:17:05] [W] [TRT]  - Subnormal FP16 values detected. 
[08/10/2023-11:17:05] [W] [TRT]  - Values less than smallest positive FP16 Subnormal value detected. Converting to FP16 minimum subnormalized value. 
[08/10/2023-11:17:05] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.
[08/10/2023-11:17:05] [V] [TRT] *************** Autotuning format combination: Half(2073600,32400:2,180,1) -> Half(2073600,32400:2,180,1) ***************
[08/10/2023-11:17:05] [W] [TRT] Weights [name=Conv_23 + Relu_24.weight] had the following issues when converted to FP16:
[08/10/2023-11:17:05] [W] [TRT]  - Subnormal FP16 values detected. 
[08/10/2023-11:17:05] [W] [TRT]  - Values less than smallest positive FP16 Subnormal value detected. Converting to FP16 minimum subnormalized value. 
[08/10/2023-11:17:05] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.
[08/10/2023-11:17:05] [V] [TRT] --------------- Timing Runner: Conv_23 + Relu_24 (FusedConvActConvolution)
[08/10/2023-11:17:05] [V] [TRT] FusedConvActConvolution has no valid tactics for this config, skipping
[08/10/2023-11:17:05] [W] [TRT] Weights [name=Conv_23 + Relu_24.weight] had the following issues when converted to FP16:
[08/10/2023-11:17:05] [W] [TRT]  - Subnormal FP16 values detected. 
[08/10/2023-11:17:05] [W] [TRT]  - Values less than smallest positive FP16 Subnormal value detected. Converting to FP16 minimum subnormalized value. 
[08/10/2023-11:17:05] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.
[08/10/2023-11:17:05] [V] [TRT] --------------- Timing Runner: Conv_23 + Relu_24 (CaskConvolution)
[08/10/2023-11:17:05] [V] [TRT] CaskConvolution has no valid tactics for this config, skipping
[08/10/2023-11:17:05] [V] [TRT] *************** Autotuning format combination: Half(518400,1:8,2880,16) -> Float(4147200,32400,180,1) ***************
[08/10/2023-11:17:05] [W] [TRT] Weights [name=Conv_23 + Relu_24.weight] had the following issues when converted to FP16:
[08/10/2023-11:17:05] [W] [TRT]  - Subnormal FP16 values detected. 
[08/10/2023-11:17:05] [W] [TRT]  - Values less than smallest positive FP16 Subnormal value detected. Converting to FP16 minimum subnormalized value. 
[08/10/2023-11:17:05] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.
[08/10/2023-11:17:05] [V] [TRT] --------------- Timing Runner: Conv_23 + Relu_24 (CaskConvolution)
[08/10/2023-11:17:05] [V] [TRT] CaskConvolution has no valid tactics for this config, skipping
[08/10/2023-11:17:05] [V] [TRT] *************** Autotuning format combination: Half(518400,1:8,2880,16) -> Half(518400,1:8,2880,16) ***************
[08/10/2023-11:17:05] [W] [TRT] Weights [name=Conv_23 + Relu_24.weight] had the following issues when converted to FP16:
[08/10/2023-11:17:05] [W] [TRT]  - Subnormal FP16 values detected. 
[08/10/2023-11:17:05] [W] [TRT]  - Values less than smallest positive FP16 Subnormal value detected. Converting to FP16 minimum subnormalized value. 
[08/10/2023-11:17:05] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.
[08/10/2023-11:17:05] [V] [TRT] *************** Autotuning format combination: Half(259200,1:16,1440,8) -> Half(259200,1:16,1440,8) ***************
[08/10/2023-11:17:05] [W] [TRT] Weights [name=Conv_23 + Relu_24.weight] had the following issues when converted to FP16:
[08/10/2023-11:17:05] [W] [TRT]  - Subnormal FP16 values detected. 
[08/10/2023-11:17:05] [W] [TRT]  - Values less than smallest positive FP16 Subnormal value detected. Converting to FP16 minimum subnormalized value. 
[08/10/2023-11:17:05] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.
[08/10/2023-11:17:05] [V] [TRT] =============== Computing costs for 
[08/10/2023-11:17:05] [V] [TRT] *************** Autotuning format combination: Float(4147200,32400,180,1) -> Float(4147200,32400,180,1) ***************
[08/10/2023-11:17:05] [V] [TRT] *************** Autotuning format combination: Float(4147200,1,23040,128) -> Float(4147200,1,23040,128) ***************
[08/10/2023-11:17:05] [V] [TRT] *************** Autotuning format combination: Float(1036800,1:4,5760,32) -> Float(1036800,1:4,5760,32) ***************
[08/10/2023-11:17:05] [V] [TRT] *************** Autotuning format combination: Half(4147200,32400,180,1) -> Half(4147200,32400,180,1) ***************
[08/10/2023-11:17:05] [W] [TRT] Weights [name=Conv_25 + Relu_26.weight] had the following issues when converted to FP16:
[08/10/2023-11:17:05] [W] [TRT]  - Subnormal FP16 values detected. 
[08/10/2023-11:17:05] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.
[08/10/2023-11:17:05] [V] [TRT] *************** Autotuning format combination: Half(2073600,32400:2,180,1) -> Half(2073600,32400:2,180,1) ***************
[08/10/2023-11:17:05] [W] [TRT] Weights [name=Conv_25 + Relu_26.weight] had the following issues when converted to FP16:
[08/10/2023-11:17:05] [W] [TRT]  - Subnormal FP16 values detected. 
[08/10/2023-11:17:05] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.
[08/10/2023-11:17:05] [V] [TRT] --------------- Timing Runner: Conv_25 + Relu_26 (FusedConvActConvolution)
[08/10/2023-11:17:05] [V] [TRT] FusedConvActConvolution has no valid tactics for this config, skipping
[08/10/2023-11:17:05] [W] [TRT] Weights [name=Conv_25 + Relu_26.weight] had the following issues when converted to FP16:
[08/10/2023-11:17:05] [W] [TRT]  - Subnormal FP16 values detected. 
[08/10/2023-11:17:05] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.
[08/10/2023-11:17:05] [V] [TRT] --------------- Timing Runner: Conv_25 + Relu_26 (CaskConvolution)
[08/10/2023-11:17:05] [V] [TRT] CaskConvolution has no valid tactics for this config, skipping
[08/10/2023-11:17:05] [V] [TRT] *************** Autotuning format combination: Half(518400,1:8,2880,16) -> Float(4147200,32400,180,1) ***************
[08/10/2023-11:17:05] [W] [TRT] Weights [name=Conv_25 + Relu_26.weight] had the following issues when converted to FP16:
[08/10/2023-11:17:05] [W] [TRT]  - Subnormal FP16 values detected. 
[08/10/2023-11:17:05] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.
[08/10/2023-11:17:05] [V] [TRT] --------------- Timing Runner: Conv_25 + Relu_26 (CaskConvolution)
[08/10/2023-11:17:05] [V] [TRT] CaskConvolution has no valid tactics for this config, skipping
[08/10/2023-11:17:05] [V] [TRT] *************** Autotuning format combination: Half(518400,1:8,2880,16) -> Half(518400,1:8,2880,16) ***************
[08/10/2023-11:17:05] [W] [TRT] Weights [name=Conv_25 + Relu_26.weight] had the following issues when converted to FP16:
[08/10/2023-11:17:05] [W] [TRT]  - Subnormal FP16 values detected. 
[08/10/2023-11:17:05] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.
[08/10/2023-11:17:05] [V] [TRT] *************** Autotuning format combination: Half(259200,1:16,1440,8) -> Half(259200,1:16,1440,8) ***************
[08/10/2023-11:17:05] [W] [TRT] Weights [name=Conv_25 + Relu_26.weight] had the following issues when converted to FP16:
[08/10/2023-11:17:05] [W] [TRT]  - Subnormal FP16 values detected. 
[08/10/2023-11:17:05] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.
[08/10/2023-11:17:05] [V] [TRT] =============== Computing costs for 
[08/10/2023-11:17:05] [V] [TRT] *************** Autotuning format combination: Float(4147200,32400,180,1) -> Float(16588800,32400,180,1) ***************
[08/10/2023-11:17:05] [V] [TRT] --------------- Timing Runner: Conv_27 + Relu_28 (CudaDepthwiseConvolution)
[08/10/2023-11:17:05] [V] [TRT] CudaDepthwiseConvolution has no valid tactics for this config, skipping
[08/10/2023-11:17:05] [V] [TRT] --------------- Timing Runner: Conv_27 + Relu_28 (FusedConvActConvolution)
[08/10/2023-11:17:05] [V] [TRT] FusedConvActConvolution has no valid tactics for this config, skipping
[08/10/2023-11:17:05] [V] [TRT] --------------- Timing Runner: Conv_27 + Relu_28 (CudnnConvolution)
[08/10/2023-11:17:05] [V] [TRT] Tactic: 0x0000000000000000 Time: 3.11933
[08/10/2023-11:17:06] [V] [TRT] Tactic: 0x0000000000000001 Time: 1.75701
[08/10/2023-11:17:06] [V] [TRT] Tactic: 0x0000000000000002 Time: 3.42853
[08/10/2023-11:17:06] [V] [TRT] Tactic: 0x0000000000000004 skipped. Scratch requested: 17381851136, available: 4294967296
[08/10/2023-11:17:06] [V] [TRT] Tactic: 0x0000000000000005 Time: 5.0323
[08/10/2023-11:17:06] [V] [TRT] Tactic: 0x0000000000000038 Time: 3.20624
[08/10/2023-11:17:06] [V] [TRT] Tactic: 0x0000000000000039 Time: 1.79239
[08/10/2023-11:17:06] [V] [TRT] Tactic: 0x000000000000003a Time: 3.42998
[08/10/2023-11:17:06] [V] [TRT] Tactic: 0x000000000000003c skipped. Scratch requested: 17381851136, available: 4294967296
[08/10/2023-11:17:06] [V] [TRT] Tactic: 0x000000000000003d Time: 4.33323
[08/10/2023-11:17:06] [V] [TRT] Tactic: 0x0000000000000070 Time: 2.87236
[08/10/2023-11:17:06] [V] [TRT] Tactic: 0x0000000000000071 Time: 2.498
[08/10/2023-11:17:06] [V] [TRT] Tactic: 0x0000000000000072 Time: 3.11608
[08/10/2023-11:17:06] [V] [TRT] Tactic: 0x0000000000000074 skipped. Scratch requested: 17381851136, available: 4294967296
[08/10/2023-11:17:06] [V] [TRT] Tactic: 0x0000000000000075 Time: 4.35547
[08/10/2023-11:17:06] [V] [TRT] Fastest Tactic: 0x0000000000000001 Time: 1.75701
[08/10/2023-11:17:06] [V] [TRT] Setting workspace to 17381851136enables more tactics for profiling
[08/10/2023-11:17:06] [V] [TRT] --------------- Timing Runner: Conv_27 + Relu_28 (CublasConvolution)
[08/10/2023-11:17:06] [V] [TRT] CublasConvolution has no valid tactics for this config, skipping
[08/10/2023-11:17:06] [V] [TRT] --------------- Timing Runner: Conv_27 + Relu_28 (CaskConvolution)
[08/10/2023-11:17:06] [V] [TRT] Conv_27 + Relu_28 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nchwkrsc_nchw_tilesize64x128x32_stage5_warpsize2x2x1_g1_tensor16x8x8_t1r1s1_aligna4_alignc4 Tactic: 0x089fce8f59784020
[08/10/2023-11:17:06] [V] [TRT] Tactic: 0x089fce8f59784020 Time: 1.2608
[08/10/2023-11:17:06] [V] [TRT] Conv_27 + Relu_28 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nchwkrsc_nchw_tilesize64x32x64_stage5_warpsize2x2x1_g1_tensor16x8x8_t1r1s1_aligna4_alignc4 Tactic: 0x126c151fbf444788
[08/10/2023-11:17:06] [V] [TRT] Tactic: 0x126c151fbf444788 Time: 2.42824
[08/10/2023-11:17:06] [V] [TRT] Conv_27 + Relu_28 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x256x8_stage3_warpsize1x4x1_g1_ffma_t1r1s1_aligna4_alignc4 Tactic: 0x2ee10e11d6651675
[08/10/2023-11:17:06] [V] [TRT] Tactic: 0x2ee10e11d6651675 Time: 1.58021
[08/10/2023-11:17:06] [V] [TRT] Conv_27 + Relu_28 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nchwkrsc_nchw_tilesize128x64x32_stage5_warpsize2x2x1_g1_tensor16x8x8_t1r1s1_aligna4_alignc4 Tactic: 0x37f7c166cac77c6f
[08/10/2023-11:17:06] [V] [TRT] Tactic: 0x37f7c166cac77c6f Time: 1.34911
[08/10/2023-11:17:06] [V] [TRT] Conv_27 + Relu_28 Set Tactic Name: ampere_scudnn_128x64_relu_small_nn_v1 Tactic: 0x3f243c490d502deb
[08/10/2023-11:17:06] [V] [TRT] Tactic: 0x3f243c490d502deb Time: 1.26998
[08/10/2023-11:17:06] [V] [TRT] Conv_27 + Relu_28 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nchwkrsc_nchw_tilesize128x64x64_stage3_warpsize2x2x1_g1_tensor16x8x8_t1r1s1_aligna4_alignc4 Tactic: 0x44429615b0244d08
[08/10/2023-11:17:06] [V] [TRT] Tactic: 0x44429615b0244d08 Time: 2.00865
[08/10/2023-11:17:06] [V] [TRT] Conv_27 + Relu_28 Set Tactic Name: ampere_scudnn_128x128_relu_small_nn_v1 Tactic: 0x503619c69ae500ff
[08/10/2023-11:17:06] [V] [TRT] Tactic: 0x503619c69ae500ff Time: 1.2372
[08/10/2023-11:17:06] [V] [TRT] Conv_27 + Relu_28 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nchwkrsc_nchw_tilesize256x128x32_stage3_warpsize4x2x1_g1_tensor16x8x8_t1r1s1_aligna4_alignc4 Tactic: 0x5104686f63bb0267
[08/10/2023-11:17:06] [V] [TRT] Tactic: 0x5104686f63bb0267 Time: 0.940594
[08/10/2023-11:17:06] [V] [TRT] Conv_27 + Relu_28 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nchwkrsc_nchw_tilesize256x64x32_stage3_warpsize4x2x1_g1_tensor16x8x8_t1r1s1_aligna4_alignc4 Tactic: 0x562e86db2b553d73
[08/10/2023-11:17:06] [V] [TRT] Tactic: 0x562e86db2b553d73 Time: 1.0471
[08/10/2023-11:17:06] [V] [TRT] Conv_27 + Relu_28 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nchwkrsc_nchw_tilesize128x128x32_stage4_warpsize2x2x1_g1_tensor16x8x8_t1r1s1_aligna4_alignc4 Tactic: 0x6fb0aa4a23519390
[08/10/2023-11:17:06] [V] [TRT] Tactic: 0x6fb0aa4a23519390 Time: 1.24681
[08/10/2023-11:17:06] [V] [TRT] Conv_27 + Relu_28 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nchwkrsc_nchw_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8_t1r1s1_aligna4_alignc4 Tactic: 0x7f0145cb49517338
[08/10/2023-11:17:06] [V] [TRT] Tactic: 0x7f0145cb49517338 Time: 0.734624
[08/10/2023-11:17:06] [V] [TRT] Conv_27 + Relu_28 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_t1r1s1_aligna4_alignc4 Tactic: 0x865894c4635db7fd
[08/10/2023-11:17:06] [V] [TRT] Tactic: 0x865894c4635db7fd Time: 1.62172
[08/10/2023-11:17:06] [V] [TRT] Conv_27 + Relu_28 Set Tactic Name: ampere_scudnn_128x32_relu_interior_nn_v1 Tactic: 0x9808072e706def96
[08/10/2023-11:17:06] [V] [TRT] Tactic: 0x9808072e706def96 Time: 1.45076
[08/10/2023-11:17:06] [V] [TRT] Conv_27 + Relu_28 Set Tactic Name: ampere_scudnn_128x32_relu_small_nn_v1 Tactic: 0xa8609adc4e0ceb90
[08/10/2023-11:17:06] [V] [TRT] Tactic: 0xa8609adc4e0ceb90 Time: 1.51512
[08/10/2023-11:17:06] [V] [TRT] Conv_27 + Relu_28 Set Tactic Name: ampere_scudnn_128x128_relu_interior_nn_v1 Tactic: 0xa8ef60e712f8ad24
[08/10/2023-11:17:06] [V] [TRT] Tactic: 0xa8ef60e712f8ad24 Time: 1.20457
[08/10/2023-11:17:06] [V] [TRT] Conv_27 + Relu_28 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x128x8_stage3_warpsize1x4x1_g1_ffma_t1r1s1_aligna4_alignc4 Tactic: 0xc0b05b61d128e46e
[08/10/2023-11:17:06] [V] [TRT] Tactic: 0xc0b05b61d128e46e Time: 1.60159
[08/10/2023-11:17:06] [V] [TRT] Conv_27 + Relu_28 Set Tactic Name: ampere_scudnn_128x64_relu_interior_nn_v1 Tactic: 0xc3cf6e1d1c6aff27
[08/10/2023-11:17:06] [V] [TRT] Tactic: 0xc3cf6e1d1c6aff27 Time: 1.18691
[08/10/2023-11:17:06] [V] [TRT] Conv_27 + Relu_28 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nchwkrsc_nchw_tilesize128x256x32_stage3_warpsize2x4x1_g1_tensor16x8x8_t1r1s1_aligna4_alignc4 Tactic: 0xd97544c2f30ecbf9
[08/10/2023-11:17:06] [V] [TRT] Tactic: 0xd97544c2f30ecbf9 Time: 0.706414
[08/10/2023-11:17:06] [V] [TRT] Conv_27 + Relu_28 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x64x8_stage3_warpsize2x2x1_g1_ffma_t1r1s1_aligna4_alignc4 Tactic: 0xe5603263b7f00303
[08/10/2023-11:17:06] [V] [TRT] Tactic: 0xe5603263b7f00303 Time: 1.63567
[08/10/2023-11:17:06] [V] [TRT] Conv_27 + Relu_28 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nchwkrsc_nchw_tilesize64x64x64_stage4_warpsize2x2x1_g1_tensor16x8x8_t1r1s1_aligna4_alignc4 Tactic: 0xed454ebf201fb878
[08/10/2023-11:17:06] [V] [TRT] Tactic: 0xed454ebf201fb878 Time: 1.70293
[08/10/2023-11:17:06] [V] [TRT] Conv_27 + Relu_28 Set Tactic Name: ampere_scudnn_128x128_relu_medium_nn_v1 Tactic: 0xf067e6205da31c2e
[08/10/2023-11:17:06] [V] [TRT] Tactic: 0xf067e6205da31c2e Time: 1.22754
[08/10/2023-11:17:06] [V] [TRT] Conv_27 + Relu_28 Set Tactic Name: ampere_scudnn_128x64_relu_medium_nn_v1 Tactic: 0xf64396b97c889179
[08/10/2023-11:17:06] [V] [TRT] Tactic: 0xf64396b97c889179 Time: 1.30399
[08/10/2023-11:17:06] [V] [TRT] Fastest Tactic: 0xd97544c2f30ecbf9 Time: 0.706414
[08/10/2023-11:17:06] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0xd97544c2f30ecbf9
[08/10/2023-11:17:06] [V] [TRT] *************** Autotuning format combination: Float(4147200,1,23040,128) -> Float(16588800,1,92160,512) ***************
[08/10/2023-11:17:06] [V] [TRT] --------------- Timing Runner: Conv_27 + Relu_28 (CublasConvolution)
[08/10/2023-11:17:06] [V] [TRT] CublasConvolution has no valid tactics for this config, skipping
[08/10/2023-11:17:06] [V] [TRT] --------------- Timing Runner: Conv_27 + Relu_28 (CaskConvolution)
[08/10/2023-11:17:06] [V] [TRT] Conv_27 + Relu_28 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x128x8_stage3_warpsize1x4x1_g1_ffma_t1r1s1_aligna4_alignc4 Tactic: 0x1022069e6f8d9aeb
[08/10/2023-11:17:06] [V] [TRT] Tactic: 0x1022069e6f8d9aeb Time: 1.46941
[08/10/2023-11:17:06] [V] [TRT] Conv_27 + Relu_28 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8_t1r1s1 Tactic: 0x130df49cb195156b
[08/10/2023-11:17:06] [V] [TRT] Tactic: 0x130df49cb195156b Time: 0.440709
[08/10/2023-11:17:06] [V] [TRT] Conv_27 + Relu_28 Set Tactic Name: ampere_scudnn_128x128_relu_exp_interior_nhwc_tn_v1 Tactic: 0x17173deba0b64484
[08/10/2023-11:17:07] [V] [TRT] Tactic: 0x17173deba0b64484 Time: 1.16075
[08/10/2023-11:17:07] [V] [TRT] Conv_27 + Relu_28 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize256x64x32_stage3_warpsize4x2x1_g1_tensor16x8x8_t1r1s1 Tactic: 0x233399c4b9cc77c1
[08/10/2023-11:17:07] [V] [TRT] Tactic: 0x233399c4b9cc77c1 Time: 0.563095
[08/10/2023-11:17:07] [V] [TRT] Conv_27 + Relu_28 Set Tactic Name: ampere_scudnn_128x64_sliced1x2_ldg4_relu_exp_small_nhwc_tn_v1 Tactic: 0x27b316f52c109002
[08/10/2023-11:17:07] [V] [TRT] Tactic: 0x27b316f52c109002 Time: 1.328
[08/10/2023-11:17:07] [V] [TRT] Conv_27 + Relu_28 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x64x8_stage3_warpsize2x2x1_g1_ffma_t1r1s1_aligna4_alignc4 Tactic: 0x35f26f9c09557d86
[08/10/2023-11:17:07] [V] [TRT] Tactic: 0x35f26f9c09557d86 Time: 1.2487
[08/10/2023-11:17:07] [V] [TRT] Conv_27 + Relu_28 Set Tactic Name: ampere_scudnn_128x32_sliced1x4_ldg4_relu_exp_medium_nhwc_tn_v1 Tactic: 0x3e191488237fab8f
[08/10/2023-11:17:07] [V] [TRT] Tactic: 0x3e191488237fab8f Time: 1.60967
[08/10/2023-11:17:07] [V] [TRT] Conv_27 + Relu_28 Set Tactic Name: ampere_scudnn_128x32_sliced1x4_ldg4_relu_exp_small_nhwc_tn_v1 Tactic: 0x3e2b881168d9689d
[08/10/2023-11:17:07] [V] [TRT] Tactic: 0x3e2b881168d9689d Time: 1.57683
[08/10/2023-11:17:07] [V] [TRT] Conv_27 + Relu_28 Set Tactic Name: ampere_scudnn_128x64_sliced1x2_ldg4_relu_exp_medium_nhwc_tn_v1 Tactic: 0x412c44dfeaf9161d
[08/10/2023-11:17:07] [V] [TRT] Tactic: 0x412c44dfeaf9161d Time: 1.32352
[08/10/2023-11:17:07] [V] [TRT] Conv_27 + Relu_28 Set Tactic Name: ampere_scudnn_128x128_relu_exp_small_nhwc_tn_v1 Tactic: 0x5030121339a48bf3
[08/10/2023-11:17:07] [V] [TRT] Tactic: 0x5030121339a48bf3 Time: 1.16895
[08/10/2023-11:17:07] [V] [TRT] Conv_27 + Relu_28 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x128x32_stage4_warpsize2x2x1_g1_tensor16x8x8_t1r1s1 Tactic: 0x53b347fe11460a8e
[08/10/2023-11:17:07] [V] [TRT] Tactic: 0x53b347fe11460a8e Time: 0.522418
[08/10/2023-11:17:07] [V] [TRT] Conv_27 + Relu_28 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_t1r1s1_aligna4_alignc4 Tactic: 0x55d80c17b1cd982d
[08/10/2023-11:17:07] [V] [TRT] Tactic: 0x55d80c17b1cd982d Time: 1.27157
[08/10/2023-11:17:07] [V] [TRT] Conv_27 + Relu_28 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize64x128x32_stage5_warpsize2x2x1_g1_tensor16x8x8_t1r1s1 Tactic: 0x5ed17eb1dfe2e6b0
[08/10/2023-11:17:07] [V] [TRT] Tactic: 0x5ed17eb1dfe2e6b0 Time: 0.661783
[08/10/2023-11:17:07] [V] [TRT] Conv_27 + Relu_28 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage3_warpsize2x2x1_g1_tensor16x8x8_t1r1s1 Tactic: 0x61f62003626e5959
[08/10/2023-11:17:07] [V] [TRT] Tactic: 0x61f62003626e5959 Time: 0.714843
[08/10/2023-11:17:07] [V] [TRT] Conv_27 + Relu_28 Set Tactic Name: ampere_scudnn_128x128_ldg4_relu_exp_medium_nhwc_tn_v1 Tactic: 0x7bc32c782b800c48
[08/10/2023-11:17:07] [V] [TRT] Tactic: 0x7bc32c782b800c48 Time: 1.17064
[08/10/2023-11:17:07] [V] [TRT] Conv_27 + Relu_28 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage4_warpsize2x2x1_g1_tensor16x8x8_t1r1s1 Tactic: 0xa71326710e3f683c
[08/10/2023-11:17:07] [V] [TRT] Tactic: 0xa71326710e3f683c Time: 0.940617
[08/10/2023-11:17:07] [V] [TRT] Conv_27 + Relu_28 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize256x128x32_stage3_warpsize4x2x1_g1_tensor16x8x8_t1r1s1 Tactic: 0xabb79847ce7b82ce
[08/10/2023-11:17:07] [V] [TRT] Tactic: 0xabb79847ce7b82ce Time: 0.491401
[08/10/2023-11:17:07] [V] [TRT] Conv_27 + Relu_28 Set Tactic Name: ampere_scudnn_128x64_sliced1x2_ldg4_relu_exp_interior_nhwc_tn_v1 Tactic: 0xae0c89d047932ba3
[08/10/2023-11:17:07] [V] [TRT] Tactic: 0xae0c89d047932ba3 Time: 1.22431
[08/10/2023-11:17:07] [V] [TRT] Conv_27 + Relu_28 Set Tactic Name: ampere_scudnn_128x32_sliced1x4_ldg4_relu_exp_interior_nhwc_tn_v1 Tactic: 0xc7feb33970feefa7
[08/10/2023-11:17:07] [V] [TRT] Tactic: 0xc7feb33970feefa7 Time: 1.53397
[08/10/2023-11:17:07] [V] [TRT] Conv_27 + Relu_28 Set Tactic Name: ampere_scudnn_128x128_relu_exp_medium_nhwc_tn_v1 Tactic: 0xd9031472c05adf51
[08/10/2023-11:17:07] [V] [TRT] Tactic: 0xd9031472c05adf51 Time: 1.17664
[08/10/2023-11:17:07] [V] [TRT] Conv_27 + Relu_28 Set Tactic Name: ampere_scudnn_128x128_ldg4_relu_exp_interior_nhwc_tn_v1 Tactic: 0xe47307053a42b3e4
[08/10/2023-11:17:07] [V] [TRT] Tactic: 0xe47307053a42b3e4 Time: 1.13546
[08/10/2023-11:17:07] [V] [TRT] Conv_27 + Relu_28 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x256x32_stage3_warpsize2x4x1_g1_tensor16x8x8_t1r1s1 Tactic: 0xebdd7d350fbaa00e
[08/10/2023-11:17:07] [V] [TRT] Tactic: 0xebdd7d350fbaa00e Time: 0.466615
[08/10/2023-11:17:07] [V] [TRT] Conv_27 + Relu_28 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize64x32x64_stage5_warpsize2x2x1_g1_tensor16x8x8_t1r1s1 Tactic: 0xed09dcddfcf4bffb
[08/10/2023-11:17:07] [V] [TRT] Tactic: 0xed09dcddfcf4bffb Time: 1.44619
[08/10/2023-11:17:07] [V] [TRT] Conv_27 + Relu_28 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x64x32_stage5_warpsize2x2x1_g1_tensor16x8x8_t1r1s1 Tactic: 0xf031e640742524d7
[08/10/2023-11:17:07] [V] [TRT] Tactic: 0xf031e640742524d7 Time: 0.672233
[08/10/2023-11:17:07] [V] [TRT] Conv_27 + Relu_28 Set Tactic Name: ampere_scudnn_128x128_ldg4_relu_exp_small_nhwc_tn_v1 Tactic: 0xf90060ce8193b811
[08/10/2023-11:17:07] [V] [TRT] Tactic: 0xf90060ce8193b811 Time: 1.14465
[08/10/2023-11:17:07] [V] [TRT] Fastest Tactic: 0x130df49cb195156b Time: 0.440709
[08/10/2023-11:17:07] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x130df49cb195156b
[08/10/2023-11:17:07] [V] [TRT] *************** Autotuning format combination: Float(1036800,1:4,5760,32) -> Float(4147200,1:4,23040,128) ***************
[08/10/2023-11:17:07] [V] [TRT] --------------- Timing Runner: Conv_27 + Relu_28 (CublasConvolution)
[08/10/2023-11:17:07] [V] [TRT] CublasConvolution has no valid tactics for this config, skipping
[08/10/2023-11:17:07] [V] [TRT] --------------- Timing Runner: Conv_27 + Relu_28 (CaskConvolution)
[08/10/2023-11:17:07] [V] [TRT] Conv_27 + Relu_28 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8_t1r1s1 Tactic: 0x130df49cb195156b
[08/10/2023-11:17:07] [V] [TRT] Tactic: 0x130df49cb195156b Time: 0.451406
[08/10/2023-11:17:07] [V] [TRT] Conv_27 + Relu_28 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize256x64x32_stage3_warpsize4x2x1_g1_tensor16x8x8_t1r1s1 Tactic: 0x233399c4b9cc77c1
[08/10/2023-11:17:07] [V] [TRT] Tactic: 0x233399c4b9cc77c1 Time: 0.563406
[08/10/2023-11:17:07] [V] [TRT] Conv_27 + Relu_28 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x128x32_stage4_warpsize2x2x1_g1_tensor16x8x8_t1r1s1 Tactic: 0x53b347fe11460a8e
[08/10/2023-11:17:07] [V] [TRT] Tactic: 0x53b347fe11460a8e Time: 0.524782
[08/10/2023-11:17:07] [V] [TRT] Conv_27 + Relu_28 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize64x128x32_stage5_warpsize2x2x1_g1_tensor16x8x8_t1r1s1 Tactic: 0x5ed17eb1dfe2e6b0
[08/10/2023-11:17:07] [V] [TRT] Tactic: 0x5ed17eb1dfe2e6b0 Time: 0.670656
[08/10/2023-11:17:07] [V] [TRT] Conv_27 + Relu_28 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage3_warpsize2x2x1_g1_tensor16x8x8_t1r1s1 Tactic: 0x61f62003626e5959
[08/10/2023-11:17:07] [V] [TRT] Tactic: 0x61f62003626e5959 Time: 0.714341
[08/10/2023-11:17:07] [V] [TRT] Conv_27 + Relu_28 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage4_warpsize2x2x1_g1_tensor16x8x8_t1r1s1 Tactic: 0xa71326710e3f683c
[08/10/2023-11:17:07] [V] [TRT] Tactic: 0xa71326710e3f683c Time: 1.0109
[08/10/2023-11:17:07] [V] [TRT] Conv_27 + Relu_28 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize256x128x32_stage3_warpsize4x2x1_g1_tensor16x8x8_t1r1s1 Tactic: 0xabb79847ce7b82ce
[08/10/2023-11:17:07] [V] [TRT] Tactic: 0xabb79847ce7b82ce Time: 0.491749
[08/10/2023-11:17:07] [V] [TRT] Conv_27 + Relu_28 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x256x32_stage3_warpsize2x4x1_g1_tensor16x8x8_t1r1s1 Tactic: 0xebdd7d350fbaa00e
[08/10/2023-11:17:07] [V] [TRT] Tactic: 0xebdd7d350fbaa00e Time: 0.469687
[08/10/2023-11:17:07] [V] [TRT] Conv_27 + Relu_28 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize64x32x64_stage5_warpsize2x2x1_g1_tensor16x8x8_t1r1s1 Tactic: 0xed09dcddfcf4bffb
[08/10/2023-11:17:07] [V] [TRT] Tactic: 0xed09dcddfcf4bffb Time: 1.45342
[08/10/2023-11:17:07] [V] [TRT] Conv_27 + Relu_28 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x64x32_stage5_warpsize2x2x1_g1_tensor16x8x8_t1r1s1 Tactic: 0xf031e640742524d7
[08/10/2023-11:17:07] [V] [TRT] Tactic: 0xf031e640742524d7 Time: 0.675182
[08/10/2023-11:17:07] [V] [TRT] Fastest Tactic: 0x130df49cb195156b Time: 0.451406
[08/10/2023-11:17:07] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x130df49cb195156b
[08/10/2023-11:17:07] [V] [TRT] *************** Autotuning format combination: Half(4147200,32400,180,1) -> Half(16588800,32400,180,1) ***************
[08/10/2023-11:17:07] [W] [TRT] Weights [name=Conv_27 + Relu_28.weight] had the following issues when converted to FP16:
[08/10/2023-11:17:07] [W] [TRT]  - Subnormal FP16 values detected. 
[08/10/2023-11:17:07] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.
[08/10/2023-11:17:07] [V] [TRT] --------------- Timing Runner: Conv_27 + Relu_28 (CudnnConvolution)
[08/10/2023-11:17:07] [V] [TRT] Tactic: 0x0000000000000000 Time: 3.00784
[08/10/2023-11:17:07] [V] [TRT] Tactic: 0x0000000000000001 Time: 2.41143
[08/10/2023-11:17:07] [V] [TRT] Tactic: 0x0000000000000002 Time: 3.06301
[08/10/2023-11:17:07] [V] [TRT] Tactic: 0x0000000000000004 skipped. Scratch requested: 17381851136, available: 4294967296
[08/10/2023-11:17:07] [V] [TRT] Tactic: 0x0000000000000005 Time: 4.89566
[08/10/2023-11:17:07] [V] [TRT] Tactic: 0x0000000000000038 Time: 2.72997
[08/10/2023-11:17:07] [V] [TRT] Tactic: 0x000000000000003a Time: 2.97099
[08/10/2023-11:17:07] [V] [TRT] Tactic: 0x000000000000003c skipped. Scratch requested: 17381851136, available: 4294967296
[08/10/2023-11:17:07] [V] [TRT] Tactic: 0x000000000000003d Time: 4.93707
[08/10/2023-11:17:07] [V] [TRT] Fastest Tactic: 0x0000000000000001 Time: 2.41143
[08/10/2023-11:17:07] [V] [TRT] Setting workspace to 17381851136enables more tactics for profiling
[08/10/2023-11:17:07] [W] [TRT] Weights [name=Conv_27 + Relu_28.weight] had the following issues when converted to FP16:
[08/10/2023-11:17:07] [W] [TRT]  - Subnormal FP16 values detected. 
[08/10/2023-11:17:07] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.
[08/10/2023-11:17:07] [V] [TRT] --------------- Timing Runner: Conv_27 + Relu_28 (CublasConvolution)
[08/10/2023-11:17:07] [V] [TRT] CublasConvolution has no valid tactics for this config, skipping
[08/10/2023-11:17:07] [W] [TRT] Weights [name=Conv_27 + Relu_28.weight] had the following issues when converted to FP16:
[08/10/2023-11:17:07] [W] [TRT]  - Subnormal FP16 values detected. 
[08/10/2023-11:17:07] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.
[08/10/2023-11:17:07] [V] [TRT] --------------- Timing Runner: Conv_27 + Relu_28 (CaskConvolution)
[08/10/2023-11:17:07] [V] [TRT] CaskConvolution has no valid tactics for this config, skipping
[08/10/2023-11:17:07] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CudnnConvolution Tactic: 0x0000000000000001
[08/10/2023-11:17:07] [V] [TRT] *************** Autotuning format combination: Half(2073600,32400:2,180,1) -> Half(16588800,32400,180,1) ***************
[08/10/2023-11:17:07] [W] [TRT] Weights [name=Conv_27 + Relu_28.weight] had the following issues when converted to FP16:
[08/10/2023-11:17:07] [W] [TRT]  - Subnormal FP16 values detected. 
[08/10/2023-11:17:07] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.
[08/10/2023-11:17:07] [V] [TRT] --------------- Timing Runner: Conv_27 + Relu_28 (CaskConvolution)
[08/10/2023-11:17:07] [V] [TRT] CaskConvolution has no valid tactics for this config, skipping
[08/10/2023-11:17:07] [V] [TRT] *************** Autotuning format combination: Half(2073600,32400:2,180,1) -> Half(8294400,32400:2,180,1) ***************
[08/10/2023-11:17:07] [W] [TRT] Weights [name=Conv_27 + Relu_28.weight] had the following issues when converted to FP16:
[08/10/2023-11:17:07] [W] [TRT]  - Subnormal FP16 values detected. 
[08/10/2023-11:17:07] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.
[08/10/2023-11:17:07] [V] [TRT] --------------- Timing Runner: Conv_27 + Relu_28 (FusedConvActConvolution)
[08/10/2023-11:17:07] [V] [TRT] FusedConvActConvolution has no valid tactics for this config, skipping
[08/10/2023-11:17:07] [W] [TRT] Weights [name=Conv_27 + Relu_28.weight] had the following issues when converted to FP16:
[08/10/2023-11:17:07] [W] [TRT]  - Subnormal FP16 values detected. 
[08/10/2023-11:17:07] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.
[08/10/2023-11:17:07] [V] [TRT] --------------- Timing Runner: Conv_27 + Relu_28 (CublasConvolution)
[08/10/2023-11:17:07] [V] [TRT] CublasConvolution has no valid tactics for this config, skipping
[08/10/2023-11:17:07] [W] [TRT] Weights [name=Conv_27 + Relu_28.weight] had the following issues when converted to FP16:
[08/10/2023-11:17:07] [W] [TRT]  - Subnormal FP16 values detected. 
[08/10/2023-11:17:07] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.
[08/10/2023-11:17:07] [V] [TRT] --------------- Timing Runner: Conv_27 + Relu_28 (CaskConvolution)
[08/10/2023-11:17:07] [V] [TRT] CaskConvolution has no valid tactics for this config, skipping
[08/10/2023-11:17:07] [V] [TRT] *************** Autotuning format combination: Half(518400,1:8,2880,16) -> Float(16588800,32400,180,1) ***************
[08/10/2023-11:17:07] [W] [TRT] Weights [name=Conv_27 + Relu_28.weight] had the following issues when converted to FP16:
[08/10/2023-11:17:07] [W] [TRT]  - Subnormal FP16 values detected. 
[08/10/2023-11:17:07] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.
[08/10/2023-11:17:07] [V] [TRT] --------------- Timing Runner: Conv_27 + Relu_28 (CublasConvolution)
[08/10/2023-11:17:07] [V] [TRT] CublasConvolution has no valid tactics for this config, skipping
[08/10/2023-11:17:07] [W] [TRT] Weights [name=Conv_27 + Relu_28.weight] had the following issues when converted to FP16:
[08/10/2023-11:17:07] [W] [TRT]  - Subnormal FP16 values detected. 
[08/10/2023-11:17:07] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.
[08/10/2023-11:17:07] [V] [TRT] --------------- Timing Runner: Conv_27 + Relu_28 (CaskConvolution)
[08/10/2023-11:17:07] [V] [TRT] CaskConvolution has no valid tactics for this config, skipping
[08/10/2023-11:17:07] [V] [TRT] *************** Autotuning format combination: Half(518400,1:8,2880,16) -> Half(2073600,1:8,11520,64) ***************
[08/10/2023-11:17:07] [W] [TRT] Weights [name=Conv_27 + Relu_28.weight] had the following issues when converted to FP16:
[08/10/2023-11:17:07] [W] [TRT]  - Subnormal FP16 values detected. 
[08/10/2023-11:17:07] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.
[08/10/2023-11:17:07] [V] [TRT] --------------- Timing Runner: Conv_27 + Relu_28 (CudaDepthwiseConvolution)
[08/10/2023-11:17:07] [V] [TRT] CudaDepthwiseConvolution has no valid tactics for this config, skipping
[08/10/2023-11:17:07] [W] [TRT] Weights [name=Conv_27 + Relu_28.weight] had the following issues when converted to FP16:
[08/10/2023-11:17:07] [W] [TRT]  - Subnormal FP16 values detected. 
[08/10/2023-11:17:07] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.
[08/10/2023-11:17:07] [V] [TRT] --------------- Timing Runner: Conv_27 + Relu_28 (CublasConvolution)
[08/10/2023-11:17:07] [V] [TRT] CublasConvolution has no valid tactics for this config, skipping
[08/10/2023-11:17:07] [W] [TRT] Weights [name=Conv_27 + Relu_28.weight] had the following issues when converted to FP16:
[08/10/2023-11:17:07] [W] [TRT]  - Subnormal FP16 values detected. 
[08/10/2023-11:17:07] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.
[08/10/2023-11:17:07] [V] [TRT] --------------- Timing Runner: Conv_27 + Relu_28 (CaskConvolution)
[08/10/2023-11:17:07] [V] [TRT] Conv_27 + Relu_28 Set Tactic Name: ampere_h16816cudnn_256x128_ldg8_relu_exp_medium_nhwc_tn_v1 Tactic: 0x03896956a39a1203
[08/10/2023-11:17:07] [V] [TRT] Tactic: 0x03896956a39a1203 Time: 0.307611
[08/10/2023-11:17:07] [V] [TRT] Conv_27 + Relu_28 Set Tactic Name: ampere_h16816cudnn_256x128_ldg8_relu_exp_stages_64x3_small_nhwc_tn_v1 Tactic: 0x0e199750c30c6928
[08/10/2023-11:17:07] [V] [TRT] Tactic: 0x0e199750c30c6928 Time: 0.342034
[08/10/2023-11:17:07] [V] [TRT] Conv_27 + Relu_28 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x64x64_stage3_warpsize4x1x1_g1_tensor16x8x16_t1r1s1 Tactic: 0x0e7d7dbf5b60a967
[08/10/2023-11:17:07] [V] [TRT] Tactic: 0x0e7d7dbf5b60a967 Time: 0.336704
[08/10/2023-11:17:07] [V] [TRT] Conv_27 + Relu_28 Set Tactic Name: ampere_h1688cudnn_128x128_ldg8_relu_exp_medium_nhwc_tn_v1 Tactic: 0x100c1ad308e08d35
[08/10/2023-11:17:07] [V] [TRT] Tactic: 0x100c1ad308e08d35 Time: 0.284137
[08/10/2023-11:17:07] [V] [TRT] Conv_27 + Relu_28 Set Tactic Name: ampere_h1688cudnn_256x64_ldg8_relu_exp_interior_nhwc_tn_v1 Tactic: 0x12e96131b90aa494
[08/10/2023-11:17:07] [V] [TRT] Tactic: 0x12e96131b90aa494 Time: 0.325088
[08/10/2023-11:17:07] [V] [TRT] Conv_27 + Relu_28 Set Tactic Name: ampere_h16816cudnn_64x128_sliced1x2_ldg8_relu_exp_stages_64x4_interior_nhwc_tn_v1 Tactic: 0x15e036703de3ebb6
[08/10/2023-11:17:07] [V] [TRT] Tactic: 0x15e036703de3ebb6 Time: 0.596969
[08/10/2023-11:17:07] [V] [TRT] Conv_27 + Relu_28 Set Tactic Name: ampere_h16816cudnn_128x64_ldg8_relu_exp_stages_64x4_medium_nhwc_tn_v1 Tactic: 0x196cbc423a9bb69e
[08/10/2023-11:17:07] [V] [TRT] Tactic: 0x196cbc423a9bb69e Time: 0.559035
[08/10/2023-11:17:07] [V] [TRT] Conv_27 + Relu_28 Set Tactic Name: ampere_h16816cudnn_128x64_sliced1x2_ldg8_relu_exp_stages_64x3_small_nhwc_tn_v1 Tactic: 0x19822de884f42a6a
[08/10/2023-11:17:07] [V] [TRT] Tactic: 0x19822de884f42a6a Time: 0.460965
[08/10/2023-11:17:07] [V] [TRT] Conv_27 + Relu_28 Set Tactic Name: ampere_h16816cudnn_64x64_sliced1x2_ldg8_relu_exp_stages_64x5_small_nhwc_tn_v1 Tactic: 0x1a5ba808ad4cc5a8
[08/10/2023-11:17:07] [V] [TRT] Tactic: 0x1a5ba808ad4cc5a8 Time: 0.694144
[08/10/2023-11:17:07] [V] [TRT] Conv_27 + Relu_28 Set Tactic Name: ampere_h16816cudnn_64x128_sliced1x2_ldg8_relu_exp_stages_64x3_interior_nhwc_tn_v1 Tactic: 0x1f114e67977d5401
[08/10/2023-11:17:07] [V] [TRT] Tactic: 0x1f114e67977d5401 Time: 0.453458
[08/10/2023-11:17:07] [V] [TRT] Conv_27 + Relu_28 Set Tactic Name: ampere_h16816cudnn_128x128_ldg8_relu_exp_medium_nhwc_tn_v1 Tactic: 0x234580e8a194335c
[08/10/2023-11:17:07] [V] [TRT] Tactic: 0x234580e8a194335c Time: 0.290898
[08/10/2023-11:17:07] [V] [TRT] Conv_27 + Relu_28 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x256x32_stage3_warpsize2x4x1_g1_tensor16x8x16_t1r1s1 Tactic: 0x23848f7cc4e20635
[08/10/2023-11:17:08] [V] [TRT] Tactic: 0x23848f7cc4e20635 Time: 0.310062
[08/10/2023-11:17:08] [V] [TRT] Conv_27 + Relu_28 Set Tactic Name: ampere_h16816cudnn_64x64_sliced1x2_ldg8_relu_exp_stages_64x6_small_nhwc_tn_v1 Tactic: 0x24e01e7405cfdfe9
[08/10/2023-11:17:08] [V] [TRT] Tactic: 0x24e01e7405cfdfe9 Time: 0.975013
[08/10/2023-11:17:08] [V] [TRT] Conv_27 + Relu_28 Set Tactic Name: ampere_h16816cudnn_128x64_sliced1x2_ldg8_relu_exp_stages_64x3_medium_nhwc_tn_v1 Tactic: 0x29329b5741ea05f2
[08/10/2023-11:17:08] [V] [TRT] Tactic: 0x29329b5741ea05f2 Time: 0.441417
[08/10/2023-11:17:08] [V] [TRT] Conv_27 + Relu_28 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x32x64_stage5_warpsize2x2x1_g1_tensor16x8x16_t1r1s1 Tactic: 0x2aa016c86360697f
[08/10/2023-11:17:08] [V] [TRT] Tactic: 0x2aa016c86360697f Time: 0.615698
[08/10/2023-11:17:08] [V] [TRT] Conv_27 + Relu_28 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x32x32_stage4_warpsize4x1x1_g1_tensor16x8x16_t1r1s1 Tactic: 0x2e9f40fea3fe4d65
[08/10/2023-11:17:08] [V] [TRT] Tactic: 0x2e9f40fea3fe4d65 Time: 0.336311
[08/10/2023-11:17:08] [V] [TRT] Conv_27 + Relu_28 Set Tactic Name: ampere_h16816cudnn_128x128_ldg8_relu_exp_stages_64x3_medium_nhwc_tn_v1 Tactic: 0x2eaa2202de9404d6
[08/10/2023-11:17:08] [V] [TRT] Tactic: 0x2eaa2202de9404d6 Time: 0.406327
[08/10/2023-11:17:08] [V] [TRT] Conv_27 + Relu_28 Set Tactic Name: ampere_h1688cudnn_256x128_ldg8_relu_exp_medium_nhwc_tn_v1 Tactic: 0x30c0f36d0aeeac6a
[08/10/2023-11:17:08] [V] [TRT] Tactic: 0x30c0f36d0aeeac6a Time: 0.362016
[08/10/2023-11:17:08] [V] [TRT] Conv_27 + Relu_28 Set Tactic Name: ampere_h1688cudnn_128x128_ldg8_relu_exp_small_nhwc_tn_v1 Tactic: 0x3369260c04f9ad73
[08/10/2023-11:17:08] [V] [TRT] Tactic: 0x3369260c04f9ad73 Time: 0.324242
[08/10/2023-11:17:08] [V] [TRT] Conv_27 + Relu_28 Set Tactic Name: ampere_h16816cudnn_64x128_ldg8_relu_exp_stages_64x4_small_nhwc_tn_v1 Tactic: 0x3b5fa0f2a8fc2410
[08/10/2023-11:17:08] [V] [TRT] Tactic: 0x3b5fa0f2a8fc2410 Time: 0.543643
[08/10/2023-11:17:08] [V] [TRT] Conv_27 + Relu_28 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x64x32_stage5_warpsize2x2x1_g1_tensor16x8x16_t1r1s1 Tactic: 0x3e2d344492eaa731
[08/10/2023-11:17:08] [V] [TRT] Tactic: 0x3e2d344492eaa731 Time: 0.288014
[08/10/2023-11:17:08] [V] [TRT] Conv_27 + Relu_28 Set Tactic Name: ampere_h16816cudnn_256x64_ldg8_relu_exp_stages_64x3_medium_nhwc_tn_v1 Tactic: 0x3f031df0e579d52c
[08/10/2023-11:17:08] [V] [TRT] Tactic: 0x3f031df0e579d52c Time: 0.409573
[08/10/2023-11:17:08] [V] [TRT] Conv_27 + Relu_28 Set Tactic Name: ampere_h1688cudnn_256x128_ldg8_relu_exp_interior_nhwc_tn_v1 Tactic: 0x3f725d7f77411cdc
[08/10/2023-11:17:08] [V] [TRT] Tactic: 0x3f725d7f77411cdc Time: 0.350514
[08/10/2023-11:17:08] [V] [TRT] Conv_27 + Relu_28 Set Tactic Name: ampere_h16816cudnn_256x64_sliced1x2_ldg8_relu_exp_small_nhwc_tn_v1 Tactic: 0x42a5b8709d0039be
[08/10/2023-11:17:08] [V] [TRT] Tactic: 0x42a5b8709d0039be Time: 0.445326
[08/10/2023-11:17:08] [V] [TRT] Conv_27 + Relu_28 Set Tactic Name: ampere_h16816cudnn_256x128_ldg8_relu_exp_small_nhwc_tn_v1 Tactic: 0x463794ee4acffd1f
[08/10/2023-11:17:08] [V] [TRT] Tactic: 0x463794ee4acffd1f Time: 0.348261
[08/10/2023-11:17:08] [V] [TRT] Conv_27 + Relu_28 Set Tactic Name: ampere_h16816cudnn_256x128_ldg8_relu_exp_stages_64x3_interior_nhwc_tn_v1 Tactic: 0x465d67d3d483219f
[08/10/2023-11:17:08] [V] [TRT] Tactic: 0x465d67d3d483219f Time: 0.383666
[08/10/2023-11:17:08] [V] [TRT] Conv_27 + Relu_28 Set Tactic Name: ampere_h16816cudnn_128x128_ldg8_relu_exp_interior_nhwc_tn_v1 Tactic: 0x483db0113d970335
[08/10/2023-11:17:08] [V] [TRT] Tactic: 0x483db0113d970335 Time: 0.292759
[08/10/2023-11:17:08] [V] [TRT] Conv_27 + Relu_28 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x128x64_stage3_warpsize2x2x1_g1_tensor16x8x16_t1r1s1 Tactic: 0x4a33d90483c0ec01
[08/10/2023-11:17:08] [V] [TRT] Tactic: 0x4a33d90483c0ec01 Time: 0.338386
[08/10/2023-11:17:08] [V] [TRT] Conv_27 + Relu_28 Set Tactic Name: ampere_h1688cudnn_256x64_sliced1x2_ldg8_relu_exp_small_nhwc_tn_v1 Tactic: 0x4aed1956bd10a795
[08/10/2023-11:17:08] [V] [TRT] Tactic: 0x4aed1956bd10a795 Time: 0.454551
[08/10/2023-11:17:08] [V] [TRT] Conv_27 + Relu_28 Set Tactic Name: ampere_h16816cudnn_64x128_ldg8_relu_exp_stages_64x3_small_nhwc_tn_v1 Tactic: 0x53be5e206184e6ad
[08/10/2023-11:17:08] [V] [TRT] Tactic: 0x53be5e206184e6ad Time: 0.412334
[08/10/2023-11:17:08] [V] [TRT] Conv_27 + Relu_28 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x128x64_stage3_warpsize4x2x1_g1_tensor16x8x16_t1r1s1 Tactic: 0x586f548ae63d09ab
[08/10/2023-11:17:08] [V] [TRT] Tactic: 0x586f548ae63d09ab Time: 0.28256
[08/10/2023-11:17:08] [V] [TRT] Conv_27 + Relu_28 Set Tactic Name: ampere_h16816cudnn_128x128_ldg8_relu_exp_stages_64x3_small_nhwc_tn_v1 Tactic: 0x58816bf2e9c36afb
[08/10/2023-11:17:08] [V] [TRT] Tactic: 0x58816bf2e9c36afb Time: 0.407502
[08/10/2023-11:17:08] [V] [TRT] Conv_27 + Relu_28 Set Tactic Name: ampere_h16816cudnn_128x64_sliced1x2_ldg8_relu_exp_stages_64x4_medium_nhwc_tn_v1 Tactic: 0x59c5c647b4c76593
[08/10/2023-11:17:08] [V] [TRT] Tactic: 0x59c5c647b4c76593 Time: 0.636073
[08/10/2023-11:17:08] [V] [TRT] Conv_27 + Relu_28 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x256x64_stage3_warpsize1x4x1_g1_tensor16x8x16_t1r1s1 Tactic: 0x5d5195388b4f5134
[08/10/2023-11:17:08] [V] [TRT] Tactic: 0x5d5195388b4f5134 Time: 0.343712
[08/10/2023-11:17:08] [V] [TRT] Conv_27 + Relu_28 Set Tactic Name: ampere_h16816cudnn_64x64_ldg8_relu_exp_stages_64x5_medium_nhwc_tn_v1 Tactic: 0x5f31c22ec167f384
[08/10/2023-11:17:08] [V] [TRT] Tactic: 0x5f31c22ec167f384 Time: 0.663442
[08/10/2023-11:17:08] [V] [TRT] Conv_27 + Relu_28 Set Tactic Name: ampere_h1688cudnn_256x64_ldg8_relu_exp_medium_nhwc_tn_v1 Tactic: 0x6426696f872a3b13
[08/10/2023-11:17:08] [V] [TRT] Tactic: 0x6426696f872a3b13 Time: 0.368059
[08/10/2023-11:17:08] [V] [TRT] Conv_27 + Relu_28 Set Tactic Name: ampere_h16816cudnn_128x64_ldg8_relu_exp_stages_64x3_medium_nhwc_tn_v1 Tactic: 0x699be152cfb6d6ff
[08/10/2023-11:17:08] [V] [TRT] Tactic: 0x699be152cfb6d6ff Time: 0.425531
[08/10/2023-11:17:08] [V] [TRT] Conv_27 + Relu_28 Set Tactic Name: ampere_h16816cudnn_256x64_ldg8_relu_exp_stages_64x3_interior_nhwc_tn_v1 Tactic: 0x705904bfa4b25aab
[08/10/2023-11:17:08] [V] [TRT] Tactic: 0x705904bfa4b25aab Time: 0.376914
[08/10/2023-11:17:08] [V] [TRT] Conv_27 + Relu_28 Set Tactic Name: ampere_h16816cudnn_64x64_sliced1x2_ldg8_relu_exp_stages_64x5_medium_nhwc_tn_v1 Tactic: 0x706f08da35c795a5
[08/10/2023-11:17:08] [V] [TRT] Tactic: 0x706f08da35c795a5 Time: 0.693893
[08/10/2023-11:17:08] [V] [TRT] Conv_27 + Relu_28 Set Tactic Name: ampere_h16816cudnn_128x64_sliced1x2_ldg8_relu_exp_stages_64x4_small_nhwc_tn_v1 Tactic: 0x7163d33a4d8ce8d7
[08/10/2023-11:17:08] [V] [TRT] Tactic: 0x7163d33a4d8ce8d7 Time: 0.609019
[08/10/2023-11:17:08] [V] [TRT] Conv_27 + Relu_28 Set Tactic Name: ampere_h1688cudnn_256x64_sliced1x2_ldg8_relu_exp_interior_nhwc_tn_v1 Tactic: 0x79a2c15a169f2ec9
[08/10/2023-11:17:08] [V] [TRT] Tactic: 0x79a2c15a169f2ec9 Time: 0.455314
[08/10/2023-11:17:08] [V] [TRT] Conv_27 + Relu_28 Set Tactic Name: ampere_h1688cudnn_256x64_ldg8_relu_exp_small_nhwc_tn_v1 Tactic: 0x7aad3976677d7155
[08/10/2023-11:17:08] [V] [TRT] Tactic: 0x7aad3976677d7155 Time: 0.362121
[08/10/2023-11:17:08] [V] [TRT] Conv_27 + Relu_28 Set Tactic Name: ampere_h16816cudnn_256x64_sliced1x2_ldg8_relu_exp_medium_nhwc_tn_v1 Tactic: 0x82f8a2214b0b4178
[08/10/2023-11:17:08] [V] [TRT] Tactic: 0x82f8a2214b0b4178 Time: 0.458409
[08/10/2023-11:17:08] [V] [TRT] Conv_27 + Relu_28 Set Tactic Name: ampere_h16816cudnn_64x64_ldg8_relu_exp_stages_64x6_medium_nhwc_tn_v1 Tactic: 0x88971eec55aba850
[08/10/2023-11:17:08] [V] [TRT] Tactic: 0x88971eec55aba850 Time: 0.903557
[08/10/2023-11:17:08] [V] [TRT] Conv_27 + Relu_28 Set Tactic Name: ampere_h16816cudnn_256x128_ldg8_relu_exp_interior_nhwc_tn_v1 Tactic: 0x8d15f485c8c7f871
[08/10/2023-11:17:08] [V] [TRT] Tactic: 0x8d15f485c8c7f871 Time: 0.342277
[08/10/2023-11:17:08] [V] [TRT] Conv_27 + Relu_28 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x128x32_stage3_warpsize4x2x1_g1_tensor16x8x16_t1r1s1 Tactic: 0x8e1e99c68a674ff4
[08/10/2023-11:17:08] [V] [TRT] Tactic: 0x8e1e99c68a674ff4 Time: 0.309929
[08/10/2023-11:17:08] [V] [TRT] Conv_27 + Relu_28 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x64x64_stage3_warpsize2x2x1_g1_tensor16x8x16_t1r1s1 Tactic: 0x8f25d6cdaeaaa100
[08/10/2023-11:17:08] [V] [TRT] Tactic: 0x8f25d6cdaeaaa100 Time: 0.297632
[08/10/2023-11:17:08] [V] [TRT] Conv_27 + Relu_28 Set Tactic Name: ampere_h16816cudnn_64x64_sliced1x2_ldg8_relu_exp_stages_64x6_interior_nhwc_tn_v1 Tactic: 0x992a591f6c1eb36f
[08/10/2023-11:17:08] [V] [TRT] Tactic: 0x992a591f6c1eb36f Time: 0.936219
[08/10/2023-11:17:08] [V] [TRT] Conv_27 + Relu_28 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x128x32_stage5_warpsize2x2x1_g1_tensor16x8x16_t1r1s1 Tactic: 0x9d78040b7e8c8ac7
[08/10/2023-11:17:08] [V] [TRT] Tactic: 0x9d78040b7e8c8ac7 Time: 0.269591
[08/10/2023-11:17:08] [V] [TRT] Conv_27 + Relu_28 Set Tactic Name: ampere_h16816cudnn_64x64_ldg8_relu_exp_stages_64x6_interior_nhwc_tn_v1 Tactic: 0x9d9e6402ba06ac05
[08/10/2023-11:17:08] [V] [TRT] Tactic: 0x9d9e6402ba06ac05 Time: 0.814981
[08/10/2023-11:17:08] [V] [TRT] Conv_27 + Relu_28 Set Tactic Name: ampere_h16816cudnn_64x64_sliced1x2_ldg8_relu_exp_stages_64x6_medium_nhwc_tn_v1 Tactic: 0xa7c9d418a10bce71
[08/10/2023-11:17:08] [V] [TRT] Tactic: 0xa7c9d418a10bce71 Time: 1.02747
[08/10/2023-11:17:08] [V] [TRT] Conv_27 + Relu_28 Set Tactic Name: ampere_h16816cudnn_128x64_ldg8_relu_exp_stages_64x4_small_nhwc_tn_v1 Tactic: 0xabd92c9ae596b545
[08/10/2023-11:17:08] [V] [TRT] Tactic: 0xabd92c9ae596b545 Time: 0.546665
[08/10/2023-11:17:08] [V] [TRT] Conv_27 + Relu_28 Set Tactic Name: ampere_h16816cudnn_128x128_ldg8_relu_exp_small_nhwc_tn_v1 Tactic: 0xb33e57fb3e8a0a56
[08/10/2023-11:17:08] [V] [TRT] Tactic: 0xb33e57fb3e8a0a56 Time: 0.290263
[08/10/2023-11:17:08] [V] [TRT] Conv_27 + Relu_28 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x32x32_stage4_warpsize4x1x1_g1_tensor16x8x16_t1r1s1 Tactic: 0xb4ed47991b2d81ae
[08/10/2023-11:17:08] [V] [TRT] Tactic: 0xb4ed47991b2d81ae Time: 0.309883
[08/10/2023-11:17:08] [V] [TRT] Conv_27 + Relu_28 Set Tactic Name: ampere_h16816cudnn_256x64_ldg8_relu_exp_stages_64x3_small_nhwc_tn_v1 Tactic: 0xb6fa5ffcc1a9d518
[08/10/2023-11:17:08] [V] [TRT] Tactic: 0xb6fa5ffcc1a9d518 Time: 0.40784
[08/10/2023-11:17:08] [V] [TRT] Conv_27 + Relu_28 Set Tactic Name: ampere_h16816cudnn_64x128_sliced1x2_ldg8_relu_exp_stages_64x3_medium_nhwc_tn_v1 Tactic: 0xb85e52e87caf60a2
[08/10/2023-11:17:08] [V] [TRT] Tactic: 0xb85e52e87caf60a2 Time: 0.455803
[08/10/2023-11:17:08] [V] [TRT] Conv_27 + Relu_28 Set Tactic Name: ampere_h16816cudnn_64x128_ldg8_relu_exp_stages_64x3_medium_nhwc_tn_v1 Tactic: 0xb8d86216e1235cda
[08/10/2023-11:17:08] [V] [TRT] Tactic: 0xb8d86216e1235cda Time: 0.412274
[08/10/2023-11:17:08] [V] [TRT] Conv_27 + Relu_28 Set Tactic Name: ampere_h16816cudnn_256x128_ldg8_relu_exp_stages_64x3_medium_nhwc_tn_v1 Tactic: 0xb9171d70ba2eda4b
[08/10/2023-11:17:08] [V] [TRT] Tactic: 0xb9171d70ba2eda4b Time: 0.395794
[08/10/2023-11:17:08] [V] [TRT] Conv_27 + Relu_28 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x64x64_stage4_warpsize2x2x1_g1_tensor16x8x16_t1r1s1 Tactic: 0xbb4ac900a7be8b4c
[08/10/2023-11:17:08] [V] [TRT] Tactic: 0xbb4ac900a7be8b4c Time: 0.406949
[08/10/2023-11:17:08] [V] [TRT] Conv_27 + Relu_28 Set Tactic Name: ampere_h16816cudnn_64x128_sliced1x2_ldg8_relu_exp_stages_64x4_small_nhwc_tn_v1 Tactic: 0xbd6f5e6f24c05c10
[08/10/2023-11:17:08] [V] [TRT] Tactic: 0xbd6f5e6f24c05c10 Time: 0.632073
[08/10/2023-11:17:08] [V] [TRT] Conv_27 + Relu_28 Set Tactic Name: ampere_h16816cudnn_64x64_sliced1x2_ldg8_relu_exp_stages_64x5_interior_nhwc_tn_v1 Tactic: 0xc0df55afc74af7ab
[08/10/2023-11:17:08] [V] [TRT] Tactic: 0xc0df55afc74af7ab Time: 0.674697
[08/10/2023-11:17:08] [V] [TRT] Conv_27 + Relu_28 Set Tactic Name: ampere_h16816cudnn_128x64_ldg8_relu_exp_stages_64x3_small_nhwc_tn_v1 Tactic: 0xc338d2482cee77f8
[08/10/2023-11:17:08] [V] [TRT] Tactic: 0xc338d2482cee77f8 Time: 0.434501
[08/10/2023-11:17:08] [V] [TRT] Conv_27 + Relu_28 Set Tactic Name: ampere_h16816cudnn_64x64_ldg8_relu_exp_stages_64x5_interior_nhwc_tn_v1 Tactic: 0xc46b68b21152e8c1
[08/10/2023-11:17:08] [V] [TRT] Tactic: 0xc46b68b21152e8c1 Time: 0.627717
[08/10/2023-11:17:08] [V] [TRT] Conv_27 + Relu_28 Set Tactic Name: ampere_h1688cudnn_256x128_ldg8_relu_exp_small_nhwc_tn_v1 Tactic: 0xc660e51970bc5a3a
[08/10/2023-11:17:08] [V] [TRT] Tactic: 0xc660e51970bc5a3a Time: 0.354757
[08/10/2023-11:17:08] [V] [TRT] Conv_27 + Relu_28 Set Tactic Name: ampere_h16816cudnn_64x128_ldg8_relu_exp_stages_64x4_medium_nhwc_tn_v1 Tactic: 0xc82f3f06140e3cbb
[08/10/2023-11:17:08] [V] [TRT] Tactic: 0xc82f3f06140e3cbb Time: 0.561481
[08/10/2023-11:17:08] [V] [TRT] Conv_27 + Relu_28 Set Tactic Name: ampere_h16816cudnn_64x128_sliced1x2_ldg8_relu_exp_stages_64x4_medium_nhwc_tn_v1 Tactic: 0xc8a90ff8898200c3
[08/10/2023-11:17:08] [V] [TRT] Tactic: 0xc8a90ff8898200c3 Time: 0.636037
[08/10/2023-11:17:08] [V] [TRT] Conv_27 + Relu_28 Set Tactic Name: ampere_h1688cudnn_256x64_sliced1x2_ldg8_relu_exp_medium_nhwc_tn_v1 Tactic: 0xc9cc55109bb4de26
[08/10/2023-11:17:08] [V] [TRT] Tactic: 0xc9cc55109bb4de26 Time: 0.471771
[08/10/2023-11:17:08] [V] [TRT] Conv_27 + Relu_28 Set Tactic Name: ampere_h16816cudnn_256x64_ldg8_relu_exp_interior_nhwc_tn_v1 Tactic: 0xc9d48aa637bf7810
[08/10/2023-11:17:08] [V] [TRT] Tactic: 0xc9d48aa637bf7810 Time: 0.345093
[08/10/2023-11:17:08] [V] [TRT] Conv_27 + Relu_28 Set Tactic Name: ampere_h16816cudnn_256x64_ldg8_relu_exp_small_nhwc_tn_v1 Tactic: 0xca5d3a11fd48f571
[08/10/2023-11:17:08] [V] [TRT] Tactic: 0xca5d3a11fd48f571 Time: 0.353029
[08/10/2023-11:17:08] [V] [TRT] Conv_27 + Relu_28 Set Tactic Name: ampere_h16816cudnn_64x64_ldg8_relu_exp_stages_64x5_small_nhwc_tn_v1 Tactic: 0xce0506e1512285c3
[08/10/2023-11:17:08] [V] [TRT] Tactic: 0xce0506e1512285c3 Time: 0.648695
[08/10/2023-11:17:08] [V] [TRT] Conv_27 + Relu_28 Set Tactic Name: ampere_h16816cudnn_64x128_sliced1x2_ldg8_relu_exp_stages_64x3_small_nhwc_tn_v1 Tactic: 0xd58ea0bdedb89ead
[08/10/2023-11:17:08] [V] [TRT] Tactic: 0xd58ea0bdedb89ead Time: 0.452859
[08/10/2023-11:17:08] [V] [TRT] Conv_27 + Relu_28 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x64x32_stage3_warpsize4x1x1_g1_tensor16x8x16_t1r1s1 Tactic: 0xd80cb0f3373aef38
[08/10/2023-11:17:08] [V] [TRT] Tactic: 0xd80cb0f3373aef38 Time: 0.272997
[08/10/2023-11:17:08] [V] [TRT] Conv_27 + Relu_28 Set Tactic Name: ampere_h16816cudnn_128x128_ldg8_relu_exp_stages_64x3_interior_nhwc_tn_v1 Tactic: 0xdbe058db40209ee1
[08/10/2023-11:17:08] [V] [TRT] Tactic: 0xdbe058db40209ee1 Time: 0.392219
[08/10/2023-11:17:08] [V] [TRT] Conv_27 + Relu_28 Set Tactic Name: ampere_h16816cudnn_256x64_sliced1x2_ldg8_relu_exp_interior_nhwc_tn_v1 Tactic: 0xdf5651a32d9311a6
[08/10/2023-11:17:08] [V] [TRT] Tactic: 0xdf5651a32d9311a6 Time: 0.448242
[08/10/2023-11:17:08] [V] [TRT] Conv_27 + Relu_28 Set Tactic Name: ampere_h16816cudnn_128x64_sliced1x2_ldg8_relu_exp_stages_64x4_interior_nhwc_tn_v1 Tactic: 0xe3cad530d257946a
[08/10/2023-11:17:08] [V] [TRT] Tactic: 0xe3cad530d257946a Time: 0.592974
[08/10/2023-11:17:08] [V] [TRT] Conv_27 + Relu_28 Set Tactic Name: ampere_h16816cudnn_64x128_ldg8_relu_exp_stages_64x3_interior_nhwc_tn_v1 Tactic: 0xe433b7de8e028d8e
[08/10/2023-11:17:08] [V] [TRT] Tactic: 0xe433b7de8e028d8e Time: 0.406619
[08/10/2023-11:17:08] [V] [TRT] Conv_27 + Relu_28 Set Tactic Name: ampere_h16816cudnn_256x64_ldg8_relu_exp_medium_nhwc_tn_v1 Tactic: 0xe4711898bd599c36
[08/10/2023-11:17:08] [V] [TRT] Tactic: 0xe4711898bd599c36 Time: 0.36
[08/10/2023-11:17:08] [V] [TRT] Conv_27 + Relu_28 Set Tactic Name: ampere_h16816cudnn_128x64_sliced1x2_ldg8_relu_exp_stages_64x3_interior_nhwc_tn_v1 Tactic: 0xe93bad2778c92bdd
[08/10/2023-11:17:08] [V] [TRT] Tactic: 0xe93bad2778c92bdd Time: 0.454738
[08/10/2023-11:17:08] [V] [TRT] Conv_27 + Relu_28 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x128x32_stage4_warpsize2x2x1_g1_tensor16x8x16_t1r1s1 Tactic: 0xea50b6d3d87bf5dd
[08/10/2023-11:17:08] [V] [TRT] Tactic: 0xea50b6d3d87bf5dd Time: 0.229696
[08/10/2023-11:17:08] [V] [TRT] Conv_27 + Relu_28 Set Tactic Name: ampere_h16816cudnn_64x128_ldg8_relu_exp_stages_64x4_interior_nhwc_tn_v1 Tactic: 0xeec2cfc9249c3239
[08/10/2023-11:17:08] [V] [TRT] Tactic: 0xeec2cfc9249c3239 Time: 0.540736
[08/10/2023-11:17:08] [V] [TRT] Conv_27 + Relu_28 Set Tactic Name: ampere_h16816cudnn_64x64_ldg8_relu_exp_stages_64x6_small_nhwc_tn_v1 Tactic: 0xf0beb09df9a19f82
[08/10/2023-11:17:08] [V] [TRT] Tactic: 0xf0beb09df9a19f82 Time: 0.852352
[08/10/2023-11:17:08] [V] [TRT] Conv_27 + Relu_28 Set Tactic Name: ampere_h16816cudnn_128x64_ldg8_relu_exp_stages_64x4_interior_nhwc_tn_v1 Tactic: 0xf1973f09f9ca8bf3
[08/10/2023-11:17:08] [V] [TRT] Tactic: 0xf1973f09f9ca8bf3 Time: 0.522976
[08/10/2023-11:17:08] [V] [TRT] Conv_27 + Relu_28 Set Tactic Name: ampere_h1688cudnn_128x128_ldg8_relu_exp_interior_nhwc_tn_v1 Tactic: 0xfa5a19eb8211e798
[08/10/2023-11:17:08] [V] [TRT] Tactic: 0xfa5a19eb8211e798 Time: 0.327392
[08/10/2023-11:17:08] [V] [TRT] Conv_27 + Relu_28 Set Tactic Name: ampere_h16816cudnn_128x64_ldg8_relu_exp_stages_64x3_interior_nhwc_tn_v1 Tactic: 0xfb66471e53543444
[08/10/2023-11:17:08] [V] [TRT] Tactic: 0xfb66471e53543444 Time: 0.413765
[08/10/2023-11:17:08] [V] [TRT] Fastest Tactic: 0xea50b6d3d87bf5dd Time: 0.229696
[08/10/2023-11:17:08] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0xea50b6d3d87bf5dd
[08/10/2023-11:17:08] [V] [TRT] *************** Autotuning format combination: Half(259200,1:16,1440,8) -> Half(1036800,1:16,5760,32) ***************
[08/10/2023-11:17:08] [W] [TRT] Weights [name=Conv_27 + Relu_28.weight] had the following issues when converted to FP16:
[08/10/2023-11:17:08] [W] [TRT]  - Subnormal FP16 values detected. 
[08/10/2023-11:17:08] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.
[08/10/2023-11:17:08] [V] [TRT] --------------- Timing Runner: Conv_27 + Relu_28 (CublasConvolution)
[08/10/2023-11:17:08] [V] [TRT] CublasConvolution has no valid tactics for this config, skipping
[08/10/2023-11:17:08] [W] [TRT] Weights [name=Conv_27 + Relu_28.weight] had the following issues when converted to FP16:
[08/10/2023-11:17:08] [W] [TRT]  - Subnormal FP16 values detected. 
[08/10/2023-11:17:08] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.
[08/10/2023-11:17:08] [V] [TRT] --------------- Timing Runner: Conv_27 + Relu_28 (CaskConvolution)
[08/10/2023-11:17:08] [V] [TRT] Conv_27 + Relu_28 Set Tactic Name: ampere_h16816cudnn_256x128_ldg8_relu_exp_medium_nhwc_tn_v1 Tactic: 0x03896956a39a1203
[08/10/2023-11:17:08] [V] [TRT] Tactic: 0x03896956a39a1203 Time: 0.355099
[08/10/2023-11:17:08] [V] [TRT] Conv_27 + Relu_28 Set Tactic Name: ampere_h16816cudnn_256x128_ldg8_relu_exp_stages_64x3_small_nhwc_tn_v1 Tactic: 0x0e199750c30c6928
[08/10/2023-11:17:08] [V] [TRT] Tactic: 0x0e199750c30c6928 Time: 0.387831
[08/10/2023-11:17:08] [V] [TRT] Conv_27 + Relu_28 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x64x64_stage3_warpsize4x1x1_g1_tensor16x8x16_t1r1s1 Tactic: 0x0e7d7dbf5b60a967
[08/10/2023-11:17:08] [V] [TRT] Tactic: 0x0e7d7dbf5b60a967 Time: 0.346066
[08/10/2023-11:17:08] [V] [TRT] Conv_27 + Relu_28 Set Tactic Name: ampere_h1688cudnn_128x128_ldg8_relu_exp_medium_nhwc_tn_v1 Tactic: 0x100c1ad308e08d35
[08/10/2023-11:17:08] [V] [TRT] Tactic: 0x100c1ad308e08d35 Time: 0.333979
[08/10/2023-11:17:08] [V] [TRT] Conv_27 + Relu_28 Set Tactic Name: ampere_h1688cudnn_256x64_ldg8_relu_exp_interior_nhwc_tn_v1 Tactic: 0x12e96131b90aa494
[08/10/2023-11:17:08] [V] [TRT] Tactic: 0x12e96131b90aa494 Time: 0.36005
[08/10/2023-11:17:08] [V] [TRT] Conv_27 + Relu_28 Set Tactic Name: ampere_h16816cudnn_64x128_sliced1x2_ldg8_relu_exp_stages_64x4_interior_nhwc_tn_v1 Tactic: 0x15e036703de3ebb6
[08/10/2023-11:17:08] [V] [TRT] Tactic: 0x15e036703de3ebb6 Time: 0.624032
[08/10/2023-11:17:08] [V] [TRT] Conv_27 + Relu_28 Set Tactic Name: ampere_h16816cudnn_128x64_ldg8_relu_exp_stages_64x4_medium_nhwc_tn_v1 Tactic: 0x196cbc423a9bb69e
[08/10/2023-11:17:08] [V] [TRT] Tactic: 0x196cbc423a9bb69e Time: 0.563145
[08/10/2023-11:17:08] [V] [TRT] Conv_27 + Relu_28 Set Tactic Name: ampere_h16816cudnn_128x64_sliced1x2_ldg8_relu_exp_stages_64x3_small_nhwc_tn_v1 Tactic: 0x19822de884f42a6a
[08/10/2023-11:17:08] [V] [TRT] Tactic: 0x19822de884f42a6a Time: 0.454446
[08/10/2023-11:17:08] [V] [TRT] Conv_27 + Relu_28 Set Tactic Name: ampere_h16816cudnn_64x64_sliced1x2_ldg8_relu_exp_stages_64x5_small_nhwc_tn_v1 Tactic: 0x1a5ba808ad4cc5a8
[08/10/2023-11:17:08] [V] [TRT] Tactic: 0x1a5ba808ad4cc5a8 Time: 0.679922
[08/10/2023-11:17:08] [V] [TRT] Conv_27 + Relu_28 Set Tactic Name: ampere_h16816cudnn_64x128_sliced1x2_ldg8_relu_exp_stages_64x3_interior_nhwc_tn_v1 Tactic: 0x1f114e67977d5401
[08/10/2023-11:17:08] [V] [TRT] Tactic: 0x1f114e67977d5401 Time: 0.443977
[08/10/2023-11:17:08] [V] [TRT] Conv_27 + Relu_28 Set Tactic Name: ampere_h16816cudnn_128x128_ldg8_relu_exp_medium_nhwc_tn_v1 Tactic: 0x234580e8a194335c
[08/10/2023-11:17:08] [V] [TRT] Tactic: 0x234580e8a194335c Time: 0.298222
[08/10/2023-11:17:08] [V] [TRT] Conv_27 + Relu_28 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x256x32_stage3_warpsize2x4x1_g1_tensor16x8x16_t1r1s1 Tactic: 0x23848f7cc4e20635
[08/10/2023-11:17:08] [V] [TRT] Tactic: 0x23848f7cc4e20635 Time: 0.311017
[08/10/2023-11:17:08] [V] [TRT] Conv_27 + Relu_28 Set Tactic Name: ampere_h16816cudnn_64x64_sliced1x2_ldg8_relu_exp_stages_64x6_small_nhwc_tn_v1 Tactic: 0x24e01e7405cfdfe9
[08/10/2023-11:17:08] [V] [TRT] Tactic: 0x24e01e7405cfdfe9 Time: 0.987941
[08/10/2023-11:17:08] [V] [TRT] Conv_27 + Relu_28 Set Tactic Name: ampere_h16816cudnn_128x64_sliced1x2_ldg8_relu_exp_stages_64x3_medium_nhwc_tn_v1 Tactic: 0x29329b5741ea05f2
[08/10/2023-11:17:08] [V] [TRT] Tactic: 0x29329b5741ea05f2 Time: 0.453874
[08/10/2023-11:17:08] [V] [TRT] Conv_27 + Relu_28 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x32x64_stage5_warpsize2x2x1_g1_tensor16x8x16_t1r1s1 Tactic: 0x2aa016c86360697f
[08/10/2023-11:17:08] [V] [TRT] Tactic: 0x2aa016c86360697f Time: 0.616722
[08/10/2023-11:17:08] [V] [TRT] Conv_27 + Relu_28 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x32x32_stage4_warpsize4x1x1_g1_tensor16x8x16_t1r1s1 Tactic: 0x2e9f40fea3fe4d65
[08/10/2023-11:17:08] [V] [TRT] Tactic: 0x2e9f40fea3fe4d65 Time: 0.334395
[08/10/2023-11:17:08] [V] [TRT] Conv_27 + Relu_28 Set Tactic Name: ampere_h16816cudnn_128x128_ldg8_relu_exp_stages_64x3_medium_nhwc_tn_v1 Tactic: 0x2eaa2202de9404d6
[08/10/2023-11:17:08] [V] [TRT] Tactic: 0x2eaa2202de9404d6 Time: 0.417033
[08/10/2023-11:17:08] [V] [TRT] Conv_27 + Relu_28 Set Tactic Name: ampere_h1688cudnn_256x128_ldg8_relu_exp_medium_nhwc_tn_v1 Tactic: 0x30c0f36d0aeeac6a
[08/10/2023-11:17:08] [V] [TRT] Tactic: 0x30c0f36d0aeeac6a Time: 0.363255
[08/10/2023-11:17:08] [V] [TRT] Conv_27 + Relu_28 Set Tactic Name: ampere_h1688cudnn_128x128_ldg8_relu_exp_small_nhwc_tn_v1 Tactic: 0x3369260c04f9ad73
[08/10/2023-11:17:08] [V] [TRT] Tactic: 0x3369260c04f9ad73 Time: 0.330135
[08/10/2023-11:17:08] [V] [TRT] Conv_27 + Relu_28 Set Tactic Name: ampere_h16816cudnn_64x128_ldg8_relu_exp_stages_64x4_small_nhwc_tn_v1 Tactic: 0x3b5fa0f2a8fc2410
[08/10/2023-11:17:08] [V] [TRT] Tactic: 0x3b5fa0f2a8fc2410 Time: 0.559323
[08/10/2023-11:17:08] [V] [TRT] Conv_27 + Relu_28 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x64x32_stage5_warpsize2x2x1_g1_tensor16x8x16_t1r1s1 Tactic: 0x3e2d344492eaa731
[08/10/2023-11:17:08] [V] [TRT] Tactic: 0x3e2d344492eaa731 Time: 0.288686
[08/10/2023-11:17:08] [V] [TRT] Conv_27 + Relu_28 Set Tactic Name: ampere_h16816cudnn_256x64_ldg8_relu_exp_stages_64x3_medium_nhwc_tn_v1 Tactic: 0x3f031df0e579d52c
[08/10/2023-11:17:08] [V] [TRT] Tactic: 0x3f031df0e579d52c Time: 0.408411
[08/10/2023-11:17:08] [V] [TRT] Conv_27 + Relu_28 Set Tactic Name: ampere_h1688cudnn_256x128_ldg8_relu_exp_interior_nhwc_tn_v1 Tactic: 0x3f725d7f77411cdc
[08/10/2023-11:17:08] [V] [TRT] Tactic: 0x3f725d7f77411cdc Time: 0.351931
[08/10/2023-11:17:08] [V] [TRT] Conv_27 + Relu_28 Set Tactic Name: ampere_h16816cudnn_256x64_sliced1x2_ldg8_relu_exp_small_nhwc_tn_v1 Tactic: 0x42a5b8709d0039be
[08/10/2023-11:17:08] [V] [TRT] Tactic: 0x42a5b8709d0039be Time: 0.445298
[08/10/2023-11:17:08] [V] [TRT] Conv_27 + Relu_28 Set Tactic Name: ampere_h16816cudnn_256x128_ldg8_relu_exp_small_nhwc_tn_v1 Tactic: 0x463794ee4acffd1f
[08/10/2023-11:17:09] [V] [TRT] Tactic: 0x463794ee4acffd1f Time: 0.347826
[08/10/2023-11:17:09] [V] [TRT] Conv_27 + Relu_28 Set Tactic Name: ampere_h16816cudnn_256x128_ldg8_relu_exp_stages_64x3_interior_nhwc_tn_v1 Tactic: 0x465d67d3d483219f
[08/10/2023-11:17:09] [V] [TRT] Tactic: 0x465d67d3d483219f Time: 0.384206
[08/10/2023-11:17:09] [V] [TRT] Conv_27 + Relu_28 Set Tactic Name: ampere_h16816cudnn_128x128_ldg8_relu_exp_interior_nhwc_tn_v1 Tactic: 0x483db0113d970335
[08/10/2023-11:17:09] [V] [TRT] Tactic: 0x483db0113d970335 Time: 0.291218
[08/10/2023-11:17:09] [V] [TRT] Conv_27 + Relu_28 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x128x64_stage3_warpsize2x2x1_g1_tensor16x8x16_t1r1s1 Tactic: 0x4a33d90483c0ec01
[08/10/2023-11:17:09] [V] [TRT] Tactic: 0x4a33d90483c0ec01 Time: 0.333307
[08/10/2023-11:17:09] [V] [TRT] Conv_27 + Relu_28 Set Tactic Name: ampere_h1688cudnn_256x64_sliced1x2_ldg8_relu_exp_small_nhwc_tn_v1 Tactic: 0x4aed1956bd10a795
[08/10/2023-11:17:09] [V] [TRT] Tactic: 0x4aed1956bd10a795 Time: 0.466418
[08/10/2023-11:17:09] [V] [TRT] Conv_27 + Relu_28 Set Tactic Name: ampere_h16816cudnn_64x128_ldg8_relu_exp_stages_64x3_small_nhwc_tn_v1 Tactic: 0x53be5e206184e6ad
[08/10/2023-11:17:09] [V] [TRT] Tactic: 0x53be5e206184e6ad Time: 0.41877
[08/10/2023-11:17:09] [V] [TRT] Conv_27 + Relu_28 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x128x64_stage3_warpsize4x2x1_g1_tensor16x8x16_t1r1s1 Tactic: 0x586f548ae63d09ab
[08/10/2023-11:17:09] [V] [TRT] Tactic: 0x586f548ae63d09ab Time: 0.2824
[08/10/2023-11:17:09] [V] [TRT] Conv_27 + Relu_28 Set Tactic Name: ampere_h16816cudnn_128x128_ldg8_relu_exp_stages_64x3_small_nhwc_tn_v1 Tactic: 0x58816bf2e9c36afb
[08/10/2023-11:17:09] [V] [TRT] Tactic: 0x58816bf2e9c36afb Time: 0.413449
[08/10/2023-11:17:09] [V] [TRT] Conv_27 + Relu_28 Set Tactic Name: ampere_h16816cudnn_128x64_sliced1x2_ldg8_relu_exp_stages_64x4_medium_nhwc_tn_v1 Tactic: 0x59c5c647b4c76593
[08/10/2023-11:17:09] [V] [TRT] Tactic: 0x59c5c647b4c76593 Time: 0.632133
[08/10/2023-11:17:09] [V] [TRT] Conv_27 + Relu_28 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x256x64_stage3_warpsize1x4x1_g1_tensor16x8x16_t1r1s1 Tactic: 0x5d5195388b4f5134
[08/10/2023-11:17:09] [V] [TRT] Tactic: 0x5d5195388b4f5134 Time: 0.339799
[08/10/2023-11:17:09] [V] [TRT] Conv_27 + Relu_28 Set Tactic Name: ampere_h16816cudnn_64x64_ldg8_relu_exp_stages_64x5_medium_nhwc_tn_v1 Tactic: 0x5f31c22ec167f384
[08/10/2023-11:17:09] [V] [TRT] Tactic: 0x5f31c22ec167f384 Time: 0.657957
[08/10/2023-11:17:09] [V] [TRT] Conv_27 + Relu_28 Set Tactic Name: ampere_h1688cudnn_256x64_ldg8_relu_exp_medium_nhwc_tn_v1 Tactic: 0x6426696f872a3b13
[08/10/2023-11:17:09] [V] [TRT] Tactic: 0x6426696f872a3b13 Time: 0.369193
[08/10/2023-11:17:09] [V] [TRT] Conv_27 + Relu_28 Set Tactic Name: ampere_h16816cudnn_128x64_ldg8_relu_exp_stages_64x3_medium_nhwc_tn_v1 Tactic: 0x699be152cfb6d6ff
[08/10/2023-11:17:09] [V] [TRT] Tactic: 0x699be152cfb6d6ff Time: 0.436114
[08/10/2023-11:17:09] [V] [TRT] Conv_27 + Relu_28 Set Tactic Name: ampere_h16816cudnn_256x64_ldg8_relu_exp_stages_64x3_interior_nhwc_tn_v1 Tactic: 0x705904bfa4b25aab
[08/10/2023-11:17:09] [V] [TRT] Tactic: 0x705904bfa4b25aab Time: 0.382089
[08/10/2023-11:17:09] [V] [TRT] Conv_27 + Relu_28 Set Tactic Name: ampere_h16816cudnn_64x64_sliced1x2_ldg8_relu_exp_stages_64x5_medium_nhwc_tn_v1 Tactic: 0x706f08da35c795a5
[08/10/2023-11:17:09] [V] [TRT] Tactic: 0x706f08da35c795a5 Time: 0.697568
[08/10/2023-11:17:09] [V] [TRT] Conv_27 + Relu_28 Set Tactic Name: ampere_h16816cudnn_128x64_sliced1x2_ldg8_relu_exp_stages_64x4_small_nhwc_tn_v1 Tactic: 0x7163d33a4d8ce8d7
[08/10/2023-11:17:09] [V] [TRT] Tactic: 0x7163d33a4d8ce8d7 Time: 0.60405
[08/10/2023-11:17:09] [V] [TRT] Conv_27 + Relu_28 Set Tactic Name: ampere_h1688cudnn_256x64_sliced1x2_ldg8_relu_exp_interior_nhwc_tn_v1 Tactic: 0x79a2c15a169f2ec9
[08/10/2023-11:17:09] [V] [TRT] Tactic: 0x79a2c15a169f2ec9 Time: 0.453001
[08/10/2023-11:17:09] [V] [TRT] Conv_27 + Relu_28 Set Tactic Name: ampere_h1688cudnn_256x64_ldg8_relu_exp_small_nhwc_tn_v1 Tactic: 0x7aad3976677d7155
[08/10/2023-11:17:09] [V] [TRT] Tactic: 0x7aad3976677d7155 Time: 0.358226
[08/10/2023-11:17:09] [V] [TRT] Conv_27 + Relu_28 Set Tactic Name: ampere_h16816cudnn_256x64_sliced1x2_ldg8_relu_exp_medium_nhwc_tn_v1 Tactic: 0x82f8a2214b0b4178
[08/10/2023-11:17:09] [V] [TRT] Tactic: 0x82f8a2214b0b4178 Time: 0.456576
[08/10/2023-11:17:09] [V] [TRT] Conv_27 + Relu_28 Set Tactic Name: ampere_h16816cudnn_64x64_ldg8_relu_exp_stages_64x6_medium_nhwc_tn_v1 Tactic: 0x88971eec55aba850
[08/10/2023-11:17:09] [V] [TRT] Tactic: 0x88971eec55aba850 Time: 0.90197
[08/10/2023-11:17:09] [V] [TRT] Conv_27 + Relu_28 Set Tactic Name: ampere_h16816cudnn_256x128_ldg8_relu_exp_interior_nhwc_tn_v1 Tactic: 0x8d15f485c8c7f871
[08/10/2023-11:17:09] [V] [TRT] Tactic: 0x8d15f485c8c7f871 Time: 0.347639
[08/10/2023-11:17:09] [V] [TRT] Conv_27 + Relu_28 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x128x32_stage3_warpsize4x2x1_g1_tensor16x8x16_t1r1s1 Tactic: 0x8e1e99c68a674ff4
[08/10/2023-11:17:09] [V] [TRT] Tactic: 0x8e1e99c68a674ff4 Time: 0.310565
[08/10/2023-11:17:09] [V] [TRT] Conv_27 + Relu_28 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x64x64_stage3_warpsize2x2x1_g1_tensor16x8x16_t1r1s1 Tactic: 0x8f25d6cdaeaaa100
[08/10/2023-11:17:09] [V] [TRT] Tactic: 0x8f25d6cdaeaaa100 Time: 0.294679
[08/10/2023-11:17:09] [V] [TRT] Conv_27 + Relu_28 Set Tactic Name: ampere_h16816cudnn_64x64_sliced1x2_ldg8_relu_exp_stages_64x6_interior_nhwc_tn_v1 Tactic: 0x992a591f6c1eb36f
[08/10/2023-11:17:09] [V] [TRT] Tactic: 0x992a591f6c1eb36f Time: 0.935173
[08/10/2023-11:17:09] [V] [TRT] Conv_27 + Relu_28 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x128x32_stage5_warpsize2x2x1_g1_tensor16x8x16_t1r1s1 Tactic: 0x9d78040b7e8c8ac7
[08/10/2023-11:17:09] [V] [TRT] Tactic: 0x9d78040b7e8c8ac7 Time: 0.269262
[08/10/2023-11:17:09] [V] [TRT] Conv_27 + Relu_28 Set Tactic Name: ampere_h16816cudnn_64x64_ldg8_relu_exp_stages_64x6_interior_nhwc_tn_v1 Tactic: 0x9d9e6402ba06ac05
[08/10/2023-11:17:09] [V] [TRT] Tactic: 0x9d9e6402ba06ac05 Time: 0.81243
[08/10/2023-11:17:09] [V] [TRT] Conv_27 + Relu_28 Set Tactic Name: ampere_h16816cudnn_64x64_sliced1x2_ldg8_relu_exp_stages_64x6_medium_nhwc_tn_v1 Tactic: 0xa7c9d418a10bce71
[08/10/2023-11:17:09] [V] [TRT] Tactic: 0xa7c9d418a10bce71 Time: 1.03288
[08/10/2023-11:17:09] [V] [TRT] Conv_27 + Relu_28 Set Tactic Name: ampere_h16816cudnn_128x64_ldg8_relu_exp_stages_64x4_small_nhwc_tn_v1 Tactic: 0xabd92c9ae596b545
[08/10/2023-11:17:09] [V] [TRT] Tactic: 0xabd92c9ae596b545 Time: 0.563063
[08/10/2023-11:17:09] [V] [TRT] Conv_27 + Relu_28 Set Tactic Name: ampere_h16816cudnn_128x128_ldg8_relu_exp_small_nhwc_tn_v1 Tactic: 0xb33e57fb3e8a0a56
[08/10/2023-11:17:09] [V] [TRT] Tactic: 0xb33e57fb3e8a0a56 Time: 0.297623
[08/10/2023-11:17:09] [V] [TRT] Conv_27 + Relu_28 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x32x32_stage4_warpsize4x1x1_g1_tensor16x8x16_t1r1s1 Tactic: 0xb4ed47991b2d81ae
[08/10/2023-11:17:09] [V] [TRT] Tactic: 0xb4ed47991b2d81ae Time: 0.308741
[08/10/2023-11:17:09] [V] [TRT] Conv_27 + Relu_28 Set Tactic Name: ampere_h16816cudnn_256x64_ldg8_relu_exp_stages_64x3_small_nhwc_tn_v1 Tactic: 0xb6fa5ffcc1a9d518
[08/10/2023-11:17:09] [V] [TRT] Tactic: 0xb6fa5ffcc1a9d518 Time: 0.406149
[08/10/2023-11:17:09] [V] [TRT] Conv_27 + Relu_28 Set Tactic Name: ampere_h16816cudnn_64x128_sliced1x2_ldg8_relu_exp_stages_64x3_medium_nhwc_tn_v1 Tactic: 0xb85e52e87caf60a2
[08/10/2023-11:17:09] [V] [TRT] Tactic: 0xb85e52e87caf60a2 Time: 0.451099
[08/10/2023-11:17:09] [V] [TRT] Conv_27 + Relu_28 Set Tactic Name: ampere_h16816cudnn_64x128_ldg8_relu_exp_stages_64x3_medium_nhwc_tn_v1 Tactic: 0xb8d86216e1235cda
[08/10/2023-11:17:09] [V] [TRT] Tactic: 0xb8d86216e1235cda Time: 0.409179
[08/10/2023-11:17:09] [V] [TRT] Conv_27 + Relu_28 Set Tactic Name: ampere_h16816cudnn_256x128_ldg8_relu_exp_stages_64x3_medium_nhwc_tn_v1 Tactic: 0xb9171d70ba2eda4b
[08/10/2023-11:17:09] [V] [TRT] Tactic: 0xb9171d70ba2eda4b Time: 0.395648
[08/10/2023-11:17:09] [V] [TRT] Conv_27 + Relu_28 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x64x64_stage4_warpsize2x2x1_g1_tensor16x8x16_t1r1s1 Tactic: 0xbb4ac900a7be8b4c
[08/10/2023-11:17:09] [V] [TRT] Tactic: 0xbb4ac900a7be8b4c Time: 0.40517
[08/10/2023-11:17:09] [V] [TRT] Conv_27 + Relu_28 Set Tactic Name: ampere_h16816cudnn_64x128_sliced1x2_ldg8_relu_exp_stages_64x4_small_nhwc_tn_v1 Tactic: 0xbd6f5e6f24c05c10
[08/10/2023-11:17:09] [V] [TRT] Tactic: 0xbd6f5e6f24c05c10 Time: 0.636517
[08/10/2023-11:17:09] [V] [TRT] Conv_27 + Relu_28 Set Tactic Name: ampere_h16816cudnn_64x64_sliced1x2_ldg8_relu_exp_stages_64x5_interior_nhwc_tn_v1 Tactic: 0xc0df55afc74af7ab
[08/10/2023-11:17:09] [V] [TRT] Tactic: 0xc0df55afc74af7ab Time: 0.670985
[08/10/2023-11:17:09] [V] [TRT] Conv_27 + Relu_28 Set Tactic Name: ampere_h16816cudnn_128x64_ldg8_relu_exp_stages_64x3_small_nhwc_tn_v1 Tactic: 0xc338d2482cee77f8
[08/10/2023-11:17:09] [V] [TRT] Tactic: 0xc338d2482cee77f8 Time: 0.429536
[08/10/2023-11:17:09] [V] [TRT] Conv_27 + Relu_28 Set Tactic Name: ampere_h16816cudnn_64x64_ldg8_relu_exp_stages_64x5_interior_nhwc_tn_v1 Tactic: 0xc46b68b21152e8c1
[08/10/2023-11:17:09] [V] [TRT] Tactic: 0xc46b68b21152e8c1 Time: 0.624544
[08/10/2023-11:17:09] [V] [TRT] Conv_27 + Relu_28 Set Tactic Name: ampere_h1688cudnn_256x128_ldg8_relu_exp_small_nhwc_tn_v1 Tactic: 0xc660e51970bc5a3a
[08/10/2023-11:17:09] [V] [TRT] Tactic: 0xc660e51970bc5a3a Time: 0.35584
[08/10/2023-11:17:09] [V] [TRT] Conv_27 + Relu_28 Set Tactic Name: ampere_h16816cudnn_64x128_ldg8_relu_exp_stages_64x4_medium_nhwc_tn_v1 Tactic: 0xc82f3f06140e3cbb
[08/10/2023-11:17:09] [V] [TRT] Tactic: 0xc82f3f06140e3cbb Time: 0.563776
[08/10/2023-11:17:09] [V] [TRT] Conv_27 + Relu_28 Set Tactic Name: ampere_h16816cudnn_64x128_sliced1x2_ldg8_relu_exp_stages_64x4_medium_nhwc_tn_v1 Tactic: 0xc8a90ff8898200c3
[08/10/2023-11:17:09] [V] [TRT] Tactic: 0xc8a90ff8898200c3 Time: 0.642624
[08/10/2023-11:17:09] [V] [TRT] Conv_27 + Relu_28 Set Tactic Name: ampere_h1688cudnn_256x64_sliced1x2_ldg8_relu_exp_medium_nhwc_tn_v1 Tactic: 0xc9cc55109bb4de26
[08/10/2023-11:17:09] [V] [TRT] Tactic: 0xc9cc55109bb4de26 Time: 0.472073
[08/10/2023-11:17:09] [V] [TRT] Conv_27 + Relu_28 Set Tactic Name: ampere_h16816cudnn_256x64_ldg8_relu_exp_interior_nhwc_tn_v1 Tactic: 0xc9d48aa637bf7810
[08/10/2023-11:17:09] [V] [TRT] Tactic: 0xc9d48aa637bf7810 Time: 0.34592
[08/10/2023-11:17:09] [V] [TRT] Conv_27 + Relu_28 Set Tactic Name: ampere_h16816cudnn_256x64_ldg8_relu_exp_small_nhwc_tn_v1 Tactic: 0xca5d3a11fd48f571
[08/10/2023-11:17:09] [V] [TRT] Tactic: 0xca5d3a11fd48f571 Time: 0.354528
[08/10/2023-11:17:09] [V] [TRT] Conv_27 + Relu_28 Set Tactic Name: ampere_h16816cudnn_64x64_ldg8_relu_exp_stages_64x5_small_nhwc_tn_v1 Tactic: 0xce0506e1512285c3
[08/10/2023-11:17:09] [V] [TRT] Tactic: 0xce0506e1512285c3 Time: 0.656448
[08/10/2023-11:17:09] [V] [TRT] Conv_27 + Relu_28 Set Tactic Name: ampere_h16816cudnn_64x128_sliced1x2_ldg8_relu_exp_stages_64x3_small_nhwc_tn_v1 Tactic: 0xd58ea0bdedb89ead
[08/10/2023-11:17:09] [V] [TRT] Tactic: 0xd58ea0bdedb89ead Time: 0.462757
[08/10/2023-11:17:09] [V] [TRT] Conv_27 + Relu_28 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x64x32_stage3_warpsize4x1x1_g1_tensor16x8x16_t1r1s1 Tactic: 0xd80cb0f3373aef38
[08/10/2023-11:17:09] [V] [TRT] Tactic: 0xd80cb0f3373aef38 Time: 0.274382
[08/10/2023-11:17:09] [V] [TRT] Conv_27 + Relu_28 Set Tactic Name: ampere_h16816cudnn_128x128_ldg8_relu_exp_stages_64x3_interior_nhwc_tn_v1 Tactic: 0xdbe058db40209ee1
[08/10/2023-11:17:09] [V] [TRT] Tactic: 0xdbe058db40209ee1 Time: 0.393943
[08/10/2023-11:17:09] [V] [TRT] Conv_27 + Relu_28 Set Tactic Name: ampere_h16816cudnn_256x64_sliced1x2_ldg8_relu_exp_interior_nhwc_tn_v1 Tactic: 0xdf5651a32d9311a6
[08/10/2023-11:17:09] [V] [TRT] Tactic: 0xdf5651a32d9311a6 Time: 0.441778
[08/10/2023-11:17:09] [V] [TRT] Conv_27 + Relu_28 Set Tactic Name: ampere_h16816cudnn_128x64_sliced1x2_ldg8_relu_exp_stages_64x4_interior_nhwc_tn_v1 Tactic: 0xe3cad530d257946a
[08/10/2023-11:17:09] [V] [TRT] Tactic: 0xe3cad530d257946a Time: 0.585385
[08/10/2023-11:17:09] [V] [TRT] Conv_27 + Relu_28 Set Tactic Name: ampere_h16816cudnn_64x128_ldg8_relu_exp_stages_64x3_interior_nhwc_tn_v1 Tactic: 0xe433b7de8e028d8e
[08/10/2023-11:17:09] [V] [TRT] Tactic: 0xe433b7de8e028d8e Time: 0.404361
[08/10/2023-11:17:09] [V] [TRT] Conv_27 + Relu_28 Set Tactic Name: ampere_h16816cudnn_256x64_ldg8_relu_exp_medium_nhwc_tn_v1 Tactic: 0xe4711898bd599c36
[08/10/2023-11:17:09] [V] [TRT] Tactic: 0xe4711898bd599c36 Time: 0.361998
[08/10/2023-11:17:09] [V] [TRT] Conv_27 + Relu_28 Set Tactic Name: ampere_h16816cudnn_128x64_sliced1x2_ldg8_relu_exp_stages_64x3_interior_nhwc_tn_v1 Tactic: 0xe93bad2778c92bdd
[08/10/2023-11:17:09] [V] [TRT] Tactic: 0xe93bad2778c92bdd Time: 0.455762
[08/10/2023-11:17:09] [V] [TRT] Conv_27 + Relu_28 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x128x32_stage4_warpsize2x2x1_g1_tensor16x8x16_t1r1s1 Tactic: 0xea50b6d3d87bf5dd
[08/10/2023-11:17:09] [V] [TRT] Tactic: 0xea50b6d3d87bf5dd Time: 0.232366
[08/10/2023-11:17:09] [V] [TRT] Conv_27 + Relu_28 Set Tactic Name: ampere_h16816cudnn_64x128_ldg8_relu_exp_stages_64x4_interior_nhwc_tn_v1 Tactic: 0xeec2cfc9249c3239
[08/10/2023-11:17:09] [V] [TRT] Tactic: 0xeec2cfc9249c3239 Time: 0.540649
[08/10/2023-11:17:09] [V] [TRT] Conv_27 + Relu_28 Set Tactic Name: ampere_h16816cudnn_64x64_ldg8_relu_exp_stages_64x6_small_nhwc_tn_v1 Tactic: 0xf0beb09df9a19f82
[08/10/2023-11:17:09] [V] [TRT] Tactic: 0xf0beb09df9a19f82 Time: 0.851246
[08/10/2023-11:17:09] [V] [TRT] Conv_27 + Relu_28 Set Tactic Name: ampere_h16816cudnn_128x64_ldg8_relu_exp_stages_64x4_interior_nhwc_tn_v1 Tactic: 0xf1973f09f9ca8bf3
[08/10/2023-11:17:09] [V] [TRT] Tactic: 0xf1973f09f9ca8bf3 Time: 0.522661
[08/10/2023-11:17:09] [V] [TRT] Conv_27 + Relu_28 Set Tactic Name: ampere_h1688cudnn_128x128_ldg8_relu_exp_interior_nhwc_tn_v1 Tactic: 0xfa5a19eb8211e798
[08/10/2023-11:17:09] [V] [TRT] Tactic: 0xfa5a19eb8211e798 Time: 0.32843
[08/10/2023-11:17:09] [V] [TRT] Conv_27 + Relu_28 Set Tactic Name: ampere_h16816cudnn_128x64_ldg8_relu_exp_stages_64x3_interior_nhwc_tn_v1 Tactic: 0xfb66471e53543444
[08/10/2023-11:17:09] [V] [TRT] Tactic: 0xfb66471e53543444 Time: 0.42485
[08/10/2023-11:17:09] [V] [TRT] Fastest Tactic: 0xea50b6d3d87bf5dd Time: 0.232366
[08/10/2023-11:17:09] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0xea50b6d3d87bf5dd
[08/10/2023-11:17:09] [V] [TRT] =============== Computing costs for 
[08/10/2023-11:17:09] [V] [TRT] *************** Autotuning format combination: Float(4147200,32400,180,1) -> Float(2073600,8100,90,1) ***************
[08/10/2023-11:17:09] [V] [TRT] --------------- Timing Runner: Conv_44 + Relu_45 (CudaDepthwiseConvolution)
[08/10/2023-11:17:09] [V] [TRT] CudaDepthwiseConvolution has no valid tactics for this config, skipping
[08/10/2023-11:17:09] [V] [TRT] --------------- Timing Runner: Conv_44 + Relu_45 (FusedConvActConvolution)
[08/10/2023-11:17:09] [V] [TRT] FusedConvActConvolution has no valid tactics for this config, skipping
[08/10/2023-11:17:09] [V] [TRT] --------------- Timing Runner: Conv_44 + Relu_45 (CudnnConvolution)
[08/10/2023-11:17:09] [V] [TRT] Tactic: 0x0000000000000000 Time: 3.06256
[08/10/2023-11:17:09] [V] [TRT] Tactic: 0x0000000000000001 Time: 1.18935
[08/10/2023-11:17:09] [V] [TRT] Tactic: 0x0000000000000002 Time: 4.80959
[08/10/2023-11:17:10] [V] [TRT] Tactic: 0x0000000000000005 Time: 31.3234
[08/10/2023-11:17:10] [V] [TRT] Tactic: 0x0000000000000038 Time: 2.97293
[08/10/2023-11:17:10] [V] [TRT] Tactic: 0x0000000000000039 Time: 0.995717
[08/10/2023-11:17:10] [V] [TRT] Tactic: 0x000000000000003a Time: 4.4958
[08/10/2023-11:17:10] [V] [TRT] Tactic: 0x000000000000003d Time: 31.2989
[08/10/2023-11:17:10] [V] [TRT] Tactic: 0x0000000000000070 Time: 3.02133
[08/10/2023-11:17:10] [V] [TRT] Tactic: 0x0000000000000071 Time: 2.83289
[08/10/2023-11:17:10] [V] [TRT] Tactic: 0x0000000000000072 Time: 4.4972
[08/10/2023-11:17:10] [V] [TRT] Tactic: 0x0000000000000075 Time: 31.3089
[08/10/2023-11:17:10] [V] [TRT] Fastest Tactic: 0x0000000000000039 Time: 0.995717
[08/10/2023-11:17:10] [V] [TRT] --------------- Timing Runner: Conv_44 + Relu_45 (CaskConvolution)
[08/10/2023-11:17:10] [V] [TRT] Conv_44 + Relu_45 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x128x8_stage3_warpsize1x4x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0x01cf8ce2da913006
[08/10/2023-11:17:10] [V] [TRT] Tactic: 0x01cf8ce2da913006 Time: 2.83244
[08/10/2023-11:17:10] [V] [TRT] Conv_44 + Relu_45 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x64x8_stage3_warpsize2x2x1_g1_ffma_aligna4_alignc4 Tactic: 0x12dbf7d94ee3696d
[08/10/2023-11:17:10] [V] [TRT] Tactic: 0x12dbf7d94ee3696d Time: 2.82167
[08/10/2023-11:17:10] [V] [TRT] Conv_44 + Relu_45 Set Tactic Name: ampere_scudnn_128x64_relu_small_nn_v1 Tactic: 0x3f243c490d502deb
[08/10/2023-11:17:10] [V] [TRT] Tactic: 0x3f243c490d502deb Time: 2.73656
[08/10/2023-11:17:10] [V] [TRT] Conv_44 + Relu_45 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0x4727434768e46395
[08/10/2023-11:17:10] [V] [TRT] Tactic: 0x4727434768e46395 Time: 3.08737
[08/10/2023-11:17:10] [V] [TRT] Conv_44 + Relu_45 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize256x128x8_stage3_warpsize4x2x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0x4efce38acc876f5c
[08/10/2023-11:17:10] [V] [TRT] Tactic: 0x4efce38acc876f5c Time: 2.71299
[08/10/2023-11:17:10] [V] [TRT] Conv_44 + Relu_45 Set Tactic Name: ampere_scudnn_128x128_relu_small_nn_v1 Tactic: 0x503619c69ae500ff
[08/10/2023-11:17:10] [V] [TRT] Tactic: 0x503619c69ae500ff Time: 2.52409
[08/10/2023-11:17:10] [V] [TRT] Conv_44 + Relu_45 Set Tactic Name: ampere_scudnn_128x128_relu_xregs_large_nn_v1 Tactic: 0x5403ad713f811a18
[08/10/2023-11:17:10] [V] [TRT] Tactic: 0x5403ad713f811a18 Time: 2.66045
[08/10/2023-11:17:10] [V] [TRT] Conv_44 + Relu_45 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x128x8_stage3_warpsize1x4x1_g1_ffma_aligna4_alignc4 Tactic: 0x5aa723e0481da855
[08/10/2023-11:17:10] [V] [TRT] Tactic: 0x5aa723e0481da855 Time: 2.66279
[08/10/2023-11:17:10] [V] [TRT] Conv_44 + Relu_45 Set Tactic Name: ampere_scudnn_128x64_relu_xregs_large_nn_v1 Tactic: 0x5deb29b7a8e275f7
[08/10/2023-11:17:11] [V] [TRT] Tactic: 0x5deb29b7a8e275f7 Time: 2.61623
[08/10/2023-11:17:11] [V] [TRT] Conv_44 + Relu_45 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_aligna4_alignc4 Tactic: 0xa31d27de74b895ff
[08/10/2023-11:17:11] [V] [TRT] Tactic: 0xa31d27de74b895ff Time: 2.99093
[08/10/2023-11:17:11] [V] [TRT] Conv_44 + Relu_45 Set Tactic Name: ampere_scudnn_128x32_relu_small_nn_v1 Tactic: 0xa8609adc4e0ceb90
[08/10/2023-11:17:11] [V] [TRT] Tactic: 0xa8609adc4e0ceb90 Time: 3.07413
[08/10/2023-11:17:11] [V] [TRT] Conv_44 + Relu_45 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize256x128x8_stage3_warpsize4x2x1_g1_ffma_aligna4_alignc4 Tactic: 0xbb8c3889c7eacd30
[08/10/2023-11:17:11] [V] [TRT] Tactic: 0xbb8c3889c7eacd30 Time: 2.74677
[08/10/2023-11:17:11] [V] [TRT] Conv_44 + Relu_45 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_aligna4_alignc4 Tactic: 0xd828f024626fa982
[08/10/2023-11:17:11] [V] [TRT] Tactic: 0xd828f024626fa982 Time: 4.30497
[08/10/2023-11:17:11] [V] [TRT] Conv_44 + Relu_45 Set Tactic Name: ampere_scudnn_128x128_relu_medium_nn_v1 Tactic: 0xf067e6205da31c2e
[08/10/2023-11:17:11] [V] [TRT] Tactic: 0xf067e6205da31c2e Time: 2.55795
[08/10/2023-11:17:11] [V] [TRT] Conv_44 + Relu_45 Set Tactic Name: ampere_scudnn_128x64_relu_medium_nn_v1 Tactic: 0xf64396b97c889179
[08/10/2023-11:17:11] [V] [TRT] Tactic: 0xf64396b97c889179 Time: 2.78926
[08/10/2023-11:17:11] [V] [TRT] Fastest Tactic: 0x503619c69ae500ff Time: 2.52409
[08/10/2023-11:17:11] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CudnnConvolution Tactic: 0x0000000000000039
[08/10/2023-11:17:11] [V] [TRT] *************** Autotuning format combination: Float(4147200,1,23040,128) -> Float(2073600,1,23040,256) ***************
[08/10/2023-11:17:11] [V] [TRT] --------------- Timing Runner: Conv_44 + Relu_45 (CaskConvolution)
[08/10/2023-11:17:11] [V] [TRT] Conv_44 + Relu_45 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize64x128x32_stage5_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0x112d7e4d280f396d
[08/10/2023-11:17:11] [V] [TRT] Tactic: 0x112d7e4d280f396d Time: 0.747854
[08/10/2023-11:17:11] [V] [TRT] Conv_44 + Relu_45 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x64x32_stage5_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0x13cc2ad8dcd32ba2
[08/10/2023-11:17:11] [V] [TRT] Tactic: 0x13cc2ad8dcd32ba2 Time: 0.785861
[08/10/2023-11:17:11] [V] [TRT] Conv_44 + Relu_45 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_aligna4_alignc4 Tactic: 0x19b688348f983aa0
[08/10/2023-11:17:11] [V] [TRT] Tactic: 0x19b688348f983aa0 Time: 2.8034
[08/10/2023-11:17:11] [V] [TRT] Conv_44 + Relu_45 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x64x8_stage3_warpsize2x2x1_g1_ffma_aligna4_alignc4 Tactic: 0x1da91d865428f237
[08/10/2023-11:17:11] [V] [TRT] Tactic: 0x1da91d865428f237 Time: 2.22415
[08/10/2023-11:17:11] [V] [TRT] Conv_44 + Relu_45 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize256x128x32_stage3_warpsize4x2x1_g1_tensor16x8x8_t1r3s3 Tactic: 0x21246c8544eff903
[08/10/2023-11:17:11] [V] [TRT] Tactic: 0x21246c8544eff903 Time: 0.500023
[08/10/2023-11:17:11] [V] [TRT] Conv_44 + Relu_45 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x128x32_stage4_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0x25b2b9d5c9d5ca0d
[08/10/2023-11:17:11] [V] [TRT] Tactic: 0x25b2b9d5c9d5ca0d Time: 0.545897
[08/10/2023-11:17:11] [V] [TRT] Conv_44 + Relu_45 Set Tactic Name: ampere_scudnn_128x64_sliced1x2_ldg4_relu_exp_small_nhwc_tn_v1 Tactic: 0x27b316f52c109002
[08/10/2023-11:17:11] [V] [TRT] Tactic: 0x27b316f52c109002 Time: 2.74539
[08/10/2023-11:17:11] [V] [TRT] Conv_44 + Relu_45 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage4_warpsize2x2x1_g1_tensor16x8x8_t1r3s3 Tactic: 0x2d80d2b384ab13f1
[08/10/2023-11:17:11] [V] [TRT] Tactic: 0x2d80d2b384ab13f1 Time: 0.829157
[08/10/2023-11:17:11] [V] [TRT] Conv_44 + Relu_45 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize256x128x32_stage3_warpsize4x2x1_g1_tensor16x8x8 Tactic: 0x36303f399d3ee4fd
[08/10/2023-11:17:11] [V] [TRT] Tactic: 0x36303f399d3ee4fd Time: 0.604517
[08/10/2023-11:17:11] [V] [TRT] Conv_44 + Relu_45 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage4_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0x385bfab9332b1413
[08/10/2023-11:17:11] [V] [TRT] Tactic: 0x385bfab9332b1413 Time: 0.907877
[08/10/2023-11:17:11] [V] [TRT] Conv_44 + Relu_45 Set Tactic Name: ampere_scudnn_128x32_sliced1x4_ldg4_relu_exp_medium_nhwc_tn_v1 Tactic: 0x3e191488237fab8f
[08/10/2023-11:17:11] [V] [TRT] Tactic: 0x3e191488237fab8f Time: 2.73946
[08/10/2023-11:17:11] [V] [TRT] Conv_44 + Relu_45 Set Tactic Name: ampere_scudnn_128x32_sliced1x4_ldg4_relu_exp_small_nhwc_tn_v1 Tactic: 0x3e2b881168d9689d
[08/10/2023-11:17:11] [V] [TRT] Tactic: 0x3e2b881168d9689d Time: 2.72572
[08/10/2023-11:17:11] [V] [TRT] Conv_44 + Relu_45 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x256x8_stage3_warpsize1x4x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0x3f0c846d6379bc98
[08/10/2023-11:17:11] [V] [TRT] Tactic: 0x3f0c846d6379bc98 Time: 2.28344
[08/10/2023-11:17:11] [V] [TRT] Conv_44 + Relu_45 Set Tactic Name: ampere_scudnn_128x64_sliced1x2_ldg4_relu_exp_medium_nhwc_tn_v1 Tactic: 0x412c44dfeaf9161d
[08/10/2023-11:17:11] [V] [TRT] Tactic: 0x412c44dfeaf9161d Time: 2.70123
[08/10/2023-11:17:11] [V] [TRT] Conv_44 + Relu_45 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x256x32_stage3_warpsize2x4x1_g1_tensor16x8x8 Tactic: 0x482b48242255b8ce
[08/10/2023-11:17:11] [V] [TRT] Tactic: 0x482b48242255b8ce Time: 0.500928
[08/10/2023-11:17:11] [V] [TRT] Conv_44 + Relu_45 Set Tactic Name: ampere_scudnn_128x128_relu_exp_small_nhwc_tn_v1 Tactic: 0x5030121339a48bf3
[08/10/2023-11:17:11] [V] [TRT] Tactic: 0x5030121339a48bf3 Time: 2.51681
[08/10/2023-11:17:11] [V] [TRT] Conv_44 + Relu_45 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x256x32_stage3_warpsize2x4x1_g1_tensor16x8x8_t1r3s3 Tactic: 0x614e89f7852edbc3
[08/10/2023-11:17:11] [V] [TRT] Tactic: 0x614e89f7852edbc3 Time: 0.490199
[08/10/2023-11:17:11] [V] [TRT] Conv_44 + Relu_45 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_aligna4_alignc4 Tactic: 0x62835fce994f06dd
[08/10/2023-11:17:11] [V] [TRT] Tactic: 0x62835fce994f06dd Time: 2.47434
[08/10/2023-11:17:11] [V] [TRT] Conv_44 + Relu_45 Set Tactic Name: ampere_scudnn_128x128_ldg4_relu_exp_large_nhwc_tn_v1 Tactic: 0x634e99502974e4da
[08/10/2023-11:17:11] [V] [TRT] Tactic: 0x634e99502974e4da Time: 2.52921
[08/10/2023-11:17:11] [V] [TRT] Conv_44 + Relu_45 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0x65e41d81f093b482
[08/10/2023-11:17:11] [V] [TRT] Tactic: 0x65e41d81f093b482 Time: 0.590359
[08/10/2023-11:17:11] [V] [TRT] Conv_44 + Relu_45 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize64x128x32_stage5_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0x65f71607d20d5438
[08/10/2023-11:17:11] [V] [TRT] Tactic: 0x65f71607d20d5438 Time: 0.670098
[08/10/2023-11:17:11] [V] [TRT] Conv_44 + Relu_45 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x64x32_stage5_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0x6716429226d146f7
[08/10/2023-11:17:11] [V] [TRT] Tactic: 0x6716429226d146f7 Time: 0.707328
[08/10/2023-11:17:11] [V] [TRT] Conv_44 + Relu_45 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize64x32x64_stage5_warpsize2x2x1_g1_tensor16x8x8_t1r3s3 Tactic: 0x679a281f7660c436
[08/10/2023-11:17:11] [V] [TRT] Tactic: 0x679a281f7660c436 Time: 1.27239
[08/10/2023-11:17:11] [V] [TRT] Conv_44 + Relu_45 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize64x32x64_stage5_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0x6e2a3c7a7fc5e4e1
[08/10/2023-11:17:11] [V] [TRT] Tactic: 0x6e2a3c7a7fc5e4e1 Time: 1.31869
[08/10/2023-11:17:11] [V] [TRT] Conv_44 + Relu_45 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x64x32_stage5_warpsize2x2x1_g1_tensor16x8x8_t1r3s3 Tactic: 0x7aa21282feb15f1a
[08/10/2023-11:17:11] [V] [TRT] Tactic: 0x7aa21282feb15f1a Time: 0.707945
[08/10/2023-11:17:11] [V] [TRT] Conv_44 + Relu_45 Set Tactic Name: ampere_scudnn_128x128_ldg4_relu_exp_medium_nhwc_tn_v1 Tactic: 0x7bc32c782b800c48
[08/10/2023-11:17:11] [V] [TRT] Tactic: 0x7bc32c782b800c48 Time: 2.45727
[08/10/2023-11:17:11] [V] [TRT] Conv_44 + Relu_45 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize256x64x8_stage3_warpsize2x2x1_g1_ffma_aligna4_alignc4 Tactic: 0x8014228ec08b4d49
[08/10/2023-11:17:11] [V] [TRT] Tactic: 0x8014228ec08b4d49 Time: 2.28399
[08/10/2023-11:17:11] [V] [TRT] Conv_44 + Relu_45 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage4_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0x818413b69874bb35
[08/10/2023-11:17:11] [V] [TRT] Tactic: 0x818413b69874bb35 Time: 0.883959
[08/10/2023-11:17:11] [V] [TRT] Conv_44 + Relu_45 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0x94a7db94ba744c45
[08/10/2023-11:17:11] [V] [TRT] Tactic: 0x94a7db94ba744c45 Time: 2.51064
[08/10/2023-11:17:11] [V] [TRT] Conv_44 + Relu_45 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x256x32_stage3_warpsize2x4x1_g1_tensor16x8x8 Tactic: 0x998c97842e775a17
[08/10/2023-11:17:11] [V] [TRT] Tactic: 0x998c97842e775a17 Time: 0.519497
[08/10/2023-11:17:11] [V] [TRT] Conv_44 + Relu_45 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8_t1r3s3 Tactic: 0x999e005e3b016ea6
[08/10/2023-11:17:11] [V] [TRT] Tactic: 0x999e005e3b016ea6 Time: 0.562999
[08/10/2023-11:17:11] [V] [TRT] Conv_44 + Relu_45 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize256x64x32_stage3_warpsize4x2x1_g1_tensor16x8x8_t1r3s3 Tactic: 0xa9a06d0633580c0c
[08/10/2023-11:17:11] [V] [TRT] Tactic: 0xa9a06d0633580c0c Time: 0.618574
[08/10/2023-11:17:11] [V] [TRT] Conv_44 + Relu_45 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0xb443c221fcb1565b
[08/10/2023-11:17:11] [V] [TRT] Tactic: 0xb443c221fcb1565b Time: 0.562734
[08/10/2023-11:17:11] [V] [TRT] Conv_44 + Relu_45 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage3_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0xb47d7d926de02dee
[08/10/2023-11:17:11] [V] [TRT] Tactic: 0xb47d7d926de02dee Time: 0.731104
[08/10/2023-11:17:11] [V] [TRT] Conv_44 + Relu_45 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize256x64x32_stage3_warpsize4x2x1_g1_tensor16x8x8 Tactic: 0xba0185279ccb2b85
[08/10/2023-11:17:11] [V] [TRT] Tactic: 0xba0185279ccb2b85 Time: 0.682427
[08/10/2023-11:17:11] [V] [TRT] Conv_44 + Relu_45 Set Tactic Name: ampere_scudnn_128x64_sliced1x2_ldg4_relu_exp_large_nhwc_tn_v1 Tactic: 0xbdfdef6b84f7ccc9
[08/10/2023-11:17:11] [V] [TRT] Tactic: 0xbdfdef6b84f7ccc9 Time: 2.6664
[08/10/2023-11:17:11] [V] [TRT] Conv_44 + Relu_45 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage3_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0xc0a715d897e240bb
[08/10/2023-11:17:11] [V] [TRT] Tactic: 0xc0a715d897e240bb Time: 0.717495
[08/10/2023-11:17:11] [V] [TRT] Conv_44 + Relu_45 Set Tactic Name: ampere_scudnn_128x128_relu_exp_large_nhwc_tn_v1 Tactic: 0xca7eeb8d9143d738
[08/10/2023-11:17:11] [V] [TRT] Tactic: 0xca7eeb8d9143d738 Time: 2.56185
[08/10/2023-11:17:11] [V] [TRT] Conv_44 + Relu_45 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize256x64x32_stage3_warpsize4x2x1_g1_tensor16x8x8 Tactic: 0xcedbed6d66c946d0
[08/10/2023-11:17:11] [V] [TRT] Tactic: 0xcedbed6d66c946d0 Time: 0.626537
[08/10/2023-11:17:11] [V] [TRT] Conv_44 + Relu_45 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x128x8_stage3_warpsize1x4x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0xd15dd11d64344e83
[08/10/2023-11:17:11] [V] [TRT] Tactic: 0xd15dd11d64344e83 Time: 2.20753
[08/10/2023-11:17:11] [V] [TRT] Conv_44 + Relu_45 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize64x128x32_stage5_warpsize2x2x1_g1_tensor16x8x8_t1r3s3 Tactic: 0xd4428a7355769d7d
[08/10/2023-11:17:11] [V] [TRT] Tactic: 0xd4428a7355769d7d Time: 0.642021
[08/10/2023-11:17:11] [V] [TRT] Conv_44 + Relu_45 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize64x32x64_stage5_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0xd7f5d575d49a4bc7
[08/10/2023-11:17:11] [V] [TRT] Tactic: 0xd7f5d575d49a4bc7 Time: 1.49213
[08/10/2023-11:17:11] [V] [TRT] Conv_44 + Relu_45 Set Tactic Name: ampere_scudnn_128x128_relu_exp_medium_nhwc_tn_v1 Tactic: 0xd9031472c05adf51
[08/10/2023-11:17:11] [V] [TRT] Tactic: 0xd9031472c05adf51 Time: 2.55115
[08/10/2023-11:17:11] [V] [TRT] Conv_44 + Relu_45 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x128x32_stage4_warpsize2x2x1_g1_tensor16x8x8_t1r3s3 Tactic: 0xd920b33c9bd27143
[08/10/2023-11:17:11] [V] [TRT] Tactic: 0xd920b33c9bd27143 Time: 0.519195
[08/10/2023-11:17:11] [V] [TRT] Conv_44 + Relu_45 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize256x128x32_stage3_warpsize4x2x1_g1_tensor16x8x8 Tactic: 0xe797e099911c0624
[08/10/2023-11:17:11] [V] [TRT] Tactic: 0xe797e099911c0624 Time: 0.528946
[08/10/2023-11:17:12] [V] [TRT] Conv_44 + Relu_45 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage3_warpsize2x2x1_g1_tensor16x8x8_t1r3s3 Tactic: 0xeb65d4c1e8fa2294
[08/10/2023-11:17:12] [V] [TRT] Tactic: 0xeb65d4c1e8fa2294 Time: 0.712809
[08/10/2023-11:17:12] [V] [TRT] Conv_44 + Relu_45 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x128x32_stage4_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0xf4156675c5f728d4
[08/10/2023-11:17:12] [V] [TRT] Tactic: 0xf4156675c5f728d4 Time: 0.813705
[08/10/2023-11:17:12] [V] [TRT] Conv_44 + Relu_45 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x64x8_stage3_warpsize2x2x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0xf48db81f02eca9ee
[08/10/2023-11:17:12] [V] [TRT] Tactic: 0xf48db81f02eca9ee Time: 2.23921
[08/10/2023-11:17:12] [V] [TRT] Conv_44 + Relu_45 Set Tactic Name: ampere_scudnn_128x128_ldg4_relu_exp_small_nhwc_tn_v1 Tactic: 0xf90060ce8193b811
[08/10/2023-11:17:12] [V] [TRT] Tactic: 0xf90060ce8193b811 Time: 2.45735
[08/10/2023-11:17:12] [V] [TRT] Fastest Tactic: 0x614e89f7852edbc3 Time: 0.490199
[08/10/2023-11:17:12] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x614e89f7852edbc3
[08/10/2023-11:17:12] [V] [TRT] *************** Autotuning format combination: Float(1036800,1:4,5760,32) -> Float(518400,1:4,5760,64) ***************
[08/10/2023-11:17:12] [V] [TRT] --------------- Timing Runner: Conv_44 + Relu_45 (CaskConvolution)
[08/10/2023-11:17:12] [V] [TRT] Conv_44 + Relu_45 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize64x128x32_stage5_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0x112d7e4d280f396d
[08/10/2023-11:17:12] [V] [TRT] Tactic: 0x112d7e4d280f396d Time: 0.746473
[08/10/2023-11:17:12] [V] [TRT] Conv_44 + Relu_45 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x64x32_stage5_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0x13cc2ad8dcd32ba2
[08/10/2023-11:17:12] [V] [TRT] Tactic: 0x13cc2ad8dcd32ba2 Time: 0.787703
[08/10/2023-11:17:12] [V] [TRT] Conv_44 + Relu_45 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize256x128x32_stage3_warpsize4x2x1_g1_tensor16x8x8_t1r3s3 Tactic: 0x21246c8544eff903
[08/10/2023-11:17:12] [V] [TRT] Tactic: 0x21246c8544eff903 Time: 0.489385
[08/10/2023-11:17:12] [V] [TRT] Conv_44 + Relu_45 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x128x32_stage4_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0x25b2b9d5c9d5ca0d
[08/10/2023-11:17:12] [V] [TRT] Tactic: 0x25b2b9d5c9d5ca0d Time: 0.534107
[08/10/2023-11:17:12] [V] [TRT] Conv_44 + Relu_45 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage4_warpsize2x2x1_g1_tensor16x8x8_t1r3s3 Tactic: 0x2d80d2b384ab13f1
[08/10/2023-11:17:12] [V] [TRT] Tactic: 0x2d80d2b384ab13f1 Time: 0.825696
[08/10/2023-11:17:12] [V] [TRT] Conv_44 + Relu_45 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize256x128x32_stage3_warpsize4x2x1_g1_tensor16x8x8 Tactic: 0x36303f399d3ee4fd
[08/10/2023-11:17:12] [V] [TRT] Tactic: 0x36303f399d3ee4fd Time: 0.601865
[08/10/2023-11:17:12] [V] [TRT] Conv_44 + Relu_45 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage4_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0x385bfab9332b1413
[08/10/2023-11:17:12] [V] [TRT] Tactic: 0x385bfab9332b1413 Time: 0.905595
[08/10/2023-11:17:12] [V] [TRT] Conv_44 + Relu_45 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x256x32_stage3_warpsize2x4x1_g1_tensor16x8x8 Tactic: 0x482b48242255b8ce
[08/10/2023-11:17:12] [V] [TRT] Tactic: 0x482b48242255b8ce Time: 0.500197
[08/10/2023-11:17:12] [V] [TRT] Conv_44 + Relu_45 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x256x32_stage3_warpsize2x4x1_g1_tensor16x8x8_t1r3s3 Tactic: 0x614e89f7852edbc3
[08/10/2023-11:17:12] [V] [TRT] Tactic: 0x614e89f7852edbc3 Time: 0.488352
[08/10/2023-11:17:12] [V] [TRT] Conv_44 + Relu_45 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0x65e41d81f093b482
[08/10/2023-11:17:12] [V] [TRT] Tactic: 0x65e41d81f093b482 Time: 0.600069
[08/10/2023-11:17:12] [V] [TRT] Conv_44 + Relu_45 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize64x128x32_stage5_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0x65f71607d20d5438
[08/10/2023-11:17:12] [V] [TRT] Tactic: 0x65f71607d20d5438 Time: 0.679323
[08/10/2023-11:17:12] [V] [TRT] Conv_44 + Relu_45 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x64x32_stage5_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0x6716429226d146f7
[08/10/2023-11:17:12] [V] [TRT] Tactic: 0x6716429226d146f7 Time: 0.719872
[08/10/2023-11:17:12] [V] [TRT] Conv_44 + Relu_45 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize64x32x64_stage5_warpsize2x2x1_g1_tensor16x8x8_t1r3s3 Tactic: 0x679a281f7660c436
[08/10/2023-11:17:12] [V] [TRT] Tactic: 0x679a281f7660c436 Time: 1.30158
[08/10/2023-11:17:12] [V] [TRT] Conv_44 + Relu_45 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize64x32x64_stage5_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0x6e2a3c7a7fc5e4e1
[08/10/2023-11:17:12] [V] [TRT] Tactic: 0x6e2a3c7a7fc5e4e1 Time: 1.31824
[08/10/2023-11:17:12] [V] [TRT] Conv_44 + Relu_45 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x64x32_stage5_warpsize2x2x1_g1_tensor16x8x8_t1r3s3 Tactic: 0x7aa21282feb15f1a
[08/10/2023-11:17:12] [V] [TRT] Tactic: 0x7aa21282feb15f1a Time: 0.712032
[08/10/2023-11:17:12] [V] [TRT] Conv_44 + Relu_45 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage4_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0x818413b69874bb35
[08/10/2023-11:17:12] [V] [TRT] Tactic: 0x818413b69874bb35 Time: 0.882299
[08/10/2023-11:17:12] [V] [TRT] Conv_44 + Relu_45 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x256x32_stage3_warpsize2x4x1_g1_tensor16x8x8 Tactic: 0x998c97842e775a17
[08/10/2023-11:17:12] [V] [TRT] Tactic: 0x998c97842e775a17 Time: 0.508896
[08/10/2023-11:17:12] [V] [TRT] Conv_44 + Relu_45 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8_t1r3s3 Tactic: 0x999e005e3b016ea6
[08/10/2023-11:17:12] [V] [TRT] Tactic: 0x999e005e3b016ea6 Time: 0.551282
[08/10/2023-11:17:12] [V] [TRT] Conv_44 + Relu_45 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize256x64x32_stage3_warpsize4x2x1_g1_tensor16x8x8_t1r3s3 Tactic: 0xa9a06d0633580c0c
[08/10/2023-11:17:12] [V] [TRT] Tactic: 0xa9a06d0633580c0c Time: 0.607813
[08/10/2023-11:17:12] [V] [TRT] Conv_44 + Relu_45 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0xb443c221fcb1565b
[08/10/2023-11:17:12] [V] [TRT] Tactic: 0xb443c221fcb1565b Time: 0.563314
[08/10/2023-11:17:12] [V] [TRT] Conv_44 + Relu_45 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage3_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0xb47d7d926de02dee
[08/10/2023-11:17:12] [V] [TRT] Tactic: 0xb47d7d926de02dee Time: 0.731721
[08/10/2023-11:17:12] [V] [TRT] Conv_44 + Relu_45 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize256x64x32_stage3_warpsize4x2x1_g1_tensor16x8x8 Tactic: 0xba0185279ccb2b85
[08/10/2023-11:17:12] [V] [TRT] Tactic: 0xba0185279ccb2b85 Time: 0.685198
[08/10/2023-11:17:12] [V] [TRT] Conv_44 + Relu_45 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage3_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0xc0a715d897e240bb
[08/10/2023-11:17:12] [V] [TRT] Tactic: 0xc0a715d897e240bb Time: 0.715671
[08/10/2023-11:17:12] [V] [TRT] Conv_44 + Relu_45 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize256x64x32_stage3_warpsize4x2x1_g1_tensor16x8x8 Tactic: 0xcedbed6d66c946d0
[08/10/2023-11:17:12] [V] [TRT] Tactic: 0xcedbed6d66c946d0 Time: 0.626528
[08/10/2023-11:17:12] [V] [TRT] Conv_44 + Relu_45 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize64x128x32_stage5_warpsize2x2x1_g1_tensor16x8x8_t1r3s3 Tactic: 0xd4428a7355769d7d
[08/10/2023-11:17:12] [V] [TRT] Tactic: 0xd4428a7355769d7d Time: 0.642171
[08/10/2023-11:17:12] [V] [TRT] Conv_44 + Relu_45 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize64x32x64_stage5_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0xd7f5d575d49a4bc7
[08/10/2023-11:17:12] [V] [TRT] Tactic: 0xd7f5d575d49a4bc7 Time: 1.4921
[08/10/2023-11:17:12] [V] [TRT] Conv_44 + Relu_45 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x128x32_stage4_warpsize2x2x1_g1_tensor16x8x8_t1r3s3 Tactic: 0xd920b33c9bd27143
[08/10/2023-11:17:12] [V] [TRT] Tactic: 0xd920b33c9bd27143 Time: 0.518149
[08/10/2023-11:17:12] [V] [TRT] Conv_44 + Relu_45 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize256x128x32_stage3_warpsize4x2x1_g1_tensor16x8x8 Tactic: 0xe797e099911c0624
[08/10/2023-11:17:12] [V] [TRT] Tactic: 0xe797e099911c0624 Time: 0.516338
[08/10/2023-11:17:12] [V] [TRT] Conv_44 + Relu_45 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage3_warpsize2x2x1_g1_tensor16x8x8_t1r3s3 Tactic: 0xeb65d4c1e8fa2294
[08/10/2023-11:17:12] [V] [TRT] Tactic: 0xeb65d4c1e8fa2294 Time: 0.697883
[08/10/2023-11:17:12] [V] [TRT] Conv_44 + Relu_45 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x128x32_stage4_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0xf4156675c5f728d4
[08/10/2023-11:17:12] [V] [TRT] Tactic: 0xf4156675c5f728d4 Time: 0.796617
[08/10/2023-11:17:12] [V] [TRT] Fastest Tactic: 0x614e89f7852edbc3 Time: 0.488352
[08/10/2023-11:17:12] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x614e89f7852edbc3
[08/10/2023-11:17:12] [V] [TRT] *************** Autotuning format combination: Half(4147200,32400,180,1) -> Half(2073600,8100,90,1) ***************
[08/10/2023-11:17:12] [W] [TRT] Weights [name=Conv_44 + Relu_45.weight] had the following issues when converted to FP16:
[08/10/2023-11:17:12] [W] [TRT]  - Subnormal FP16 values detected. 
[08/10/2023-11:17:12] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.
[08/10/2023-11:17:12] [V] [TRT] --------------- Timing Runner: Conv_44 + Relu_45 (CudnnConvolution)
[08/10/2023-11:17:12] [V] [TRT] Tactic: 0x0000000000000000 Time: 3.72475
[08/10/2023-11:17:12] [V] [TRT] Tactic: 0x0000000000000001 Time: 2.89611
[08/10/2023-11:17:12] [V] [TRT] Tactic: 0x0000000000000002 Time: 4.02019
[08/10/2023-11:17:12] [V] [TRT] Tactic: 0x0000000000000005 Time: 32.2483
[08/10/2023-11:17:12] [V] [TRT] Tactic: 0x0000000000000038 Time: 3.67986
[08/10/2023-11:17:12] [V] [TRT] Tactic: 0x000000000000003a Time: 3.63957
[08/10/2023-11:17:13] [V] [TRT] Tactic: 0x000000000000003d Time: 31.8527
[08/10/2023-11:17:13] [V] [TRT] Fastest Tactic: 0x0000000000000001 Time: 2.89611
[08/10/2023-11:17:13] [W] [TRT] Weights [name=Conv_44 + Relu_45.weight] had the following issues when converted to FP16:
[08/10/2023-11:17:13] [W] [TRT]  - Subnormal FP16 values detected. 
[08/10/2023-11:17:13] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.
[08/10/2023-11:17:13] [V] [TRT] --------------- Timing Runner: Conv_44 + Relu_45 (CaskConvolution)
[08/10/2023-11:17:13] [V] [TRT] CaskConvolution has no valid tactics for this config, skipping
[08/10/2023-11:17:13] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CudnnConvolution Tactic: 0x0000000000000001
[08/10/2023-11:17:13] [V] [TRT] *************** Autotuning format combination: Half(2073600,32400:2,180,1) -> Half(1036800,8100:2,90,1) ***************
[08/10/2023-11:17:13] [W] [TRT] Weights [name=Conv_44 + Relu_45.weight] had the following issues when converted to FP16:
[08/10/2023-11:17:13] [W] [TRT]  - Subnormal FP16 values detected. 
[08/10/2023-11:17:13] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.
[08/10/2023-11:17:13] [V] [TRT] --------------- Timing Runner: Conv_44 + Relu_45 (FusedConvActConvolution)
[08/10/2023-11:17:13] [V] [TRT] FusedConvActConvolution has no valid tactics for this config, skipping
[08/10/2023-11:17:13] [W] [TRT] Weights [name=Conv_44 + Relu_45.weight] had the following issues when converted to FP16:
[08/10/2023-11:17:13] [W] [TRT]  - Subnormal FP16 values detected. 
[08/10/2023-11:17:13] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.
[08/10/2023-11:17:13] [V] [TRT] --------------- Timing Runner: Conv_44 + Relu_45 (CaskConvolution)
[08/10/2023-11:17:13] [V] [TRT] CaskConvolution has no valid tactics for this config, skipping
[08/10/2023-11:17:13] [V] [TRT] *************** Autotuning format combination: Half(518400,1:8,2880,16) -> Float(2073600,8100,90,1) ***************
[08/10/2023-11:17:13] [W] [TRT] Weights [name=Conv_44 + Relu_45.weight] had the following issues when converted to FP16:
[08/10/2023-11:17:13] [W] [TRT]  - Subnormal FP16 values detected. 
[08/10/2023-11:17:13] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.
[08/10/2023-11:17:13] [V] [TRT] --------------- Timing Runner: Conv_44 + Relu_45 (CaskConvolution)
[08/10/2023-11:17:13] [V] [TRT] CaskConvolution has no valid tactics for this config, skipping
[08/10/2023-11:17:13] [V] [TRT] *************** Autotuning format combination: Half(518400,1:8,2880,16) -> Half(259200,1:8,2880,32) ***************
[08/10/2023-11:17:13] [W] [TRT] Weights [name=Conv_44 + Relu_45.weight] had the following issues when converted to FP16:
[08/10/2023-11:17:13] [W] [TRT]  - Subnormal FP16 values detected. 
[08/10/2023-11:17:13] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.
[08/10/2023-11:17:13] [V] [TRT] --------------- Timing Runner: Conv_44 + Relu_45 (CudaDepthwiseConvolution)
[08/10/2023-11:17:13] [V] [TRT] CudaDepthwiseConvolution has no valid tactics for this config, skipping
[08/10/2023-11:17:13] [W] [TRT] Weights [name=Conv_44 + Relu_45.weight] had the following issues when converted to FP16:
[08/10/2023-11:17:13] [W] [TRT]  - Subnormal FP16 values detected. 
[08/10/2023-11:17:13] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.
[08/10/2023-11:17:13] [V] [TRT] --------------- Timing Runner: Conv_44 + Relu_45 (CaskConvolution)
[08/10/2023-11:17:13] [V] [TRT] Conv_44 + Relu_45 Set Tactic Name: ampere_h16816cudnn_256x128_ldg8_relu_exp_medium_nhwc_tn_v1 Tactic: 0x03896956a39a1203
[08/10/2023-11:17:13] [V] [TRT] Tactic: 0x03896956a39a1203 Time: 0.285943
[08/10/2023-11:17:13] [V] [TRT] Conv_44 + Relu_45 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x128x32_stage3_warpsize4x2x1_g1_tensor16x8x16_t1r3s3 Tactic: 0x048d6d0400f33439
[08/10/2023-11:17:13] [V] [TRT] Tactic: 0x048d6d0400f33439 Time: 0.268704
[08/10/2023-11:17:13] [V] [TRT] Conv_44 + Relu_45 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x64x64_stage3_warpsize2x2x1_g1_tensor16x8x16_t1r3s3 Tactic: 0x05b6220f243edacd
[08/10/2023-11:17:13] [V] [TRT] Tactic: 0x05b6220f243edacd Time: 0.303506
[08/10/2023-11:17:13] [V] [TRT] Conv_44 + Relu_45 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x64x32_stage3_warpsize4x1x1_g1_tensor16x8x16_t1r3s3_aACCESS Tactic: 0x0866ddee325d07a6
[08/10/2023-11:17:13] [V] [TRT] Tactic: 0x0866ddee325d07a6 Time: 0.286702
[08/10/2023-11:17:13] [V] [TRT] Conv_44 + Relu_45 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x128x32_stage4_warpsize2x2x1_g1_tensor16x8x16 Tactic: 0x0e07ff4f4f7c1ac9
[08/10/2023-11:17:13] [V] [TRT] Tactic: 0x0e07ff4f4f7c1ac9 Time: 0.26176
[08/10/2023-11:17:13] [V] [TRT] Conv_44 + Relu_45 Set Tactic Name: ampere_h16816cudnn_256x128_ldg8_relu_exp_stages_64x3_small_nhwc_tn_v1 Tactic: 0x0e199750c30c6928
[08/10/2023-11:17:13] [V] [TRT] Tactic: 0x0e199750c30c6928 Time: 0.274345
[08/10/2023-11:17:13] [V] [TRT] Conv_44 + Relu_45 Set Tactic Name: ampere_h1688cudnn_128x128_ldg8_relu_exp_medium_nhwc_tn_v1 Tactic: 0x100c1ad308e08d35
[08/10/2023-11:17:13] [V] [TRT] Tactic: 0x100c1ad308e08d35 Time: 0.326514
[08/10/2023-11:17:13] [V] [TRT] Conv_44 + Relu_45 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x128x32_stage5_warpsize2x2x1_g1_tensor16x8x16_t1r3s3 Tactic: 0x17ebf0c9f418f10a
[08/10/2023-11:17:13] [V] [TRT] Tactic: 0x17ebf0c9f418f10a Time: 0.292069
[08/10/2023-11:17:13] [V] [TRT] Conv_44 + Relu_45 Set Tactic Name: ampere_h16816cudnn_128x64_ldg8_relu_exp_stages_64x4_medium_nhwc_tn_v1 Tactic: 0x196cbc423a9bb69e
[08/10/2023-11:17:13] [V] [TRT] Tactic: 0x196cbc423a9bb69e Time: 0.362354
[08/10/2023-11:17:13] [V] [TRT] Conv_44 + Relu_45 Set Tactic Name: ampere_h16816cudnn_128x64_sliced1x2_ldg8_relu_exp_stages_64x3_small_nhwc_tn_v1 Tactic: 0x19822de884f42a6a
[08/10/2023-11:17:13] [V] [TRT] Tactic: 0x19822de884f42a6a Time: 0.305065
[08/10/2023-11:17:13] [V] [TRT] Conv_44 + Relu_45 Set Tactic Name: ampere_h16816cudnn_64x64_sliced1x2_ldg8_relu_exp_stages_64x5_small_nhwc_tn_v1 Tactic: 0x1a5ba808ad4cc5a8
[08/10/2023-11:17:13] [V] [TRT] Tactic: 0x1a5ba808ad4cc5a8 Time: 0.410034
[08/10/2023-11:17:13] [V] [TRT] Conv_44 + Relu_45 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x64x64_stage3_warpsize2x2x1_g1_tensor16x8x16 Tactic: 0x21b295c0c8f6c95a
[08/10/2023-11:17:13] [V] [TRT] Tactic: 0x21b295c0c8f6c95a Time: 0.305253
[08/10/2023-11:17:13] [V] [TRT] Conv_44 + Relu_45 Set Tactic Name: ampere_h16816cudnn_128x128_ldg8_relu_exp_medium_nhwc_tn_v1 Tactic: 0x234580e8a194335c
[08/10/2023-11:17:13] [V] [TRT] Tactic: 0x234580e8a194335c Time: 0.266162
[08/10/2023-11:17:13] [V] [TRT] Conv_44 + Relu_45 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x128x32_stage4_warpsize2x2x1_g1_tensor16x8x16_t1r3s3_aACCESS Tactic: 0x245530c34bd6090f
[08/10/2023-11:17:13] [V] [TRT] Tactic: 0x245530c34bd6090f Time: 0.248288
[08/10/2023-11:17:13] [V] [TRT] Conv_44 + Relu_45 Set Tactic Name: ampere_h16816cudnn_64x64_sliced1x2_ldg8_relu_exp_stages_64x6_small_nhwc_tn_v1 Tactic: 0x24e01e7405cfdfe9
[08/10/2023-11:17:13] [V] [TRT] Tactic: 0x24e01e7405cfdfe9 Time: 0.486478
[08/10/2023-11:17:13] [V] [TRT] Conv_44 + Relu_45 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x64x64_stage3_warpsize2x2x1_g1_tensor16x8x16_t1r3s3_aACCESS Tactic: 0x263a38afd75e3a43
[08/10/2023-11:17:13] [V] [TRT] Tactic: 0x263a38afd75e3a43 Time: 0.301673
[08/10/2023-11:17:13] [V] [TRT] Conv_44 + Relu_45 Set Tactic Name: ampere_h16816cudnn_64x128_sliced1x2_ldg8_relu_exp_stages_64x4_large_nhwc_tn_v1 Tactic: 0x2721a7f18c2700db
[08/10/2023-11:17:13] [V] [TRT] Tactic: 0x2721a7f18c2700db Time: 0.383877
[08/10/2023-11:17:13] [V] [TRT] Conv_44 + Relu_45 Set Tactic Name: ampere_h16816cudnn_128x64_sliced1x2_ldg8_relu_exp_stages_64x3_medium_nhwc_tn_v1 Tactic: 0x29329b5741ea05f2
[08/10/2023-11:17:13] [V] [TRT] Tactic: 0x29329b5741ea05f2 Time: 0.303675
[08/10/2023-11:17:13] [V] [TRT] Conv_44 + Relu_45 Set Tactic Name: ampere_h16816cudnn_128x128_ldg8_relu_exp_large_nhwc_tn_v1 Tactic: 0x2970ae65966d569d
[08/10/2023-11:17:13] [V] [TRT] Tactic: 0x2970ae65966d569d Time: 0.266208
[08/10/2023-11:17:13] [V] [TRT] Conv_44 + Relu_45 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x256x64_stage3_warpsize1x4x1_g1_tensor16x8x16 Tactic: 0x29dc14520e0e4eb6
[08/10/2023-11:17:13] [V] [TRT] Tactic: 0x29dc14520e0e4eb6 Time: 0.300969
[08/10/2023-11:17:13] [V] [TRT] Conv_44 + Relu_45 Set Tactic Name: ampere_h16816cudnn_256x64_ldg8_relu_exp_stages_64x3_large_nhwc_tn_v1 Tactic: 0x2cb4a662694e89d3
[08/10/2023-11:17:13] [V] [TRT] Tactic: 0x2cb4a662694e89d3 Time: 0.314377
[08/10/2023-11:17:13] [V] [TRT] Conv_44 + Relu_45 Set Tactic Name: ampere_h16816cudnn_128x128_ldg8_relu_exp_stages_64x3_medium_nhwc_tn_v1 Tactic: 0x2eaa2202de9404d6
[08/10/2023-11:17:13] [V] [TRT] Tactic: 0x2eaa2202de9404d6 Time: 0.291959
[08/10/2023-11:17:13] [V] [TRT] Conv_44 + Relu_45 Set Tactic Name: ampere_h1688cudnn_256x128_ldg8_relu_exp_medium_nhwc_tn_v1 Tactic: 0x30c0f36d0aeeac6a
[08/10/2023-11:17:13] [V] [TRT] Tactic: 0x30c0f36d0aeeac6a Time: 0.331849
[08/10/2023-11:17:13] [V] [TRT] Conv_44 + Relu_45 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x64x64_stage4_warpsize2x2x1_g1_tensor16x8x16_t1r3s3_aACCESS Tactic: 0x30e8a8d7a953e5e9
[08/10/2023-11:17:13] [V] [TRT] Tactic: 0x30e8a8d7a953e5e9 Time: 0.339383
[08/10/2023-11:17:13] [V] [TRT] Conv_44 + Relu_45 Set Tactic Name: ampere_h16816cudnn_128x64_ldg8_relu_exp_stages_64x4_large_nhwc_tn_v1 Tactic: 0x3197d5044d71e98e
[08/10/2023-11:17:13] [V] [TRT] Tactic: 0x3197d5044d71e98e Time: 0.371365
[08/10/2023-11:17:13] [V] [TRT] Conv_44 + Relu_45 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x64x64_stage4_warpsize2x2x1_g1_tensor16x8x16_t1r3s3 Tactic: 0x31d93dc22d2af081
[08/10/2023-11:17:13] [V] [TRT] Tactic: 0x31d93dc22d2af081 Time: 0.373975
[08/10/2023-11:17:13] [V] [TRT] Conv_44 + Relu_45 Set Tactic Name: ampere_h1688cudnn_128x128_ldg8_relu_exp_small_nhwc_tn_v1 Tactic: 0x3369260c04f9ad73
[08/10/2023-11:17:13] [V] [TRT] Tactic: 0x3369260c04f9ad73 Time: 0.311246
[08/10/2023-11:17:13] [V] [TRT] Conv_44 + Relu_45 Set Tactic Name: ampere_h16816cudnn_64x128_ldg8_relu_exp_stages_64x4_small_nhwc_tn_v1 Tactic: 0x3b5fa0f2a8fc2410
[08/10/2023-11:17:13] [V] [TRT] Tactic: 0x3b5fa0f2a8fc2410 Time: 0.35541
[08/10/2023-11:17:13] [V] [TRT] Conv_44 + Relu_45 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x32x32_stage4_warpsize4x1x1_g1_tensor16x8x16_t1r3s3 Tactic: 0x3e7eb35b91b9fa63
[08/10/2023-11:17:13] [V] [TRT] Tactic: 0x3e7eb35b91b9fa63 Time: 0.456288
[08/10/2023-11:17:13] [V] [TRT] Conv_44 + Relu_45 Set Tactic Name: ampere_h16816cudnn_256x64_ldg8_relu_exp_stages_64x3_medium_nhwc_tn_v1 Tactic: 0x3f031df0e579d52c
[08/10/2023-11:17:13] [V] [TRT] Tactic: 0x3f031df0e579d52c Time: 0.317358
[08/10/2023-11:17:13] [V] [TRT] Conv_44 + Relu_45 Set Tactic Name: ampere_h16816cudnn_256x64_sliced1x2_ldg8_relu_exp_small_nhwc_tn_v1 Tactic: 0x42a5b8709d0039be
[08/10/2023-11:17:13] [V] [TRT] Tactic: 0x42a5b8709d0039be Time: 0.374903
[08/10/2023-11:17:13] [V] [TRT] Conv_44 + Relu_45 Set Tactic Name: ampere_h16816cudnn_256x128_ldg8_relu_exp_small_nhwc_tn_v1 Tactic: 0x463794ee4acffd1f
[08/10/2023-11:17:13] [V] [TRT] Tactic: 0x463794ee4acffd1f Time: 0.283689
[08/10/2023-11:17:13] [V] [TRT] Conv_44 + Relu_45 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x64x64_stage4_warpsize2x2x1_g1_tensor16x8x16 Tactic: 0x4a81ea1e51436a30
[08/10/2023-11:17:13] [V] [TRT] Tactic: 0x4a81ea1e51436a30 Time: 0.381349
[08/10/2023-11:17:13] [V] [TRT] Conv_44 + Relu_45 Set Tactic Name: ampere_h1688cudnn_256x64_sliced1x2_ldg8_relu_exp_small_nhwc_tn_v1 Tactic: 0x4aed1956bd10a795
[08/10/2023-11:17:13] [V] [TRT] Tactic: 0x4aed1956bd10a795 Time: 0.40299
[08/10/2023-11:17:13] [V] [TRT] Conv_44 + Relu_45 Set Tactic Name: ampere_h16816cudnn_64x128_sliced1x2_ldg8_relu_exp_stages_64x3_large_nhwc_tn_v1 Tactic: 0x4fc05923455fc266
[08/10/2023-11:17:13] [V] [TRT] Tactic: 0x4fc05923455fc266 Time: 0.310299
[08/10/2023-11:17:13] [V] [TRT] Conv_44 + Relu_45 Set Tactic Name: ampere_h16816cudnn_256x64_ldg8_relu_exp_large_nhwc_tn_v1 Tactic: 0x5013c38f55afa9ba
[08/10/2023-11:17:13] [V] [TRT] Tactic: 0x5013c38f55afa9ba Time: 0.322528
[08/10/2023-11:17:13] [V] [TRT] Conv_44 + Relu_45 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x64x32_stage3_warpsize4x1x1_g1_tensor16x8x16_t1r3s3 Tactic: 0x529f4431bdae94f5
[08/10/2023-11:17:13] [V] [TRT] Tactic: 0x529f4431bdae94f5 Time: 0.263515
[08/10/2023-11:17:13] [V] [TRT] Conv_44 + Relu_45 Set Tactic Name: ampere_h16816cudnn_64x128_ldg8_relu_exp_stages_64x3_small_nhwc_tn_v1 Tactic: 0x53be5e206184e6ad
[08/10/2023-11:17:13] [V] [TRT] Tactic: 0x53be5e206184e6ad Time: 0.316965
[08/10/2023-11:17:13] [V] [TRT] Conv_44 + Relu_45 Set Tactic Name: ampere_h16816cudnn_64x64_ldg8_relu_exp_stages_64x5_large_nhwc_tn_v1 Tactic: 0x544bff7ff9c5d908
[08/10/2023-11:17:13] [V] [TRT] Tactic: 0x544bff7ff9c5d908 Time: 0.443479
[08/10/2023-11:17:13] [V] [TRT] Conv_44 + Relu_45 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x64x64_stage3_warpsize2x2x1_g1_tensor16x8x16 Tactic: 0x5568fd8a32f4a40f
[08/10/2023-11:17:13] [V] [TRT] Tactic: 0x5568fd8a32f4a40f Time: 0.304183
[08/10/2023-11:17:13] [V] [TRT] Conv_44 + Relu_45 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x128x64_stage3_warpsize2x2x1_g1_tensor16x8x16 Tactic: 0x57c9a5ff682354a6
[08/10/2023-11:17:13] [V] [TRT] Tactic: 0x57c9a5ff682354a6 Time: 0.283639
[08/10/2023-11:17:13] [V] [TRT] Conv_44 + Relu_45 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x32x64_stage5_warpsize2x2x1_g1_tensor16x8x16 Tactic: 0x5820b3dda403c4d0
[08/10/2023-11:17:13] [V] [TRT] Tactic: 0x5820b3dda403c4d0 Time: 0.546633
[08/10/2023-11:17:13] [V] [TRT] Conv_44 + Relu_45 Set Tactic Name: ampere_h16816cudnn_128x128_ldg8_relu_exp_stages_64x3_small_nhwc_tn_v1 Tactic: 0x58816bf2e9c36afb
[08/10/2023-11:17:13] [V] [TRT] Tactic: 0x58816bf2e9c36afb Time: 0.294354
[08/10/2023-11:17:13] [V] [TRT] Conv_44 + Relu_45 Set Tactic Name: ampere_h16816cudnn_128x64_ldg8_relu_exp_stages_64x3_large_nhwc_tn_v1 Tactic: 0x59762bd684092b33
[08/10/2023-11:17:13] [V] [TRT] Tactic: 0x59762bd684092b33 Time: 0.317024
[08/10/2023-11:17:13] [V] [TRT] Conv_44 + Relu_45 Set Tactic Name: ampere_h16816cudnn_128x64_sliced1x2_ldg8_relu_exp_stages_64x4_medium_nhwc_tn_v1 Tactic: 0x59c5c647b4c76593
[08/10/2023-11:17:13] [V] [TRT] Tactic: 0x59c5c647b4c76593 Time: 0.360594
[08/10/2023-11:17:13] [V] [TRT] Conv_44 + Relu_45 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x128x64_stage3_warpsize4x2x1_g1_tensor16x8x16 Tactic: 0x5a55274960724ba8
[08/10/2023-11:17:13] [V] [TRT] Tactic: 0x5a55274960724ba8 Time: 0.262757
[08/10/2023-11:17:13] [V] [TRT] Conv_44 + Relu_45 Set Tactic Name: ampere_h1688cudnn_256x128_ldg8_relu_exp_large_nhwc_tn_v1 Tactic: 0x5c2e1c87d85b06f1
[08/10/2023-11:17:13] [V] [TRT] Tactic: 0x5c2e1c87d85b06f1 Time: 0.337637
[08/10/2023-11:17:13] [V] [TRT] Conv_44 + Relu_45 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x256x64_stage3_warpsize1x4x1_g1_tensor16x8x16 Tactic: 0x5d067c18f40c23e3
[08/10/2023-11:17:13] [V] [TRT] Tactic: 0x5d067c18f40c23e3 Time: 0.300151
[08/10/2023-11:17:13] [V] [TRT] Conv_44 + Relu_45 Set Tactic Name: ampere_h16816cudnn_64x64_ldg8_relu_exp_stages_64x5_medium_nhwc_tn_v1 Tactic: 0x5f31c22ec167f384
[08/10/2023-11:17:13] [V] [TRT] Tactic: 0x5f31c22ec167f384 Time: 0.44229
[08/10/2023-11:17:13] [V] [TRT] Conv_44 + Relu_45 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x128x32_stage4_warpsize2x2x1_g1_tensor16x8x16_t1r3s3 Tactic: 0x60c3421152ef8e10
[08/10/2023-11:17:13] [V] [TRT] Tactic: 0x60c3421152ef8e10 Time: 0.244288
[08/10/2023-11:17:13] [V] [TRT] Conv_44 + Relu_45 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x128x32_stage3_warpsize4x2x1_g1_tensor16x8x16 Tactic: 0x60da8c7151d91e47
[08/10/2023-11:17:13] [V] [TRT] Tactic: 0x60da8c7151d91e47 Time: 0.270043
[08/10/2023-11:17:13] [V] [TRT] Conv_44 + Relu_45 Set Tactic Name: ampere_h1688cudnn_256x64_ldg8_relu_exp_medium_nhwc_tn_v1 Tactic: 0x6426696f872a3b13
[08/10/2023-11:17:13] [V] [TRT] Tactic: 0x6426696f872a3b13 Time: 0.409719
[08/10/2023-11:17:13] [V] [TRT] Conv_44 + Relu_45 Set Tactic Name: ampere_h16816cudnn_128x64_ldg8_relu_exp_stages_64x3_medium_nhwc_tn_v1 Tactic: 0x699be152cfb6d6ff
[08/10/2023-11:17:13] [V] [TRT] Tactic: 0x699be152cfb6d6ff Time: 0.317362
[08/10/2023-11:17:13] [V] [TRT] Conv_44 + Relu_45 Set Tactic Name: ampere_h16816cudnn_64x64_ldg8_relu_exp_stages_64x6_large_nhwc_tn_v1 Tactic: 0x6af049035146c349
[08/10/2023-11:17:13] [V] [TRT] Tactic: 0x6af049035146c349 Time: 0.532183
[08/10/2023-11:17:13] [V] [TRT] Conv_44 + Relu_45 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x256x32_stage3_warpsize2x4x1_g1_tensor16x8x16 Tactic: 0x7005d10718f6c22d
[08/10/2023-11:17:13] [V] [TRT] Tactic: 0x7005d10718f6c22d Time: 0.271963
[08/10/2023-11:17:13] [V] [TRT] Conv_44 + Relu_45 Set Tactic Name: ampere_h16816cudnn_64x64_sliced1x2_ldg8_relu_exp_stages_64x5_medium_nhwc_tn_v1 Tactic: 0x706f08da35c795a5
[08/10/2023-11:17:13] [V] [TRT] Tactic: 0x706f08da35c795a5 Time: 0.413193
[08/10/2023-11:17:13] [V] [TRT] Conv_44 + Relu_45 Set Tactic Name: ampere_h16816cudnn_128x64_sliced1x2_ldg8_relu_exp_stages_64x4_small_nhwc_tn_v1 Tactic: 0x7163d33a4d8ce8d7
[08/10/2023-11:17:13] [V] [TRT] Tactic: 0x7163d33a4d8ce8d7 Time: 0.349874
[08/10/2023-11:17:13] [V] [TRT] Conv_44 + Relu_45 Set Tactic Name: ampere_h1688cudnn_256x64_ldg8_relu_exp_small_nhwc_tn_v1 Tactic: 0x7aad3976677d7155
[08/10/2023-11:17:13] [V] [TRT] Tactic: 0x7aad3976677d7155 Time: 0.384306
[08/10/2023-11:17:13] [V] [TRT] Conv_44 + Relu_45 Set Tactic Name: ampere_h16816cudnn_64x64_sliced1x2_ldg8_relu_exp_stages_64x5_large_nhwc_tn_v1 Tactic: 0x8015519605ab9963
[08/10/2023-11:17:13] [V] [TRT] Tactic: 0x8015519605ab9963 Time: 0.410345
[08/10/2023-11:17:13] [V] [TRT] Conv_44 + Relu_45 Set Tactic Name: ampere_h16816cudnn_256x64_sliced1x2_ldg8_relu_exp_medium_nhwc_tn_v1 Tactic: 0x82f8a2214b0b4178
[08/10/2023-11:17:13] [V] [TRT] Tactic: 0x82f8a2214b0b4178 Time: 0.415273
[08/10/2023-11:17:13] [V] [TRT] Conv_44 + Relu_45 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x32x32_stage4_warpsize4x1x1_g1_tensor16x8x16 Tactic: 0x834e11ecd4ab9454
[08/10/2023-11:17:13] [V] [TRT] Tactic: 0x834e11ecd4ab9454 Time: 0.450249
[08/10/2023-11:17:13] [V] [TRT] Conv_44 + Relu_45 Set Tactic Name: ampere_h16816cudnn_128x64_sliced1x2_ldg8_relu_exp_stages_64x3_large_nhwc_tn_v1 Tactic: 0x83ccd4762c1376a1
[08/10/2023-11:17:13] [V] [TRT] Tactic: 0x83ccd4762c1376a1 Time: 0.307808
[08/10/2023-11:17:13] [V] [TRT] Conv_44 + Relu_45 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x64x64_stage3_warpsize4x1x1_g1_tensor16x8x16_t1r3s3 Tactic: 0x84ee897dd1f4d2aa
[08/10/2023-11:17:13] [V] [TRT] Tactic: 0x84ee897dd1f4d2aa Time: 0.312517
[08/10/2023-11:17:13] [V] [TRT] Conv_44 + Relu_45 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x128x64_stage3_warpsize2x2x1_g1_tensor16x8x16 Tactic: 0x866e7a5f6401b67f
[08/10/2023-11:17:13] [V] [TRT] Tactic: 0x866e7a5f6401b67f Time: 0.277307
[08/10/2023-11:17:13] [V] [TRT] Conv_44 + Relu_45 Set Tactic Name: ampere_h16816cudnn_64x64_ldg8_relu_exp_stages_64x6_medium_nhwc_tn_v1 Tactic: 0x88971eec55aba850
[08/10/2023-11:17:13] [V] [TRT] Tactic: 0x88971eec55aba850 Time: 0.518153
[08/10/2023-11:17:13] [V] [TRT] Conv_44 + Relu_45 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x128x64_stage3_warpsize4x2x1_g1_tensor16x8x16 Tactic: 0x8bf2f8e96c50a971
[08/10/2023-11:17:13] [V] [TRT] Tactic: 0x8bf2f8e96c50a971 Time: 0.259269
[08/10/2023-11:17:13] [V] [TRT] Conv_44 + Relu_45 Set Tactic Name: ampere_h16816cudnn_256x128_ldg8_relu_exp_stages_64x3_large_nhwc_tn_v1 Tactic: 0x94576ece6beb35e3
[08/10/2023-11:17:13] [V] [TRT] Tactic: 0x94576ece6beb35e3 Time: 0.274363
[08/10/2023-11:17:13] [V] [TRT] Conv_44 + Relu_45 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x64x64_stage3_warpsize4x1x1_g1_tensor16x8x16 Tactic: 0x9fff6e65019cc310
[08/10/2023-11:17:13] [V] [TRT] Tactic: 0x9fff6e65019cc310 Time: 0.309998
[08/10/2023-11:17:13] [V] [TRT] Conv_44 + Relu_45 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x32x64_stage5_warpsize2x2x1_g1_tensor16x8x16_t1r3s3 Tactic: 0xa033e20ae9f412b2
[08/10/2023-11:17:13] [V] [TRT] Tactic: 0xa033e20ae9f412b2 Time: 0.536338
[08/10/2023-11:17:13] [V] [TRT] Conv_44 + Relu_45 Set Tactic Name: ampere_h16816cudnn_64x128_ldg8_relu_exp_stages_64x4_large_nhwc_tn_v1 Tactic: 0xa111596c001b78db
[08/10/2023-11:17:13] [V] [TRT] Tactic: 0xa111596c001b78db Time: 0.359685
[08/10/2023-11:17:13] [V] [TRT] Conv_44 + Relu_45 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x256x32_stage3_warpsize2x4x1_g1_tensor16x8x16 Tactic: 0xa1a20ea714d420f4
[08/10/2023-11:17:13] [V] [TRT] Tactic: 0xa1a20ea714d420f4 Time: 0.272178
[08/10/2023-11:17:13] [V] [TRT] Conv_44 + Relu_45 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x32x32_stage4_warpsize4x1x1_g1_tensor16x8x16_t1r3s3 Tactic: 0xa40cb43c296a36a8
[08/10/2023-11:17:13] [V] [TRT] Tactic: 0xa40cb43c296a36a8 Time: 0.407273
[08/10/2023-11:17:13] [V] [TRT] Conv_44 + Relu_45 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x64x32_stage3_warpsize4x1x1_g1_tensor16x8x16 Tactic: 0xa570c55d303796ff
[08/10/2023-11:17:13] [V] [TRT] Tactic: 0xa570c55d303796ff Time: 0.277915
[08/10/2023-11:17:13] [V] [TRT] Conv_44 + Relu_45 Set Tactic Name: ampere_h16816cudnn_64x64_sliced1x2_ldg8_relu_exp_stages_64x6_medium_nhwc_tn_v1 Tactic: 0xa7c9d418a10bce71
[08/10/2023-11:17:13] [V] [TRT] Tactic: 0xa7c9d418a10bce71 Time: 0.522007
[08/10/2023-11:17:13] [V] [TRT] Conv_44 + Relu_45 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x32x32_stage4_warpsize4x1x1_g1_tensor16x8x16 Tactic: 0xa83b68f30462f971
[08/10/2023-11:17:13] [V] [TRT] Tactic: 0xa83b68f30462f971 Time: 0.414313
[08/10/2023-11:17:13] [V] [TRT] Conv_44 + Relu_45 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x256x32_stage3_warpsize2x4x1_g1_tensor16x8x16_t1r3s3 Tactic: 0xa9177bbe4e767df8
[08/10/2023-11:17:13] [V] [TRT] Tactic: 0xa9177bbe4e767df8 Time: 0.272311
[08/10/2023-11:17:13] [V] [TRT] Conv_44 + Relu_45 Set Tactic Name: ampere_h1688cudnn_128x128_ldg8_relu_exp_large_nhwc_tn_v1 Tactic: 0xa927df92ac1ef1b8
[08/10/2023-11:17:13] [V] [TRT] Tactic: 0xa927df92ac1ef1b8 Time: 0.325243
[08/10/2023-11:17:13] [V] [TRT] Conv_44 + Relu_45 Set Tactic Name: ampere_h16816cudnn_128x64_ldg8_relu_exp_stages_64x4_small_nhwc_tn_v1 Tactic: 0xabd92c9ae596b545
[08/10/2023-11:17:13] [V] [TRT] Tactic: 0xabd92c9ae596b545 Time: 0.355333
[08/10/2023-11:17:13] [V] [TRT] Conv_44 + Relu_45 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x128x32_stage5_warpsize2x2x1_g1_tensor16x8x16 Tactic: 0xafd1e8bf6bd3d638
[08/10/2023-11:17:13] [V] [TRT] Tactic: 0xafd1e8bf6bd3d638 Time: 0.291346
[08/10/2023-11:17:13] [V] [TRT] Conv_44 + Relu_45 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x128x32_stage3_warpsize4x2x1_g1_tensor16x8x16 Tactic: 0xb17d53d15dfbfc9e
[08/10/2023-11:17:13] [V] [TRT] Tactic: 0xb17d53d15dfbfc9e Time: 0.286359
[08/10/2023-11:17:13] [V] [TRT] Conv_44 + Relu_45 Set Tactic Name: ampere_h16816cudnn_128x128_ldg8_relu_exp_small_nhwc_tn_v1 Tactic: 0xb33e57fb3e8a0a56
[08/10/2023-11:17:13] [V] [TRT] Tactic: 0xb33e57fb3e8a0a56 Time: 0.270304
[08/10/2023-11:17:13] [V] [TRT] Conv_44 + Relu_45 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x64x32_stage5_warpsize2x2x1_g1_tensor16x8x16_t1r3s3 Tactic: 0xb4bec086187edcfc
[08/10/2023-11:17:13] [V] [TRT] Tactic: 0xb4bec086187edcfc Time: 0.301595
[08/10/2023-11:17:13] [V] [TRT] Conv_44 + Relu_45 Set Tactic Name: ampere_h16816cudnn_256x64_ldg8_relu_exp_stages_64x3_small_nhwc_tn_v1 Tactic: 0xb6fa5ffcc1a9d518
[08/10/2023-11:17:13] [V] [TRT] Tactic: 0xb6fa5ffcc1a9d518 Time: 0.316878
[08/10/2023-11:17:13] [V] [TRT] Conv_44 + Relu_45 Set Tactic Name: ampere_h16816cudnn_64x128_sliced1x2_ldg8_relu_exp_stages_64x3_medium_nhwc_tn_v1 Tactic: 0xb85e52e87caf60a2
[08/10/2023-11:17:13] [V] [TRT] Tactic: 0xb85e52e87caf60a2 Time: 0.312622
[08/10/2023-11:17:13] [V] [TRT] Conv_44 + Relu_45 Set Tactic Name: ampere_h16816cudnn_64x128_ldg8_relu_exp_stages_64x3_medium_nhwc_tn_v1 Tactic: 0xb8d86216e1235cda
[08/10/2023-11:17:13] [V] [TRT] Tactic: 0xb8d86216e1235cda Time: 0.325559
[08/10/2023-11:17:13] [V] [TRT] Conv_44 + Relu_45 Set Tactic Name: ampere_h16816cudnn_256x128_ldg8_relu_exp_stages_64x3_medium_nhwc_tn_v1 Tactic: 0xb9171d70ba2eda4b
[08/10/2023-11:17:13] [V] [TRT] Tactic: 0xb9171d70ba2eda4b Time: 0.279621
[08/10/2023-11:17:13] [V] [TRT] Conv_44 + Relu_45 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x64x32_stage5_warpsize2x2x1_g1_tensor16x8x16 Tactic: 0xbd08239a9317f2fd
[08/10/2023-11:17:13] [V] [TRT] Tactic: 0xbd08239a9317f2fd Time: 0.30917
[08/10/2023-11:17:13] [V] [TRT] Conv_44 + Relu_45 Set Tactic Name: ampere_h16816cudnn_64x128_sliced1x2_ldg8_relu_exp_stages_64x4_small_nhwc_tn_v1 Tactic: 0xbd6f5e6f24c05c10
[08/10/2023-11:17:13] [V] [TRT] Tactic: 0xbd6f5e6f24c05c10 Time: 0.383337
[08/10/2023-11:17:13] [V] [TRT] Conv_44 + Relu_45 Set Tactic Name: ampere_h16816cudnn_64x64_sliced1x2_ldg8_relu_exp_stages_64x6_large_nhwc_tn_v1 Tactic: 0xbeaee7eaad288322
[08/10/2023-11:17:13] [V] [TRT] Tactic: 0xbeaee7eaad288322 Time: 0.529673
[08/10/2023-11:17:13] [V] [TRT] Conv_44 + Relu_45 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x128x64_stage3_warpsize2x2x1_g1_tensor16x8x16_t1r3s3 Tactic: 0xc0a02dc6095497cc
[08/10/2023-11:17:13] [V] [TRT] Tactic: 0xc0a02dc6095497cc Time: 0.279415
[08/10/2023-11:17:13] [V] [TRT] Conv_44 + Relu_45 Set Tactic Name: ampere_h16816cudnn_128x128_ldg8_relu_exp_stages_64x3_large_nhwc_tn_v1 Tactic: 0xc2cf926c41243630
[08/10/2023-11:17:13] [V] [TRT] Tactic: 0xc2cf926c41243630 Time: 0.292919
[08/10/2023-11:17:13] [V] [TRT] Conv_44 + Relu_45 Set Tactic Name: ampere_h16816cudnn_128x64_ldg8_relu_exp_stages_64x3_small_nhwc_tn_v1 Tactic: 0xc338d2482cee77f8
[08/10/2023-11:17:13] [V] [TRT] Tactic: 0xc338d2482cee77f8 Time: 0.315264
[08/10/2023-11:17:13] [V] [TRT] Conv_44 + Relu_45 Set Tactic Name: ampere_h1688cudnn_256x128_ldg8_relu_exp_small_nhwc_tn_v1 Tactic: 0xc660e51970bc5a3a
[08/10/2023-11:17:13] [V] [TRT] Tactic: 0xc660e51970bc5a3a Time: 0.312672
[08/10/2023-11:17:13] [V] [TRT] Conv_44 + Relu_45 Set Tactic Name: ampere_h16816cudnn_64x128_ldg8_relu_exp_stages_64x4_medium_nhwc_tn_v1 Tactic: 0xc82f3f06140e3cbb
[08/10/2023-11:17:13] [V] [TRT] Tactic: 0xc82f3f06140e3cbb Time: 0.359159
[08/10/2023-11:17:13] [V] [TRT] Conv_44 + Relu_45 Set Tactic Name: ampere_h16816cudnn_64x128_sliced1x2_ldg8_relu_exp_stages_64x4_medium_nhwc_tn_v1 Tactic: 0xc8a90ff8898200c3
[08/10/2023-11:17:13] [V] [TRT] Tactic: 0xc8a90ff8898200c3 Time: 0.3784
[08/10/2023-11:17:13] [V] [TRT] Conv_44 + Relu_45 Set Tactic Name: ampere_h1688cudnn_256x64_sliced1x2_ldg8_relu_exp_medium_nhwc_tn_v1 Tactic: 0xc9cc55109bb4de26
[08/10/2023-11:17:13] [V] [TRT] Tactic: 0xc9cc55109bb4de26 Time: 0.42939
[08/10/2023-11:17:13] [V] [TRT] Conv_44 + Relu_45 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x64x32_stage5_warpsize2x2x1_g1_tensor16x8x16 Tactic: 0xc9d24bd069159fa8
[08/10/2023-11:17:13] [V] [TRT] Tactic: 0xc9d24bd069159fa8 Time: 0.309458
[08/10/2023-11:17:13] [V] [TRT] Conv_44 + Relu_45 Set Tactic Name: ampere_h16816cudnn_64x128_ldg8_relu_exp_stages_64x3_large_nhwc_tn_v1 Tactic: 0xc9f0a7bec963ba66
[08/10/2023-11:17:13] [V] [TRT] Tactic: 0xc9f0a7bec963ba66 Time: 0.318981
[08/10/2023-11:17:13] [V] [TRT] Conv_44 + Relu_45 Set Tactic Name: ampere_h16816cudnn_256x64_ldg8_relu_exp_small_nhwc_tn_v1 Tactic: 0xca5d3a11fd48f571
[08/10/2023-11:17:13] [V] [TRT] Tactic: 0xca5d3a11fd48f571 Time: 0.306213
[08/10/2023-11:17:13] [V] [TRT] Conv_44 + Relu_45 Set Tactic Name: ampere_h16816cudnn_64x64_ldg8_relu_exp_stages_64x5_small_nhwc_tn_v1 Tactic: 0xce0506e1512285c3
[08/10/2023-11:17:13] [V] [TRT] Tactic: 0xce0506e1512285c3 Time: 0.448032
[08/10/2023-11:17:13] [V] [TRT] Conv_44 + Relu_45 Set Tactic Name: ampere_h1688cudnn_256x64_sliced1x2_ldg8_relu_exp_large_nhwc_tn_v1 Tactic: 0xd0a3e0c815f7fb5e
[08/10/2023-11:17:13] [V] [TRT] Tactic: 0xd0a3e0c815f7fb5e Time: 0.43413
[08/10/2023-11:17:13] [V] [TRT] Conv_44 + Relu_45 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x64x32_stage3_warpsize4x1x1_g1_tensor16x8x16 Tactic: 0xd1aaad17ca35fbaa
[08/10/2023-11:17:13] [V] [TRT] Tactic: 0xd1aaad17ca35fbaa Time: 0.265582
[08/10/2023-11:17:13] [V] [TRT] Conv_44 + Relu_45 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x128x64_stage3_warpsize4x2x1_g1_tensor16x8x16_t1r3s3 Tactic: 0xd2fca0486ca97266
[08/10/2023-11:17:13] [V] [TRT] Tactic: 0xd2fca0486ca97266 Time: 0.263552
[08/10/2023-11:17:13] [V] [TRT] Conv_44 + Relu_45 Set Tactic Name: ampere_h16816cudnn_64x128_sliced1x2_ldg8_relu_exp_stages_64x3_small_nhwc_tn_v1 Tactic: 0xd58ea0bdedb89ead
[08/10/2023-11:17:13] [V] [TRT] Tactic: 0xd58ea0bdedb89ead Time: 0.307195
[08/10/2023-11:17:13] [V] [TRT] Conv_44 + Relu_45 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x256x64_stage3_warpsize1x4x1_g1_tensor16x8x16_t1r3s3 Tactic: 0xd7c261fa01db2af9
[08/10/2023-11:17:13] [V] [TRT] Tactic: 0xd7c261fa01db2af9 Time: 0.302848
[08/10/2023-11:17:13] [V] [TRT] Conv_44 + Relu_45 Set Tactic Name: ampere_h16816cudnn_256x64_sliced1x2_ldg8_relu_exp_large_nhwc_tn_v1 Tactic: 0xd8eb41ee35e76575
[08/10/2023-11:17:13] [V] [TRT] Tactic: 0xd8eb41ee35e76575 Time: 0.417851
[08/10/2023-11:17:13] [V] [TRT] Conv_44 + Relu_45 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x128x32_stage5_warpsize2x2x1_g1_tensor16x8x16 Tactic: 0xdb0b80f591d1bb6d
[08/10/2023-11:17:13] [V] [TRT] Tactic: 0xdb0b80f591d1bb6d Time: 0.292101
[08/10/2023-11:17:13] [V] [TRT] Conv_44 + Relu_45 Set Tactic Name: ampere_h16816cudnn_256x128_ldg8_relu_exp_large_nhwc_tn_v1 Tactic: 0xdc796d70e228a1d4
[08/10/2023-11:17:13] [V] [TRT] Tactic: 0xdc796d70e228a1d4 Time: 0.282126
[08/10/2023-11:17:13] [V] [TRT] Conv_44 + Relu_45 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x32x32_stage4_warpsize4x1x1_g1_tensor16x8x16 Tactic: 0xdce100b9fe609424
[08/10/2023-11:17:13] [V] [TRT] Tactic: 0xdce100b9fe609424 Time: 0.420229
[08/10/2023-11:17:13] [V] [TRT] Conv_44 + Relu_45 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x128x32_stage4_warpsize2x2x1_g1_tensor16x8x16 Tactic: 0xdfa020ef435ef810
[08/10/2023-11:17:13] [V] [TRT] Tactic: 0xdfa020ef435ef810 Time: 0.243694
[08/10/2023-11:17:13] [V] [TRT] Conv_44 + Relu_45 Set Tactic Name: ampere_h1688cudnn_256x64_ldg8_relu_exp_large_nhwc_tn_v1 Tactic: 0xe0e3c0e8cf9a2d9e
[08/10/2023-11:17:13] [V] [TRT] Tactic: 0xe0e3c0e8cf9a2d9e Time: 0.408133
[08/10/2023-11:17:13] [V] [TRT] Conv_44 + Relu_45 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x32x64_stage5_warpsize2x2x1_g1_tensor16x8x16 Tactic: 0xe1ff5ad20f5c6bf6
[08/10/2023-11:17:13] [V] [TRT] Tactic: 0xe1ff5ad20f5c6bf6 Time: 0.557787
[08/10/2023-11:17:13] [V] [TRT] Conv_44 + Relu_45 Set Tactic Name: ampere_h16816cudnn_256x64_ldg8_relu_exp_medium_nhwc_tn_v1 Tactic: 0xe4711898bd599c36
[08/10/2023-11:17:13] [V] [TRT] Tactic: 0xe4711898bd599c36 Time: 0.313751
[08/10/2023-11:17:13] [V] [TRT] Conv_44 + Relu_45 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x64x64_stage3_warpsize4x1x1_g1_tensor16x8x16 Tactic: 0xeb25062ffb9eae45
[08/10/2023-11:17:13] [V] [TRT] Tactic: 0xeb25062ffb9eae45 Time: 0.315959
[08/10/2023-11:17:13] [V] [TRT] Conv_44 + Relu_45 Set Tactic Name: ampere_h16816cudnn_128x64_sliced1x2_ldg8_relu_exp_stages_64x4_large_nhwc_tn_v1 Tactic: 0xeb2d2aa4e56bb41c
[08/10/2023-11:17:13] [V] [TRT] Tactic: 0xeb2d2aa4e56bb41c Time: 0.37259
[08/10/2023-11:17:13] [V] [TRT] Conv_44 + Relu_45 Set Tactic Name: ampere_h16816cudnn_64x64_ldg8_relu_exp_stages_64x6_small_nhwc_tn_v1 Tactic: 0xf0beb09df9a19f82
[08/10/2023-11:17:13] [V] [TRT] Tactic: 0xf0beb09df9a19f82 Time: 0.487072
[08/10/2023-11:17:13] [V] [TRT] Conv_44 + Relu_45 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x64x64_stage4_warpsize2x2x1_g1_tensor16x8x16 Tactic: 0xf35e0311fa1cc516
[08/10/2023-11:17:13] [V] [TRT] Tactic: 0xf35e0311fa1cc516 Time: 0.380741
[08/10/2023-11:17:13] [V] [TRT] Conv_44 + Relu_45 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x32x32_stage4_warpsize4x1x1_g1_tensor16x8x16 Tactic: 0xf79479a62ea9f901
[08/10/2023-11:17:13] [V] [TRT] Tactic: 0xf79479a62ea9f901 Time: 0.45488
[08/10/2023-11:17:13] [V] [TRT] Fastest Tactic: 0xdfa020ef435ef810 Time: 0.243694
[08/10/2023-11:17:13] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0xdfa020ef435ef810
[08/10/2023-11:17:13] [V] [TRT] *************** Autotuning format combination: Half(259200,1:16,1440,8) -> Half(129600,1:16,1440,16) ***************
[08/10/2023-11:17:13] [W] [TRT] Weights [name=Conv_44 + Relu_45.weight] had the following issues when converted to FP16:
[08/10/2023-11:17:13] [W] [TRT]  - Subnormal FP16 values detected. 
[08/10/2023-11:17:13] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.
[08/10/2023-11:17:13] [V] [TRT] --------------- Timing Runner: Conv_44 + Relu_45 (CaskConvolution)
[08/10/2023-11:17:13] [V] [TRT] Conv_44 + Relu_45 Set Tactic Name: ampere_h16816cudnn_256x128_ldg8_relu_exp_medium_nhwc_tn_v1 Tactic: 0x03896956a39a1203
[08/10/2023-11:17:13] [V] [TRT] Tactic: 0x03896956a39a1203 Time: 0.285047
[08/10/2023-11:17:13] [V] [TRT] Conv_44 + Relu_45 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x128x32_stage3_warpsize4x2x1_g1_tensor16x8x16_t1r3s3 Tactic: 0x048d6d0400f33439
[08/10/2023-11:17:13] [V] [TRT] Tactic: 0x048d6d0400f33439 Time: 0.268786
[08/10/2023-11:17:13] [V] [TRT] Conv_44 + Relu_45 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x64x64_stage3_warpsize2x2x1_g1_tensor16x8x16_t1r3s3 Tactic: 0x05b6220f243edacd
[08/10/2023-11:17:13] [V] [TRT] Tactic: 0x05b6220f243edacd Time: 0.309961
[08/10/2023-11:17:13] [V] [TRT] Conv_44 + Relu_45 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x64x32_stage3_warpsize4x1x1_g1_tensor16x8x16_t1r3s3_aACCESS Tactic: 0x0866ddee325d07a6
[08/10/2023-11:17:13] [V] [TRT] Tactic: 0x0866ddee325d07a6 Time: 0.290441
[08/10/2023-11:17:13] [V] [TRT] Conv_44 + Relu_45 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x128x32_stage4_warpsize2x2x1_g1_tensor16x8x16 Tactic: 0x0e07ff4f4f7c1ac9
[08/10/2023-11:17:13] [V] [TRT] Tactic: 0x0e07ff4f4f7c1ac9 Time: 0.267899
[08/10/2023-11:17:13] [V] [TRT] Conv_44 + Relu_45 Set Tactic Name: ampere_h16816cudnn_256x128_ldg8_relu_exp_stages_64x3_small_nhwc_tn_v1 Tactic: 0x0e199750c30c6928
[08/10/2023-11:17:13] [V] [TRT] Tactic: 0x0e199750c30c6928 Time: 0.280539
[08/10/2023-11:17:13] [V] [TRT] Conv_44 + Relu_45 Set Tactic Name: ampere_h1688cudnn_128x128_ldg8_relu_exp_medium_nhwc_tn_v1 Tactic: 0x100c1ad308e08d35
[08/10/2023-11:17:13] [V] [TRT] Tactic: 0x100c1ad308e08d35 Time: 0.331675
[08/10/2023-11:17:13] [V] [TRT] Conv_44 + Relu_45 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x128x32_stage5_warpsize2x2x1_g1_tensor16x8x16_t1r3s3 Tactic: 0x17ebf0c9f418f10a
[08/10/2023-11:17:13] [V] [TRT] Tactic: 0x17ebf0c9f418f10a Time: 0.298962
[08/10/2023-11:17:13] [V] [TRT] Conv_44 + Relu_45 Set Tactic Name: ampere_h16816cudnn_128x64_ldg8_relu_exp_stages_64x4_medium_nhwc_tn_v1 Tactic: 0x196cbc423a9bb69e
[08/10/2023-11:17:13] [V] [TRT] Tactic: 0x196cbc423a9bb69e Time: 0.368649
[08/10/2023-11:17:13] [V] [TRT] Conv_44 + Relu_45 Set Tactic Name: ampere_h16816cudnn_128x64_sliced1x2_ldg8_relu_exp_stages_64x3_small_nhwc_tn_v1 Tactic: 0x19822de884f42a6a
[08/10/2023-11:17:13] [V] [TRT] Tactic: 0x19822de884f42a6a Time: 0.311419
[08/10/2023-11:17:13] [V] [TRT] Conv_44 + Relu_45 Set Tactic Name: ampere_h16816cudnn_64x64_sliced1x2_ldg8_relu_exp_stages_64x5_small_nhwc_tn_v1 Tactic: 0x1a5ba808ad4cc5a8
[08/10/2023-11:17:13] [V] [TRT] Tactic: 0x1a5ba808ad4cc5a8 Time: 0.419209
[08/10/2023-11:17:13] [V] [TRT] Conv_44 + Relu_45 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x64x64_stage3_warpsize2x2x1_g1_tensor16x8x16 Tactic: 0x21b295c0c8f6c95a
[08/10/2023-11:17:13] [V] [TRT] Tactic: 0x21b295c0c8f6c95a Time: 0.305271
[08/10/2023-11:17:13] [V] [TRT] Conv_44 + Relu_45 Set Tactic Name: ampere_h16816cudnn_128x128_ldg8_relu_exp_medium_nhwc_tn_v1 Tactic: 0x234580e8a194335c
[08/10/2023-11:17:13] [V] [TRT] Tactic: 0x234580e8a194335c Time: 0.265083
[08/10/2023-11:17:13] [V] [TRT] Conv_44 + Relu_45 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x128x32_stage4_warpsize2x2x1_g1_tensor16x8x16_t1r3s3_aACCESS Tactic: 0x245530c34bd6090f
[08/10/2023-11:17:13] [V] [TRT] Tactic: 0x245530c34bd6090f Time: 0.249714
[08/10/2023-11:17:13] [V] [TRT] Conv_44 + Relu_45 Set Tactic Name: ampere_h16816cudnn_64x64_sliced1x2_ldg8_relu_exp_stages_64x6_small_nhwc_tn_v1 Tactic: 0x24e01e7405cfdfe9
[08/10/2023-11:17:13] [V] [TRT] Tactic: 0x24e01e7405cfdfe9 Time: 0.488891
[08/10/2023-11:17:13] [V] [TRT] Conv_44 + Relu_45 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x64x64_stage3_warpsize2x2x1_g1_tensor16x8x16_t1r3s3_aACCESS Tactic: 0x263a38afd75e3a43
[08/10/2023-11:17:13] [V] [TRT] Tactic: 0x263a38afd75e3a43 Time: 0.304402
[08/10/2023-11:17:13] [V] [TRT] Conv_44 + Relu_45 Set Tactic Name: ampere_h16816cudnn_64x128_sliced1x2_ldg8_relu_exp_stages_64x4_large_nhwc_tn_v1 Tactic: 0x2721a7f18c2700db
[08/10/2023-11:17:13] [V] [TRT] Tactic: 0x2721a7f18c2700db Time: 0.380133
[08/10/2023-11:17:13] [V] [TRT] Conv_44 + Relu_45 Set Tactic Name: ampere_h16816cudnn_128x64_sliced1x2_ldg8_relu_exp_stages_64x3_medium_nhwc_tn_v1 Tactic: 0x29329b5741ea05f2
[08/10/2023-11:17:13] [V] [TRT] Tactic: 0x29329b5741ea05f2 Time: 0.307675
[08/10/2023-11:17:13] [V] [TRT] Conv_44 + Relu_45 Set Tactic Name: ampere_h16816cudnn_128x128_ldg8_relu_exp_large_nhwc_tn_v1 Tactic: 0x2970ae65966d569d
[08/10/2023-11:17:13] [V] [TRT] Tactic: 0x2970ae65966d569d Time: 0.267058
[08/10/2023-11:17:13] [V] [TRT] Conv_44 + Relu_45 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x256x64_stage3_warpsize1x4x1_g1_tensor16x8x16 Tactic: 0x29dc14520e0e4eb6
[08/10/2023-11:17:13] [V] [TRT] Tactic: 0x29dc14520e0e4eb6 Time: 0.304923
[08/10/2023-11:17:13] [V] [TRT] Conv_44 + Relu_45 Set Tactic Name: ampere_h16816cudnn_256x64_ldg8_relu_exp_stages_64x3_large_nhwc_tn_v1 Tactic: 0x2cb4a662694e89d3
[08/10/2023-11:17:13] [V] [TRT] Tactic: 0x2cb4a662694e89d3 Time: 0.312494
[08/10/2023-11:17:13] [V] [TRT] Conv_44 + Relu_45 Set Tactic Name: ampere_h16816cudnn_128x128_ldg8_relu_exp_stages_64x3_medium_nhwc_tn_v1 Tactic: 0x2eaa2202de9404d6
[08/10/2023-11:17:13] [V] [TRT] Tactic: 0x2eaa2202de9404d6 Time: 0.291849
[08/10/2023-11:17:13] [V] [TRT] Conv_44 + Relu_45 Set Tactic Name: ampere_h1688cudnn_256x128_ldg8_relu_exp_medium_nhwc_tn_v1 Tactic: 0x30c0f36d0aeeac6a
[08/10/2023-11:17:14] [V] [TRT] Tactic: 0x30c0f36d0aeeac6a Time: 0.334519
[08/10/2023-11:17:14] [V] [TRT] Conv_44 + Relu_45 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x64x64_stage4_warpsize2x2x1_g1_tensor16x8x16_t1r3s3_aACCESS Tactic: 0x30e8a8d7a953e5e9
[08/10/2023-11:17:14] [V] [TRT] Tactic: 0x30e8a8d7a953e5e9 Time: 0.339525
[08/10/2023-11:17:14] [V] [TRT] Conv_44 + Relu_45 Set Tactic Name: ampere_h16816cudnn_128x64_ldg8_relu_exp_stages_64x4_large_nhwc_tn_v1 Tactic: 0x3197d5044d71e98e
[08/10/2023-11:17:14] [V] [TRT] Tactic: 0x3197d5044d71e98e Time: 0.370693
[08/10/2023-11:17:14] [V] [TRT] Conv_44 + Relu_45 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x64x64_stage4_warpsize2x2x1_g1_tensor16x8x16_t1r3s3 Tactic: 0x31d93dc22d2af081
[08/10/2023-11:17:14] [V] [TRT] Tactic: 0x31d93dc22d2af081 Time: 0.373627
[08/10/2023-11:17:14] [V] [TRT] Conv_44 + Relu_45 Set Tactic Name: ampere_h1688cudnn_128x128_ldg8_relu_exp_small_nhwc_tn_v1 Tactic: 0x3369260c04f9ad73
[08/10/2023-11:17:14] [V] [TRT] Tactic: 0x3369260c04f9ad73 Time: 0.313554
[08/10/2023-11:17:14] [V] [TRT] Conv_44 + Relu_45 Set Tactic Name: ampere_h16816cudnn_64x128_ldg8_relu_exp_stages_64x4_small_nhwc_tn_v1 Tactic: 0x3b5fa0f2a8fc2410
[08/10/2023-11:17:14] [V] [TRT] Tactic: 0x3b5fa0f2a8fc2410 Time: 0.359282
[08/10/2023-11:17:14] [V] [TRT] Conv_44 + Relu_45 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x32x32_stage4_warpsize4x1x1_g1_tensor16x8x16_t1r3s3 Tactic: 0x3e7eb35b91b9fa63
[08/10/2023-11:17:14] [V] [TRT] Tactic: 0x3e7eb35b91b9fa63 Time: 0.448005
[08/10/2023-11:17:14] [V] [TRT] Conv_44 + Relu_45 Set Tactic Name: ampere_h16816cudnn_256x64_ldg8_relu_exp_stages_64x3_medium_nhwc_tn_v1 Tactic: 0x3f031df0e579d52c
[08/10/2023-11:17:14] [V] [TRT] Tactic: 0x3f031df0e579d52c Time: 0.311977
[08/10/2023-11:17:14] [V] [TRT] Conv_44 + Relu_45 Set Tactic Name: ampere_h16816cudnn_256x64_sliced1x2_ldg8_relu_exp_small_nhwc_tn_v1 Tactic: 0x42a5b8709d0039be
[08/10/2023-11:17:14] [V] [TRT] Tactic: 0x42a5b8709d0039be Time: 0.374427
[08/10/2023-11:17:14] [V] [TRT] Conv_44 + Relu_45 Set Tactic Name: ampere_h16816cudnn_256x128_ldg8_relu_exp_small_nhwc_tn_v1 Tactic: 0x463794ee4acffd1f
[08/10/2023-11:17:14] [V] [TRT] Tactic: 0x463794ee4acffd1f Time: 0.298098
[08/10/2023-11:17:14] [V] [TRT] Conv_44 + Relu_45 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x64x64_stage4_warpsize2x2x1_g1_tensor16x8x16 Tactic: 0x4a81ea1e51436a30
[08/10/2023-11:17:14] [V] [TRT] Tactic: 0x4a81ea1e51436a30 Time: 0.376375
[08/10/2023-11:17:14] [V] [TRT] Conv_44 + Relu_45 Set Tactic Name: ampere_h1688cudnn_256x64_sliced1x2_ldg8_relu_exp_small_nhwc_tn_v1 Tactic: 0x4aed1956bd10a795
[08/10/2023-11:17:14] [V] [TRT] Tactic: 0x4aed1956bd10a795 Time: 0.399195
[08/10/2023-11:17:14] [V] [TRT] Conv_44 + Relu_45 Set Tactic Name: ampere_h16816cudnn_64x128_sliced1x2_ldg8_relu_exp_stages_64x3_large_nhwc_tn_v1 Tactic: 0x4fc05923455fc266
[08/10/2023-11:17:14] [V] [TRT] Tactic: 0x4fc05923455fc266 Time: 0.304087
[08/10/2023-11:17:14] [V] [TRT] Conv_44 + Relu_45 Set Tactic Name: ampere_h16816cudnn_256x64_ldg8_relu_exp_large_nhwc_tn_v1 Tactic: 0x5013c38f55afa9ba
[08/10/2023-11:17:14] [V] [TRT] Tactic: 0x5013c38f55afa9ba Time: 0.315264
[08/10/2023-11:17:14] [V] [TRT] Conv_44 + Relu_45 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x64x32_stage3_warpsize4x1x1_g1_tensor16x8x16_t1r3s3 Tactic: 0x529f4431bdae94f5
[08/10/2023-11:17:14] [V] [TRT] Tactic: 0x529f4431bdae94f5 Time: 0.263566
[08/10/2023-11:17:14] [V] [TRT] Conv_44 + Relu_45 Set Tactic Name: ampere_h16816cudnn_64x128_ldg8_relu_exp_stages_64x3_small_nhwc_tn_v1 Tactic: 0x53be5e206184e6ad
[08/10/2023-11:17:14] [V] [TRT] Tactic: 0x53be5e206184e6ad Time: 0.325714
[08/10/2023-11:17:14] [V] [TRT] Conv_44 + Relu_45 Set Tactic Name: ampere_h16816cudnn_64x64_ldg8_relu_exp_stages_64x5_large_nhwc_tn_v1 Tactic: 0x544bff7ff9c5d908
[08/10/2023-11:17:14] [V] [TRT] Tactic: 0x544bff7ff9c5d908 Time: 0.449445
[08/10/2023-11:17:14] [V] [TRT] Conv_44 + Relu_45 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x64x64_stage3_warpsize2x2x1_g1_tensor16x8x16 Tactic: 0x5568fd8a32f4a40f
[08/10/2023-11:17:14] [V] [TRT] Tactic: 0x5568fd8a32f4a40f Time: 0.309472
[08/10/2023-11:17:14] [V] [TRT] Conv_44 + Relu_45 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x128x64_stage3_warpsize2x2x1_g1_tensor16x8x16 Tactic: 0x57c9a5ff682354a6
[08/10/2023-11:17:14] [V] [TRT] Tactic: 0x57c9a5ff682354a6 Time: 0.287712
[08/10/2023-11:17:14] [V] [TRT] Conv_44 + Relu_45 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x32x64_stage5_warpsize2x2x1_g1_tensor16x8x16 Tactic: 0x5820b3dda403c4d0
[08/10/2023-11:17:14] [V] [TRT] Tactic: 0x5820b3dda403c4d0 Time: 0.558747
[08/10/2023-11:17:14] [V] [TRT] Conv_44 + Relu_45 Set Tactic Name: ampere_h16816cudnn_128x128_ldg8_relu_exp_stages_64x3_small_nhwc_tn_v1 Tactic: 0x58816bf2e9c36afb
[08/10/2023-11:17:14] [V] [TRT] Tactic: 0x58816bf2e9c36afb Time: 0.298578
[08/10/2023-11:17:14] [V] [TRT] Conv_44 + Relu_45 Set Tactic Name: ampere_h16816cudnn_128x64_ldg8_relu_exp_stages_64x3_large_nhwc_tn_v1 Tactic: 0x59762bd684092b33
[08/10/2023-11:17:14] [V] [TRT] Tactic: 0x59762bd684092b33 Time: 0.323904
[08/10/2023-11:17:14] [V] [TRT] Conv_44 + Relu_45 Set Tactic Name: ampere_h16816cudnn_128x64_sliced1x2_ldg8_relu_exp_stages_64x4_medium_nhwc_tn_v1 Tactic: 0x59c5c647b4c76593
[08/10/2023-11:17:14] [V] [TRT] Tactic: 0x59c5c647b4c76593 Time: 0.366871
[08/10/2023-11:17:14] [V] [TRT] Conv_44 + Relu_45 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x128x64_stage3_warpsize4x2x1_g1_tensor16x8x16 Tactic: 0x5a55274960724ba8
[08/10/2023-11:17:14] [V] [TRT] Tactic: 0x5a55274960724ba8 Time: 0.262473
[08/10/2023-11:17:14] [V] [TRT] Conv_44 + Relu_45 Set Tactic Name: ampere_h1688cudnn_256x128_ldg8_relu_exp_large_nhwc_tn_v1 Tactic: 0x5c2e1c87d85b06f1
[08/10/2023-11:17:14] [V] [TRT] Tactic: 0x5c2e1c87d85b06f1 Time: 0.336411
[08/10/2023-11:17:14] [V] [TRT] Conv_44 + Relu_45 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x256x64_stage3_warpsize1x4x1_g1_tensor16x8x16 Tactic: 0x5d067c18f40c23e3
[08/10/2023-11:17:14] [V] [TRT] Tactic: 0x5d067c18f40c23e3 Time: 0.296
[08/10/2023-11:17:14] [V] [TRT] Conv_44 + Relu_45 Set Tactic Name: ampere_h16816cudnn_64x64_ldg8_relu_exp_stages_64x5_medium_nhwc_tn_v1 Tactic: 0x5f31c22ec167f384
[08/10/2023-11:17:14] [V] [TRT] Tactic: 0x5f31c22ec167f384 Time: 0.440457
[08/10/2023-11:17:14] [V] [TRT] Conv_44 + Relu_45 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x128x32_stage4_warpsize2x2x1_g1_tensor16x8x16_t1r3s3 Tactic: 0x60c3421152ef8e10
[08/10/2023-11:17:14] [V] [TRT] Tactic: 0x60c3421152ef8e10 Time: 0.244366
[08/10/2023-11:17:14] [V] [TRT] Conv_44 + Relu_45 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x128x32_stage3_warpsize4x2x1_g1_tensor16x8x16 Tactic: 0x60da8c7151d91e47
[08/10/2023-11:17:14] [V] [TRT] Tactic: 0x60da8c7151d91e47 Time: 0.27275
[08/10/2023-11:17:14] [V] [TRT] Conv_44 + Relu_45 Set Tactic Name: ampere_h1688cudnn_256x64_ldg8_relu_exp_medium_nhwc_tn_v1 Tactic: 0x6426696f872a3b13
[08/10/2023-11:17:14] [V] [TRT] Tactic: 0x6426696f872a3b13 Time: 0.410299
[08/10/2023-11:17:14] [V] [TRT] Conv_44 + Relu_45 Set Tactic Name: ampere_h16816cudnn_128x64_ldg8_relu_exp_stages_64x3_medium_nhwc_tn_v1 Tactic: 0x699be152cfb6d6ff
[08/10/2023-11:17:14] [V] [TRT] Tactic: 0x699be152cfb6d6ff Time: 0.318144
[08/10/2023-11:17:14] [V] [TRT] Conv_44 + Relu_45 Set Tactic Name: ampere_h16816cudnn_64x64_ldg8_relu_exp_stages_64x6_large_nhwc_tn_v1 Tactic: 0x6af049035146c349
[08/10/2023-11:17:14] [V] [TRT] Tactic: 0x6af049035146c349 Time: 0.532306
[08/10/2023-11:17:14] [V] [TRT] Conv_44 + Relu_45 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x256x32_stage3_warpsize2x4x1_g1_tensor16x8x16 Tactic: 0x7005d10718f6c22d
[08/10/2023-11:17:14] [V] [TRT] Tactic: 0x7005d10718f6c22d Time: 0.273335
[08/10/2023-11:17:14] [V] [TRT] Conv_44 + Relu_45 Set Tactic Name: ampere_h16816cudnn_64x64_sliced1x2_ldg8_relu_exp_stages_64x5_medium_nhwc_tn_v1 Tactic: 0x706f08da35c795a5
[08/10/2023-11:17:14] [V] [TRT] Tactic: 0x706f08da35c795a5 Time: 0.414386
[08/10/2023-11:17:14] [V] [TRT] Conv_44 + Relu_45 Set Tactic Name: ampere_h16816cudnn_128x64_sliced1x2_ldg8_relu_exp_stages_64x4_small_nhwc_tn_v1 Tactic: 0x7163d33a4d8ce8d7
[08/10/2023-11:17:14] [V] [TRT] Tactic: 0x7163d33a4d8ce8d7 Time: 0.349573
[08/10/2023-11:17:14] [V] [TRT] Conv_44 + Relu_45 Set Tactic Name: ampere_h1688cudnn_256x64_ldg8_relu_exp_small_nhwc_tn_v1 Tactic: 0x7aad3976677d7155
[08/10/2023-11:17:14] [V] [TRT] Tactic: 0x7aad3976677d7155 Time: 0.383589
[08/10/2023-11:17:14] [V] [TRT] Conv_44 + Relu_45 Set Tactic Name: ampere_h16816cudnn_64x64_sliced1x2_ldg8_relu_exp_stages_64x5_large_nhwc_tn_v1 Tactic: 0x8015519605ab9963
[08/10/2023-11:17:14] [V] [TRT] Tactic: 0x8015519605ab9963 Time: 0.410647
[08/10/2023-11:17:14] [V] [TRT] Conv_44 + Relu_45 Set Tactic Name: ampere_h16816cudnn_256x64_sliced1x2_ldg8_relu_exp_medium_nhwc_tn_v1 Tactic: 0x82f8a2214b0b4178
[08/10/2023-11:17:14] [V] [TRT] Tactic: 0x82f8a2214b0b4178 Time: 0.410519
[08/10/2023-11:17:14] [V] [TRT] Conv_44 + Relu_45 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x32x32_stage4_warpsize4x1x1_g1_tensor16x8x16 Tactic: 0x834e11ecd4ab9454
[08/10/2023-11:17:14] [V] [TRT] Tactic: 0x834e11ecd4ab9454 Time: 0.449705
[08/10/2023-11:17:14] [V] [TRT] Conv_44 + Relu_45 Set Tactic Name: ampere_h16816cudnn_128x64_sliced1x2_ldg8_relu_exp_stages_64x3_large_nhwc_tn_v1 Tactic: 0x83ccd4762c1376a1
[08/10/2023-11:17:14] [V] [TRT] Tactic: 0x83ccd4762c1376a1 Time: 0.305957
[08/10/2023-11:17:14] [V] [TRT] Conv_44 + Relu_45 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x64x64_stage3_warpsize4x1x1_g1_tensor16x8x16_t1r3s3 Tactic: 0x84ee897dd1f4d2aa
[08/10/2023-11:17:14] [V] [TRT] Tactic: 0x84ee897dd1f4d2aa Time: 0.311141
[08/10/2023-11:17:14] [V] [TRT] Conv_44 + Relu_45 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x128x64_stage3_warpsize2x2x1_g1_tensor16x8x16 Tactic: 0x866e7a5f6401b67f
[08/10/2023-11:17:14] [V] [TRT] Tactic: 0x866e7a5f6401b67f Time: 0.277417
[08/10/2023-11:17:14] [V] [TRT] Conv_44 + Relu_45 Set Tactic Name: ampere_h16816cudnn_64x64_ldg8_relu_exp_stages_64x6_medium_nhwc_tn_v1 Tactic: 0x88971eec55aba850
[08/10/2023-11:17:14] [V] [TRT] Tactic: 0x88971eec55aba850 Time: 0.517481
[08/10/2023-11:17:14] [V] [TRT] Conv_44 + Relu_45 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x128x64_stage3_warpsize4x2x1_g1_tensor16x8x16 Tactic: 0x8bf2f8e96c50a971
[08/10/2023-11:17:14] [V] [TRT] Tactic: 0x8bf2f8e96c50a971 Time: 0.260384
[08/10/2023-11:17:14] [V] [TRT] Conv_44 + Relu_45 Set Tactic Name: ampere_h16816cudnn_256x128_ldg8_relu_exp_stages_64x3_large_nhwc_tn_v1 Tactic: 0x94576ece6beb35e3
[08/10/2023-11:17:14] [V] [TRT] Tactic: 0x94576ece6beb35e3 Time: 0.275095
[08/10/2023-11:17:14] [V] [TRT] Conv_44 + Relu_45 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x64x64_stage3_warpsize4x1x1_g1_tensor16x8x16 Tactic: 0x9fff6e65019cc310
[08/10/2023-11:17:14] [V] [TRT] Tactic: 0x9fff6e65019cc310 Time: 0.310679
[08/10/2023-11:17:14] [V] [TRT] Conv_44 + Relu_45 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x32x64_stage5_warpsize2x2x1_g1_tensor16x8x16_t1r3s3 Tactic: 0xa033e20ae9f412b2
[08/10/2023-11:17:14] [V] [TRT] Tactic: 0xa033e20ae9f412b2 Time: 0.535333
[08/10/2023-11:17:14] [V] [TRT] Conv_44 + Relu_45 Set Tactic Name: ampere_h16816cudnn_64x128_ldg8_relu_exp_stages_64x4_large_nhwc_tn_v1 Tactic: 0xa111596c001b78db
[08/10/2023-11:17:14] [V] [TRT] Tactic: 0xa111596c001b78db Time: 0.359163
[08/10/2023-11:17:14] [V] [TRT] Conv_44 + Relu_45 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x256x32_stage3_warpsize2x4x1_g1_tensor16x8x16 Tactic: 0xa1a20ea714d420f4
[08/10/2023-11:17:14] [V] [TRT] Tactic: 0xa1a20ea714d420f4 Time: 0.269545
[08/10/2023-11:17:14] [V] [TRT] Conv_44 + Relu_45 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x32x32_stage4_warpsize4x1x1_g1_tensor16x8x16_t1r3s3 Tactic: 0xa40cb43c296a36a8
[08/10/2023-11:17:14] [V] [TRT] Tactic: 0xa40cb43c296a36a8 Time: 0.408613
[08/10/2023-11:17:14] [V] [TRT] Conv_44 + Relu_45 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x64x32_stage3_warpsize4x1x1_g1_tensor16x8x16 Tactic: 0xa570c55d303796ff
[08/10/2023-11:17:14] [V] [TRT] Tactic: 0xa570c55d303796ff Time: 0.27776
[08/10/2023-11:17:14] [V] [TRT] Conv_44 + Relu_45 Set Tactic Name: ampere_h16816cudnn_64x64_sliced1x2_ldg8_relu_exp_stages_64x6_medium_nhwc_tn_v1 Tactic: 0xa7c9d418a10bce71
[08/10/2023-11:17:14] [V] [TRT] Tactic: 0xa7c9d418a10bce71 Time: 0.521435
[08/10/2023-11:17:14] [V] [TRT] Conv_44 + Relu_45 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x32x32_stage4_warpsize4x1x1_g1_tensor16x8x16 Tactic: 0xa83b68f30462f971
[08/10/2023-11:17:14] [V] [TRT] Tactic: 0xa83b68f30462f971 Time: 0.416471
[08/10/2023-11:17:14] [V] [TRT] Conv_44 + Relu_45 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x256x32_stage3_warpsize2x4x1_g1_tensor16x8x16_t1r3s3 Tactic: 0xa9177bbe4e767df8
[08/10/2023-11:17:14] [V] [TRT] Tactic: 0xa9177bbe4e767df8 Time: 0.269088
[08/10/2023-11:17:14] [V] [TRT] Conv_44 + Relu_45 Set Tactic Name: ampere_h1688cudnn_128x128_ldg8_relu_exp_large_nhwc_tn_v1 Tactic: 0xa927df92ac1ef1b8
[08/10/2023-11:17:14] [V] [TRT] Tactic: 0xa927df92ac1ef1b8 Time: 0.326683
[08/10/2023-11:17:14] [V] [TRT] Conv_44 + Relu_45 Set Tactic Name: ampere_h16816cudnn_128x64_ldg8_relu_exp_stages_64x4_small_nhwc_tn_v1 Tactic: 0xabd92c9ae596b545
[08/10/2023-11:17:14] [V] [TRT] Tactic: 0xabd92c9ae596b545 Time: 0.355182
[08/10/2023-11:17:14] [V] [TRT] Conv_44 + Relu_45 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x128x32_stage5_warpsize2x2x1_g1_tensor16x8x16 Tactic: 0xafd1e8bf6bd3d638
[08/10/2023-11:17:14] [V] [TRT] Tactic: 0xafd1e8bf6bd3d638 Time: 0.291232
[08/10/2023-11:17:14] [V] [TRT] Conv_44 + Relu_45 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x128x32_stage3_warpsize4x2x1_g1_tensor16x8x16 Tactic: 0xb17d53d15dfbfc9e
[08/10/2023-11:17:14] [V] [TRT] Tactic: 0xb17d53d15dfbfc9e Time: 0.28491
[08/10/2023-11:17:14] [V] [TRT] Conv_44 + Relu_45 Set Tactic Name: ampere_h16816cudnn_128x128_ldg8_relu_exp_small_nhwc_tn_v1 Tactic: 0xb33e57fb3e8a0a56
[08/10/2023-11:17:14] [V] [TRT] Tactic: 0xb33e57fb3e8a0a56 Time: 0.269275
[08/10/2023-11:17:14] [V] [TRT] Conv_44 + Relu_45 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x64x32_stage5_warpsize2x2x1_g1_tensor16x8x16_t1r3s3 Tactic: 0xb4bec086187edcfc
[08/10/2023-11:17:14] [V] [TRT] Tactic: 0xb4bec086187edcfc Time: 0.302418
[08/10/2023-11:17:14] [V] [TRT] Conv_44 + Relu_45 Set Tactic Name: ampere_h16816cudnn_256x64_ldg8_relu_exp_stages_64x3_small_nhwc_tn_v1 Tactic: 0xb6fa5ffcc1a9d518
[08/10/2023-11:17:14] [V] [TRT] Tactic: 0xb6fa5ffcc1a9d518 Time: 0.316251
[08/10/2023-11:17:14] [V] [TRT] Conv_44 + Relu_45 Set Tactic Name: ampere_h16816cudnn_64x128_sliced1x2_ldg8_relu_exp_stages_64x3_medium_nhwc_tn_v1 Tactic: 0xb85e52e87caf60a2
[08/10/2023-11:17:14] [V] [TRT] Tactic: 0xb85e52e87caf60a2 Time: 0.312206
[08/10/2023-11:17:14] [V] [TRT] Conv_44 + Relu_45 Set Tactic Name: ampere_h16816cudnn_64x128_ldg8_relu_exp_stages_64x3_medium_nhwc_tn_v1 Tactic: 0xb8d86216e1235cda
[08/10/2023-11:17:14] [V] [TRT] Tactic: 0xb8d86216e1235cda Time: 0.326373
[08/10/2023-11:17:14] [V] [TRT] Conv_44 + Relu_45 Set Tactic Name: ampere_h16816cudnn_256x128_ldg8_relu_exp_stages_64x3_medium_nhwc_tn_v1 Tactic: 0xb9171d70ba2eda4b
[08/10/2023-11:17:14] [V] [TRT] Tactic: 0xb9171d70ba2eda4b Time: 0.279803
[08/10/2023-11:17:14] [V] [TRT] Conv_44 + Relu_45 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x64x32_stage5_warpsize2x2x1_g1_tensor16x8x16 Tactic: 0xbd08239a9317f2fd
[08/10/2023-11:17:14] [V] [TRT] Tactic: 0xbd08239a9317f2fd Time: 0.309207
[08/10/2023-11:17:14] [V] [TRT] Conv_44 + Relu_45 Set Tactic Name: ampere_h16816cudnn_64x128_sliced1x2_ldg8_relu_exp_stages_64x4_small_nhwc_tn_v1 Tactic: 0xbd6f5e6f24c05c10
[08/10/2023-11:17:14] [V] [TRT] Tactic: 0xbd6f5e6f24c05c10 Time: 0.380334
[08/10/2023-11:17:14] [V] [TRT] Conv_44 + Relu_45 Set Tactic Name: ampere_h16816cudnn_64x64_sliced1x2_ldg8_relu_exp_stages_64x6_large_nhwc_tn_v1 Tactic: 0xbeaee7eaad288322
[08/10/2023-11:17:14] [V] [TRT] Tactic: 0xbeaee7eaad288322 Time: 0.530149
[08/10/2023-11:17:14] [V] [TRT] Conv_44 + Relu_45 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x128x64_stage3_warpsize2x2x1_g1_tensor16x8x16_t1r3s3 Tactic: 0xc0a02dc6095497cc
[08/10/2023-11:17:14] [V] [TRT] Tactic: 0xc0a02dc6095497cc Time: 0.279045
[08/10/2023-11:17:14] [V] [TRT] Conv_44 + Relu_45 Set Tactic Name: ampere_h16816cudnn_128x128_ldg8_relu_exp_stages_64x3_large_nhwc_tn_v1 Tactic: 0xc2cf926c41243630
[08/10/2023-11:17:14] [V] [TRT] Tactic: 0xc2cf926c41243630 Time: 0.293993
[08/10/2023-11:17:14] [V] [TRT] Conv_44 + Relu_45 Set Tactic Name: ampere_h16816cudnn_128x64_ldg8_relu_exp_stages_64x3_small_nhwc_tn_v1 Tactic: 0xc338d2482cee77f8
[08/10/2023-11:17:14] [V] [TRT] Tactic: 0xc338d2482cee77f8 Time: 0.314848
[08/10/2023-11:17:14] [V] [TRT] Conv_44 + Relu_45 Set Tactic Name: ampere_h1688cudnn_256x128_ldg8_relu_exp_small_nhwc_tn_v1 Tactic: 0xc660e51970bc5a3a
[08/10/2023-11:17:14] [V] [TRT] Tactic: 0xc660e51970bc5a3a Time: 0.3104
[08/10/2023-11:17:14] [V] [TRT] Conv_44 + Relu_45 Set Tactic Name: ampere_h16816cudnn_64x128_ldg8_relu_exp_stages_64x4_medium_nhwc_tn_v1 Tactic: 0xc82f3f06140e3cbb
[08/10/2023-11:17:14] [V] [TRT] Tactic: 0xc82f3f06140e3cbb Time: 0.357326
[08/10/2023-11:17:14] [V] [TRT] Conv_44 + Relu_45 Set Tactic Name: ampere_h16816cudnn_64x128_sliced1x2_ldg8_relu_exp_stages_64x4_medium_nhwc_tn_v1 Tactic: 0xc8a90ff8898200c3
[08/10/2023-11:17:14] [V] [TRT] Tactic: 0xc8a90ff8898200c3 Time: 0.374523
[08/10/2023-11:17:14] [V] [TRT] Conv_44 + Relu_45 Set Tactic Name: ampere_h1688cudnn_256x64_sliced1x2_ldg8_relu_exp_medium_nhwc_tn_v1 Tactic: 0xc9cc55109bb4de26
[08/10/2023-11:17:14] [V] [TRT] Tactic: 0xc9cc55109bb4de26 Time: 0.428983
[08/10/2023-11:17:14] [V] [TRT] Conv_44 + Relu_45 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x64x32_stage5_warpsize2x2x1_g1_tensor16x8x16 Tactic: 0xc9d24bd069159fa8
[08/10/2023-11:17:14] [V] [TRT] Tactic: 0xc9d24bd069159fa8 Time: 0.308837
[08/10/2023-11:17:14] [V] [TRT] Conv_44 + Relu_45 Set Tactic Name: ampere_h16816cudnn_64x128_ldg8_relu_exp_stages_64x3_large_nhwc_tn_v1 Tactic: 0xc9f0a7bec963ba66
[08/10/2023-11:17:14] [V] [TRT] Tactic: 0xc9f0a7bec963ba66 Time: 0.316288
[08/10/2023-11:17:14] [V] [TRT] Conv_44 + Relu_45 Set Tactic Name: ampere_h16816cudnn_256x64_ldg8_relu_exp_small_nhwc_tn_v1 Tactic: 0xca5d3a11fd48f571
[08/10/2023-11:17:14] [V] [TRT] Tactic: 0xca5d3a11fd48f571 Time: 0.303854
[08/10/2023-11:17:14] [V] [TRT] Conv_44 + Relu_45 Set Tactic Name: ampere_h16816cudnn_64x64_ldg8_relu_exp_stages_64x5_small_nhwc_tn_v1 Tactic: 0xce0506e1512285c3
[08/10/2023-11:17:14] [V] [TRT] Tactic: 0xce0506e1512285c3 Time: 0.441317
[08/10/2023-11:17:14] [V] [TRT] Conv_44 + Relu_45 Set Tactic Name: ampere_h1688cudnn_256x64_sliced1x2_ldg8_relu_exp_large_nhwc_tn_v1 Tactic: 0xd0a3e0c815f7fb5e
[08/10/2023-11:17:14] [V] [TRT] Tactic: 0xd0a3e0c815f7fb5e Time: 0.431314
[08/10/2023-11:17:14] [V] [TRT] Conv_44 + Relu_45 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x64x32_stage3_warpsize4x1x1_g1_tensor16x8x16 Tactic: 0xd1aaad17ca35fbaa
[08/10/2023-11:17:14] [V] [TRT] Tactic: 0xd1aaad17ca35fbaa Time: 0.264608
[08/10/2023-11:17:14] [V] [TRT] Conv_44 + Relu_45 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x128x64_stage3_warpsize4x2x1_g1_tensor16x8x16_t1r3s3 Tactic: 0xd2fca0486ca97266
[08/10/2023-11:17:14] [V] [TRT] Tactic: 0xd2fca0486ca97266 Time: 0.262674
[08/10/2023-11:17:14] [V] [TRT] Conv_44 + Relu_45 Set Tactic Name: ampere_h16816cudnn_64x128_sliced1x2_ldg8_relu_exp_stages_64x3_small_nhwc_tn_v1 Tactic: 0xd58ea0bdedb89ead
[08/10/2023-11:17:14] [V] [TRT] Tactic: 0xd58ea0bdedb89ead Time: 0.301198
[08/10/2023-11:17:14] [V] [TRT] Conv_44 + Relu_45 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x256x64_stage3_warpsize1x4x1_g1_tensor16x8x16_t1r3s3 Tactic: 0xd7c261fa01db2af9
[08/10/2023-11:17:14] [V] [TRT] Tactic: 0xd7c261fa01db2af9 Time: 0.297362
[08/10/2023-11:17:14] [V] [TRT] Conv_44 + Relu_45 Set Tactic Name: ampere_h16816cudnn_256x64_sliced1x2_ldg8_relu_exp_large_nhwc_tn_v1 Tactic: 0xd8eb41ee35e76575
[08/10/2023-11:17:14] [V] [TRT] Tactic: 0xd8eb41ee35e76575 Time: 0.415913
[08/10/2023-11:17:14] [V] [TRT] Conv_44 + Relu_45 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x128x32_stage5_warpsize2x2x1_g1_tensor16x8x16 Tactic: 0xdb0b80f591d1bb6d
[08/10/2023-11:17:14] [V] [TRT] Tactic: 0xdb0b80f591d1bb6d Time: 0.292073
[08/10/2023-11:17:14] [V] [TRT] Conv_44 + Relu_45 Set Tactic Name: ampere_h16816cudnn_256x128_ldg8_relu_exp_large_nhwc_tn_v1 Tactic: 0xdc796d70e228a1d4
[08/10/2023-11:17:14] [V] [TRT] Tactic: 0xdc796d70e228a1d4 Time: 0.293445
[08/10/2023-11:17:14] [V] [TRT] Conv_44 + Relu_45 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x32x32_stage4_warpsize4x1x1_g1_tensor16x8x16 Tactic: 0xdce100b9fe609424
[08/10/2023-11:17:14] [V] [TRT] Tactic: 0xdce100b9fe609424 Time: 0.418606
[08/10/2023-11:17:14] [V] [TRT] Conv_44 + Relu_45 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x128x32_stage4_warpsize2x2x1_g1_tensor16x8x16 Tactic: 0xdfa020ef435ef810
[08/10/2023-11:17:14] [V] [TRT] Tactic: 0xdfa020ef435ef810 Time: 0.24293
[08/10/2023-11:17:14] [V] [TRT] Conv_44 + Relu_45 Set Tactic Name: ampere_h1688cudnn_256x64_ldg8_relu_exp_large_nhwc_tn_v1 Tactic: 0xe0e3c0e8cf9a2d9e
[08/10/2023-11:17:14] [V] [TRT] Tactic: 0xe0e3c0e8cf9a2d9e Time: 0.406939
[08/10/2023-11:17:14] [V] [TRT] Conv_44 + Relu_45 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x32x64_stage5_warpsize2x2x1_g1_tensor16x8x16 Tactic: 0xe1ff5ad20f5c6bf6
[08/10/2023-11:17:14] [V] [TRT] Tactic: 0xe1ff5ad20f5c6bf6 Time: 0.556649
[08/10/2023-11:17:14] [V] [TRT] Conv_44 + Relu_45 Set Tactic Name: ampere_h16816cudnn_256x64_ldg8_relu_exp_medium_nhwc_tn_v1 Tactic: 0xe4711898bd599c36
[08/10/2023-11:17:14] [V] [TRT] Tactic: 0xe4711898bd599c36 Time: 0.315365
[08/10/2023-11:17:14] [V] [TRT] Conv_44 + Relu_45 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x64x64_stage3_warpsize4x1x1_g1_tensor16x8x16 Tactic: 0xeb25062ffb9eae45
[08/10/2023-11:17:14] [V] [TRT] Tactic: 0xeb25062ffb9eae45 Time: 0.316078
[08/10/2023-11:17:14] [V] [TRT] Conv_44 + Relu_45 Set Tactic Name: ampere_h16816cudnn_128x64_sliced1x2_ldg8_relu_exp_stages_64x4_large_nhwc_tn_v1 Tactic: 0xeb2d2aa4e56bb41c
[08/10/2023-11:17:14] [V] [TRT] Tactic: 0xeb2d2aa4e56bb41c Time: 0.380421
[08/10/2023-11:17:14] [V] [TRT] Conv_44 + Relu_45 Set Tactic Name: ampere_h16816cudnn_64x64_ldg8_relu_exp_stages_64x6_small_nhwc_tn_v1 Tactic: 0xf0beb09df9a19f82
[08/10/2023-11:17:14] [V] [TRT] Tactic: 0xf0beb09df9a19f82 Time: 0.497138
[08/10/2023-11:17:14] [V] [TRT] Conv_44 + Relu_45 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x64x64_stage4_warpsize2x2x1_g1_tensor16x8x16 Tactic: 0xf35e0311fa1cc516
[08/10/2023-11:17:14] [V] [TRT] Tactic: 0xf35e0311fa1cc516 Time: 0.38885
[08/10/2023-11:17:14] [V] [TRT] Conv_44 + Relu_45 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x32x32_stage4_warpsize4x1x1_g1_tensor16x8x16 Tactic: 0xf79479a62ea9f901
[08/10/2023-11:17:14] [V] [TRT] Tactic: 0xf79479a62ea9f901 Time: 0.465678
[08/10/2023-11:17:14] [V] [TRT] Fastest Tactic: 0xdfa020ef435ef810 Time: 0.24293
[08/10/2023-11:17:14] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0xdfa020ef435ef810
[08/10/2023-11:17:14] [V] [TRT] =============== Computing costs for 
[08/10/2023-11:17:14] [V] [TRT] *************** Autotuning format combination: Float(2073600,8100,90,1) -> Float(2073600,8100,90,1) ***************
[08/10/2023-11:17:14] [V] [TRT] --------------- Timing Runner: Conv_46 + Relu_47 (CudaDepthwiseConvolution)
[08/10/2023-11:17:14] [V] [TRT] CudaDepthwiseConvolution has no valid tactics for this config, skipping
[08/10/2023-11:17:14] [V] [TRT] --------------- Timing Runner: Conv_46 + Relu_47 (FusedConvActConvolution)
[08/10/2023-11:17:14] [V] [TRT] FusedConvActConvolution has no valid tactics for this config, skipping
[08/10/2023-11:17:14] [V] [TRT] --------------- Timing Runner: Conv_46 + Relu_47 (CudnnConvolution)
[08/10/2023-11:17:14] [V] [TRT] Tactic: 0x0000000000000000 Time: 7.07466
[08/10/2023-11:17:14] [V] [TRT] Tactic: 0x0000000000000001 Time: 1.37169
[08/10/2023-11:17:14] [V] [TRT] Tactic: 0x0000000000000002 Time: 8.46353
[08/10/2023-11:17:14] [V] [TRT] Tactic: 0x0000000000000004 skipped. Scratch requested: 8760590336, available: 4294967296
[08/10/2023-11:17:14] [V] [TRT] Tactic: 0x0000000000000005 Time: 18.6561
[08/10/2023-11:17:14] [V] [TRT] Tactic: 0x0000000000000006 Time: 3.35015
[08/10/2023-11:17:15] [V] [TRT] Tactic: 0x0000000000000038 Time: 7.20248
[08/10/2023-11:17:15] [V] [TRT] Tactic: 0x0000000000000039 Time: 1.37564
[08/10/2023-11:17:15] [V] [TRT] Tactic: 0x000000000000003a Time: 8.47155
[08/10/2023-11:17:15] [V] [TRT] Tactic: 0x000000000000003c skipped. Scratch requested: 8760590336, available: 4294967296
[08/10/2023-11:17:15] [V] [TRT] Tactic: 0x000000000000003d Time: 18.5239
[08/10/2023-11:17:15] [V] [TRT] Tactic: 0x000000000000003e Time: 3.34967
[08/10/2023-11:17:15] [V] [TRT] Tactic: 0x0000000000000070 Time: 7.10643
[08/10/2023-11:17:15] [V] [TRT] Tactic: 0x0000000000000071 Time: 5.58347
[08/10/2023-11:17:15] [V] [TRT] Tactic: 0x0000000000000072 Time: 8.47428
[08/10/2023-11:17:15] [V] [TRT] Tactic: 0x0000000000000074 skipped. Scratch requested: 8760590336, available: 4294967296
[08/10/2023-11:17:15] [V] [TRT] Tactic: 0x0000000000000075 Time: 18.7181
[08/10/2023-11:17:15] [V] [TRT] Tactic: 0x0000000000000076 Time: 3.34691
[08/10/2023-11:17:15] [V] [TRT] Fastest Tactic: 0x0000000000000001 Time: 1.37169
[08/10/2023-11:17:15] [V] [TRT] Setting workspace to 8760590336enables more tactics for profiling
[08/10/2023-11:17:15] [V] [TRT] --------------- Timing Runner: Conv_46 + Relu_47 (CaskConvolution)
[08/10/2023-11:17:15] [V] [TRT] Conv_46 + Relu_47 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x128x8_stage3_warpsize1x4x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0x01cf8ce2da913006
[08/10/2023-11:17:15] [V] [TRT] Tactic: 0x01cf8ce2da913006 Time: 5.1944
[08/10/2023-11:17:15] [V] [TRT] Conv_46 + Relu_47 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x64x8_stage3_warpsize2x2x1_g1_ffma_aligna4_alignc4 Tactic: 0x12dbf7d94ee3696d
[08/10/2023-11:17:15] [V] [TRT] Tactic: 0x12dbf7d94ee3696d Time: 5.63048
[08/10/2023-11:17:15] [V] [TRT] Conv_46 + Relu_47 Set Tactic Name: ampere_scudnn_128x64_relu_small_nn_v1 Tactic: 0x3f243c490d502deb
[08/10/2023-11:17:15] [V] [TRT] Tactic: 0x3f243c490d502deb Time: 5.42852
[08/10/2023-11:17:15] [V] [TRT] Conv_46 + Relu_47 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0x4727434768e46395
[08/10/2023-11:17:16] [V] [TRT] Tactic: 0x4727434768e46395 Time: 5.93045
[08/10/2023-11:17:16] [V] [TRT] Conv_46 + Relu_47 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize256x128x8_stage3_warpsize4x2x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0x4efce38acc876f5c
[08/10/2023-11:17:16] [V] [TRT] Tactic: 0x4efce38acc876f5c Time: 5.1914
[08/10/2023-11:17:16] [V] [TRT] Conv_46 + Relu_47 Set Tactic Name: ampere_scudnn_128x128_relu_small_nn_v1 Tactic: 0x503619c69ae500ff
[08/10/2023-11:17:16] [V] [TRT] Tactic: 0x503619c69ae500ff Time: 4.98327
[08/10/2023-11:17:16] [V] [TRT] Conv_46 + Relu_47 Set Tactic Name: ampere_scudnn_128x128_relu_xregs_large_nn_v1 Tactic: 0x5403ad713f811a18
[08/10/2023-11:17:16] [V] [TRT] Tactic: 0x5403ad713f811a18 Time: 5.34456
[08/10/2023-11:17:16] [V] [TRT] Conv_46 + Relu_47 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x128x8_stage3_warpsize1x4x1_g1_ffma_aligna4_alignc4 Tactic: 0x5aa723e0481da855
[08/10/2023-11:17:16] [V] [TRT] Tactic: 0x5aa723e0481da855 Time: 5.20169
[08/10/2023-11:17:16] [V] [TRT] Conv_46 + Relu_47 Set Tactic Name: ampere_scudnn_128x64_relu_xregs_large_nn_v1 Tactic: 0x5deb29b7a8e275f7
[08/10/2023-11:17:16] [V] [TRT] Tactic: 0x5deb29b7a8e275f7 Time: 5.19261
[08/10/2023-11:17:16] [V] [TRT] Conv_46 + Relu_47 Set Tactic Name: ampere_scudnn_winograd_128x128_ldg1_ldg4_relu_tile148t_nt_v1 Tactic: 0x94119b4c514b211a
[08/10/2023-11:17:16] [V] [TRT] Tactic: 0x94119b4c514b211a Time: 3.02729
[08/10/2023-11:17:16] [V] [TRT] Conv_46 + Relu_47 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_aligna4_alignc4 Tactic: 0xa31d27de74b895ff
[08/10/2023-11:17:16] [V] [TRT] Tactic: 0xa31d27de74b895ff Time: 5.96016
[08/10/2023-11:17:16] [V] [TRT] Conv_46 + Relu_47 Set Tactic Name: ampere_scudnn_128x32_relu_small_nn_v1 Tactic: 0xa8609adc4e0ceb90
[08/10/2023-11:17:16] [V] [TRT] Tactic: 0xa8609adc4e0ceb90 Time: 6.00839
[08/10/2023-11:17:16] [V] [TRT] Conv_46 + Relu_47 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize256x128x8_stage3_warpsize4x2x1_g1_ffma_aligna4_alignc4 Tactic: 0xbb8c3889c7eacd30
[08/10/2023-11:17:16] [V] [TRT] Tactic: 0xbb8c3889c7eacd30 Time: 5.23716
[08/10/2023-11:17:16] [V] [TRT] Conv_46 + Relu_47 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_aligna4_alignc4 Tactic: 0xd828f024626fa982
[08/10/2023-11:17:16] [V] [TRT] Tactic: 0xd828f024626fa982 Time: 8.30721
[08/10/2023-11:17:16] [V] [TRT] Conv_46 + Relu_47 Set Tactic Name: ampere_scudnn_128x128_relu_medium_nn_v1 Tactic: 0xf067e6205da31c2e
[08/10/2023-11:17:16] [V] [TRT] Tactic: 0xf067e6205da31c2e Time: 5.11749
[08/10/2023-11:17:16] [V] [TRT] Conv_46 + Relu_47 Set Tactic Name: ampere_scudnn_128x64_relu_medium_nn_v1 Tactic: 0xf64396b97c889179
[08/10/2023-11:17:16] [V] [TRT] Tactic: 0xf64396b97c889179 Time: 5.58853
[08/10/2023-11:17:16] [V] [TRT] Fastest Tactic: 0x94119b4c514b211a Time: 3.02729
[08/10/2023-11:17:16] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CudnnConvolution Tactic: 0x0000000000000001
[08/10/2023-11:17:16] [V] [TRT] *************** Autotuning format combination: Float(2073600,1,23040,256) -> Float(2073600,1,23040,256) ***************
[08/10/2023-11:17:16] [V] [TRT] --------------- Timing Runner: Conv_46 + Relu_47 (CaskConvolution)
[08/10/2023-11:17:16] [V] [TRT] Conv_46 + Relu_47 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize64x128x32_stage5_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0x112d7e4d280f396d
[08/10/2023-11:17:16] [V] [TRT] Tactic: 0x112d7e4d280f396d Time: 1.38869
[08/10/2023-11:17:16] [V] [TRT] Conv_46 + Relu_47 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x64x32_stage5_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0x13cc2ad8dcd32ba2
[08/10/2023-11:17:16] [V] [TRT] Tactic: 0x13cc2ad8dcd32ba2 Time: 1.4604
[08/10/2023-11:17:16] [V] [TRT] Conv_46 + Relu_47 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_aligna4_alignc4 Tactic: 0x19b688348f983aa0
[08/10/2023-11:17:16] [V] [TRT] Tactic: 0x19b688348f983aa0 Time: 5.5252
[08/10/2023-11:17:16] [V] [TRT] Conv_46 + Relu_47 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x64x8_stage3_warpsize2x2x1_g1_ffma_aligna4_alignc4 Tactic: 0x1da91d865428f237
[08/10/2023-11:17:16] [V] [TRT] Tactic: 0x1da91d865428f237 Time: 4.35271
[08/10/2023-11:17:16] [V] [TRT] Conv_46 + Relu_47 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize256x128x32_stage3_warpsize4x2x1_g1_tensor16x8x8_t1r3s3 Tactic: 0x21246c8544eff903
[08/10/2023-11:17:16] [V] [TRT] Tactic: 0x21246c8544eff903 Time: 0.921742
[08/10/2023-11:17:16] [V] [TRT] Conv_46 + Relu_47 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x128x32_stage4_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0x25b2b9d5c9d5ca0d
[08/10/2023-11:17:16] [V] [TRT] Tactic: 0x25b2b9d5c9d5ca0d Time: 0.998537
[08/10/2023-11:17:16] [V] [TRT] Conv_46 + Relu_47 Set Tactic Name: ampere_scudnn_128x64_sliced1x2_ldg4_relu_exp_small_nhwc_tn_v1 Tactic: 0x27b316f52c109002
[08/10/2023-11:17:16] [V] [TRT] Tactic: 0x27b316f52c109002 Time: 5.31859
[08/10/2023-11:17:16] [V] [TRT] Conv_46 + Relu_47 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage4_warpsize2x2x1_g1_tensor16x8x8_t1r3s3 Tactic: 0x2d80d2b384ab13f1
[08/10/2023-11:17:16] [V] [TRT] Tactic: 0x2d80d2b384ab13f1 Time: 1.47603
[08/10/2023-11:17:16] [V] [TRT] Conv_46 + Relu_47 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize256x128x32_stage3_warpsize4x2x1_g1_tensor16x8x8 Tactic: 0x36303f399d3ee4fd
[08/10/2023-11:17:16] [V] [TRT] Tactic: 0x36303f399d3ee4fd Time: 1.13789
[08/10/2023-11:17:16] [V] [TRT] Conv_46 + Relu_47 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage4_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0x385bfab9332b1413
[08/10/2023-11:17:16] [V] [TRT] Tactic: 0x385bfab9332b1413 Time: 1.65475
[08/10/2023-11:17:16] [V] [TRT] Conv_46 + Relu_47 Set Tactic Name: ampere_scudnn_128x32_sliced1x4_ldg4_relu_exp_medium_nhwc_tn_v1 Tactic: 0x3e191488237fab8f
[08/10/2023-11:17:16] [V] [TRT] Tactic: 0x3e191488237fab8f Time: 5.36968
[08/10/2023-11:17:16] [V] [TRT] Conv_46 + Relu_47 Set Tactic Name: ampere_scudnn_128x32_sliced1x4_ldg4_relu_exp_small_nhwc_tn_v1 Tactic: 0x3e2b881168d9689d
[08/10/2023-11:17:17] [V] [TRT] Tactic: 0x3e2b881168d9689d Time: 5.3517
[08/10/2023-11:17:17] [V] [TRT] Conv_46 + Relu_47 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x256x8_stage3_warpsize1x4x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0x3f0c846d6379bc98
[08/10/2023-11:17:17] [V] [TRT] Tactic: 0x3f0c846d6379bc98 Time: 4.54686
[08/10/2023-11:17:17] [V] [TRT] Conv_46 + Relu_47 Set Tactic Name: ampere_scudnn_128x64_sliced1x2_ldg4_relu_exp_medium_nhwc_tn_v1 Tactic: 0x412c44dfeaf9161d
[08/10/2023-11:17:17] [V] [TRT] Tactic: 0x412c44dfeaf9161d Time: 5.37662
[08/10/2023-11:17:17] [V] [TRT] Conv_46 + Relu_47 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x256x32_stage3_warpsize2x4x1_g1_tensor16x8x8 Tactic: 0x482b48242255b8ce
[08/10/2023-11:17:17] [V] [TRT] Tactic: 0x482b48242255b8ce Time: 0.929829
[08/10/2023-11:17:17] [V] [TRT] Conv_46 + Relu_47 Set Tactic Name: ampere_scudnn_128x128_relu_exp_small_nhwc_tn_v1 Tactic: 0x5030121339a48bf3
[08/10/2023-11:17:17] [V] [TRT] Tactic: 0x5030121339a48bf3 Time: 5.23365
[08/10/2023-11:17:17] [V] [TRT] Conv_46 + Relu_47 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x256x32_stage3_warpsize2x4x1_g1_tensor16x8x8_t1r3s3 Tactic: 0x614e89f7852edbc3
[08/10/2023-11:17:17] [V] [TRT] Tactic: 0x614e89f7852edbc3 Time: 0.9064
[08/10/2023-11:17:17] [V] [TRT] Conv_46 + Relu_47 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_aligna4_alignc4 Tactic: 0x62835fce994f06dd
[08/10/2023-11:17:17] [V] [TRT] Tactic: 0x62835fce994f06dd Time: 4.74376
[08/10/2023-11:17:17] [V] [TRT] Conv_46 + Relu_47 Set Tactic Name: ampere_scudnn_128x128_ldg4_relu_exp_large_nhwc_tn_v1 Tactic: 0x634e99502974e4da
[08/10/2023-11:17:17] [V] [TRT] Tactic: 0x634e99502974e4da Time: 5.01879
[08/10/2023-11:17:17] [V] [TRT] Conv_46 + Relu_47 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0x65e41d81f093b482
[08/10/2023-11:17:17] [V] [TRT] Tactic: 0x65e41d81f093b482 Time: 1.09714
[08/10/2023-11:17:17] [V] [TRT] Conv_46 + Relu_47 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize64x128x32_stage5_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0x65f71607d20d5438
[08/10/2023-11:17:17] [V] [TRT] Tactic: 0x65f71607d20d5438 Time: 1.23023
[08/10/2023-11:17:17] [V] [TRT] Conv_46 + Relu_47 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x64x32_stage5_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0x6716429226d146f7
[08/10/2023-11:17:17] [V] [TRT] Tactic: 0x6716429226d146f7 Time: 1.28828
[08/10/2023-11:17:17] [V] [TRT] Conv_46 + Relu_47 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize64x32x64_stage5_warpsize2x2x1_g1_tensor16x8x8_t1r3s3 Tactic: 0x679a281f7660c436
[08/10/2023-11:17:17] [V] [TRT] Tactic: 0x679a281f7660c436 Time: 2.21778
[08/10/2023-11:17:17] [V] [TRT] Conv_46 + Relu_47 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize64x32x64_stage5_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0x6e2a3c7a7fc5e4e1
[08/10/2023-11:17:17] [V] [TRT] Tactic: 0x6e2a3c7a7fc5e4e1 Time: 2.28369
[08/10/2023-11:17:17] [V] [TRT] Conv_46 + Relu_47 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x64x32_stage5_warpsize2x2x1_g1_tensor16x8x8_t1r3s3 Tactic: 0x7aa21282feb15f1a
[08/10/2023-11:17:17] [V] [TRT] Tactic: 0x7aa21282feb15f1a Time: 1.30555
[08/10/2023-11:17:17] [V] [TRT] Conv_46 + Relu_47 Set Tactic Name: ampere_scudnn_128x128_ldg4_relu_exp_medium_nhwc_tn_v1 Tactic: 0x7bc32c782b800c48
[08/10/2023-11:17:17] [V] [TRT] Tactic: 0x7bc32c782b800c48 Time: 4.88205
[08/10/2023-11:17:17] [V] [TRT] Conv_46 + Relu_47 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize256x64x8_stage3_warpsize2x2x1_g1_ffma_aligna4_alignc4 Tactic: 0x8014228ec08b4d49
[08/10/2023-11:17:17] [V] [TRT] Tactic: 0x8014228ec08b4d49 Time: 4.40661
[08/10/2023-11:17:17] [V] [TRT] Conv_46 + Relu_47 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage4_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0x818413b69874bb35
[08/10/2023-11:17:17] [V] [TRT] Tactic: 0x818413b69874bb35 Time: 1.60331
[08/10/2023-11:17:17] [V] [TRT] Conv_46 + Relu_47 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0x94a7db94ba744c45
[08/10/2023-11:17:17] [V] [TRT] Tactic: 0x94a7db94ba744c45 Time: 4.9654
[08/10/2023-11:17:17] [V] [TRT] Conv_46 + Relu_47 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x256x32_stage3_warpsize2x4x1_g1_tensor16x8x8 Tactic: 0x998c97842e775a17
[08/10/2023-11:17:17] [V] [TRT] Tactic: 0x998c97842e775a17 Time: 0.941787
[08/10/2023-11:17:17] [V] [TRT] Conv_46 + Relu_47 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8_t1r3s3 Tactic: 0x999e005e3b016ea6
[08/10/2023-11:17:17] [V] [TRT] Tactic: 0x999e005e3b016ea6 Time: 1.03319
[08/10/2023-11:17:17] [V] [TRT] Conv_46 + Relu_47 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize256x64x32_stage3_warpsize4x2x1_g1_tensor16x8x8_t1r3s3 Tactic: 0xa9a06d0633580c0c
[08/10/2023-11:17:17] [V] [TRT] Tactic: 0xa9a06d0633580c0c Time: 1.10436
[08/10/2023-11:17:17] [V] [TRT] Conv_46 + Relu_47 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0xb443c221fcb1565b
[08/10/2023-11:17:17] [V] [TRT] Tactic: 0xb443c221fcb1565b Time: 1.05405
[08/10/2023-11:17:17] [V] [TRT] Conv_46 + Relu_47 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage3_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0xb47d7d926de02dee
[08/10/2023-11:17:17] [V] [TRT] Tactic: 0xb47d7d926de02dee Time: 1.33826
[08/10/2023-11:17:17] [V] [TRT] Conv_46 + Relu_47 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize256x64x32_stage3_warpsize4x2x1_g1_tensor16x8x8 Tactic: 0xba0185279ccb2b85
[08/10/2023-11:17:17] [V] [TRT] Tactic: 0xba0185279ccb2b85 Time: 1.28043
[08/10/2023-11:17:17] [V] [TRT] Conv_46 + Relu_47 Set Tactic Name: ampere_scudnn_128x64_sliced1x2_ldg4_relu_exp_large_nhwc_tn_v1 Tactic: 0xbdfdef6b84f7ccc9
[08/10/2023-11:17:17] [V] [TRT] Tactic: 0xbdfdef6b84f7ccc9 Time: 5.27391
[08/10/2023-11:17:17] [V] [TRT] Conv_46 + Relu_47 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage3_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0xc0a715d897e240bb
[08/10/2023-11:17:17] [V] [TRT] Tactic: 0xc0a715d897e240bb Time: 1.3141
[08/10/2023-11:17:17] [V] [TRT] Conv_46 + Relu_47 Set Tactic Name: ampere_scudnn_128x128_relu_exp_large_nhwc_tn_v1 Tactic: 0xca7eeb8d9143d738
[08/10/2023-11:17:17] [V] [TRT] Tactic: 0xca7eeb8d9143d738 Time: 5.17307
[08/10/2023-11:17:17] [V] [TRT] Conv_46 + Relu_47 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize256x64x32_stage3_warpsize4x2x1_g1_tensor16x8x8 Tactic: 0xcedbed6d66c946d0
[08/10/2023-11:17:17] [V] [TRT] Tactic: 0xcedbed6d66c946d0 Time: 1.15533
[08/10/2023-11:17:17] [V] [TRT] Conv_46 + Relu_47 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x128x8_stage3_warpsize1x4x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0xd15dd11d64344e83
[08/10/2023-11:17:17] [V] [TRT] Tactic: 0xd15dd11d64344e83 Time: 4.32584
[08/10/2023-11:17:17] [V] [TRT] Conv_46 + Relu_47 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize64x128x32_stage5_warpsize2x2x1_g1_tensor16x8x8_t1r3s3 Tactic: 0xd4428a7355769d7d
[08/10/2023-11:17:17] [V] [TRT] Tactic: 0xd4428a7355769d7d Time: 1.1851
[08/10/2023-11:17:17] [V] [TRT] Conv_46 + Relu_47 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize64x32x64_stage5_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0xd7f5d575d49a4bc7
[08/10/2023-11:17:17] [V] [TRT] Tactic: 0xd7f5d575d49a4bc7 Time: 2.70271
[08/10/2023-11:17:17] [V] [TRT] Conv_46 + Relu_47 Set Tactic Name: ampere_scudnn_128x128_relu_exp_medium_nhwc_tn_v1 Tactic: 0xd9031472c05adf51
[08/10/2023-11:17:17] [V] [TRT] Tactic: 0xd9031472c05adf51 Time: 5.06003
[08/10/2023-11:17:17] [V] [TRT] Conv_46 + Relu_47 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x128x32_stage4_warpsize2x2x1_g1_tensor16x8x8_t1r3s3 Tactic: 0xd920b33c9bd27143
[08/10/2023-11:17:17] [V] [TRT] Tactic: 0xd920b33c9bd27143 Time: 0.946921
[08/10/2023-11:17:17] [V] [TRT] Conv_46 + Relu_47 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize256x128x32_stage3_warpsize4x2x1_g1_tensor16x8x8 Tactic: 0xe797e099911c0624
[08/10/2023-11:17:17] [V] [TRT] Tactic: 0xe797e099911c0624 Time: 0.954766
[08/10/2023-11:17:17] [V] [TRT] Conv_46 + Relu_47 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage3_warpsize2x2x1_g1_tensor16x8x8_t1r3s3 Tactic: 0xeb65d4c1e8fa2294
[08/10/2023-11:17:17] [V] [TRT] Tactic: 0xeb65d4c1e8fa2294 Time: 1.23872
[08/10/2023-11:17:17] [V] [TRT] Conv_46 + Relu_47 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x128x32_stage4_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0xf4156675c5f728d4
[08/10/2023-11:17:17] [V] [TRT] Tactic: 0xf4156675c5f728d4 Time: 1.54306
[08/10/2023-11:17:17] [V] [TRT] Conv_46 + Relu_47 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x64x8_stage3_warpsize2x2x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0xf48db81f02eca9ee
[08/10/2023-11:17:17] [V] [TRT] Tactic: 0xf48db81f02eca9ee Time: 4.44634
[08/10/2023-11:17:17] [V] [TRT] Conv_46 + Relu_47 Set Tactic Name: ampere_scudnn_128x128_ldg4_relu_exp_small_nhwc_tn_v1 Tactic: 0xf90060ce8193b811
[08/10/2023-11:17:18] [V] [TRT] Tactic: 0xf90060ce8193b811 Time: 4.87681
[08/10/2023-11:17:18] [V] [TRT] Fastest Tactic: 0x614e89f7852edbc3 Time: 0.9064
[08/10/2023-11:17:18] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x614e89f7852edbc3
[08/10/2023-11:17:18] [V] [TRT] *************** Autotuning format combination: Float(518400,1:4,5760,64) -> Float(518400,1:4,5760,64) ***************
[08/10/2023-11:17:18] [V] [TRT] --------------- Timing Runner: Conv_46 + Relu_47 (CaskConvolution)
[08/10/2023-11:17:18] [V] [TRT] Conv_46 + Relu_47 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize64x128x32_stage5_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0x112d7e4d280f396d
[08/10/2023-11:17:18] [V] [TRT] Tactic: 0x112d7e4d280f396d Time: 1.39576
[08/10/2023-11:17:18] [V] [TRT] Conv_46 + Relu_47 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x64x32_stage5_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0x13cc2ad8dcd32ba2
[08/10/2023-11:17:18] [V] [TRT] Tactic: 0x13cc2ad8dcd32ba2 Time: 1.4683
[08/10/2023-11:17:18] [V] [TRT] Conv_46 + Relu_47 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize256x128x32_stage3_warpsize4x2x1_g1_tensor16x8x8_t1r3s3 Tactic: 0x21246c8544eff903
[08/10/2023-11:17:18] [V] [TRT] Tactic: 0x21246c8544eff903 Time: 0.90261
[08/10/2023-11:17:18] [V] [TRT] Conv_46 + Relu_47 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x128x32_stage4_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0x25b2b9d5c9d5ca0d
[08/10/2023-11:17:18] [V] [TRT] Tactic: 0x25b2b9d5c9d5ca0d Time: 0.973161
[08/10/2023-11:17:18] [V] [TRT] Conv_46 + Relu_47 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage4_warpsize2x2x1_g1_tensor16x8x8_t1r3s3 Tactic: 0x2d80d2b384ab13f1
[08/10/2023-11:17:18] [V] [TRT] Tactic: 0x2d80d2b384ab13f1 Time: 1.50752
[08/10/2023-11:17:18] [V] [TRT] Conv_46 + Relu_47 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize256x128x32_stage3_warpsize4x2x1_g1_tensor16x8x8 Tactic: 0x36303f399d3ee4fd
[08/10/2023-11:17:18] [V] [TRT] Tactic: 0x36303f399d3ee4fd Time: 1.13456
[08/10/2023-11:17:18] [V] [TRT] Conv_46 + Relu_47 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage4_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0x385bfab9332b1413
[08/10/2023-11:17:18] [V] [TRT] Tactic: 0x385bfab9332b1413 Time: 1.69329
[08/10/2023-11:17:18] [V] [TRT] Conv_46 + Relu_47 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x256x32_stage3_warpsize2x4x1_g1_tensor16x8x8 Tactic: 0x482b48242255b8ce
[08/10/2023-11:17:18] [V] [TRT] Tactic: 0x482b48242255b8ce Time: 0.950322
[08/10/2023-11:17:18] [V] [TRT] Conv_46 + Relu_47 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x256x32_stage3_warpsize2x4x1_g1_tensor16x8x8_t1r3s3 Tactic: 0x614e89f7852edbc3
[08/10/2023-11:17:18] [V] [TRT] Tactic: 0x614e89f7852edbc3 Time: 0.927918
[08/10/2023-11:17:18] [V] [TRT] Conv_46 + Relu_47 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0x65e41d81f093b482
[08/10/2023-11:17:18] [V] [TRT] Tactic: 0x65e41d81f093b482 Time: 1.09871
[08/10/2023-11:17:18] [V] [TRT] Conv_46 + Relu_47 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize64x128x32_stage5_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0x65f71607d20d5438
[08/10/2023-11:17:18] [V] [TRT] Tactic: 0x65f71607d20d5438 Time: 1.23683
[08/10/2023-11:17:18] [V] [TRT] Conv_46 + Relu_47 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x64x32_stage5_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0x6716429226d146f7
[08/10/2023-11:17:18] [V] [TRT] Tactic: 0x6716429226d146f7 Time: 1.28839
[08/10/2023-11:17:18] [V] [TRT] Conv_46 + Relu_47 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize64x32x64_stage5_warpsize2x2x1_g1_tensor16x8x8_t1r3s3 Tactic: 0x679a281f7660c436
[08/10/2023-11:17:18] [V] [TRT] Tactic: 0x679a281f7660c436 Time: 2.23225
[08/10/2023-11:17:18] [V] [TRT] Conv_46 + Relu_47 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize64x32x64_stage5_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0x6e2a3c7a7fc5e4e1
[08/10/2023-11:17:18] [V] [TRT] Tactic: 0x6e2a3c7a7fc5e4e1 Time: 2.29013
[08/10/2023-11:17:18] [V] [TRT] Conv_46 + Relu_47 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x64x32_stage5_warpsize2x2x1_g1_tensor16x8x8_t1r3s3 Tactic: 0x7aa21282feb15f1a
[08/10/2023-11:17:18] [V] [TRT] Tactic: 0x7aa21282feb15f1a Time: 1.30587
[08/10/2023-11:17:18] [V] [TRT] Conv_46 + Relu_47 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage4_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0x818413b69874bb35
[08/10/2023-11:17:18] [V] [TRT] Tactic: 0x818413b69874bb35 Time: 1.56605
[08/10/2023-11:17:18] [V] [TRT] Conv_46 + Relu_47 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x256x32_stage3_warpsize2x4x1_g1_tensor16x8x8 Tactic: 0x998c97842e775a17
[08/10/2023-11:17:18] [V] [TRT] Tactic: 0x998c97842e775a17 Time: 0.936677
[08/10/2023-11:17:18] [V] [TRT] Conv_46 + Relu_47 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8_t1r3s3 Tactic: 0x999e005e3b016ea6
[08/10/2023-11:17:18] [V] [TRT] Tactic: 0x999e005e3b016ea6 Time: 1.02949
[08/10/2023-11:17:18] [V] [TRT] Conv_46 + Relu_47 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize256x64x32_stage3_warpsize4x2x1_g1_tensor16x8x8_t1r3s3 Tactic: 0xa9a06d0633580c0c
[08/10/2023-11:17:18] [V] [TRT] Tactic: 0xa9a06d0633580c0c Time: 1.10133
[08/10/2023-11:17:18] [V] [TRT] Conv_46 + Relu_47 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0xb443c221fcb1565b
[08/10/2023-11:17:18] [V] [TRT] Tactic: 0xb443c221fcb1565b Time: 1.05579
[08/10/2023-11:17:18] [V] [TRT] Conv_46 + Relu_47 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage3_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0xb47d7d926de02dee
[08/10/2023-11:17:18] [V] [TRT] Tactic: 0xb47d7d926de02dee Time: 1.33963
[08/10/2023-11:17:18] [V] [TRT] Conv_46 + Relu_47 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize256x64x32_stage3_warpsize4x2x1_g1_tensor16x8x8 Tactic: 0xba0185279ccb2b85
[08/10/2023-11:17:18] [V] [TRT] Tactic: 0xba0185279ccb2b85 Time: 1.31541
[08/10/2023-11:17:18] [V] [TRT] Conv_46 + Relu_47 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage3_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0xc0a715d897e240bb
[08/10/2023-11:17:18] [V] [TRT] Tactic: 0xc0a715d897e240bb Time: 1.31622
[08/10/2023-11:17:18] [V] [TRT] Conv_46 + Relu_47 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize256x64x32_stage3_warpsize4x2x1_g1_tensor16x8x8 Tactic: 0xcedbed6d66c946d0
[08/10/2023-11:17:18] [V] [TRT] Tactic: 0xcedbed6d66c946d0 Time: 1.17765
[08/10/2023-11:17:18] [V] [TRT] Conv_46 + Relu_47 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize64x128x32_stage5_warpsize2x2x1_g1_tensor16x8x8_t1r3s3 Tactic: 0xd4428a7355769d7d
[08/10/2023-11:17:18] [V] [TRT] Tactic: 0xd4428a7355769d7d Time: 1.1835
[08/10/2023-11:17:18] [V] [TRT] Conv_46 + Relu_47 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize64x32x64_stage5_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0xd7f5d575d49a4bc7
[08/10/2023-11:17:18] [V] [TRT] Tactic: 0xd7f5d575d49a4bc7 Time: 2.70214
[08/10/2023-11:17:18] [V] [TRT] Conv_46 + Relu_47 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x128x32_stage4_warpsize2x2x1_g1_tensor16x8x8_t1r3s3 Tactic: 0xd920b33c9bd27143
[08/10/2023-11:17:18] [V] [TRT] Tactic: 0xd920b33c9bd27143 Time: 0.946281
[08/10/2023-11:17:18] [V] [TRT] Conv_46 + Relu_47 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize256x128x32_stage3_warpsize4x2x1_g1_tensor16x8x8 Tactic: 0xe797e099911c0624
[08/10/2023-11:17:18] [V] [TRT] Tactic: 0xe797e099911c0624 Time: 0.955931
[08/10/2023-11:17:18] [V] [TRT] Conv_46 + Relu_47 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage3_warpsize2x2x1_g1_tensor16x8x8_t1r3s3 Tactic: 0xeb65d4c1e8fa2294
[08/10/2023-11:17:18] [V] [TRT] Tactic: 0xeb65d4c1e8fa2294 Time: 1.24075
[08/10/2023-11:17:18] [V] [TRT] Conv_46 + Relu_47 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x128x32_stage4_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0xf4156675c5f728d4
[08/10/2023-11:17:18] [V] [TRT] Tactic: 0xf4156675c5f728d4 Time: 1.51316
[08/10/2023-11:17:18] [V] [TRT] Fastest Tactic: 0x21246c8544eff903 Time: 0.90261
[08/10/2023-11:17:18] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x21246c8544eff903
[08/10/2023-11:17:18] [V] [TRT] *************** Autotuning format combination: Half(2073600,8100,90,1) -> Half(2073600,8100,90,1) ***************
[08/10/2023-11:17:18] [W] [TRT] Weights [name=Conv_46 + Relu_47.weight] had the following issues when converted to FP16:
[08/10/2023-11:17:18] [W] [TRT]  - Subnormal FP16 values detected. 
[08/10/2023-11:17:18] [W] [TRT]  - Values less than smallest positive FP16 Subnormal value detected. Converting to FP16 minimum subnormalized value. 
[08/10/2023-11:17:18] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.
[08/10/2023-11:17:18] [V] [TRT] --------------- Timing Runner: Conv_46 + Relu_47 (CudnnConvolution)
[08/10/2023-11:17:18] [V] [TRT] Tactic: 0x0000000000000000 Time: 7.16963
[08/10/2023-11:17:18] [V] [TRT] Tactic: 0x0000000000000001 Time: 6.43808
[08/10/2023-11:17:18] [V] [TRT] Tactic: 0x0000000000000002 Time: 7.64864
[08/10/2023-11:17:18] [V] [TRT] Tactic: 0x0000000000000004 skipped. Scratch requested: 8760590336, available: 4294967296
[08/10/2023-11:17:19] [V] [TRT] Tactic: 0x0000000000000005 Time: 25.7729
[08/10/2023-11:17:19] [V] [TRT] Tactic: 0x0000000000000006 Time: 3.84846
[08/10/2023-11:17:19] [V] [TRT] Tactic: 0x0000000000000038 Time: 7.02895
[08/10/2023-11:17:19] [V] [TRT] Tactic: 0x000000000000003a Time: 7.62043
[08/10/2023-11:17:19] [V] [TRT] Tactic: 0x000000000000003c skipped. Scratch requested: 8760590336, available: 4294967296
[08/10/2023-11:17:19] [V] [TRT] Tactic: 0x000000000000003d Time: 25.9936
[08/10/2023-11:17:19] [V] [TRT] Tactic: 0x000000000000003e Time: 3.6627
[08/10/2023-11:17:19] [V] [TRT] Fastest Tactic: 0x000000000000003e Time: 3.6627
[08/10/2023-11:17:19] [V] [TRT] Setting workspace to 8760590336enables more tactics for profiling
[08/10/2023-11:17:19] [W] [TRT] Weights [name=Conv_46 + Relu_47.weight] had the following issues when converted to FP16:
[08/10/2023-11:17:19] [W] [TRT]  - Subnormal FP16 values detected. 
[08/10/2023-11:17:19] [W] [TRT]  - Values less than smallest positive FP16 Subnormal value detected. Converting to FP16 minimum subnormalized value. 
[08/10/2023-11:17:19] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.
[08/10/2023-11:17:19] [V] [TRT] --------------- Timing Runner: Conv_46 + Relu_47 (CaskConvolution)
[08/10/2023-11:17:19] [V] [TRT] CaskConvolution has no valid tactics for this config, skipping
[08/10/2023-11:17:19] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CudnnConvolution Tactic: 0x000000000000003e
[08/10/2023-11:17:19] [V] [TRT] *************** Autotuning format combination: Half(1036800,8100:2,90,1) -> Half(1036800,8100:2,90,1) ***************
[08/10/2023-11:17:19] [W] [TRT] Weights [name=Conv_46 + Relu_47.weight] had the following issues when converted to FP16:
[08/10/2023-11:17:19] [W] [TRT]  - Subnormal FP16 values detected. 
[08/10/2023-11:17:19] [W] [TRT]  - Values less than smallest positive FP16 Subnormal value detected. Converting to FP16 minimum subnormalized value. 
[08/10/2023-11:17:19] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.
[08/10/2023-11:17:19] [V] [TRT] --------------- Timing Runner: Conv_46 + Relu_47 (FusedConvActConvolution)
[08/10/2023-11:17:19] [V] [TRT] FusedConvActConvolution has no valid tactics for this config, skipping
[08/10/2023-11:17:19] [W] [TRT] Weights [name=Conv_46 + Relu_47.weight] had the following issues when converted to FP16:
[08/10/2023-11:17:19] [W] [TRT]  - Subnormal FP16 values detected. 
[08/10/2023-11:17:19] [W] [TRT]  - Values less than smallest positive FP16 Subnormal value detected. Converting to FP16 minimum subnormalized value. 
[08/10/2023-11:17:19] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.
[08/10/2023-11:17:19] [V] [TRT] --------------- Timing Runner: Conv_46 + Relu_47 (CaskConvolution)
[08/10/2023-11:17:19] [V] [TRT] CaskConvolution has no valid tactics for this config, skipping
[08/10/2023-11:17:19] [V] [TRT] *************** Autotuning format combination: Half(259200,1:8,2880,32) -> Float(2073600,8100,90,1) ***************
[08/10/2023-11:17:19] [W] [TRT] Weights [name=Conv_46 + Relu_47.weight] had the following issues when converted to FP16:
[08/10/2023-11:17:19] [W] [TRT]  - Subnormal FP16 values detected. 
[08/10/2023-11:17:19] [W] [TRT]  - Values less than smallest positive FP16 Subnormal value detected. Converting to FP16 minimum subnormalized value. 
[08/10/2023-11:17:19] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.
[08/10/2023-11:17:19] [V] [TRT] --------------- Timing Runner: Conv_46 + Relu_47 (CaskConvolution)
[08/10/2023-11:17:19] [V] [TRT] CaskConvolution has no valid tactics for this config, skipping
[08/10/2023-11:17:19] [V] [TRT] *************** Autotuning format combination: Half(259200,1:8,2880,32) -> Half(259200,1:8,2880,32) ***************
[08/10/2023-11:17:19] [W] [TRT] Weights [name=Conv_46 + Relu_47.weight] had the following issues when converted to FP16:
[08/10/2023-11:17:19] [W] [TRT]  - Subnormal FP16 values detected. 
[08/10/2023-11:17:19] [W] [TRT]  - Values less than smallest positive FP16 Subnormal value detected. Converting to FP16 minimum subnormalized value. 
[08/10/2023-11:17:19] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.
[08/10/2023-11:17:19] [V] [TRT] --------------- Timing Runner: Conv_46 + Relu_47 (CudaDepthwiseConvolution)
[08/10/2023-11:17:19] [V] [TRT] CudaDepthwiseConvolution has no valid tactics for this config, skipping
[08/10/2023-11:17:19] [W] [TRT] Weights [name=Conv_46 + Relu_47.weight] had the following issues when converted to FP16:
[08/10/2023-11:17:19] [W] [TRT]  - Subnormal FP16 values detected. 
[08/10/2023-11:17:19] [W] [TRT]  - Values less than smallest positive FP16 Subnormal value detected. Converting to FP16 minimum subnormalized value. 
[08/10/2023-11:17:19] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.
[08/10/2023-11:17:19] [V] [TRT] --------------- Timing Runner: Conv_46 + Relu_47 (CaskConvolution)
[08/10/2023-11:17:19] [V] [TRT] Conv_46 + Relu_47 Set Tactic Name: ampere_h16816cudnn_256x128_ldg8_relu_exp_medium_nhwc_tn_v1 Tactic: 0x03896956a39a1203
[08/10/2023-11:17:19] [V] [TRT] Tactic: 0x03896956a39a1203 Time: 0.492914
[08/10/2023-11:17:19] [V] [TRT] Conv_46 + Relu_47 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x128x32_stage3_warpsize4x2x1_g1_tensor16x8x16_t1r3s3 Tactic: 0x048d6d0400f33439
[08/10/2023-11:17:19] [V] [TRT] Tactic: 0x048d6d0400f33439 Time: 0.463159
[08/10/2023-11:17:19] [V] [TRT] Conv_46 + Relu_47 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x64x64_stage3_warpsize2x2x1_g1_tensor16x8x16_t1r3s3 Tactic: 0x05b6220f243edacd
[08/10/2023-11:17:19] [V] [TRT] Tactic: 0x05b6220f243edacd Time: 0.53941
[08/10/2023-11:17:19] [V] [TRT] Conv_46 + Relu_47 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x64x32_stage3_warpsize4x1x1_g1_tensor16x8x16_t1r3s3_aACCESS Tactic: 0x0866ddee325d07a6
[08/10/2023-11:17:19] [V] [TRT] Tactic: 0x0866ddee325d07a6 Time: 0.495771
[08/10/2023-11:17:19] [V] [TRT] Conv_46 + Relu_47 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x128x32_stage4_warpsize2x2x1_g1_tensor16x8x16 Tactic: 0x0e07ff4f4f7c1ac9
[08/10/2023-11:17:19] [V] [TRT] Tactic: 0x0e07ff4f4f7c1ac9 Time: 0.47824
[08/10/2023-11:17:19] [V] [TRT] Conv_46 + Relu_47 Set Tactic Name: ampere_h16816cudnn_256x128_ldg8_relu_exp_stages_64x3_small_nhwc_tn_v1 Tactic: 0x0e199750c30c6928
[08/10/2023-11:17:19] [V] [TRT] Tactic: 0x0e199750c30c6928 Time: 0.47243
[08/10/2023-11:17:19] [V] [TRT] Conv_46 + Relu_47 Set Tactic Name: ampere_h1688cudnn_128x128_ldg8_relu_exp_medium_nhwc_tn_v1 Tactic: 0x100c1ad308e08d35
[08/10/2023-11:17:19] [V] [TRT] Tactic: 0x100c1ad308e08d35 Time: 0.596009
[08/10/2023-11:17:19] [V] [TRT] Conv_46 + Relu_47 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x128x32_stage5_warpsize2x2x1_g1_tensor16x8x16_t1r3s3 Tactic: 0x17ebf0c9f418f10a
[08/10/2023-11:17:19] [V] [TRT] Tactic: 0x17ebf0c9f418f10a Time: 0.530702
[08/10/2023-11:17:19] [V] [TRT] Conv_46 + Relu_47 Set Tactic Name: ampere_h16816cudnn_128x64_ldg8_relu_exp_stages_64x4_medium_nhwc_tn_v1 Tactic: 0x196cbc423a9bb69e
[08/10/2023-11:17:19] [V] [TRT] Tactic: 0x196cbc423a9bb69e Time: 0.612974
[08/10/2023-11:17:19] [V] [TRT] Conv_46 + Relu_47 Set Tactic Name: ampere_h16816cudnn_128x64_sliced1x2_ldg8_relu_exp_stages_64x3_small_nhwc_tn_v1 Tactic: 0x19822de884f42a6a
[08/10/2023-11:17:19] [V] [TRT] Tactic: 0x19822de884f42a6a Time: 0.517015
[08/10/2023-11:17:19] [V] [TRT] Conv_46 + Relu_47 Set Tactic Name: ampere_h16816cudnn_64x64_sliced1x2_ldg8_relu_exp_stages_64x5_small_nhwc_tn_v1 Tactic: 0x1a5ba808ad4cc5a8
[08/10/2023-11:17:19] [V] [TRT] Tactic: 0x1a5ba808ad4cc5a8 Time: 0.676306
[08/10/2023-11:17:19] [V] [TRT] Conv_46 + Relu_47 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x64x64_stage3_warpsize2x2x1_g1_tensor16x8x16 Tactic: 0x21b295c0c8f6c95a
[08/10/2023-11:17:19] [V] [TRT] Tactic: 0x21b295c0c8f6c95a Time: 0.547735
[08/10/2023-11:17:19] [V] [TRT] Conv_46 + Relu_47 Set Tactic Name: ampere_h16816cudnn_128x128_ldg8_relu_exp_medium_nhwc_tn_v1 Tactic: 0x234580e8a194335c
[08/10/2023-11:17:19] [V] [TRT] Tactic: 0x234580e8a194335c Time: 0.465426
[08/10/2023-11:17:19] [V] [TRT] Conv_46 + Relu_47 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x128x32_stage4_warpsize2x2x1_g1_tensor16x8x16_t1r3s3_aACCESS Tactic: 0x245530c34bd6090f
[08/10/2023-11:17:19] [V] [TRT] Tactic: 0x245530c34bd6090f Time: 0.445371
[08/10/2023-11:17:19] [V] [TRT] Conv_46 + Relu_47 Set Tactic Name: ampere_h16816cudnn_64x64_sliced1x2_ldg8_relu_exp_stages_64x6_small_nhwc_tn_v1 Tactic: 0x24e01e7405cfdfe9
[08/10/2023-11:17:19] [V] [TRT] Tactic: 0x24e01e7405cfdfe9 Time: 0.739959
[08/10/2023-11:17:19] [V] [TRT] Conv_46 + Relu_47 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x64x64_stage3_warpsize2x2x1_g1_tensor16x8x16_t1r3s3_aACCESS Tactic: 0x263a38afd75e3a43
[08/10/2023-11:17:19] [V] [TRT] Tactic: 0x263a38afd75e3a43 Time: 0.494153
[08/10/2023-11:17:19] [V] [TRT] Conv_46 + Relu_47 Set Tactic Name: ampere_h16816cudnn_64x128_sliced1x2_ldg8_relu_exp_stages_64x4_large_nhwc_tn_v1 Tactic: 0x2721a7f18c2700db
[08/10/2023-11:17:19] [V] [TRT] Tactic: 0x2721a7f18c2700db Time: 0.589243
[08/10/2023-11:17:19] [V] [TRT] Conv_46 + Relu_47 Set Tactic Name: ampere_h16816cudnn_128x64_sliced1x2_ldg8_relu_exp_stages_64x3_medium_nhwc_tn_v1 Tactic: 0x29329b5741ea05f2
[08/10/2023-11:17:19] [V] [TRT] Tactic: 0x29329b5741ea05f2 Time: 0.532745
[08/10/2023-11:17:19] [V] [TRT] Conv_46 + Relu_47 Set Tactic Name: ampere_h16816cudnn_128x128_ldg8_relu_exp_large_nhwc_tn_v1 Tactic: 0x2970ae65966d569d
[08/10/2023-11:17:19] [V] [TRT] Tactic: 0x2970ae65966d569d Time: 0.474286
[08/10/2023-11:17:19] [V] [TRT] Conv_46 + Relu_47 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x256x64_stage3_warpsize1x4x1_g1_tensor16x8x16 Tactic: 0x29dc14520e0e4eb6
[08/10/2023-11:17:19] [V] [TRT] Tactic: 0x29dc14520e0e4eb6 Time: 0.515433
[08/10/2023-11:17:19] [V] [TRT] Conv_46 + Relu_47 Set Tactic Name: ampere_h16816cudnn_256x64_ldg8_relu_exp_stages_64x3_large_nhwc_tn_v1 Tactic: 0x2cb4a662694e89d3
[08/10/2023-11:17:19] [V] [TRT] Tactic: 0x2cb4a662694e89d3 Time: 0.518203
[08/10/2023-11:17:19] [V] [TRT] Conv_46 + Relu_47 Set Tactic Name: ampere_h16816cudnn_128x128_ldg8_relu_exp_stages_64x3_medium_nhwc_tn_v1 Tactic: 0x2eaa2202de9404d6
[08/10/2023-11:17:19] [V] [TRT] Tactic: 0x2eaa2202de9404d6 Time: 0.509467
[08/10/2023-11:17:19] [V] [TRT] Conv_46 + Relu_47 Set Tactic Name: ampere_h1688cudnn_256x128_ldg8_relu_exp_medium_nhwc_tn_v1 Tactic: 0x30c0f36d0aeeac6a
[08/10/2023-11:17:19] [V] [TRT] Tactic: 0x30c0f36d0aeeac6a Time: 0.612786
[08/10/2023-11:17:19] [V] [TRT] Conv_46 + Relu_47 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x64x64_stage4_warpsize2x2x1_g1_tensor16x8x16_t1r3s3_aACCESS Tactic: 0x30e8a8d7a953e5e9
[08/10/2023-11:17:19] [V] [TRT] Tactic: 0x30e8a8d7a953e5e9 Time: 0.588558
[08/10/2023-11:17:19] [V] [TRT] Conv_46 + Relu_47 Set Tactic Name: ampere_h16816cudnn_128x64_ldg8_relu_exp_stages_64x4_large_nhwc_tn_v1 Tactic: 0x3197d5044d71e98e
[08/10/2023-11:17:19] [V] [TRT] Tactic: 0x3197d5044d71e98e Time: 0.64539
[08/10/2023-11:17:19] [V] [TRT] Conv_46 + Relu_47 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x64x64_stage4_warpsize2x2x1_g1_tensor16x8x16_t1r3s3 Tactic: 0x31d93dc22d2af081
[08/10/2023-11:17:19] [V] [TRT] Tactic: 0x31d93dc22d2af081 Time: 0.705755
[08/10/2023-11:17:19] [V] [TRT] Conv_46 + Relu_47 Set Tactic Name: ampere_h1688cudnn_128x128_ldg8_relu_exp_small_nhwc_tn_v1 Tactic: 0x3369260c04f9ad73
[08/10/2023-11:17:19] [V] [TRT] Tactic: 0x3369260c04f9ad73 Time: 0.587456
[08/10/2023-11:17:19] [V] [TRT] Conv_46 + Relu_47 Set Tactic Name: ampere_h16816cudnn_64x128_ldg8_relu_exp_stages_64x4_small_nhwc_tn_v1 Tactic: 0x3b5fa0f2a8fc2410
[08/10/2023-11:17:19] [V] [TRT] Tactic: 0x3b5fa0f2a8fc2410 Time: 0.601266
[08/10/2023-11:17:19] [V] [TRT] Conv_46 + Relu_47 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x32x32_stage4_warpsize4x1x1_g1_tensor16x8x16_t1r3s3 Tactic: 0x3e7eb35b91b9fa63
[08/10/2023-11:17:19] [V] [TRT] Tactic: 0x3e7eb35b91b9fa63 Time: 0.870304
[08/10/2023-11:17:19] [V] [TRT] Conv_46 + Relu_47 Set Tactic Name: ampere_h16816cudnn_256x64_ldg8_relu_exp_stages_64x3_medium_nhwc_tn_v1 Tactic: 0x3f031df0e579d52c
[08/10/2023-11:17:19] [V] [TRT] Tactic: 0x3f031df0e579d52c Time: 0.519991
[08/10/2023-11:17:19] [V] [TRT] Conv_46 + Relu_47 Set Tactic Name: ampere_h16816cudnn_256x64_sliced1x2_ldg8_relu_exp_small_nhwc_tn_v1 Tactic: 0x42a5b8709d0039be
[08/10/2023-11:17:19] [V] [TRT] Tactic: 0x42a5b8709d0039be Time: 0.63173
[08/10/2023-11:17:19] [V] [TRT] Conv_46 + Relu_47 Set Tactic Name: ampere_h16816cudnn_256x128_ldg8_relu_exp_small_nhwc_tn_v1 Tactic: 0x463794ee4acffd1f
[08/10/2023-11:17:19] [V] [TRT] Tactic: 0x463794ee4acffd1f Time: 0.503419
[08/10/2023-11:17:19] [V] [TRT] Conv_46 + Relu_47 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x64x64_stage4_warpsize2x2x1_g1_tensor16x8x16 Tactic: 0x4a81ea1e51436a30
[08/10/2023-11:17:19] [V] [TRT] Tactic: 0x4a81ea1e51436a30 Time: 0.693015
[08/10/2023-11:17:19] [V] [TRT] Conv_46 + Relu_47 Set Tactic Name: ampere_h1688cudnn_256x64_sliced1x2_ldg8_relu_exp_small_nhwc_tn_v1 Tactic: 0x4aed1956bd10a795
[08/10/2023-11:17:19] [V] [TRT] Tactic: 0x4aed1956bd10a795 Time: 0.699758
[08/10/2023-11:17:19] [V] [TRT] Conv_46 + Relu_47 Set Tactic Name: ampere_h16816cudnn_64x128_sliced1x2_ldg8_relu_exp_stages_64x3_large_nhwc_tn_v1 Tactic: 0x4fc05923455fc266
[08/10/2023-11:17:19] [V] [TRT] Tactic: 0x4fc05923455fc266 Time: 0.553906
[08/10/2023-11:17:19] [V] [TRT] Conv_46 + Relu_47 Set Tactic Name: ampere_h16816cudnn_256x64_ldg8_relu_exp_large_nhwc_tn_v1 Tactic: 0x5013c38f55afa9ba
[08/10/2023-11:17:19] [V] [TRT] Tactic: 0x5013c38f55afa9ba Time: 0.562478
[08/10/2023-11:17:19] [V] [TRT] Conv_46 + Relu_47 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x64x32_stage3_warpsize4x1x1_g1_tensor16x8x16_t1r3s3 Tactic: 0x529f4431bdae94f5
[08/10/2023-11:17:19] [V] [TRT] Tactic: 0x529f4431bdae94f5 Time: 0.470194
[08/10/2023-11:17:19] [V] [TRT] Conv_46 + Relu_47 Set Tactic Name: ampere_h16816cudnn_64x128_ldg8_relu_exp_stages_64x3_small_nhwc_tn_v1 Tactic: 0x53be5e206184e6ad
[08/10/2023-11:17:19] [V] [TRT] Tactic: 0x53be5e206184e6ad Time: 0.565934
[08/10/2023-11:17:19] [V] [TRT] Conv_46 + Relu_47 Set Tactic Name: ampere_h16816cudnn_64x64_ldg8_relu_exp_stages_64x5_large_nhwc_tn_v1 Tactic: 0x544bff7ff9c5d908
[08/10/2023-11:17:19] [V] [TRT] Tactic: 0x544bff7ff9c5d908 Time: 0.745929
[08/10/2023-11:17:19] [V] [TRT] Conv_46 + Relu_47 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x64x64_stage3_warpsize2x2x1_g1_tensor16x8x16 Tactic: 0x5568fd8a32f4a40f
[08/10/2023-11:17:19] [V] [TRT] Tactic: 0x5568fd8a32f4a40f Time: 0.540073
[08/10/2023-11:17:19] [V] [TRT] Conv_46 + Relu_47 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x128x64_stage3_warpsize2x2x1_g1_tensor16x8x16 Tactic: 0x57c9a5ff682354a6
[08/10/2023-11:17:19] [V] [TRT] Tactic: 0x57c9a5ff682354a6 Time: 0.484709
[08/10/2023-11:17:19] [V] [TRT] Conv_46 + Relu_47 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x32x64_stage5_warpsize2x2x1_g1_tensor16x8x16 Tactic: 0x5820b3dda403c4d0
[08/10/2023-11:17:19] [V] [TRT] Tactic: 0x5820b3dda403c4d0 Time: 1.01501
[08/10/2023-11:17:19] [V] [TRT] Conv_46 + Relu_47 Set Tactic Name: ampere_h16816cudnn_128x128_ldg8_relu_exp_stages_64x3_small_nhwc_tn_v1 Tactic: 0x58816bf2e9c36afb
[08/10/2023-11:17:19] [V] [TRT] Tactic: 0x58816bf2e9c36afb Time: 0.515214
[08/10/2023-11:17:19] [V] [TRT] Conv_46 + Relu_47 Set Tactic Name: ampere_h16816cudnn_128x64_ldg8_relu_exp_stages_64x3_large_nhwc_tn_v1 Tactic: 0x59762bd684092b33
[08/10/2023-11:17:19] [V] [TRT] Tactic: 0x59762bd684092b33 Time: 0.563566
[08/10/2023-11:17:19] [V] [TRT] Conv_46 + Relu_47 Set Tactic Name: ampere_h16816cudnn_128x64_sliced1x2_ldg8_relu_exp_stages_64x4_medium_nhwc_tn_v1 Tactic: 0x59c5c647b4c76593
[08/10/2023-11:17:19] [V] [TRT] Tactic: 0x59c5c647b4c76593 Time: 0.589787
[08/10/2023-11:17:19] [V] [TRT] Conv_46 + Relu_47 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x128x64_stage3_warpsize4x2x1_g1_tensor16x8x16 Tactic: 0x5a55274960724ba8
[08/10/2023-11:17:19] [V] [TRT] Tactic: 0x5a55274960724ba8 Time: 0.477733
[08/10/2023-11:17:19] [V] [TRT] Conv_46 + Relu_47 Set Tactic Name: ampere_h1688cudnn_256x128_ldg8_relu_exp_large_nhwc_tn_v1 Tactic: 0x5c2e1c87d85b06f1
[08/10/2023-11:17:19] [V] [TRT] Tactic: 0x5c2e1c87d85b06f1 Time: 0.63344
[08/10/2023-11:17:19] [V] [TRT] Conv_46 + Relu_47 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x256x64_stage3_warpsize1x4x1_g1_tensor16x8x16 Tactic: 0x5d067c18f40c23e3
[08/10/2023-11:17:19] [V] [TRT] Tactic: 0x5d067c18f40c23e3 Time: 0.525655
[08/10/2023-11:17:19] [V] [TRT] Conv_46 + Relu_47 Set Tactic Name: ampere_h16816cudnn_64x64_ldg8_relu_exp_stages_64x5_medium_nhwc_tn_v1 Tactic: 0x5f31c22ec167f384
[08/10/2023-11:17:19] [V] [TRT] Tactic: 0x5f31c22ec167f384 Time: 0.765362
[08/10/2023-11:17:19] [V] [TRT] Conv_46 + Relu_47 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x128x32_stage4_warpsize2x2x1_g1_tensor16x8x16_t1r3s3 Tactic: 0x60c3421152ef8e10
[08/10/2023-11:17:19] [V] [TRT] Tactic: 0x60c3421152ef8e10 Time: 0.455369
[08/10/2023-11:17:19] [V] [TRT] Conv_46 + Relu_47 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x128x32_stage3_warpsize4x2x1_g1_tensor16x8x16 Tactic: 0x60da8c7151d91e47
[08/10/2023-11:17:19] [V] [TRT] Tactic: 0x60da8c7151d91e47 Time: 0.477989
[08/10/2023-11:17:19] [V] [TRT] Conv_46 + Relu_47 Set Tactic Name: ampere_h1688cudnn_256x64_ldg8_relu_exp_medium_nhwc_tn_v1 Tactic: 0x6426696f872a3b13
[08/10/2023-11:17:19] [V] [TRT] Tactic: 0x6426696f872a3b13 Time: 0.751657
[08/10/2023-11:17:20] [V] [TRT] Conv_46 + Relu_47 Set Tactic Name: ampere_h16816cudnn_128x64_ldg8_relu_exp_stages_64x3_medium_nhwc_tn_v1 Tactic: 0x699be152cfb6d6ff
[08/10/2023-11:17:20] [V] [TRT] Tactic: 0x699be152cfb6d6ff Time: 0.56469
[08/10/2023-11:17:20] [V] [TRT] Conv_46 + Relu_47 Set Tactic Name: ampere_h16816cudnn_64x64_ldg8_relu_exp_stages_64x6_large_nhwc_tn_v1 Tactic: 0x6af049035146c349
[08/10/2023-11:17:20] [V] [TRT] Tactic: 0x6af049035146c349 Time: 0.878162
[08/10/2023-11:17:20] [V] [TRT] Conv_46 + Relu_47 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x256x32_stage3_warpsize2x4x1_g1_tensor16x8x16 Tactic: 0x7005d10718f6c22d
[08/10/2023-11:17:20] [V] [TRT] Tactic: 0x7005d10718f6c22d Time: 0.478391
[08/10/2023-11:17:20] [V] [TRT] Conv_46 + Relu_47 Set Tactic Name: ampere_h16816cudnn_64x64_sliced1x2_ldg8_relu_exp_stages_64x5_medium_nhwc_tn_v1 Tactic: 0x706f08da35c795a5
[08/10/2023-11:17:20] [V] [TRT] Tactic: 0x706f08da35c795a5 Time: 0.703232
[08/10/2023-11:17:20] [V] [TRT] Conv_46 + Relu_47 Set Tactic Name: ampere_h16816cudnn_128x64_sliced1x2_ldg8_relu_exp_stages_64x4_small_nhwc_tn_v1 Tactic: 0x7163d33a4d8ce8d7
[08/10/2023-11:17:20] [V] [TRT] Tactic: 0x7163d33a4d8ce8d7 Time: 0.571589
[08/10/2023-11:17:20] [V] [TRT] Conv_46 + Relu_47 Set Tactic Name: ampere_h1688cudnn_256x64_ldg8_relu_exp_small_nhwc_tn_v1 Tactic: 0x7aad3976677d7155
[08/10/2023-11:17:20] [V] [TRT] Tactic: 0x7aad3976677d7155 Time: 0.693659
[08/10/2023-11:17:20] [V] [TRT] Conv_46 + Relu_47 Set Tactic Name: ampere_h16816cudnn_64x64_sliced1x2_ldg8_relu_exp_stages_64x5_large_nhwc_tn_v1 Tactic: 0x8015519605ab9963
[08/10/2023-11:17:20] [V] [TRT] Tactic: 0x8015519605ab9963 Time: 0.701248
[08/10/2023-11:17:20] [V] [TRT] Conv_46 + Relu_47 Set Tactic Name: ampere_h16816cudnn_256x64_sliced1x2_ldg8_relu_exp_medium_nhwc_tn_v1 Tactic: 0x82f8a2214b0b4178
[08/10/2023-11:17:20] [V] [TRT] Tactic: 0x82f8a2214b0b4178 Time: 0.721367
[08/10/2023-11:17:20] [V] [TRT] Conv_46 + Relu_47 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x32x32_stage4_warpsize4x1x1_g1_tensor16x8x16 Tactic: 0x834e11ecd4ab9454
[08/10/2023-11:17:20] [V] [TRT] Tactic: 0x834e11ecd4ab9454 Time: 0.849422
[08/10/2023-11:17:20] [V] [TRT] Conv_46 + Relu_47 Set Tactic Name: ampere_h16816cudnn_128x64_sliced1x2_ldg8_relu_exp_stages_64x3_large_nhwc_tn_v1 Tactic: 0x83ccd4762c1376a1
[08/10/2023-11:17:20] [V] [TRT] Tactic: 0x83ccd4762c1376a1 Time: 0.537079
[08/10/2023-11:17:20] [V] [TRT] Conv_46 + Relu_47 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x64x64_stage3_warpsize4x1x1_g1_tensor16x8x16_t1r3s3 Tactic: 0x84ee897dd1f4d2aa
[08/10/2023-11:17:20] [V] [TRT] Tactic: 0x84ee897dd1f4d2aa Time: 0.526254
[08/10/2023-11:17:20] [V] [TRT] Conv_46 + Relu_47 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x128x64_stage3_warpsize2x2x1_g1_tensor16x8x16 Tactic: 0x866e7a5f6401b67f
[08/10/2023-11:17:20] [V] [TRT] Tactic: 0x866e7a5f6401b67f Time: 0.485838
[08/10/2023-11:17:20] [V] [TRT] Conv_46 + Relu_47 Set Tactic Name: ampere_h16816cudnn_64x64_ldg8_relu_exp_stages_64x6_medium_nhwc_tn_v1 Tactic: 0x88971eec55aba850
[08/10/2023-11:17:20] [V] [TRT] Tactic: 0x88971eec55aba850 Time: 0.854304
[08/10/2023-11:17:20] [V] [TRT] Conv_46 + Relu_47 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x128x64_stage3_warpsize4x2x1_g1_tensor16x8x16 Tactic: 0x8bf2f8e96c50a971
[08/10/2023-11:17:20] [V] [TRT] Tactic: 0x8bf2f8e96c50a971 Time: 0.470907
[08/10/2023-11:17:20] [V] [TRT] Conv_46 + Relu_47 Set Tactic Name: ampere_h16816cudnn_256x128_ldg8_relu_exp_stages_64x3_large_nhwc_tn_v1 Tactic: 0x94576ece6beb35e3
[08/10/2023-11:17:20] [V] [TRT] Tactic: 0x94576ece6beb35e3 Time: 0.48619
[08/10/2023-11:17:20] [V] [TRT] Conv_46 + Relu_47 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x64x64_stage3_warpsize4x1x1_g1_tensor16x8x16 Tactic: 0x9fff6e65019cc310
[08/10/2023-11:17:20] [V] [TRT] Tactic: 0x9fff6e65019cc310 Time: 0.532165
[08/10/2023-11:17:20] [V] [TRT] Conv_46 + Relu_47 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x32x64_stage5_warpsize2x2x1_g1_tensor16x8x16_t1r3s3 Tactic: 0xa033e20ae9f412b2
[08/10/2023-11:17:20] [V] [TRT] Tactic: 0xa033e20ae9f412b2 Time: 1.0103
[08/10/2023-11:17:20] [V] [TRT] Conv_46 + Relu_47 Set Tactic Name: ampere_h16816cudnn_64x128_ldg8_relu_exp_stages_64x4_large_nhwc_tn_v1 Tactic: 0xa111596c001b78db
[08/10/2023-11:17:20] [V] [TRT] Tactic: 0xa111596c001b78db Time: 0.591209
[08/10/2023-11:17:20] [V] [TRT] Conv_46 + Relu_47 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x256x32_stage3_warpsize2x4x1_g1_tensor16x8x16 Tactic: 0xa1a20ea714d420f4
[08/10/2023-11:17:20] [V] [TRT] Tactic: 0xa1a20ea714d420f4 Time: 0.477605
[08/10/2023-11:17:20] [V] [TRT] Conv_46 + Relu_47 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x32x32_stage4_warpsize4x1x1_g1_tensor16x8x16_t1r3s3 Tactic: 0xa40cb43c296a36a8
[08/10/2023-11:17:20] [V] [TRT] Tactic: 0xa40cb43c296a36a8 Time: 0.778885
[08/10/2023-11:17:20] [V] [TRT] Conv_46 + Relu_47 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x64x32_stage3_warpsize4x1x1_g1_tensor16x8x16 Tactic: 0xa570c55d303796ff
[08/10/2023-11:17:20] [V] [TRT] Tactic: 0xa570c55d303796ff Time: 0.514665
[08/10/2023-11:17:20] [V] [TRT] Conv_46 + Relu_47 Set Tactic Name: ampere_h16816cudnn_64x64_sliced1x2_ldg8_relu_exp_stages_64x6_medium_nhwc_tn_v1 Tactic: 0xa7c9d418a10bce71
[08/10/2023-11:17:20] [V] [TRT] Tactic: 0xa7c9d418a10bce71 Time: 0.855113
[08/10/2023-11:17:20] [V] [TRT] Conv_46 + Relu_47 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x32x32_stage4_warpsize4x1x1_g1_tensor16x8x16 Tactic: 0xa83b68f30462f971
[08/10/2023-11:17:20] [V] [TRT] Tactic: 0xa83b68f30462f971 Time: 0.805093
[08/10/2023-11:17:20] [V] [TRT] Conv_46 + Relu_47 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x256x32_stage3_warpsize2x4x1_g1_tensor16x8x16_t1r3s3 Tactic: 0xa9177bbe4e767df8
[08/10/2023-11:17:20] [V] [TRT] Tactic: 0xa9177bbe4e767df8 Time: 0.481403
[08/10/2023-11:17:20] [V] [TRT] Conv_46 + Relu_47 Set Tactic Name: ampere_h1688cudnn_128x128_ldg8_relu_exp_large_nhwc_tn_v1 Tactic: 0xa927df92ac1ef1b8
[08/10/2023-11:17:20] [V] [TRT] Tactic: 0xa927df92ac1ef1b8 Time: 0.61749
[08/10/2023-11:17:20] [V] [TRT] Conv_46 + Relu_47 Set Tactic Name: ampere_h16816cudnn_128x64_ldg8_relu_exp_stages_64x4_small_nhwc_tn_v1 Tactic: 0xabd92c9ae596b545
[08/10/2023-11:17:20] [V] [TRT] Tactic: 0xabd92c9ae596b545 Time: 0.611712
[08/10/2023-11:17:20] [V] [TRT] Conv_46 + Relu_47 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x128x32_stage5_warpsize2x2x1_g1_tensor16x8x16 Tactic: 0xafd1e8bf6bd3d638
[08/10/2023-11:17:20] [V] [TRT] Tactic: 0xafd1e8bf6bd3d638 Time: 0.539159
[08/10/2023-11:17:20] [V] [TRT] Conv_46 + Relu_47 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x128x32_stage3_warpsize4x2x1_g1_tensor16x8x16 Tactic: 0xb17d53d15dfbfc9e
[08/10/2023-11:17:20] [V] [TRT] Tactic: 0xb17d53d15dfbfc9e Time: 0.488018
[08/10/2023-11:17:20] [V] [TRT] Conv_46 + Relu_47 Set Tactic Name: ampere_h16816cudnn_128x128_ldg8_relu_exp_small_nhwc_tn_v1 Tactic: 0xb33e57fb3e8a0a56
[08/10/2023-11:17:20] [V] [TRT] Tactic: 0xb33e57fb3e8a0a56 Time: 0.465906
[08/10/2023-11:17:20] [V] [TRT] Conv_46 + Relu_47 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x64x32_stage5_warpsize2x2x1_g1_tensor16x8x16_t1r3s3 Tactic: 0xb4bec086187edcfc
[08/10/2023-11:17:20] [V] [TRT] Tactic: 0xb4bec086187edcfc Time: 0.535461
[08/10/2023-11:17:20] [V] [TRT] Conv_46 + Relu_47 Set Tactic Name: ampere_h16816cudnn_256x64_ldg8_relu_exp_stages_64x3_small_nhwc_tn_v1 Tactic: 0xb6fa5ffcc1a9d518
[08/10/2023-11:17:20] [V] [TRT] Tactic: 0xb6fa5ffcc1a9d518 Time: 0.515067
[08/10/2023-11:17:20] [V] [TRT] Conv_46 + Relu_47 Set Tactic Name: ampere_h16816cudnn_64x128_sliced1x2_ldg8_relu_exp_stages_64x3_medium_nhwc_tn_v1 Tactic: 0xb85e52e87caf60a2
[08/10/2023-11:17:20] [V] [TRT] Tactic: 0xb85e52e87caf60a2 Time: 0.548649
[08/10/2023-11:17:20] [V] [TRT] Conv_46 + Relu_47 Set Tactic Name: ampere_h16816cudnn_64x128_ldg8_relu_exp_stages_64x3_medium_nhwc_tn_v1 Tactic: 0xb8d86216e1235cda
[08/10/2023-11:17:20] [V] [TRT] Tactic: 0xb8d86216e1235cda Time: 0.575337
[08/10/2023-11:17:20] [V] [TRT] Conv_46 + Relu_47 Set Tactic Name: ampere_h16816cudnn_256x128_ldg8_relu_exp_stages_64x3_medium_nhwc_tn_v1 Tactic: 0xb9171d70ba2eda4b
[08/10/2023-11:17:20] [V] [TRT] Tactic: 0xb9171d70ba2eda4b Time: 0.482295
[08/10/2023-11:17:20] [V] [TRT] Conv_46 + Relu_47 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x64x32_stage5_warpsize2x2x1_g1_tensor16x8x16 Tactic: 0xbd08239a9317f2fd
[08/10/2023-11:17:20] [V] [TRT] Tactic: 0xbd08239a9317f2fd Time: 0.542299
[08/10/2023-11:17:20] [V] [TRT] Conv_46 + Relu_47 Set Tactic Name: ampere_h16816cudnn_64x128_sliced1x2_ldg8_relu_exp_stages_64x4_small_nhwc_tn_v1 Tactic: 0xbd6f5e6f24c05c10
[08/10/2023-11:17:20] [V] [TRT] Tactic: 0xbd6f5e6f24c05c10 Time: 0.571657
[08/10/2023-11:17:20] [V] [TRT] Conv_46 + Relu_47 Set Tactic Name: ampere_h16816cudnn_64x64_sliced1x2_ldg8_relu_exp_stages_64x6_large_nhwc_tn_v1 Tactic: 0xbeaee7eaad288322
[08/10/2023-11:17:20] [V] [TRT] Tactic: 0xbeaee7eaad288322 Time: 0.830651
[08/10/2023-11:17:20] [V] [TRT] Conv_46 + Relu_47 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x128x64_stage3_warpsize2x2x1_g1_tensor16x8x16_t1r3s3 Tactic: 0xc0a02dc6095497cc
[08/10/2023-11:17:20] [V] [TRT] Tactic: 0xc0a02dc6095497cc Time: 0.47877
[08/10/2023-11:17:20] [V] [TRT] Conv_46 + Relu_47 Set Tactic Name: ampere_h16816cudnn_128x128_ldg8_relu_exp_stages_64x3_large_nhwc_tn_v1 Tactic: 0xc2cf926c41243630
[08/10/2023-11:17:20] [V] [TRT] Tactic: 0xc2cf926c41243630 Time: 0.507031
[08/10/2023-11:17:20] [V] [TRT] Conv_46 + Relu_47 Set Tactic Name: ampere_h16816cudnn_128x64_ldg8_relu_exp_stages_64x3_small_nhwc_tn_v1 Tactic: 0xc338d2482cee77f8
[08/10/2023-11:17:20] [V] [TRT] Tactic: 0xc338d2482cee77f8 Time: 0.561509
[08/10/2023-11:17:20] [V] [TRT] Conv_46 + Relu_47 Set Tactic Name: ampere_h1688cudnn_256x128_ldg8_relu_exp_small_nhwc_tn_v1 Tactic: 0xc660e51970bc5a3a
[08/10/2023-11:17:20] [V] [TRT] Tactic: 0xc660e51970bc5a3a Time: 0.570789
[08/10/2023-11:17:20] [V] [TRT] Conv_46 + Relu_47 Set Tactic Name: ampere_h16816cudnn_64x128_ldg8_relu_exp_stages_64x4_medium_nhwc_tn_v1 Tactic: 0xc82f3f06140e3cbb
[08/10/2023-11:17:20] [V] [TRT] Tactic: 0xc82f3f06140e3cbb Time: 0.587872
[08/10/2023-11:17:20] [V] [TRT] Conv_46 + Relu_47 Set Tactic Name: ampere_h16816cudnn_64x128_sliced1x2_ldg8_relu_exp_stages_64x4_medium_nhwc_tn_v1 Tactic: 0xc8a90ff8898200c3
[08/10/2023-11:17:20] [V] [TRT] Tactic: 0xc8a90ff8898200c3 Time: 0.572279
[08/10/2023-11:17:20] [V] [TRT] Conv_46 + Relu_47 Set Tactic Name: ampere_h1688cudnn_256x64_sliced1x2_ldg8_relu_exp_medium_nhwc_tn_v1 Tactic: 0xc9cc55109bb4de26
[08/10/2023-11:17:20] [V] [TRT] Tactic: 0xc9cc55109bb4de26 Time: 0.774688
[08/10/2023-11:17:20] [V] [TRT] Conv_46 + Relu_47 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x64x32_stage5_warpsize2x2x1_g1_tensor16x8x16 Tactic: 0xc9d24bd069159fa8
[08/10/2023-11:17:20] [V] [TRT] Tactic: 0xc9d24bd069159fa8 Time: 0.57301
[08/10/2023-11:17:20] [V] [TRT] Conv_46 + Relu_47 Set Tactic Name: ampere_h16816cudnn_64x128_ldg8_relu_exp_stages_64x3_large_nhwc_tn_v1 Tactic: 0xc9f0a7bec963ba66
[08/10/2023-11:17:20] [V] [TRT] Tactic: 0xc9f0a7bec963ba66 Time: 0.586551
[08/10/2023-11:17:20] [V] [TRT] Conv_46 + Relu_47 Set Tactic Name: ampere_h16816cudnn_256x64_ldg8_relu_exp_small_nhwc_tn_v1 Tactic: 0xca5d3a11fd48f571
[08/10/2023-11:17:20] [V] [TRT] Tactic: 0xca5d3a11fd48f571 Time: 0.556133
[08/10/2023-11:17:20] [V] [TRT] Conv_46 + Relu_47 Set Tactic Name: ampere_h16816cudnn_64x64_ldg8_relu_exp_stages_64x5_small_nhwc_tn_v1 Tactic: 0xce0506e1512285c3
[08/10/2023-11:17:20] [V] [TRT] Tactic: 0xce0506e1512285c3 Time: 0.756466
[08/10/2023-11:17:20] [V] [TRT] Conv_46 + Relu_47 Set Tactic Name: ampere_h1688cudnn_256x64_sliced1x2_ldg8_relu_exp_large_nhwc_tn_v1 Tactic: 0xd0a3e0c815f7fb5e
[08/10/2023-11:17:20] [V] [TRT] Tactic: 0xd0a3e0c815f7fb5e Time: 0.769134
[08/10/2023-11:17:20] [V] [TRT] Conv_46 + Relu_47 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x64x32_stage3_warpsize4x1x1_g1_tensor16x8x16 Tactic: 0xd1aaad17ca35fbaa
[08/10/2023-11:17:20] [V] [TRT] Tactic: 0xd1aaad17ca35fbaa Time: 0.471566
[08/10/2023-11:17:20] [V] [TRT] Conv_46 + Relu_47 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x128x64_stage3_warpsize4x2x1_g1_tensor16x8x16_t1r3s3 Tactic: 0xd2fca0486ca97266
[08/10/2023-11:17:20] [V] [TRT] Tactic: 0xd2fca0486ca97266 Time: 0.469234
[08/10/2023-11:17:20] [V] [TRT] Conv_46 + Relu_47 Set Tactic Name: ampere_h16816cudnn_64x128_sliced1x2_ldg8_relu_exp_stages_64x3_small_nhwc_tn_v1 Tactic: 0xd58ea0bdedb89ead
[08/10/2023-11:17:20] [V] [TRT] Tactic: 0xd58ea0bdedb89ead Time: 0.553317
[08/10/2023-11:17:20] [V] [TRT] Conv_46 + Relu_47 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x256x64_stage3_warpsize1x4x1_g1_tensor16x8x16_t1r3s3 Tactic: 0xd7c261fa01db2af9
[08/10/2023-11:17:20] [V] [TRT] Tactic: 0xd7c261fa01db2af9 Time: 0.515712
[08/10/2023-11:17:20] [V] [TRT] Conv_46 + Relu_47 Set Tactic Name: ampere_h16816cudnn_256x64_sliced1x2_ldg8_relu_exp_large_nhwc_tn_v1 Tactic: 0xd8eb41ee35e76575
[08/10/2023-11:17:20] [V] [TRT] Tactic: 0xd8eb41ee35e76575 Time: 0.730469
[08/10/2023-11:17:20] [V] [TRT] Conv_46 + Relu_47 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x128x32_stage5_warpsize2x2x1_g1_tensor16x8x16 Tactic: 0xdb0b80f591d1bb6d
[08/10/2023-11:17:20] [V] [TRT] Tactic: 0xdb0b80f591d1bb6d Time: 0.532443
[08/10/2023-11:17:20] [V] [TRT] Conv_46 + Relu_47 Set Tactic Name: ampere_h16816cudnn_256x128_ldg8_relu_exp_large_nhwc_tn_v1 Tactic: 0xdc796d70e228a1d4
[08/10/2023-11:17:20] [V] [TRT] Tactic: 0xdc796d70e228a1d4 Time: 0.505147
[08/10/2023-11:17:20] [V] [TRT] Conv_46 + Relu_47 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x32x32_stage4_warpsize4x1x1_g1_tensor16x8x16 Tactic: 0xdce100b9fe609424
[08/10/2023-11:17:20] [V] [TRT] Tactic: 0xdce100b9fe609424 Time: 0.768521
[08/10/2023-11:17:20] [V] [TRT] Conv_46 + Relu_47 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x128x32_stage4_warpsize2x2x1_g1_tensor16x8x16 Tactic: 0xdfa020ef435ef810
[08/10/2023-11:17:20] [V] [TRT] Tactic: 0xdfa020ef435ef810 Time: 0.444306
[08/10/2023-11:17:20] [V] [TRT] Conv_46 + Relu_47 Set Tactic Name: ampere_h1688cudnn_256x64_ldg8_relu_exp_large_nhwc_tn_v1 Tactic: 0xe0e3c0e8cf9a2d9e
[08/10/2023-11:17:20] [V] [TRT] Tactic: 0xe0e3c0e8cf9a2d9e Time: 0.753842
[08/10/2023-11:17:20] [V] [TRT] Conv_46 + Relu_47 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x32x64_stage5_warpsize2x2x1_g1_tensor16x8x16 Tactic: 0xe1ff5ad20f5c6bf6
[08/10/2023-11:17:20] [V] [TRT] Tactic: 0xe1ff5ad20f5c6bf6 Time: 1.01051
[08/10/2023-11:17:20] [V] [TRT] Conv_46 + Relu_47 Set Tactic Name: ampere_h16816cudnn_256x64_ldg8_relu_exp_medium_nhwc_tn_v1 Tactic: 0xe4711898bd599c36
[08/10/2023-11:17:20] [V] [TRT] Tactic: 0xe4711898bd599c36 Time: 0.564626
[08/10/2023-11:17:20] [V] [TRT] Conv_46 + Relu_47 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x64x64_stage3_warpsize4x1x1_g1_tensor16x8x16 Tactic: 0xeb25062ffb9eae45
[08/10/2023-11:17:20] [V] [TRT] Tactic: 0xeb25062ffb9eae45 Time: 0.530761
[08/10/2023-11:17:20] [V] [TRT] Conv_46 + Relu_47 Set Tactic Name: ampere_h16816cudnn_128x64_sliced1x2_ldg8_relu_exp_stages_64x4_large_nhwc_tn_v1 Tactic: 0xeb2d2aa4e56bb41c
[08/10/2023-11:17:20] [V] [TRT] Tactic: 0xeb2d2aa4e56bb41c Time: 0.610277
[08/10/2023-11:17:20] [V] [TRT] Conv_46 + Relu_47 Set Tactic Name: ampere_h16816cudnn_64x64_ldg8_relu_exp_stages_64x6_small_nhwc_tn_v1 Tactic: 0xf0beb09df9a19f82
[08/10/2023-11:17:20] [V] [TRT] Tactic: 0xf0beb09df9a19f82 Time: 0.80192
[08/10/2023-11:17:20] [V] [TRT] Conv_46 + Relu_47 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x64x64_stage4_warpsize2x2x1_g1_tensor16x8x16 Tactic: 0xf35e0311fa1cc516
[08/10/2023-11:17:20] [V] [TRT] Tactic: 0xf35e0311fa1cc516 Time: 0.691305
[08/10/2023-11:17:20] [V] [TRT] Conv_46 + Relu_47 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x32x32_stage4_warpsize4x1x1_g1_tensor16x8x16 Tactic: 0xf79479a62ea9f901
[08/10/2023-11:17:20] [V] [TRT] Tactic: 0xf79479a62ea9f901 Time: 0.854551
[08/10/2023-11:17:20] [V] [TRT] Fastest Tactic: 0xdfa020ef435ef810 Time: 0.444306
[08/10/2023-11:17:20] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0xdfa020ef435ef810
[08/10/2023-11:17:20] [V] [TRT] *************** Autotuning format combination: Half(129600,1:16,1440,16) -> Half(129600,1:16,1440,16) ***************
[08/10/2023-11:17:20] [W] [TRT] Weights [name=Conv_46 + Relu_47.weight] had the following issues when converted to FP16:
[08/10/2023-11:17:20] [W] [TRT]  - Subnormal FP16 values detected. 
[08/10/2023-11:17:20] [W] [TRT]  - Values less than smallest positive FP16 Subnormal value detected. Converting to FP16 minimum subnormalized value. 
[08/10/2023-11:17:20] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.
[08/10/2023-11:17:20] [V] [TRT] --------------- Timing Runner: Conv_46 + Relu_47 (CaskConvolution)
[08/10/2023-11:17:20] [V] [TRT] Conv_46 + Relu_47 Set Tactic Name: ampere_h16816cudnn_256x128_ldg8_relu_exp_medium_nhwc_tn_v1 Tactic: 0x03896956a39a1203
[08/10/2023-11:17:20] [V] [TRT] Tactic: 0x03896956a39a1203 Time: 0.510281
[08/10/2023-11:17:20] [V] [TRT] Conv_46 + Relu_47 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x128x32_stage3_warpsize4x2x1_g1_tensor16x8x16_t1r3s3 Tactic: 0x048d6d0400f33439
[08/10/2023-11:17:20] [V] [TRT] Tactic: 0x048d6d0400f33439 Time: 0.481083
[08/10/2023-11:17:20] [V] [TRT] Conv_46 + Relu_47 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x64x64_stage3_warpsize2x2x1_g1_tensor16x8x16_t1r3s3 Tactic: 0x05b6220f243edacd
[08/10/2023-11:17:20] [V] [TRT] Tactic: 0x05b6220f243edacd Time: 0.551877
[08/10/2023-11:17:20] [V] [TRT] Conv_46 + Relu_47 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x64x32_stage3_warpsize4x1x1_g1_tensor16x8x16_t1r3s3_aACCESS Tactic: 0x0866ddee325d07a6
[08/10/2023-11:17:20] [V] [TRT] Tactic: 0x0866ddee325d07a6 Time: 0.510258
[08/10/2023-11:17:20] [V] [TRT] Conv_46 + Relu_47 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x128x32_stage4_warpsize2x2x1_g1_tensor16x8x16 Tactic: 0x0e07ff4f4f7c1ac9
[08/10/2023-11:17:20] [V] [TRT] Tactic: 0x0e07ff4f4f7c1ac9 Time: 0.492123
[08/10/2023-11:17:20] [V] [TRT] Conv_46 + Relu_47 Set Tactic Name: ampere_h16816cudnn_256x128_ldg8_relu_exp_stages_64x3_small_nhwc_tn_v1 Tactic: 0x0e199750c30c6928
[08/10/2023-11:17:20] [V] [TRT] Tactic: 0x0e199750c30c6928 Time: 0.491945
[08/10/2023-11:17:20] [V] [TRT] Conv_46 + Relu_47 Set Tactic Name: ampere_h1688cudnn_128x128_ldg8_relu_exp_medium_nhwc_tn_v1 Tactic: 0x100c1ad308e08d35
[08/10/2023-11:17:20] [V] [TRT] Tactic: 0x100c1ad308e08d35 Time: 0.615488
[08/10/2023-11:17:20] [V] [TRT] Conv_46 + Relu_47 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x128x32_stage5_warpsize2x2x1_g1_tensor16x8x16_t1r3s3 Tactic: 0x17ebf0c9f418f10a
[08/10/2023-11:17:20] [V] [TRT] Tactic: 0x17ebf0c9f418f10a Time: 0.531547
[08/10/2023-11:17:20] [V] [TRT] Conv_46 + Relu_47 Set Tactic Name: ampere_h16816cudnn_128x64_ldg8_relu_exp_stages_64x4_medium_nhwc_tn_v1 Tactic: 0x196cbc423a9bb69e
[08/10/2023-11:17:20] [V] [TRT] Tactic: 0x196cbc423a9bb69e Time: 0.617563
[08/10/2023-11:17:20] [V] [TRT] Conv_46 + Relu_47 Set Tactic Name: ampere_h16816cudnn_128x64_sliced1x2_ldg8_relu_exp_stages_64x3_small_nhwc_tn_v1 Tactic: 0x19822de884f42a6a
[08/10/2023-11:17:20] [V] [TRT] Tactic: 0x19822de884f42a6a Time: 0.524512
[08/10/2023-11:17:20] [V] [TRT] Conv_46 + Relu_47 Set Tactic Name: ampere_h16816cudnn_64x64_sliced1x2_ldg8_relu_exp_stages_64x5_small_nhwc_tn_v1 Tactic: 0x1a5ba808ad4cc5a8
[08/10/2023-11:17:20] [V] [TRT] Tactic: 0x1a5ba808ad4cc5a8 Time: 0.688722
[08/10/2023-11:17:20] [V] [TRT] Conv_46 + Relu_47 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x64x64_stage3_warpsize2x2x1_g1_tensor16x8x16 Tactic: 0x21b295c0c8f6c95a
[08/10/2023-11:17:20] [V] [TRT] Tactic: 0x21b295c0c8f6c95a Time: 0.551223
[08/10/2023-11:17:20] [V] [TRT] Conv_46 + Relu_47 Set Tactic Name: ampere_h16816cudnn_128x128_ldg8_relu_exp_medium_nhwc_tn_v1 Tactic: 0x234580e8a194335c
[08/10/2023-11:17:20] [V] [TRT] Tactic: 0x234580e8a194335c Time: 0.472791
[08/10/2023-11:17:20] [V] [TRT] Conv_46 + Relu_47 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x128x32_stage4_warpsize2x2x1_g1_tensor16x8x16_t1r3s3_aACCESS Tactic: 0x245530c34bd6090f
[08/10/2023-11:17:20] [V] [TRT] Tactic: 0x245530c34bd6090f Time: 0.447776
[08/10/2023-11:17:20] [V] [TRT] Conv_46 + Relu_47 Set Tactic Name: ampere_h16816cudnn_64x64_sliced1x2_ldg8_relu_exp_stages_64x6_small_nhwc_tn_v1 Tactic: 0x24e01e7405cfdfe9
[08/10/2023-11:17:20] [V] [TRT] Tactic: 0x24e01e7405cfdfe9 Time: 0.749783
[08/10/2023-11:17:20] [V] [TRT] Conv_46 + Relu_47 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x64x64_stage3_warpsize2x2x1_g1_tensor16x8x16_t1r3s3_aACCESS Tactic: 0x263a38afd75e3a43
[08/10/2023-11:17:20] [V] [TRT] Tactic: 0x263a38afd75e3a43 Time: 0.49664
[08/10/2023-11:17:20] [V] [TRT] Conv_46 + Relu_47 Set Tactic Name: ampere_h16816cudnn_64x128_sliced1x2_ldg8_relu_exp_stages_64x4_large_nhwc_tn_v1 Tactic: 0x2721a7f18c2700db
[08/10/2023-11:17:20] [V] [TRT] Tactic: 0x2721a7f18c2700db Time: 0.593787
[08/10/2023-11:17:20] [V] [TRT] Conv_46 + Relu_47 Set Tactic Name: ampere_h16816cudnn_128x64_sliced1x2_ldg8_relu_exp_stages_64x3_medium_nhwc_tn_v1 Tactic: 0x29329b5741ea05f2
[08/10/2023-11:17:20] [V] [TRT] Tactic: 0x29329b5741ea05f2 Time: 0.530738
[08/10/2023-11:17:20] [V] [TRT] Conv_46 + Relu_47 Set Tactic Name: ampere_h16816cudnn_128x128_ldg8_relu_exp_large_nhwc_tn_v1 Tactic: 0x2970ae65966d569d
[08/10/2023-11:17:20] [V] [TRT] Tactic: 0x2970ae65966d569d Time: 0.474432
[08/10/2023-11:17:20] [V] [TRT] Conv_46 + Relu_47 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x256x64_stage3_warpsize1x4x1_g1_tensor16x8x16 Tactic: 0x29dc14520e0e4eb6
[08/10/2023-11:17:20] [V] [TRT] Tactic: 0x29dc14520e0e4eb6 Time: 0.523264
[08/10/2023-11:17:20] [V] [TRT] Conv_46 + Relu_47 Set Tactic Name: ampere_h16816cudnn_256x64_ldg8_relu_exp_stages_64x3_large_nhwc_tn_v1 Tactic: 0x2cb4a662694e89d3
[08/10/2023-11:17:20] [V] [TRT] Tactic: 0x2cb4a662694e89d3 Time: 0.518062
[08/10/2023-11:17:20] [V] [TRT] Conv_46 + Relu_47 Set Tactic Name: ampere_h16816cudnn_128x128_ldg8_relu_exp_stages_64x3_medium_nhwc_tn_v1 Tactic: 0x2eaa2202de9404d6
[08/10/2023-11:17:20] [V] [TRT] Tactic: 0x2eaa2202de9404d6 Time: 0.512206
[08/10/2023-11:17:20] [V] [TRT] Conv_46 + Relu_47 Set Tactic Name: ampere_h1688cudnn_256x128_ldg8_relu_exp_medium_nhwc_tn_v1 Tactic: 0x30c0f36d0aeeac6a
[08/10/2023-11:17:20] [V] [TRT] Tactic: 0x30c0f36d0aeeac6a Time: 0.612841
[08/10/2023-11:17:20] [V] [TRT] Conv_46 + Relu_47 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x64x64_stage4_warpsize2x2x1_g1_tensor16x8x16_t1r3s3_aACCESS Tactic: 0x30e8a8d7a953e5e9
[08/10/2023-11:17:20] [V] [TRT] Tactic: 0x30e8a8d7a953e5e9 Time: 0.587447
[08/10/2023-11:17:20] [V] [TRT] Conv_46 + Relu_47 Set Tactic Name: ampere_h16816cudnn_128x64_ldg8_relu_exp_stages_64x4_large_nhwc_tn_v1 Tactic: 0x3197d5044d71e98e
[08/10/2023-11:17:20] [V] [TRT] Tactic: 0x3197d5044d71e98e Time: 0.646542
[08/10/2023-11:17:20] [V] [TRT] Conv_46 + Relu_47 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x64x64_stage4_warpsize2x2x1_g1_tensor16x8x16_t1r3s3 Tactic: 0x31d93dc22d2af081
[08/10/2023-11:17:20] [V] [TRT] Tactic: 0x31d93dc22d2af081 Time: 0.704448
[08/10/2023-11:17:20] [V] [TRT] Conv_46 + Relu_47 Set Tactic Name: ampere_h1688cudnn_128x128_ldg8_relu_exp_small_nhwc_tn_v1 Tactic: 0x3369260c04f9ad73
[08/10/2023-11:17:20] [V] [TRT] Tactic: 0x3369260c04f9ad73 Time: 0.588773
[08/10/2023-11:17:20] [V] [TRT] Conv_46 + Relu_47 Set Tactic Name: ampere_h16816cudnn_64x128_ldg8_relu_exp_stages_64x4_small_nhwc_tn_v1 Tactic: 0x3b5fa0f2a8fc2410
[08/10/2023-11:17:20] [V] [TRT] Tactic: 0x3b5fa0f2a8fc2410 Time: 0.599525
[08/10/2023-11:17:20] [V] [TRT] Conv_46 + Relu_47 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x32x32_stage4_warpsize4x1x1_g1_tensor16x8x16_t1r3s3 Tactic: 0x3e7eb35b91b9fa63
[08/10/2023-11:17:20] [V] [TRT] Tactic: 0x3e7eb35b91b9fa63 Time: 0.869275
[08/10/2023-11:17:20] [V] [TRT] Conv_46 + Relu_47 Set Tactic Name: ampere_h16816cudnn_256x64_ldg8_relu_exp_stages_64x3_medium_nhwc_tn_v1 Tactic: 0x3f031df0e579d52c
[08/10/2023-11:17:20] [V] [TRT] Tactic: 0x3f031df0e579d52c Time: 0.523758
[08/10/2023-11:17:20] [V] [TRT] Conv_46 + Relu_47 Set Tactic Name: ampere_h16816cudnn_256x64_sliced1x2_ldg8_relu_exp_small_nhwc_tn_v1 Tactic: 0x42a5b8709d0039be
[08/10/2023-11:17:20] [V] [TRT] Tactic: 0x42a5b8709d0039be Time: 0.632855
[08/10/2023-11:17:20] [V] [TRT] Conv_46 + Relu_47 Set Tactic Name: ampere_h16816cudnn_256x128_ldg8_relu_exp_small_nhwc_tn_v1 Tactic: 0x463794ee4acffd1f
[08/10/2023-11:17:20] [V] [TRT] Tactic: 0x463794ee4acffd1f Time: 0.496859
[08/10/2023-11:17:20] [V] [TRT] Conv_46 + Relu_47 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x64x64_stage4_warpsize2x2x1_g1_tensor16x8x16 Tactic: 0x4a81ea1e51436a30
[08/10/2023-11:17:20] [V] [TRT] Tactic: 0x4a81ea1e51436a30 Time: 0.695255
[08/10/2023-11:17:20] [V] [TRT] Conv_46 + Relu_47 Set Tactic Name: ampere_h1688cudnn_256x64_sliced1x2_ldg8_relu_exp_small_nhwc_tn_v1 Tactic: 0x4aed1956bd10a795
[08/10/2023-11:17:20] [V] [TRT] Tactic: 0x4aed1956bd10a795 Time: 0.694478
[08/10/2023-11:17:20] [V] [TRT] Conv_46 + Relu_47 Set Tactic Name: ampere_h16816cudnn_64x128_sliced1x2_ldg8_relu_exp_stages_64x3_large_nhwc_tn_v1 Tactic: 0x4fc05923455fc266
[08/10/2023-11:17:20] [V] [TRT] Tactic: 0x4fc05923455fc266 Time: 0.555227
[08/10/2023-11:17:20] [V] [TRT] Conv_46 + Relu_47 Set Tactic Name: ampere_h16816cudnn_256x64_ldg8_relu_exp_large_nhwc_tn_v1 Tactic: 0x5013c38f55afa9ba
[08/10/2023-11:17:20] [V] [TRT] Tactic: 0x5013c38f55afa9ba Time: 0.563282
[08/10/2023-11:17:20] [V] [TRT] Conv_46 + Relu_47 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x64x32_stage3_warpsize4x1x1_g1_tensor16x8x16_t1r3s3 Tactic: 0x529f4431bdae94f5
[08/10/2023-11:17:20] [V] [TRT] Tactic: 0x529f4431bdae94f5 Time: 0.47184
[08/10/2023-11:17:20] [V] [TRT] Conv_46 + Relu_47 Set Tactic Name: ampere_h16816cudnn_64x128_ldg8_relu_exp_stages_64x3_small_nhwc_tn_v1 Tactic: 0x53be5e206184e6ad
[08/10/2023-11:17:20] [V] [TRT] Tactic: 0x53be5e206184e6ad Time: 0.58032
[08/10/2023-11:17:20] [V] [TRT] Conv_46 + Relu_47 Set Tactic Name: ampere_h16816cudnn_64x64_ldg8_relu_exp_stages_64x5_large_nhwc_tn_v1 Tactic: 0x544bff7ff9c5d908
[08/10/2023-11:17:20] [V] [TRT] Tactic: 0x544bff7ff9c5d908 Time: 0.748978
[08/10/2023-11:17:20] [V] [TRT] Conv_46 + Relu_47 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x64x64_stage3_warpsize2x2x1_g1_tensor16x8x16 Tactic: 0x5568fd8a32f4a40f
[08/10/2023-11:17:20] [V] [TRT] Tactic: 0x5568fd8a32f4a40f Time: 0.542222
[08/10/2023-11:17:20] [V] [TRT] Conv_46 + Relu_47 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x128x64_stage3_warpsize2x2x1_g1_tensor16x8x16 Tactic: 0x57c9a5ff682354a6
[08/10/2023-11:17:20] [V] [TRT] Tactic: 0x57c9a5ff682354a6 Time: 0.485317
[08/10/2023-11:17:20] [V] [TRT] Conv_46 + Relu_47 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x32x64_stage5_warpsize2x2x1_g1_tensor16x8x16 Tactic: 0x5820b3dda403c4d0
[08/10/2023-11:17:21] [V] [TRT] Tactic: 0x5820b3dda403c4d0 Time: 1.01369
[08/10/2023-11:17:21] [V] [TRT] Conv_46 + Relu_47 Set Tactic Name: ampere_h16816cudnn_128x128_ldg8_relu_exp_stages_64x3_small_nhwc_tn_v1 Tactic: 0x58816bf2e9c36afb
[08/10/2023-11:17:21] [V] [TRT] Tactic: 0x58816bf2e9c36afb Time: 0.513152
[08/10/2023-11:17:21] [V] [TRT] Conv_46 + Relu_47 Set Tactic Name: ampere_h16816cudnn_128x64_ldg8_relu_exp_stages_64x3_large_nhwc_tn_v1 Tactic: 0x59762bd684092b33
[08/10/2023-11:17:21] [V] [TRT] Tactic: 0x59762bd684092b33 Time: 0.563657
[08/10/2023-11:17:21] [V] [TRT] Conv_46 + Relu_47 Set Tactic Name: ampere_h16816cudnn_128x64_sliced1x2_ldg8_relu_exp_stages_64x4_medium_nhwc_tn_v1 Tactic: 0x59c5c647b4c76593
[08/10/2023-11:17:21] [V] [TRT] Tactic: 0x59c5c647b4c76593 Time: 0.58912
[08/10/2023-11:17:21] [V] [TRT] Conv_46 + Relu_47 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x128x64_stage3_warpsize4x2x1_g1_tensor16x8x16 Tactic: 0x5a55274960724ba8
[08/10/2023-11:17:21] [V] [TRT] Tactic: 0x5a55274960724ba8 Time: 0.466661
[08/10/2023-11:17:21] [V] [TRT] Conv_46 + Relu_47 Set Tactic Name: ampere_h1688cudnn_256x128_ldg8_relu_exp_large_nhwc_tn_v1 Tactic: 0x5c2e1c87d85b06f1
[08/10/2023-11:17:21] [V] [TRT] Tactic: 0x5c2e1c87d85b06f1 Time: 0.618473
[08/10/2023-11:17:21] [V] [TRT] Conv_46 + Relu_47 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x256x64_stage3_warpsize1x4x1_g1_tensor16x8x16 Tactic: 0x5d067c18f40c23e3
[08/10/2023-11:17:21] [V] [TRT] Tactic: 0x5d067c18f40c23e3 Time: 0.518217
[08/10/2023-11:17:21] [V] [TRT] Conv_46 + Relu_47 Set Tactic Name: ampere_h16816cudnn_64x64_ldg8_relu_exp_stages_64x5_medium_nhwc_tn_v1 Tactic: 0x5f31c22ec167f384
[08/10/2023-11:17:21] [V] [TRT] Tactic: 0x5f31c22ec167f384 Time: 0.744137
[08/10/2023-11:17:21] [V] [TRT] Conv_46 + Relu_47 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x128x32_stage4_warpsize2x2x1_g1_tensor16x8x16_t1r3s3 Tactic: 0x60c3421152ef8e10
[08/10/2023-11:17:21] [V] [TRT] Tactic: 0x60c3421152ef8e10 Time: 0.443735
[08/10/2023-11:17:21] [V] [TRT] Conv_46 + Relu_47 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x128x32_stage3_warpsize4x2x1_g1_tensor16x8x16 Tactic: 0x60da8c7151d91e47
[08/10/2023-11:17:21] [V] [TRT] Tactic: 0x60da8c7151d91e47 Time: 0.471218
[08/10/2023-11:17:21] [V] [TRT] Conv_46 + Relu_47 Set Tactic Name: ampere_h1688cudnn_256x64_ldg8_relu_exp_medium_nhwc_tn_v1 Tactic: 0x6426696f872a3b13
[08/10/2023-11:17:21] [V] [TRT] Tactic: 0x6426696f872a3b13 Time: 0.765179
[08/10/2023-11:17:21] [V] [TRT] Conv_46 + Relu_47 Set Tactic Name: ampere_h16816cudnn_128x64_ldg8_relu_exp_stages_64x3_medium_nhwc_tn_v1 Tactic: 0x699be152cfb6d6ff
[08/10/2023-11:17:21] [V] [TRT] Tactic: 0x699be152cfb6d6ff Time: 0.578016
[08/10/2023-11:17:21] [V] [TRT] Conv_46 + Relu_47 Set Tactic Name: ampere_h16816cudnn_64x64_ldg8_relu_exp_stages_64x6_large_nhwc_tn_v1 Tactic: 0x6af049035146c349
[08/10/2023-11:17:21] [V] [TRT] Tactic: 0x6af049035146c349 Time: 0.894926
[08/10/2023-11:17:21] [V] [TRT] Conv_46 + Relu_47 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x256x32_stage3_warpsize2x4x1_g1_tensor16x8x16 Tactic: 0x7005d10718f6c22d
[08/10/2023-11:17:21] [V] [TRT] Tactic: 0x7005d10718f6c22d Time: 0.488805
[08/10/2023-11:17:21] [V] [TRT] Conv_46 + Relu_47 Set Tactic Name: ampere_h16816cudnn_64x64_sliced1x2_ldg8_relu_exp_stages_64x5_medium_nhwc_tn_v1 Tactic: 0x706f08da35c795a5
[08/10/2023-11:17:21] [V] [TRT] Tactic: 0x706f08da35c795a5 Time: 0.719159
[08/10/2023-11:17:21] [V] [TRT] Conv_46 + Relu_47 Set Tactic Name: ampere_h16816cudnn_128x64_sliced1x2_ldg8_relu_exp_stages_64x4_small_nhwc_tn_v1 Tactic: 0x7163d33a4d8ce8d7
[08/10/2023-11:17:21] [V] [TRT] Tactic: 0x7163d33a4d8ce8d7 Time: 0.578647
[08/10/2023-11:17:21] [V] [TRT] Conv_46 + Relu_47 Set Tactic Name: ampere_h1688cudnn_256x64_ldg8_relu_exp_small_nhwc_tn_v1 Tactic: 0x7aad3976677d7155
[08/10/2023-11:17:21] [V] [TRT] Tactic: 0x7aad3976677d7155 Time: 0.691104
[08/10/2023-11:17:21] [V] [TRT] Conv_46 + Relu_47 Set Tactic Name: ampere_h16816cudnn_64x64_sliced1x2_ldg8_relu_exp_stages_64x5_large_nhwc_tn_v1 Tactic: 0x8015519605ab9963
[08/10/2023-11:17:21] [V] [TRT] Tactic: 0x8015519605ab9963 Time: 0.703689
[08/10/2023-11:17:21] [V] [TRT] Conv_46 + Relu_47 Set Tactic Name: ampere_h16816cudnn_256x64_sliced1x2_ldg8_relu_exp_medium_nhwc_tn_v1 Tactic: 0x82f8a2214b0b4178
[08/10/2023-11:17:21] [V] [TRT] Tactic: 0x82f8a2214b0b4178 Time: 0.719013
[08/10/2023-11:17:21] [V] [TRT] Conv_46 + Relu_47 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x32x32_stage4_warpsize4x1x1_g1_tensor16x8x16 Tactic: 0x834e11ecd4ab9454
[08/10/2023-11:17:21] [V] [TRT] Tactic: 0x834e11ecd4ab9454 Time: 0.850784
[08/10/2023-11:17:21] [V] [TRT] Conv_46 + Relu_47 Set Tactic Name: ampere_h16816cudnn_128x64_sliced1x2_ldg8_relu_exp_stages_64x3_large_nhwc_tn_v1 Tactic: 0x83ccd4762c1376a1
[08/10/2023-11:17:21] [V] [TRT] Tactic: 0x83ccd4762c1376a1 Time: 0.536846
[08/10/2023-11:17:21] [V] [TRT] Conv_46 + Relu_47 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x64x64_stage3_warpsize4x1x1_g1_tensor16x8x16_t1r3s3 Tactic: 0x84ee897dd1f4d2aa
[08/10/2023-11:17:21] [V] [TRT] Tactic: 0x84ee897dd1f4d2aa Time: 0.525445
[08/10/2023-11:17:21] [V] [TRT] Conv_46 + Relu_47 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x128x64_stage3_warpsize2x2x1_g1_tensor16x8x16 Tactic: 0x866e7a5f6401b67f
[08/10/2023-11:17:21] [V] [TRT] Tactic: 0x866e7a5f6401b67f Time: 0.484942
[08/10/2023-11:17:21] [V] [TRT] Conv_46 + Relu_47 Set Tactic Name: ampere_h16816cudnn_64x64_ldg8_relu_exp_stages_64x6_medium_nhwc_tn_v1 Tactic: 0x88971eec55aba850
[08/10/2023-11:17:21] [V] [TRT] Tactic: 0x88971eec55aba850 Time: 0.85515
[08/10/2023-11:17:21] [V] [TRT] Conv_46 + Relu_47 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x128x64_stage3_warpsize4x2x1_g1_tensor16x8x16 Tactic: 0x8bf2f8e96c50a971
[08/10/2023-11:17:21] [V] [TRT] Tactic: 0x8bf2f8e96c50a971 Time: 0.468585
[08/10/2023-11:17:21] [V] [TRT] Conv_46 + Relu_47 Set Tactic Name: ampere_h16816cudnn_256x128_ldg8_relu_exp_stages_64x3_large_nhwc_tn_v1 Tactic: 0x94576ece6beb35e3
[08/10/2023-11:17:21] [V] [TRT] Tactic: 0x94576ece6beb35e3 Time: 0.486025
[08/10/2023-11:17:21] [V] [TRT] Conv_46 + Relu_47 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x64x64_stage3_warpsize4x1x1_g1_tensor16x8x16 Tactic: 0x9fff6e65019cc310
[08/10/2023-11:17:21] [V] [TRT] Tactic: 0x9fff6e65019cc310 Time: 0.532777
[08/10/2023-11:17:21] [V] [TRT] Conv_46 + Relu_47 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x32x64_stage5_warpsize2x2x1_g1_tensor16x8x16_t1r3s3 Tactic: 0xa033e20ae9f412b2
[08/10/2023-11:17:21] [V] [TRT] Tactic: 0xa033e20ae9f412b2 Time: 1.01056
[08/10/2023-11:17:21] [V] [TRT] Conv_46 + Relu_47 Set Tactic Name: ampere_h16816cudnn_64x128_ldg8_relu_exp_stages_64x4_large_nhwc_tn_v1 Tactic: 0xa111596c001b78db
[08/10/2023-11:17:21] [V] [TRT] Tactic: 0xa111596c001b78db Time: 0.591858
[08/10/2023-11:17:21] [V] [TRT] Conv_46 + Relu_47 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x256x32_stage3_warpsize2x4x1_g1_tensor16x8x16 Tactic: 0xa1a20ea714d420f4
[08/10/2023-11:17:21] [V] [TRT] Tactic: 0xa1a20ea714d420f4 Time: 0.469687
[08/10/2023-11:17:21] [V] [TRT] Conv_46 + Relu_47 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x32x32_stage4_warpsize4x1x1_g1_tensor16x8x16_t1r3s3 Tactic: 0xa40cb43c296a36a8
[08/10/2023-11:17:21] [V] [TRT] Tactic: 0xa40cb43c296a36a8 Time: 0.773307
[08/10/2023-11:17:21] [V] [TRT] Conv_46 + Relu_47 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x64x32_stage3_warpsize4x1x1_g1_tensor16x8x16 Tactic: 0xa570c55d303796ff
[08/10/2023-11:17:21] [V] [TRT] Tactic: 0xa570c55d303796ff Time: 0.507918
[08/10/2023-11:17:21] [V] [TRT] Conv_46 + Relu_47 Set Tactic Name: ampere_h16816cudnn_64x64_sliced1x2_ldg8_relu_exp_stages_64x6_medium_nhwc_tn_v1 Tactic: 0xa7c9d418a10bce71
[08/10/2023-11:17:21] [V] [TRT] Tactic: 0xa7c9d418a10bce71 Time: 0.854811
[08/10/2023-11:17:21] [V] [TRT] Conv_46 + Relu_47 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x32x32_stage4_warpsize4x1x1_g1_tensor16x8x16 Tactic: 0xa83b68f30462f971
[08/10/2023-11:17:21] [V] [TRT] Tactic: 0xa83b68f30462f971 Time: 0.809874
[08/10/2023-11:17:21] [V] [TRT] Conv_46 + Relu_47 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x256x32_stage3_warpsize2x4x1_g1_tensor16x8x16_t1r3s3 Tactic: 0xa9177bbe4e767df8
[08/10/2023-11:17:21] [V] [TRT] Tactic: 0xa9177bbe4e767df8 Time: 0.481655
[08/10/2023-11:17:21] [V] [TRT] Conv_46 + Relu_47 Set Tactic Name: ampere_h1688cudnn_128x128_ldg8_relu_exp_large_nhwc_tn_v1 Tactic: 0xa927df92ac1ef1b8
[08/10/2023-11:17:21] [V] [TRT] Tactic: 0xa927df92ac1ef1b8 Time: 0.61637
[08/10/2023-11:17:21] [V] [TRT] Conv_46 + Relu_47 Set Tactic Name: ampere_h16816cudnn_128x64_ldg8_relu_exp_stages_64x4_small_nhwc_tn_v1 Tactic: 0xabd92c9ae596b545
[08/10/2023-11:17:21] [V] [TRT] Tactic: 0xabd92c9ae596b545 Time: 0.620887
[08/10/2023-11:17:21] [V] [TRT] Conv_46 + Relu_47 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x128x32_stage5_warpsize2x2x1_g1_tensor16x8x16 Tactic: 0xafd1e8bf6bd3d638
[08/10/2023-11:17:21] [V] [TRT] Tactic: 0xafd1e8bf6bd3d638 Time: 0.537321
[08/10/2023-11:17:21] [V] [TRT] Conv_46 + Relu_47 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x128x32_stage3_warpsize4x2x1_g1_tensor16x8x16 Tactic: 0xb17d53d15dfbfc9e
[08/10/2023-11:17:21] [V] [TRT] Tactic: 0xb17d53d15dfbfc9e Time: 0.487054
[08/10/2023-11:17:21] [V] [TRT] Conv_46 + Relu_47 Set Tactic Name: ampere_h16816cudnn_128x128_ldg8_relu_exp_small_nhwc_tn_v1 Tactic: 0xb33e57fb3e8a0a56
[08/10/2023-11:17:21] [V] [TRT] Tactic: 0xb33e57fb3e8a0a56 Time: 0.466185
[08/10/2023-11:17:21] [V] [TRT] Conv_46 + Relu_47 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x64x32_stage5_warpsize2x2x1_g1_tensor16x8x16_t1r3s3 Tactic: 0xb4bec086187edcfc
[08/10/2023-11:17:21] [V] [TRT] Tactic: 0xb4bec086187edcfc Time: 0.536521
[08/10/2023-11:17:21] [V] [TRT] Conv_46 + Relu_47 Set Tactic Name: ampere_h16816cudnn_256x64_ldg8_relu_exp_stages_64x3_small_nhwc_tn_v1 Tactic: 0xb6fa5ffcc1a9d518
[08/10/2023-11:17:21] [V] [TRT] Tactic: 0xb6fa5ffcc1a9d518 Time: 0.517787
[08/10/2023-11:17:21] [V] [TRT] Conv_46 + Relu_47 Set Tactic Name: ampere_h16816cudnn_64x128_sliced1x2_ldg8_relu_exp_stages_64x3_medium_nhwc_tn_v1 Tactic: 0xb85e52e87caf60a2
[08/10/2023-11:17:21] [V] [TRT] Tactic: 0xb85e52e87caf60a2 Time: 0.549234
[08/10/2023-11:17:21] [V] [TRT] Conv_46 + Relu_47 Set Tactic Name: ampere_h16816cudnn_64x128_ldg8_relu_exp_stages_64x3_medium_nhwc_tn_v1 Tactic: 0xb8d86216e1235cda
[08/10/2023-11:17:21] [V] [TRT] Tactic: 0xb8d86216e1235cda Time: 0.579584
[08/10/2023-11:17:21] [V] [TRT] Conv_46 + Relu_47 Set Tactic Name: ampere_h16816cudnn_256x128_ldg8_relu_exp_stages_64x3_medium_nhwc_tn_v1 Tactic: 0xb9171d70ba2eda4b
[08/10/2023-11:17:21] [V] [TRT] Tactic: 0xb9171d70ba2eda4b Time: 0.486354
[08/10/2023-11:17:21] [V] [TRT] Conv_46 + Relu_47 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x64x32_stage5_warpsize2x2x1_g1_tensor16x8x16 Tactic: 0xbd08239a9317f2fd
[08/10/2023-11:17:21] [V] [TRT] Tactic: 0xbd08239a9317f2fd Time: 0.545408
[08/10/2023-11:17:21] [V] [TRT] Conv_46 + Relu_47 Set Tactic Name: ampere_h16816cudnn_64x128_sliced1x2_ldg8_relu_exp_stages_64x4_small_nhwc_tn_v1 Tactic: 0xbd6f5e6f24c05c10
[08/10/2023-11:17:21] [V] [TRT] Tactic: 0xbd6f5e6f24c05c10 Time: 0.56933
[08/10/2023-11:17:21] [V] [TRT] Conv_46 + Relu_47 Set Tactic Name: ampere_h16816cudnn_64x64_sliced1x2_ldg8_relu_exp_stages_64x6_large_nhwc_tn_v1 Tactic: 0xbeaee7eaad288322
[08/10/2023-11:17:21] [V] [TRT] Tactic: 0xbeaee7eaad288322 Time: 0.82987
[08/10/2023-11:17:21] [V] [TRT] Conv_46 + Relu_47 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x128x64_stage3_warpsize2x2x1_g1_tensor16x8x16_t1r3s3 Tactic: 0xc0a02dc6095497cc
[08/10/2023-11:17:21] [V] [TRT] Tactic: 0xc0a02dc6095497cc Time: 0.47979
[08/10/2023-11:17:21] [V] [TRT] Conv_46 + Relu_47 Set Tactic Name: ampere_h16816cudnn_128x128_ldg8_relu_exp_stages_64x3_large_nhwc_tn_v1 Tactic: 0xc2cf926c41243630
[08/10/2023-11:17:21] [V] [TRT] Tactic: 0xc2cf926c41243630 Time: 0.50955
[08/10/2023-11:17:21] [V] [TRT] Conv_46 + Relu_47 Set Tactic Name: ampere_h16816cudnn_128x64_ldg8_relu_exp_stages_64x3_small_nhwc_tn_v1 Tactic: 0xc338d2482cee77f8
[08/10/2023-11:17:21] [V] [TRT] Tactic: 0xc338d2482cee77f8 Time: 0.558683
[08/10/2023-11:17:21] [V] [TRT] Conv_46 + Relu_47 Set Tactic Name: ampere_h1688cudnn_256x128_ldg8_relu_exp_small_nhwc_tn_v1 Tactic: 0xc660e51970bc5a3a
[08/10/2023-11:17:21] [V] [TRT] Tactic: 0xc660e51970bc5a3a Time: 0.572571
[08/10/2023-11:17:21] [V] [TRT] Conv_46 + Relu_47 Set Tactic Name: ampere_h16816cudnn_64x128_ldg8_relu_exp_stages_64x4_medium_nhwc_tn_v1 Tactic: 0xc82f3f06140e3cbb
[08/10/2023-11:17:21] [V] [TRT] Tactic: 0xc82f3f06140e3cbb Time: 0.585906
[08/10/2023-11:17:21] [V] [TRT] Conv_46 + Relu_47 Set Tactic Name: ampere_h16816cudnn_64x128_sliced1x2_ldg8_relu_exp_stages_64x4_medium_nhwc_tn_v1 Tactic: 0xc8a90ff8898200c3
[08/10/2023-11:17:21] [V] [TRT] Tactic: 0xc8a90ff8898200c3 Time: 0.560594
[08/10/2023-11:17:21] [V] [TRT] Conv_46 + Relu_47 Set Tactic Name: ampere_h1688cudnn_256x64_sliced1x2_ldg8_relu_exp_medium_nhwc_tn_v1 Tactic: 0xc9cc55109bb4de26
[08/10/2023-11:17:21] [V] [TRT] Tactic: 0xc9cc55109bb4de26 Time: 0.762391
[08/10/2023-11:17:21] [V] [TRT] Conv_46 + Relu_47 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x64x32_stage5_warpsize2x2x1_g1_tensor16x8x16 Tactic: 0xc9d24bd069159fa8
[08/10/2023-11:17:21] [V] [TRT] Tactic: 0xc9d24bd069159fa8 Time: 0.557138
[08/10/2023-11:17:21] [V] [TRT] Conv_46 + Relu_47 Set Tactic Name: ampere_h16816cudnn_64x128_ldg8_relu_exp_stages_64x3_large_nhwc_tn_v1 Tactic: 0xc9f0a7bec963ba66
[08/10/2023-11:17:21] [V] [TRT] Tactic: 0xc9f0a7bec963ba66 Time: 0.570245
[08/10/2023-11:17:21] [V] [TRT] Conv_46 + Relu_47 Set Tactic Name: ampere_h16816cudnn_256x64_ldg8_relu_exp_small_nhwc_tn_v1 Tactic: 0xca5d3a11fd48f571
[08/10/2023-11:17:21] [V] [TRT] Tactic: 0xca5d3a11fd48f571 Time: 0.540402
[08/10/2023-11:17:21] [V] [TRT] Conv_46 + Relu_47 Set Tactic Name: ampere_h16816cudnn_64x64_ldg8_relu_exp_stages_64x5_small_nhwc_tn_v1 Tactic: 0xce0506e1512285c3
[08/10/2023-11:17:21] [V] [TRT] Tactic: 0xce0506e1512285c3 Time: 0.742702
[08/10/2023-11:17:21] [V] [TRT] Conv_46 + Relu_47 Set Tactic Name: ampere_h1688cudnn_256x64_sliced1x2_ldg8_relu_exp_large_nhwc_tn_v1 Tactic: 0xd0a3e0c815f7fb5e
[08/10/2023-11:17:21] [V] [TRT] Tactic: 0xd0a3e0c815f7fb5e Time: 0.788704
[08/10/2023-11:17:21] [V] [TRT] Conv_46 + Relu_47 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x64x32_stage3_warpsize4x1x1_g1_tensor16x8x16 Tactic: 0xd1aaad17ca35fbaa
[08/10/2023-11:17:21] [V] [TRT] Tactic: 0xd1aaad17ca35fbaa Time: 0.484119
[08/10/2023-11:17:21] [V] [TRT] Conv_46 + Relu_47 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x128x64_stage3_warpsize4x2x1_g1_tensor16x8x16_t1r3s3 Tactic: 0xd2fca0486ca97266
[08/10/2023-11:17:21] [V] [TRT] Tactic: 0xd2fca0486ca97266 Time: 0.477774
[08/10/2023-11:17:21] [V] [TRT] Conv_46 + Relu_47 Set Tactic Name: ampere_h16816cudnn_64x128_sliced1x2_ldg8_relu_exp_stages_64x3_small_nhwc_tn_v1 Tactic: 0xd58ea0bdedb89ead
[08/10/2023-11:17:21] [V] [TRT] Tactic: 0xd58ea0bdedb89ead Time: 0.560901
[08/10/2023-11:17:21] [V] [TRT] Conv_46 + Relu_47 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x256x64_stage3_warpsize1x4x1_g1_tensor16x8x16_t1r3s3 Tactic: 0xd7c261fa01db2af9
[08/10/2023-11:17:21] [V] [TRT] Tactic: 0xd7c261fa01db2af9 Time: 0.521559
[08/10/2023-11:17:21] [V] [TRT] Conv_46 + Relu_47 Set Tactic Name: ampere_h16816cudnn_256x64_sliced1x2_ldg8_relu_exp_large_nhwc_tn_v1 Tactic: 0xd8eb41ee35e76575
[08/10/2023-11:17:21] [V] [TRT] Tactic: 0xd8eb41ee35e76575 Time: 0.736704
[08/10/2023-11:17:21] [V] [TRT] Conv_46 + Relu_47 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x128x32_stage5_warpsize2x2x1_g1_tensor16x8x16 Tactic: 0xdb0b80f591d1bb6d
[08/10/2023-11:17:21] [V] [TRT] Tactic: 0xdb0b80f591d1bb6d Time: 0.532759
[08/10/2023-11:17:21] [V] [TRT] Conv_46 + Relu_47 Set Tactic Name: ampere_h16816cudnn_256x128_ldg8_relu_exp_large_nhwc_tn_v1 Tactic: 0xdc796d70e228a1d4
[08/10/2023-11:17:21] [V] [TRT] Tactic: 0xdc796d70e228a1d4 Time: 0.511735
[08/10/2023-11:17:21] [V] [TRT] Conv_46 + Relu_47 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x32x32_stage4_warpsize4x1x1_g1_tensor16x8x16 Tactic: 0xdce100b9fe609424
[08/10/2023-11:17:21] [V] [TRT] Tactic: 0xdce100b9fe609424 Time: 0.768891
[08/10/2023-11:17:21] [V] [TRT] Conv_46 + Relu_47 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x128x32_stage4_warpsize2x2x1_g1_tensor16x8x16 Tactic: 0xdfa020ef435ef810
[08/10/2023-11:17:21] [V] [TRT] Tactic: 0xdfa020ef435ef810 Time: 0.443301
[08/10/2023-11:17:21] [V] [TRT] Conv_46 + Relu_47 Set Tactic Name: ampere_h1688cudnn_256x64_ldg8_relu_exp_large_nhwc_tn_v1 Tactic: 0xe0e3c0e8cf9a2d9e
[08/10/2023-11:17:21] [V] [TRT] Tactic: 0xe0e3c0e8cf9a2d9e Time: 0.765074
[08/10/2023-11:17:21] [V] [TRT] Conv_46 + Relu_47 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x32x64_stage5_warpsize2x2x1_g1_tensor16x8x16 Tactic: 0xe1ff5ad20f5c6bf6
[08/10/2023-11:17:21] [V] [TRT] Tactic: 0xe1ff5ad20f5c6bf6 Time: 1.01321
[08/10/2023-11:17:21] [V] [TRT] Conv_46 + Relu_47 Set Tactic Name: ampere_h16816cudnn_256x64_ldg8_relu_exp_medium_nhwc_tn_v1 Tactic: 0xe4711898bd599c36
[08/10/2023-11:17:21] [V] [TRT] Tactic: 0xe4711898bd599c36 Time: 0.557367
[08/10/2023-11:17:21] [V] [TRT] Conv_46 + Relu_47 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x64x64_stage3_warpsize4x1x1_g1_tensor16x8x16 Tactic: 0xeb25062ffb9eae45
[08/10/2023-11:17:21] [V] [TRT] Tactic: 0xeb25062ffb9eae45 Time: 0.530043
[08/10/2023-11:17:21] [V] [TRT] Conv_46 + Relu_47 Set Tactic Name: ampere_h16816cudnn_128x64_sliced1x2_ldg8_relu_exp_stages_64x4_large_nhwc_tn_v1 Tactic: 0xeb2d2aa4e56bb41c
[08/10/2023-11:17:21] [V] [TRT] Tactic: 0xeb2d2aa4e56bb41c Time: 0.612864
[08/10/2023-11:17:21] [V] [TRT] Conv_46 + Relu_47 Set Tactic Name: ampere_h16816cudnn_64x64_ldg8_relu_exp_stages_64x6_small_nhwc_tn_v1 Tactic: 0xf0beb09df9a19f82
[08/10/2023-11:17:21] [V] [TRT] Tactic: 0xf0beb09df9a19f82 Time: 0.799223
[08/10/2023-11:17:21] [V] [TRT] Conv_46 + Relu_47 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x64x64_stage4_warpsize2x2x1_g1_tensor16x8x16 Tactic: 0xf35e0311fa1cc516
[08/10/2023-11:17:21] [V] [TRT] Tactic: 0xf35e0311fa1cc516 Time: 0.692494
[08/10/2023-11:17:21] [V] [TRT] Conv_46 + Relu_47 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x32x32_stage4_warpsize4x1x1_g1_tensor16x8x16 Tactic: 0xf79479a62ea9f901
[08/10/2023-11:17:21] [V] [TRT] Tactic: 0xf79479a62ea9f901 Time: 0.855488
[08/10/2023-11:17:21] [V] [TRT] Fastest Tactic: 0xdfa020ef435ef810 Time: 0.443301
[08/10/2023-11:17:21] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0xdfa020ef435ef810
[08/10/2023-11:17:21] [V] [TRT] =============== Computing costs for 
[08/10/2023-11:17:21] [V] [TRT] *************** Autotuning format combination: Float(2073600,8100,90,1) -> Float(2073600,8100,90,1) ***************
[08/10/2023-11:17:21] [V] [TRT] *************** Autotuning format combination: Float(2073600,1,23040,256) -> Float(2073600,1,23040,256) ***************
[08/10/2023-11:17:21] [V] [TRT] *************** Autotuning format combination: Float(518400,1:4,5760,64) -> Float(518400,1:4,5760,64) ***************
[08/10/2023-11:17:21] [V] [TRT] *************** Autotuning format combination: Half(2073600,8100,90,1) -> Half(2073600,8100,90,1) ***************
[08/10/2023-11:17:21] [W] [TRT] Weights [name=Conv_48 + Relu_49.weight] had the following issues when converted to FP16:
[08/10/2023-11:17:21] [W] [TRT]  - Subnormal FP16 values detected. 
[08/10/2023-11:17:21] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.
[08/10/2023-11:17:21] [V] [TRT] *************** Autotuning format combination: Half(1036800,8100:2,90,1) -> Half(1036800,8100:2,90,1) ***************
[08/10/2023-11:17:21] [W] [TRT] Weights [name=Conv_48 + Relu_49.weight] had the following issues when converted to FP16:
[08/10/2023-11:17:21] [W] [TRT]  - Subnormal FP16 values detected. 
[08/10/2023-11:17:21] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.
[08/10/2023-11:17:21] [V] [TRT] --------------- Timing Runner: Conv_48 + Relu_49 (FusedConvActConvolution)
[08/10/2023-11:17:21] [V] [TRT] FusedConvActConvolution has no valid tactics for this config, skipping
[08/10/2023-11:17:21] [W] [TRT] Weights [name=Conv_48 + Relu_49.weight] had the following issues when converted to FP16:
[08/10/2023-11:17:21] [W] [TRT]  - Subnormal FP16 values detected. 
[08/10/2023-11:17:21] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.
[08/10/2023-11:17:21] [V] [TRT] --------------- Timing Runner: Conv_48 + Relu_49 (CaskConvolution)
[08/10/2023-11:17:21] [V] [TRT] CaskConvolution has no valid tactics for this config, skipping
[08/10/2023-11:17:21] [V] [TRT] *************** Autotuning format combination: Half(259200,1:8,2880,32) -> Float(2073600,8100,90,1) ***************
[08/10/2023-11:17:21] [W] [TRT] Weights [name=Conv_48 + Relu_49.weight] had the following issues when converted to FP16:
[08/10/2023-11:17:21] [W] [TRT]  - Subnormal FP16 values detected. 
[08/10/2023-11:17:21] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.
[08/10/2023-11:17:21] [V] [TRT] --------------- Timing Runner: Conv_48 + Relu_49 (CaskConvolution)
[08/10/2023-11:17:21] [V] [TRT] CaskConvolution has no valid tactics for this config, skipping
[08/10/2023-11:17:21] [V] [TRT] *************** Autotuning format combination: Half(259200,1:8,2880,32) -> Half(259200,1:8,2880,32) ***************
[08/10/2023-11:17:21] [W] [TRT] Weights [name=Conv_48 + Relu_49.weight] had the following issues when converted to FP16:
[08/10/2023-11:17:21] [W] [TRT]  - Subnormal FP16 values detected. 
[08/10/2023-11:17:21] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.
[08/10/2023-11:17:21] [V] [TRT] *************** Autotuning format combination: Half(129600,1:16,1440,16) -> Half(129600,1:16,1440,16) ***************
[08/10/2023-11:17:21] [W] [TRT] Weights [name=Conv_48 + Relu_49.weight] had the following issues when converted to FP16:
[08/10/2023-11:17:21] [W] [TRT]  - Subnormal FP16 values detected. 
[08/10/2023-11:17:21] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.
[08/10/2023-11:17:21] [V] [TRT] =============== Computing costs for 
[08/10/2023-11:17:21] [V] [TRT] *************** Autotuning format combination: Float(2073600,8100,90,1) -> Float(2073600,8100,90,1) ***************
[08/10/2023-11:17:21] [V] [TRT] *************** Autotuning format combination: Float(2073600,1,23040,256) -> Float(2073600,1,23040,256) ***************
[08/10/2023-11:17:21] [V] [TRT] *************** Autotuning format combination: Float(518400,1:4,5760,64) -> Float(518400,1:4,5760,64) ***************
[08/10/2023-11:17:21] [V] [TRT] *************** Autotuning format combination: Half(2073600,8100,90,1) -> Half(2073600,8100,90,1) ***************
[08/10/2023-11:17:21] [W] [TRT] Weights [name=Conv_50 + Relu_51.weight] had the following issues when converted to FP16:
[08/10/2023-11:17:21] [W] [TRT]  - Subnormal FP16 values detected. 
[08/10/2023-11:17:21] [W] [TRT]  - Values less than smallest positive FP16 Subnormal value detected. Converting to FP16 minimum subnormalized value. 
[08/10/2023-11:17:21] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.
[08/10/2023-11:17:21] [V] [TRT] *************** Autotuning format combination: Half(1036800,8100:2,90,1) -> Half(1036800,8100:2,90,1) ***************
[08/10/2023-11:17:21] [W] [TRT] Weights [name=Conv_50 + Relu_51.weight] had the following issues when converted to FP16:
[08/10/2023-11:17:21] [W] [TRT]  - Subnormal FP16 values detected. 
[08/10/2023-11:17:21] [W] [TRT]  - Values less than smallest positive FP16 Subnormal value detected. Converting to FP16 minimum subnormalized value. 
[08/10/2023-11:17:21] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.
[08/10/2023-11:17:21] [V] [TRT] --------------- Timing Runner: Conv_50 + Relu_51 (FusedConvActConvolution)
[08/10/2023-11:17:21] [V] [TRT] FusedConvActConvolution has no valid tactics for this config, skipping
[08/10/2023-11:17:21] [W] [TRT] Weights [name=Conv_50 + Relu_51.weight] had the following issues when converted to FP16:
[08/10/2023-11:17:21] [W] [TRT]  - Subnormal FP16 values detected. 
[08/10/2023-11:17:21] [W] [TRT]  - Values less than smallest positive FP16 Subnormal value detected. Converting to FP16 minimum subnormalized value. 
[08/10/2023-11:17:21] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.
[08/10/2023-11:17:21] [V] [TRT] --------------- Timing Runner: Conv_50 + Relu_51 (CaskConvolution)
[08/10/2023-11:17:21] [V] [TRT] CaskConvolution has no valid tactics for this config, skipping
[08/10/2023-11:17:21] [V] [TRT] *************** Autotuning format combination: Half(259200,1:8,2880,32) -> Float(2073600,8100,90,1) ***************
[08/10/2023-11:17:21] [W] [TRT] Weights [name=Conv_50 + Relu_51.weight] had the following issues when converted to FP16:
[08/10/2023-11:17:21] [W] [TRT]  - Subnormal FP16 values detected. 
[08/10/2023-11:17:21] [W] [TRT]  - Values less than smallest positive FP16 Subnormal value detected. Converting to FP16 minimum subnormalized value. 
[08/10/2023-11:17:21] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.
[08/10/2023-11:17:21] [V] [TRT] --------------- Timing Runner: Conv_50 + Relu_51 (CaskConvolution)
[08/10/2023-11:17:21] [V] [TRT] CaskConvolution has no valid tactics for this config, skipping
[08/10/2023-11:17:21] [V] [TRT] *************** Autotuning format combination: Half(259200,1:8,2880,32) -> Half(259200,1:8,2880,32) ***************
[08/10/2023-11:17:21] [W] [TRT] Weights [name=Conv_50 + Relu_51.weight] had the following issues when converted to FP16:
[08/10/2023-11:17:21] [W] [TRT]  - Subnormal FP16 values detected. 
[08/10/2023-11:17:21] [W] [TRT]  - Values less than smallest positive FP16 Subnormal value detected. Converting to FP16 minimum subnormalized value. 
[08/10/2023-11:17:21] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.
[08/10/2023-11:17:21] [V] [TRT] *************** Autotuning format combination: Half(129600,1:16,1440,16) -> Half(129600,1:16,1440,16) ***************
[08/10/2023-11:17:21] [W] [TRT] Weights [name=Conv_50 + Relu_51.weight] had the following issues when converted to FP16:
[08/10/2023-11:17:21] [W] [TRT]  - Subnormal FP16 values detected. 
[08/10/2023-11:17:21] [W] [TRT]  - Values less than smallest positive FP16 Subnormal value detected. Converting to FP16 minimum subnormalized value. 
[08/10/2023-11:17:21] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.
[08/10/2023-11:17:21] [V] [TRT] =============== Computing costs for 
[08/10/2023-11:17:21] [V] [TRT] *************** Autotuning format combination: Float(2073600,8100,90,1) -> Float(2073600,8100,90,1) ***************
[08/10/2023-11:17:21] [V] [TRT] *************** Autotuning format combination: Float(2073600,1,23040,256) -> Float(2073600,1,23040,256) ***************
[08/10/2023-11:17:21] [V] [TRT] *************** Autotuning format combination: Float(518400,1:4,5760,64) -> Float(518400,1:4,5760,64) ***************
[08/10/2023-11:17:21] [V] [TRT] *************** Autotuning format combination: Half(2073600,8100,90,1) -> Half(2073600,8100,90,1) ***************
[08/10/2023-11:17:21] [W] [TRT] Weights [name=Conv_52 + Relu_53.weight] had the following issues when converted to FP16:
[08/10/2023-11:17:21] [W] [TRT]  - Subnormal FP16 values detected. 
[08/10/2023-11:17:21] [W] [TRT]  - Values less than smallest positive FP16 Subnormal value detected. Converting to FP16 minimum subnormalized value. 
[08/10/2023-11:17:21] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.
[08/10/2023-11:17:21] [V] [TRT] *************** Autotuning format combination: Half(1036800,8100:2,90,1) -> Half(1036800,8100:2,90,1) ***************
[08/10/2023-11:17:21] [W] [TRT] Weights [name=Conv_52 + Relu_53.weight] had the following issues when converted to FP16:
[08/10/2023-11:17:21] [W] [TRT]  - Subnormal FP16 values detected. 
[08/10/2023-11:17:21] [W] [TRT]  - Values less than smallest positive FP16 Subnormal value detected. Converting to FP16 minimum subnormalized value. 
[08/10/2023-11:17:21] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.
[08/10/2023-11:17:21] [V] [TRT] --------------- Timing Runner: Conv_52 + Relu_53 (FusedConvActConvolution)
[08/10/2023-11:17:21] [V] [TRT] FusedConvActConvolution has no valid tactics for this config, skipping
[08/10/2023-11:17:21] [W] [TRT] Weights [name=Conv_52 + Relu_53.weight] had the following issues when converted to FP16:
[08/10/2023-11:17:21] [W] [TRT]  - Subnormal FP16 values detected. 
[08/10/2023-11:17:21] [W] [TRT]  - Values less than smallest positive FP16 Subnormal value detected. Converting to FP16 minimum subnormalized value. 
[08/10/2023-11:17:21] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.
[08/10/2023-11:17:21] [V] [TRT] --------------- Timing Runner: Conv_52 + Relu_53 (CaskConvolution)
[08/10/2023-11:17:21] [V] [TRT] CaskConvolution has no valid tactics for this config, skipping
[08/10/2023-11:17:21] [V] [TRT] *************** Autotuning format combination: Half(259200,1:8,2880,32) -> Float(2073600,8100,90,1) ***************
[08/10/2023-11:17:21] [W] [TRT] Weights [name=Conv_52 + Relu_53.weight] had the following issues when converted to FP16:
[08/10/2023-11:17:21] [W] [TRT]  - Subnormal FP16 values detected. 
[08/10/2023-11:17:21] [W] [TRT]  - Values less than smallest positive FP16 Subnormal value detected. Converting to FP16 minimum subnormalized value. 
[08/10/2023-11:17:21] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.
[08/10/2023-11:17:21] [V] [TRT] --------------- Timing Runner: Conv_52 + Relu_53 (CaskConvolution)
[08/10/2023-11:17:21] [V] [TRT] CaskConvolution has no valid tactics for this config, skipping
[08/10/2023-11:17:21] [V] [TRT] *************** Autotuning format combination: Half(259200,1:8,2880,32) -> Half(259200,1:8,2880,32) ***************
[08/10/2023-11:17:21] [W] [TRT] Weights [name=Conv_52 + Relu_53.weight] had the following issues when converted to FP16:
[08/10/2023-11:17:21] [W] [TRT]  - Subnormal FP16 values detected. 
[08/10/2023-11:17:21] [W] [TRT]  - Values less than smallest positive FP16 Subnormal value detected. Converting to FP16 minimum subnormalized value. 
[08/10/2023-11:17:21] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.
[08/10/2023-11:17:21] [V] [TRT] *************** Autotuning format combination: Half(129600,1:16,1440,16) -> Half(129600,1:16,1440,16) ***************
[08/10/2023-11:17:21] [W] [TRT] Weights [name=Conv_52 + Relu_53.weight] had the following issues when converted to FP16:
[08/10/2023-11:17:21] [W] [TRT]  - Subnormal FP16 values detected. 
[08/10/2023-11:17:21] [W] [TRT]  - Values less than smallest positive FP16 Subnormal value detected. Converting to FP16 minimum subnormalized value. 
[08/10/2023-11:17:21] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.
[08/10/2023-11:17:21] [V] [TRT] =============== Computing costs for 
[08/10/2023-11:17:21] [V] [TRT] *************** Autotuning format combination: Float(2073600,8100,90,1) -> Float(2073600,8100,90,1) ***************
[08/10/2023-11:17:21] [V] [TRT] *************** Autotuning format combination: Float(2073600,1,23040,256) -> Float(2073600,1,23040,256) ***************
[08/10/2023-11:17:21] [V] [TRT] *************** Autotuning format combination: Float(518400,1:4,5760,64) -> Float(518400,1:4,5760,64) ***************
[08/10/2023-11:17:21] [V] [TRT] *************** Autotuning format combination: Half(2073600,8100,90,1) -> Half(2073600,8100,90,1) ***************
[08/10/2023-11:17:21] [W] [TRT] Weights [name=Conv_54 + Relu_55.weight] had the following issues when converted to FP16:
[08/10/2023-11:17:21] [W] [TRT]  - Subnormal FP16 values detected. 
[08/10/2023-11:17:21] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.
[08/10/2023-11:17:21] [V] [TRT] *************** Autotuning format combination: Half(1036800,8100:2,90,1) -> Half(1036800,8100:2,90,1) ***************
[08/10/2023-11:17:21] [W] [TRT] Weights [name=Conv_54 + Relu_55.weight] had the following issues when converted to FP16:
[08/10/2023-11:17:21] [W] [TRT]  - Subnormal FP16 values detected. 
[08/10/2023-11:17:21] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.
[08/10/2023-11:17:21] [V] [TRT] --------------- Timing Runner: Conv_54 + Relu_55 (FusedConvActConvolution)
[08/10/2023-11:17:21] [V] [TRT] FusedConvActConvolution has no valid tactics for this config, skipping
[08/10/2023-11:17:21] [W] [TRT] Weights [name=Conv_54 + Relu_55.weight] had the following issues when converted to FP16:
[08/10/2023-11:17:21] [W] [TRT]  - Subnormal FP16 values detected. 
[08/10/2023-11:17:21] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.
[08/10/2023-11:17:21] [V] [TRT] --------------- Timing Runner: Conv_54 + Relu_55 (CaskConvolution)
[08/10/2023-11:17:21] [V] [TRT] CaskConvolution has no valid tactics for this config, skipping
[08/10/2023-11:17:21] [V] [TRT] *************** Autotuning format combination: Half(259200,1:8,2880,32) -> Float(2073600,8100,90,1) ***************
[08/10/2023-11:17:21] [W] [TRT] Weights [name=Conv_54 + Relu_55.weight] had the following issues when converted to FP16:
[08/10/2023-11:17:21] [W] [TRT]  - Subnormal FP16 values detected. 
[08/10/2023-11:17:21] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.
[08/10/2023-11:17:21] [V] [TRT] --------------- Timing Runner: Conv_54 + Relu_55 (CaskConvolution)
[08/10/2023-11:17:21] [V] [TRT] CaskConvolution has no valid tactics for this config, skipping
[08/10/2023-11:17:21] [V] [TRT] *************** Autotuning format combination: Half(259200,1:8,2880,32) -> Half(259200,1:8,2880,32) ***************
[08/10/2023-11:17:21] [W] [TRT] Weights [name=Conv_54 + Relu_55.weight] had the following issues when converted to FP16:
[08/10/2023-11:17:21] [W] [TRT]  - Subnormal FP16 values detected. 
[08/10/2023-11:17:21] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.
[08/10/2023-11:17:21] [V] [TRT] *************** Autotuning format combination: Half(129600,1:16,1440,16) -> Half(129600,1:16,1440,16) ***************
[08/10/2023-11:17:21] [W] [TRT] Weights [name=Conv_54 + Relu_55.weight] had the following issues when converted to FP16:
[08/10/2023-11:17:21] [W] [TRT]  - Subnormal FP16 values detected. 
[08/10/2023-11:17:21] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.
[08/10/2023-11:17:21] [V] [TRT] =============== Computing costs for 
[08/10/2023-11:17:21] [V] [TRT] *************** Autotuning format combination: Float(2073600,8100,90,1) -> Float(8294400,32400,180,1) ***************
[08/10/2023-11:17:21] [V] [TRT] --------------- Timing Runner: ConvTranspose_56 + BatchNormalization_57 + Relu_58 (CudnnDeconvolution)
[08/10/2023-11:17:21] [V] [TRT] Tactic: 0x0000000000000000 Time: 4.62528
[08/10/2023-11:17:22] [V] [TRT] Tactic: 0x0000000000000001 Time: 2.88829
[08/10/2023-11:17:22] [V] [TRT] Tactic: 0x0000000000000003 Time: 76.3255
[08/10/2023-11:17:22] [V] [TRT] Fastest Tactic: 0x0000000000000001 Time: 2.88829
[08/10/2023-11:17:22] [V] [TRT] --------------- Timing Runner: ConvTranspose_56 + BatchNormalization_57 + Relu_58 (GemmDeconvolution)
[08/10/2023-11:17:22] [V] [TRT] Tactic: 0x0000000000000000 Time: 3.60601
[08/10/2023-11:17:22] [V] [TRT] Fastest Tactic: 0x0000000000000000 Time: 3.60601
[08/10/2023-11:17:22] [V] [TRT] --------------- Timing Runner: ConvTranspose_56 + BatchNormalization_57 + Relu_58 (CaskDeconvolution)
[08/10/2023-11:17:22] [V] [TRT] CaskDeconvolution has no valid tactics for this config, skipping
[08/10/2023-11:17:22] [V] [TRT] --------------- Timing Runner: ConvTranspose_56 + BatchNormalization_57 + Relu_58 (CaskDeconvolutionV2)
[08/10/2023-11:17:22] [V] [TRT] ConvTranspose_56 + BatchNormalization_57 + Relu_58 Set Tactic Name: sm50_xmma_deconv_generic_f32f32_f32_f32_nchwkcrs_nchw Tactic: 0x0f630dccfe13bf53
[08/10/2023-11:17:22] [V] [TRT] Tactic: 0x0f630dccfe13bf53 Time: 10000
[08/10/2023-11:17:22] [V] [TRT] Fastest Tactic: 0x0f630dccfe13bf53 Time: 10000
[08/10/2023-11:17:22] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CudnnDeconvolution Tactic: 0x0000000000000001
[08/10/2023-11:17:22] [V] [TRT] *************** Autotuning format combination: Float(2073600,1,23040,256) -> Float(8294400,1,46080,256) ***************
[08/10/2023-11:17:22] [V] [TRT] --------------- Timing Runner: ConvTranspose_56 + BatchNormalization_57 + Relu_58 (CudnnDeconvolution)
[08/10/2023-11:17:22] [V] [TRT] CudnnDeconvolution has no valid tactics for this config, skipping
[08/10/2023-11:17:22] [V] [TRT] --------------- Timing Runner: ConvTranspose_56 + BatchNormalization_57 + Relu_58 (GemmDeconvolution)
[08/10/2023-11:17:22] [V] [TRT] GemmDeconvolution has no valid tactics for this config, skipping
[08/10/2023-11:17:22] [V] [TRT] --------------- Timing Runner: ConvTranspose_56 + BatchNormalization_57 + Relu_58 (CaskDeconvolution)
[08/10/2023-11:17:22] [V] [TRT] CaskDeconvolution has no valid tactics for this config, skipping
[08/10/2023-11:17:22] [V] [TRT] --------------- Timing Runner: ConvTranspose_56 + BatchNormalization_57 + Relu_58 (CaskDeconvolutionV2)
[08/10/2023-11:17:22] [V] [TRT] ConvTranspose_56 + BatchNormalization_57 + Relu_58 Set Tactic Name: sm80_xmma_deconv_implicit_gemm_indexed_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x64x32_stage5_warpsize2x2x1_g1_tensor16x8x8_strided Tactic: 0x0134c35d7654e474
[08/10/2023-11:17:22] [V] [TRT] Tactic: 0x0134c35d7654e474 Time: 1.06368
[08/10/2023-11:17:22] [V] [TRT] ConvTranspose_56 + BatchNormalization_57 + Relu_58 Set Tactic Name: sm80_xmma_deconv_implicit_gemm_indexed_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize32x32x8_stage3_warpsize1x2x1_g1_ffma_strided_aligna4_alignc4 Tactic: 0x043c81cea95f3c97
[08/10/2023-11:17:22] [V] [TRT] Tactic: 0x043c81cea95f3c97 Time: 3.95672
[08/10/2023-11:17:22] [V] [TRT] ConvTranspose_56 + BatchNormalization_57 + Relu_58 Set Tactic Name: sm80_xmma_deconv_implicit_gemm_indexed_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x32x8_stage3_warpsize2x2x1_g1_ffma_strided_aligna4_alignc4 Tactic: 0x0af5b8971b78dc1f
[08/10/2023-11:17:22] [V] [TRT] Tactic: 0x0af5b8971b78dc1f Time: 3.46055
[08/10/2023-11:17:22] [V] [TRT] ConvTranspose_56 + BatchNormalization_57 + Relu_58 Set Tactic Name: sm80_xmma_deconv_implicit_gemm_indexed_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize64x32x64_stage5_warpsize2x2x1_g1_tensor16x8x8_strided Tactic: 0x0c64ea8aec3c7cf9
[08/10/2023-11:17:22] [V] [TRT] Tactic: 0x0c64ea8aec3c7cf9 Time: 2.67719
[08/10/2023-11:17:22] [V] [TRT] ConvTranspose_56 + BatchNormalization_57 + Relu_58 Set Tactic Name: sm80_xmma_deconv_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize32x32x8_stage3_warpsize1x2x1_g1_ffma_aligna4_alignc4 Tactic: 0x14499f757787b157
[08/10/2023-11:17:23] [V] [TRT] Tactic: 0x14499f757787b157 Time: 10.7761
[08/10/2023-11:17:23] [V] [TRT] ConvTranspose_56 + BatchNormalization_57 + Relu_58 Set Tactic Name: sm80_xmma_deconv_implicit_gemm_indexed_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x128x8_stage3_warpsize1x4x1_g1_ffma_strided_aligna4_alignc4 Tactic: 0x1a82ab99d94518f2
[08/10/2023-11:17:23] [V] [TRT] Tactic: 0x1a82ab99d94518f2 Time: 2.3697
[08/10/2023-11:17:23] [V] [TRT] ConvTranspose_56 + BatchNormalization_57 + Relu_58 Set Tactic Name: sm80_xmma_deconv_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x128x8_stage3_warpsize2x2x1_g1_ffma_aligna4_alignc4 Tactic: 0x24bd5d7c8284eeec
[08/10/2023-11:17:23] [V] [TRT] Tactic: 0x24bd5d7c8284eeec Time: 7.16396
[08/10/2023-11:17:23] [V] [TRT] ConvTranspose_56 + BatchNormalization_57 + Relu_58 Set Tactic Name: sm86_xmma_deconv_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x256x32_stage2_warpsize2x4x1_g1_tensor16x8x8 Tactic: 0x4298039fe7d925bf
[08/10/2023-11:17:23] [V] [TRT] Tactic: 0x4298039fe7d925bf Time: 1.74978
[08/10/2023-11:17:23] [V] [TRT] ConvTranspose_56 + BatchNormalization_57 + Relu_58 Set Tactic Name: sm80_xmma_deconv_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x256x32_stage3_warpsize2x4x1_g1_tensor16x8x8 Tactic: 0x42aceb94c466c376
[08/10/2023-11:17:23] [V] [TRT] Tactic: 0x42aceb94c466c376 Time: 1.66208
[08/10/2023-11:17:23] [V] [TRT] ConvTranspose_56 + BatchNormalization_57 + Relu_58 Set Tactic Name: sm80_xmma_deconv_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage4_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0x47d5b98d5a29addb
[08/10/2023-11:17:23] [V] [TRT] Tactic: 0x47d5b98d5a29addb Time: 3.03848
[08/10/2023-11:17:23] [V] [TRT] ConvTranspose_56 + BatchNormalization_57 + Relu_58 Set Tactic Name: sm80_xmma_deconv_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x64x32_stage5_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0x49c85fcd2be01d9e
[08/10/2023-11:17:23] [V] [TRT] Tactic: 0x49c85fcd2be01d9e Time: 2.59958
[08/10/2023-11:17:23] [V] [TRT] ConvTranspose_56 + BatchNormalization_57 + Relu_58 Set Tactic Name: sm80_xmma_deconv_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize64x128x32_stage5_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0x4b290b58df3c0f51
[08/10/2023-11:17:23] [V] [TRT] Tactic: 0x4b290b58df3c0f51 Time: 2.42389
[08/10/2023-11:17:23] [V] [TRT] ConvTranspose_56 + BatchNormalization_57 + Relu_58 Set Tactic Name: sm80_xmma_deconv_implicit_gemm_indexed_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_strided_aligna4_alignc4 Tactic: 0x55c5f197e3b7e8aa
[08/10/2023-11:17:23] [V] [TRT] Tactic: 0x55c5f197e3b7e8aa Time: 2.87395
[08/10/2023-11:17:23] [V] [TRT] ConvTranspose_56 + BatchNormalization_57 + Relu_58 Set Tactic Name: sm80_xmma_deconv_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0x6f63be3116a0cf3a
[08/10/2023-11:17:23] [V] [TRT] Tactic: 0x6f63be3116a0cf3a Time: 1.59739
[08/10/2023-11:17:23] [V] [TRT] ConvTranspose_56 + BatchNormalization_57 + Relu_58 Set Tactic Name: sm80_xmma_deconv_implicit_gemm_indexed_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage4_warpsize2x2x1_g1_tensor16x8x8_strided Tactic: 0x9d8e354228e29eca
[08/10/2023-11:17:23] [V] [TRT] Tactic: 0x9d8e354228e29eca Time: 1.54495
[08/10/2023-11:17:23] [V] [TRT] ConvTranspose_56 + BatchNormalization_57 + Relu_58 Set Tactic Name: sm80_xmma_deconv_implicit_gemm_indexed_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8_strided Tactic: 0xa17395d1f0a7b52b
[08/10/2023-11:17:23] [V] [TRT] Tactic: 0xa17395d1f0a7b52b Time: 0.710217
[08/10/2023-11:17:23] [V] [TRT] ConvTranspose_56 + BatchNormalization_57 + Relu_58 Set Tactic Name: sm80_xmma_deconv_implicit_gemm_indexed_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize64x128x32_stage5_warpsize2x2x1_g1_tensor16x8x8_strided Tactic: 0xa261f3129a32c982
[08/10/2023-11:17:23] [V] [TRT] Tactic: 0xa261f3129a32c982 Time: 0.9992
[08/10/2023-11:17:23] [V] [TRT] ConvTranspose_56 + BatchNormalization_57 + Relu_58 Set Tactic Name: sm80_xmma_deconv_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize64x32x64_stage5_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0xa87b9641bd98f20f
[08/10/2023-11:17:23] [V] [TRT] Tactic: 0xa87b9641bd98f20f Time: 5.60099
[08/10/2023-11:17:23] [V] [TRT] ConvTranspose_56 + BatchNormalization_57 + Relu_58 Set Tactic Name: sm80_xmma_deconv_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize256x128x8_stage3_warpsize4x2x1_g1_ffma_aligna4_alignc4 Tactic: 0xb1d8242d50afdff0
[08/10/2023-11:17:23] [V] [TRT] Tactic: 0xb1d8242d50afdff0 Time: 7.30529
[08/10/2023-11:17:23] [V] [TRT] ConvTranspose_56 + BatchNormalization_57 + Relu_58 Set Tactic Name: sm80_xmma_deconv_implicit_gemm_indexed_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x64x8_stage3_warpsize2x2x1_g1_ffma_strided_aligna4_alignc4 Tactic: 0xba791a7991b0361c
[08/10/2023-11:17:23] [V] [TRT] Tactic: 0xba791a7991b0361c Time: 2.6178
[08/10/2023-11:17:23] [V] [TRT] ConvTranspose_56 + BatchNormalization_57 + Relu_58 Set Tactic Name: sm80_xmma_deconv_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize256x64x32_stage3_warpsize4x2x1_g1_tensor16x8x8 Tactic: 0xe005f0326bf81db9
[08/10/2023-11:17:23] [V] [TRT] Tactic: 0xe005f0326bf81db9 Time: 2.03888
[08/10/2023-11:17:23] [V] [TRT] ConvTranspose_56 + BatchNormalization_57 + Relu_58 Set Tactic Name: sm80_xmma_deconv_implicit_gemm_indexed_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize256x64x32_stage3_warpsize4x2x1_g1_tensor16x8x8_strided Tactic: 0xf32895c9f4659506
[08/10/2023-11:17:23] [V] [TRT] Tactic: 0xf32895c9f4659506 Time: 0.886885
[08/10/2023-11:17:23] [V] [TRT] ConvTranspose_56 + BatchNormalization_57 + Relu_58 Set Tactic Name: sm80_xmma_deconv_implicit_gemm_indexed_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x256x32_stage3_warpsize2x4x1_g1_tensor16x8x8_strided Tactic: 0xf6f4a3d8ec94d0f1
[08/10/2023-11:17:23] [V] [TRT] Tactic: 0xf6f4a3d8ec94d0f1 Time: 0.767922
[08/10/2023-11:17:23] [V] [TRT] ConvTranspose_56 + BatchNormalization_57 + Relu_58 Set Tactic Name: sm80_xmma_deconv_implicit_gemm_indexed_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x32x8_stage3_warpsize1x2x1_g1_ffma_strided_aligna4_alignc4 Tactic: 0xf7c9fe3fcf824969
[08/10/2023-11:17:23] [V] [TRT] Tactic: 0xf7c9fe3fcf824969 Time: 3.22592
[08/10/2023-11:17:23] [V] [TRT] Fastest Tactic: 0xa17395d1f0a7b52b Time: 0.710217
[08/10/2023-11:17:23] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskDeconvolutionV2 Tactic: 0xa17395d1f0a7b52b
[08/10/2023-11:17:23] [V] [TRT] *************** Autotuning format combination: Float(518400,1:4,5760,64) -> Float(2073600,1:4,11520,64) ***************
[08/10/2023-11:17:23] [V] [TRT] --------------- Timing Runner: ConvTranspose_56 + BatchNormalization_57 + Relu_58 (CudnnDeconvolution)
[08/10/2023-11:17:23] [V] [TRT] CudnnDeconvolution has no valid tactics for this config, skipping
[08/10/2023-11:17:23] [V] [TRT] --------------- Timing Runner: ConvTranspose_56 + BatchNormalization_57 + Relu_58 (GemmDeconvolution)
[08/10/2023-11:17:23] [V] [TRT] GemmDeconvolution has no valid tactics for this config, skipping
[08/10/2023-11:17:23] [V] [TRT] --------------- Timing Runner: ConvTranspose_56 + BatchNormalization_57 + Relu_58 (CaskDeconvolution)
[08/10/2023-11:17:23] [V] [TRT] CaskDeconvolution has no valid tactics for this config, skipping
[08/10/2023-11:17:23] [V] [TRT] --------------- Timing Runner: ConvTranspose_56 + BatchNormalization_57 + Relu_58 (CaskDeconvolutionV2)
[08/10/2023-11:17:23] [V] [TRT] ConvTranspose_56 + BatchNormalization_57 + Relu_58 Set Tactic Name: sm80_xmma_deconv_implicit_gemm_indexed_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x64x32_stage5_warpsize2x2x1_g1_tensor16x8x8_strided Tactic: 0x0134c35d7654e474
[08/10/2023-11:17:23] [V] [TRT] Tactic: 0x0134c35d7654e474 Time: 1.06471
[08/10/2023-11:17:23] [V] [TRT] ConvTranspose_56 + BatchNormalization_57 + Relu_58 Set Tactic Name: sm80_xmma_deconv_implicit_gemm_indexed_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize32x32x8_stage3_warpsize1x2x1_g1_ffma_strided_aligna4_alignc4 Tactic: 0x043c81cea95f3c97
[08/10/2023-11:17:23] [V] [TRT] Tactic: 0x043c81cea95f3c97 Time: 3.93695
[08/10/2023-11:17:23] [V] [TRT] ConvTranspose_56 + BatchNormalization_57 + Relu_58 Set Tactic Name: sm80_xmma_deconv_implicit_gemm_indexed_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x32x8_stage3_warpsize2x2x1_g1_ffma_strided_aligna4_alignc4 Tactic: 0x0af5b8971b78dc1f
[08/10/2023-11:17:23] [V] [TRT] Tactic: 0x0af5b8971b78dc1f Time: 3.38032
[08/10/2023-11:17:23] [V] [TRT] ConvTranspose_56 + BatchNormalization_57 + Relu_58 Set Tactic Name: sm80_xmma_deconv_implicit_gemm_indexed_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize64x32x64_stage5_warpsize2x2x1_g1_tensor16x8x8_strided Tactic: 0x0c64ea8aec3c7cf9
[08/10/2023-11:17:23] [V] [TRT] Tactic: 0x0c64ea8aec3c7cf9 Time: 2.67026
[08/10/2023-11:17:23] [V] [TRT] ConvTranspose_56 + BatchNormalization_57 + Relu_58 Set Tactic Name: sm80_xmma_deconv_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize32x32x8_stage3_warpsize1x2x1_g1_ffma_aligna4_alignc4 Tactic: 0x14499f757787b157
[08/10/2023-11:17:23] [V] [TRT] Tactic: 0x14499f757787b157 Time: 10.9119
[08/10/2023-11:17:23] [V] [TRT] ConvTranspose_56 + BatchNormalization_57 + Relu_58 Set Tactic Name: sm80_xmma_deconv_implicit_gemm_indexed_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x128x8_stage3_warpsize1x4x1_g1_ffma_strided_aligna4_alignc4 Tactic: 0x1a82ab99d94518f2
[08/10/2023-11:17:23] [V] [TRT] Tactic: 0x1a82ab99d94518f2 Time: 2.42579
[08/10/2023-11:17:23] [V] [TRT] ConvTranspose_56 + BatchNormalization_57 + Relu_58 Set Tactic Name: sm80_xmma_deconv_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x128x8_stage3_warpsize2x2x1_g1_ffma_aligna4_alignc4 Tactic: 0x24bd5d7c8284eeec
[08/10/2023-11:17:23] [V] [TRT] Tactic: 0x24bd5d7c8284eeec Time: 7.05782
[08/10/2023-11:17:23] [V] [TRT] ConvTranspose_56 + BatchNormalization_57 + Relu_58 Set Tactic Name: sm86_xmma_deconv_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x256x32_stage2_warpsize2x4x1_g1_tensor16x8x8 Tactic: 0x4298039fe7d925bf
[08/10/2023-11:17:23] [V] [TRT] Tactic: 0x4298039fe7d925bf Time: 1.86602
[08/10/2023-11:17:23] [V] [TRT] ConvTranspose_56 + BatchNormalization_57 + Relu_58 Set Tactic Name: sm80_xmma_deconv_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x256x32_stage3_warpsize2x4x1_g1_tensor16x8x8 Tactic: 0x42aceb94c466c376
[08/10/2023-11:17:23] [V] [TRT] Tactic: 0x42aceb94c466c376 Time: 1.68443
[08/10/2023-11:17:23] [V] [TRT] ConvTranspose_56 + BatchNormalization_57 + Relu_58 Set Tactic Name: sm80_xmma_deconv_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage4_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0x47d5b98d5a29addb
[08/10/2023-11:17:24] [V] [TRT] Tactic: 0x47d5b98d5a29addb Time: 3.09345
[08/10/2023-11:17:24] [V] [TRT] ConvTranspose_56 + BatchNormalization_57 + Relu_58 Set Tactic Name: sm80_xmma_deconv_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x64x32_stage5_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0x49c85fcd2be01d9e
[08/10/2023-11:17:24] [V] [TRT] Tactic: 0x49c85fcd2be01d9e Time: 2.66973
[08/10/2023-11:17:24] [V] [TRT] ConvTranspose_56 + BatchNormalization_57 + Relu_58 Set Tactic Name: sm80_xmma_deconv_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize64x128x32_stage5_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0x4b290b58df3c0f51
[08/10/2023-11:17:24] [V] [TRT] Tactic: 0x4b290b58df3c0f51 Time: 2.45794
[08/10/2023-11:17:24] [V] [TRT] ConvTranspose_56 + BatchNormalization_57 + Relu_58 Set Tactic Name: sm80_xmma_deconv_implicit_gemm_indexed_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_strided_aligna4_alignc4 Tactic: 0x55c5f197e3b7e8aa
[08/10/2023-11:17:24] [V] [TRT] Tactic: 0x55c5f197e3b7e8aa Time: 2.88228
[08/10/2023-11:17:24] [V] [TRT] ConvTranspose_56 + BatchNormalization_57 + Relu_58 Set Tactic Name: sm80_xmma_deconv_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0x6f63be3116a0cf3a
[08/10/2023-11:17:24] [V] [TRT] Tactic: 0x6f63be3116a0cf3a Time: 1.60448
[08/10/2023-11:17:24] [V] [TRT] ConvTranspose_56 + BatchNormalization_57 + Relu_58 Set Tactic Name: sm80_xmma_deconv_implicit_gemm_indexed_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage4_warpsize2x2x1_g1_tensor16x8x8_strided Tactic: 0x9d8e354228e29eca
[08/10/2023-11:17:24] [V] [TRT] Tactic: 0x9d8e354228e29eca Time: 1.55911
[08/10/2023-11:17:24] [V] [TRT] ConvTranspose_56 + BatchNormalization_57 + Relu_58 Set Tactic Name: sm80_xmma_deconv_implicit_gemm_indexed_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8_strided Tactic: 0xa17395d1f0a7b52b
[08/10/2023-11:17:24] [V] [TRT] Tactic: 0xa17395d1f0a7b52b Time: 0.87349
[08/10/2023-11:17:24] [V] [TRT] ConvTranspose_56 + BatchNormalization_57 + Relu_58 Set Tactic Name: sm80_xmma_deconv_implicit_gemm_indexed_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize64x128x32_stage5_warpsize2x2x1_g1_tensor16x8x8_strided Tactic: 0xa261f3129a32c982
[08/10/2023-11:17:24] [V] [TRT] Tactic: 0xa261f3129a32c982 Time: 1.02679
[08/10/2023-11:17:24] [V] [TRT] ConvTranspose_56 + BatchNormalization_57 + Relu_58 Set Tactic Name: sm80_xmma_deconv_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize64x32x64_stage5_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0xa87b9641bd98f20f
[08/10/2023-11:17:24] [V] [TRT] Tactic: 0xa87b9641bd98f20f Time: 5.59843
[08/10/2023-11:17:24] [V] [TRT] ConvTranspose_56 + BatchNormalization_57 + Relu_58 Set Tactic Name: sm80_xmma_deconv_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize256x128x8_stage3_warpsize4x2x1_g1_ffma_aligna4_alignc4 Tactic: 0xb1d8242d50afdff0
[08/10/2023-11:17:24] [V] [TRT] Tactic: 0xb1d8242d50afdff0 Time: 7.38784
[08/10/2023-11:17:24] [V] [TRT] ConvTranspose_56 + BatchNormalization_57 + Relu_58 Set Tactic Name: sm80_xmma_deconv_implicit_gemm_indexed_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x64x8_stage3_warpsize2x2x1_g1_ffma_strided_aligna4_alignc4 Tactic: 0xba791a7991b0361c
[08/10/2023-11:17:24] [V] [TRT] Tactic: 0xba791a7991b0361c Time: 2.701
[08/10/2023-11:17:24] [V] [TRT] ConvTranspose_56 + BatchNormalization_57 + Relu_58 Set Tactic Name: sm80_xmma_deconv_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize256x64x32_stage3_warpsize4x2x1_g1_tensor16x8x8 Tactic: 0xe005f0326bf81db9
[08/10/2023-11:17:24] [V] [TRT] Tactic: 0xe005f0326bf81db9 Time: 2.06169
[08/10/2023-11:17:24] [V] [TRT] ConvTranspose_56 + BatchNormalization_57 + Relu_58 Set Tactic Name: sm80_xmma_deconv_implicit_gemm_indexed_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize256x64x32_stage3_warpsize4x2x1_g1_tensor16x8x8_strided Tactic: 0xf32895c9f4659506
[08/10/2023-11:17:24] [V] [TRT] Tactic: 0xf32895c9f4659506 Time: 0.887227
[08/10/2023-11:17:24] [V] [TRT] ConvTranspose_56 + BatchNormalization_57 + Relu_58 Set Tactic Name: sm80_xmma_deconv_implicit_gemm_indexed_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x256x32_stage3_warpsize2x4x1_g1_tensor16x8x8_strided Tactic: 0xf6f4a3d8ec94d0f1
[08/10/2023-11:17:24] [V] [TRT] Tactic: 0xf6f4a3d8ec94d0f1 Time: 0.847118
[08/10/2023-11:17:24] [V] [TRT] ConvTranspose_56 + BatchNormalization_57 + Relu_58 Set Tactic Name: sm80_xmma_deconv_implicit_gemm_indexed_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x32x8_stage3_warpsize1x2x1_g1_ffma_strided_aligna4_alignc4 Tactic: 0xf7c9fe3fcf824969
[08/10/2023-11:17:24] [V] [TRT] Tactic: 0xf7c9fe3fcf824969 Time: 3.23019
[08/10/2023-11:17:24] [V] [TRT] Fastest Tactic: 0xf6f4a3d8ec94d0f1 Time: 0.847118
[08/10/2023-11:17:24] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskDeconvolutionV2 Tactic: 0xf6f4a3d8ec94d0f1
[08/10/2023-11:17:24] [V] [TRT] *************** Autotuning format combination: Half(2073600,8100,90,1) -> Half(8294400,32400,180,1) ***************
[08/10/2023-11:17:24] [W] [TRT] Weights [name=ConvTranspose_56 + BatchNormalization_57 + Relu_58.weight] had the following issues when converted to FP16:
[08/10/2023-11:17:24] [W] [TRT]  - Subnormal FP16 values detected. 
[08/10/2023-11:17:24] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.
[08/10/2023-11:17:24] [V] [TRT] --------------- Timing Runner: ConvTranspose_56 + BatchNormalization_57 + Relu_58 (CudnnDeconvolution)
[08/10/2023-11:17:24] [V] [TRT] Tactic: 0x0000000000000000 Time: 6.84962
[08/10/2023-11:17:24] [V] [TRT] Tactic: 0x0000000000000001 Time: 22.2079
[08/10/2023-11:17:25] [V] [TRT] Tactic: 0x0000000000000003 Time: 71.5695
[08/10/2023-11:17:25] [V] [TRT] Fastest Tactic: 0x0000000000000000 Time: 6.84962
[08/10/2023-11:17:25] [W] [TRT] Weights [name=ConvTranspose_56 + BatchNormalization_57 + Relu_58.weight] had the following issues when converted to FP16:
[08/10/2023-11:17:25] [W] [TRT]  - Subnormal FP16 values detected. 
[08/10/2023-11:17:25] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.
[08/10/2023-11:17:25] [V] [TRT] --------------- Timing Runner: ConvTranspose_56 + BatchNormalization_57 + Relu_58 (GemmDeconvolution)
[08/10/2023-11:17:25] [V] [TRT] Tactic: 0x0000000000000000 Time: 1.23396
[08/10/2023-11:17:25] [V] [TRT] Fastest Tactic: 0x0000000000000000 Time: 1.23396
[08/10/2023-11:17:25] [W] [TRT] Weights [name=ConvTranspose_56 + BatchNormalization_57 + Relu_58.weight] had the following issues when converted to FP16:
[08/10/2023-11:17:25] [W] [TRT]  - Subnormal FP16 values detected. 
[08/10/2023-11:17:25] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.
[08/10/2023-11:17:25] [V] [TRT] --------------- Timing Runner: ConvTranspose_56 + BatchNormalization_57 + Relu_58 (CaskDeconvolution)
[08/10/2023-11:17:25] [V] [TRT] CaskDeconvolution has no valid tactics for this config, skipping
[08/10/2023-11:17:25] [W] [TRT] Weights [name=ConvTranspose_56 + BatchNormalization_57 + Relu_58.weight] had the following issues when converted to FP16:
[08/10/2023-11:17:25] [W] [TRT]  - Subnormal FP16 values detected. 
[08/10/2023-11:17:25] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.
[08/10/2023-11:17:25] [V] [TRT] --------------- Timing Runner: ConvTranspose_56 + BatchNormalization_57 + Relu_58 (CaskDeconvolutionV2)
[08/10/2023-11:17:25] [V] [TRT] CaskDeconvolutionV2 has no valid tactics for this config, skipping
[08/10/2023-11:17:25] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: GemmDeconvolution Tactic: 0x0000000000000000
[08/10/2023-11:17:25] [V] [TRT] *************** Autotuning format combination: Half(1036800,8100:2,90,1) -> Half(4147200,32400:2,180,1) ***************
[08/10/2023-11:17:25] [W] [TRT] Weights [name=ConvTranspose_56 + BatchNormalization_57 + Relu_58.weight] had the following issues when converted to FP16:
[08/10/2023-11:17:25] [W] [TRT]  - Subnormal FP16 values detected. 
[08/10/2023-11:17:25] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.
[08/10/2023-11:17:25] [V] [TRT] --------------- Timing Runner: ConvTranspose_56 + BatchNormalization_57 + Relu_58 (CudnnDeconvolution)
[08/10/2023-11:17:25] [V] [TRT] CudnnDeconvolution has no valid tactics for this config, skipping
[08/10/2023-11:17:25] [W] [TRT] Weights [name=ConvTranspose_56 + BatchNormalization_57 + Relu_58.weight] had the following issues when converted to FP16:
[08/10/2023-11:17:25] [W] [TRT]  - Subnormal FP16 values detected. 
[08/10/2023-11:17:25] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.
[08/10/2023-11:17:25] [V] [TRT] --------------- Timing Runner: ConvTranspose_56 + BatchNormalization_57 + Relu_58 (GemmDeconvolution)
[08/10/2023-11:17:25] [V] [TRT] Tactic: 0x0000000000000000 Time: 2.32974
[08/10/2023-11:17:25] [V] [TRT] Fastest Tactic: 0x0000000000000000 Time: 2.32974
[08/10/2023-11:17:25] [W] [TRT] Weights [name=ConvTranspose_56 + BatchNormalization_57 + Relu_58.weight] had the following issues when converted to FP16:
[08/10/2023-11:17:25] [W] [TRT]  - Subnormal FP16 values detected. 
[08/10/2023-11:17:25] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.
[08/10/2023-11:17:25] [V] [TRT] --------------- Timing Runner: ConvTranspose_56 + BatchNormalization_57 + Relu_58 (CaskDeconvolution)
[08/10/2023-11:17:25] [V] [TRT] CaskDeconvolution has no valid tactics for this config, skipping
[08/10/2023-11:17:25] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: GemmDeconvolution Tactic: 0x0000000000000000
[08/10/2023-11:17:25] [V] [TRT] *************** Autotuning format combination: Half(259200,1:8,2880,32) -> Half(1036800,1:8,5760,32) ***************
[08/10/2023-11:17:25] [W] [TRT] Weights [name=ConvTranspose_56 + BatchNormalization_57 + Relu_58.weight] had the following issues when converted to FP16:
[08/10/2023-11:17:25] [W] [TRT]  - Subnormal FP16 values detected. 
[08/10/2023-11:17:25] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.
[08/10/2023-11:17:25] [V] [TRT] --------------- Timing Runner: ConvTranspose_56 + BatchNormalization_57 + Relu_58 (CudnnDeconvolution)
[08/10/2023-11:17:25] [V] [TRT] CudnnDeconvolution has no valid tactics for this config, skipping
[08/10/2023-11:17:25] [W] [TRT] Weights [name=ConvTranspose_56 + BatchNormalization_57 + Relu_58.weight] had the following issues when converted to FP16:
[08/10/2023-11:17:25] [W] [TRT]  - Subnormal FP16 values detected. 
[08/10/2023-11:17:25] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.
[08/10/2023-11:17:25] [V] [TRT] --------------- Timing Runner: ConvTranspose_56 + BatchNormalization_57 + Relu_58 (GemmDeconvolution)
[08/10/2023-11:17:25] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.574112
[08/10/2023-11:17:25] [V] [TRT] Fastest Tactic: 0x0000000000000000 Time: 0.574112
[08/10/2023-11:17:25] [W] [TRT] Weights [name=ConvTranspose_56 + BatchNormalization_57 + Relu_58.weight] had the following issues when converted to FP16:
[08/10/2023-11:17:25] [W] [TRT]  - Subnormal FP16 values detected. 
[08/10/2023-11:17:25] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.
[08/10/2023-11:17:25] [V] [TRT] --------------- Timing Runner: ConvTranspose_56 + BatchNormalization_57 + Relu_58 (CaskDeconvolution)
[08/10/2023-11:17:25] [V] [TRT] CaskDeconvolution has no valid tactics for this config, skipping
[08/10/2023-11:17:25] [W] [TRT] Weights [name=ConvTranspose_56 + BatchNormalization_57 + Relu_58.weight] had the following issues when converted to FP16:
[08/10/2023-11:17:25] [W] [TRT]  - Subnormal FP16 values detected. 
[08/10/2023-11:17:25] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.
[08/10/2023-11:17:25] [V] [TRT] --------------- Timing Runner: ConvTranspose_56 + BatchNormalization_57 + Relu_58 (CaskDeconvolutionV2)
[08/10/2023-11:17:25] [V] [TRT] ConvTranspose_56 + BatchNormalization_57 + Relu_58 Set Tactic Name: sm80_xmma_deconv_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x64x64_stage4_warpsize2x2x1_g1_tensor16x8x16 Tactic: 0x350fa92a3841d3f8
[08/10/2023-11:17:25] [V] [TRT] Tactic: 0x350fa92a3841d3f8 Time: 1.12186
[08/10/2023-11:17:25] [V] [TRT] ConvTranspose_56 + BatchNormalization_57 + Relu_58 Set Tactic Name: sm80_xmma_deconv_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x128x64_stage3_warpsize2x2x1_g1_tensor16x8x16_strided Tactic: 0x3d8e7318766eb7dc
[08/10/2023-11:17:25] [V] [TRT] Tactic: 0x3d8e7318766eb7dc Time: 0.439657
[08/10/2023-11:17:25] [V] [TRT] ConvTranspose_56 + BatchNormalization_57 + Relu_58 Set Tactic Name: sm80_xmma_deconv_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x128x32_stage4_warpsize2x2x1_g1_tensor16x8x16_strided Tactic: 0x611e899dccd3003a
[08/10/2023-11:17:25] [V] [TRT] Tactic: 0x611e899dccd3003a Time: 0.303794
[08/10/2023-11:17:25] [V] [TRT] ConvTranspose_56 + BatchNormalization_57 + Relu_58 Set Tactic Name: sm80_xmma_deconv_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x64x64_stage3_warpsize2x2x1_g1_tensor16x8x16 Tactic: 0x7bb6e0d53fc5ff66
[08/10/2023-11:17:25] [V] [TRT] Tactic: 0x7bb6e0d53fc5ff66 Time: 0.887881
[08/10/2023-11:17:25] [V] [TRT] ConvTranspose_56 + BatchNormalization_57 + Relu_58 Set Tactic Name: sm80_xmma_deconv_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x128x32_stage5_warpsize2x2x1_g1_tensor16x8x16_strided Tactic: 0x7cc22b876c6d2b93
[08/10/2023-11:17:25] [V] [TRT] Tactic: 0x7cc22b876c6d2b93 Time: 0.405801
[08/10/2023-11:17:25] [V] [TRT] ConvTranspose_56 + BatchNormalization_57 + Relu_58 Set Tactic Name: sm80_xmma_deconv_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x64x64_stage4_warpsize2x2x1_g1_tensor16x8x16_strided Tactic: 0x7f88a42f41dd3253
[08/10/2023-11:17:25] [V] [TRT] Tactic: 0x7f88a42f41dd3253 Time: 0.649938
[08/10/2023-11:17:25] [V] [TRT] ConvTranspose_56 + BatchNormalization_57 + Relu_58 Set Tactic Name: sm80_xmma_deconv_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x32x32_stage4_warpsize4x1x1_g1_tensor16x8x16 Tactic: 0x86e575ac0953a218
[08/10/2023-11:17:25] [V] [TRT] Tactic: 0x86e575ac0953a218 Time: 1.10021
[08/10/2023-11:17:25] [V] [TRT] ConvTranspose_56 + BatchNormalization_57 + Relu_58 Set Tactic Name: sm80_xmma_deconv_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x64x32_stage5_warpsize2x2x1_g1_tensor16x8x16 Tactic: 0x93d63ec59e26a994
[08/10/2023-11:17:25] [V] [TRT] Tactic: 0x93d63ec59e26a994 Time: 0.914126
[08/10/2023-11:17:25] [V] [TRT] ConvTranspose_56 + BatchNormalization_57 + Relu_58 Set Tactic Name: sm80_xmma_deconv_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x64x32_stage3_warpsize4x1x1_g1_tensor16x8x16_strided Tactic: 0x9c2afe40f043a2af
[08/10/2023-11:17:25] [V] [TRT] Tactic: 0x9c2afe40f043a2af Time: 0.365417
[08/10/2023-11:17:25] [V] [TRT] ConvTranspose_56 + BatchNormalization_57 + Relu_58 Set Tactic Name: sm80_xmma_deconv_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x32x64_stage5_warpsize2x2x1_g1_tensor16x8x16 Tactic: 0x9e7119e6665ed23e
[08/10/2023-11:17:25] [V] [TRT] Tactic: 0x9e7119e6665ed23e Time: 2.02451
[08/10/2023-11:17:25] [V] [TRT] ConvTranspose_56 + BatchNormalization_57 + Relu_58 Set Tactic Name: sm80_xmma_deconv_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x32x32_stage4_warpsize4x1x1_g1_tensor16x8x16_strided Tactic: 0xa7e51cc94b1422a2
[08/10/2023-11:17:25] [V] [TRT] Tactic: 0xa7e51cc94b1422a2 Time: 0.564274
[08/10/2023-11:17:25] [V] [TRT] ConvTranspose_56 + BatchNormalization_57 + Relu_58 Set Tactic Name: sm80_xmma_deconv_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x32x32_stage4_warpsize4x1x1_g1_tensor16x8x16 Tactic: 0xad900cb3d99acf3d
[08/10/2023-11:17:25] [V] [TRT] Tactic: 0xad900cb3d99acf3d Time: 1.26404
[08/10/2023-11:17:25] [V] [TRT] ConvTranspose_56 + BatchNormalization_57 + Relu_58 Set Tactic Name: sm80_xmma_deconv_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x32x64_stage5_warpsize2x2x1_g1_tensor16x8x16_strided Tactic: 0xbe1fd751d1abb95e
[08/10/2023-11:17:25] [V] [TRT] Tactic: 0xbe1fd751d1abb95e Time: 1.21313
[08/10/2023-11:17:25] [V] [TRT] ConvTranspose_56 + BatchNormalization_57 + Relu_58 Set Tactic Name: sm80_xmma_deconv_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x64x32_stage5_warpsize2x2x1_g1_tensor16x8x16_strided Tactic: 0xd8431473bb5f91ee
[08/10/2023-11:17:25] [V] [TRT] Tactic: 0xd8431473bb5f91ee Time: 0.448969
[08/10/2023-11:17:25] [V] [TRT] ConvTranspose_56 + BatchNormalization_57 + Relu_58 Set Tactic Name: sm80_xmma_deconv_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x128x32_stage3_warpsize4x2x1_g1_tensor16x8x16_strided Tactic: 0xdec24e75875e331b
[08/10/2023-11:17:25] [V] [TRT] Tactic: 0xdec24e75875e331b Time: 0.375694
[08/10/2023-11:17:25] [V] [TRT] ConvTranspose_56 + BatchNormalization_57 + Relu_58 Set Tactic Name: sm80_xmma_deconv_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x64x32_stage3_warpsize4x1x1_g1_tensor16x8x16 Tactic: 0xff74b048c704a0c3
[08/10/2023-11:17:25] [V] [TRT] Tactic: 0xff74b048c704a0c3 Time: 0.796713
[08/10/2023-11:17:25] [V] [TRT] Fastest Tactic: 0x611e899dccd3003a Time: 0.303794
[08/10/2023-11:17:25] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskDeconvolutionV2 Tactic: 0x611e899dccd3003a
[08/10/2023-11:17:25] [V] [TRT] *************** Autotuning format combination: Half(129600,1:16,1440,16) -> Half(518400,1:16,2880,16) ***************
[08/10/2023-11:17:25] [W] [TRT] Weights [name=ConvTranspose_56 + BatchNormalization_57 + Relu_58.weight] had the following issues when converted to FP16:
[08/10/2023-11:17:25] [W] [TRT]  - Subnormal FP16 values detected. 
[08/10/2023-11:17:25] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.
[08/10/2023-11:17:25] [V] [TRT] --------------- Timing Runner: ConvTranspose_56 + BatchNormalization_57 + Relu_58 (CudnnDeconvolution)
[08/10/2023-11:17:25] [V] [TRT] CudnnDeconvolution has no valid tactics for this config, skipping
[08/10/2023-11:17:25] [W] [TRT] Weights [name=ConvTranspose_56 + BatchNormalization_57 + Relu_58.weight] had the following issues when converted to FP16:
[08/10/2023-11:17:25] [W] [TRT]  - Subnormal FP16 values detected. 
[08/10/2023-11:17:25] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.
[08/10/2023-11:17:25] [V] [TRT] --------------- Timing Runner: ConvTranspose_56 + BatchNormalization_57 + Relu_58 (GemmDeconvolution)
[08/10/2023-11:17:25] [V] [TRT] GemmDeconvolution has no valid tactics for this config, skipping
[08/10/2023-11:17:25] [W] [TRT] Weights [name=ConvTranspose_56 + BatchNormalization_57 + Relu_58.weight] had the following issues when converted to FP16:
[08/10/2023-11:17:25] [W] [TRT]  - Subnormal FP16 values detected. 
[08/10/2023-11:17:25] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.
[08/10/2023-11:17:25] [V] [TRT] --------------- Timing Runner: ConvTranspose_56 + BatchNormalization_57 + Relu_58 (CaskDeconvolution)
[08/10/2023-11:17:25] [V] [TRT] CaskDeconvolution has no valid tactics for this config, skipping
[08/10/2023-11:17:25] [W] [TRT] Weights [name=ConvTranspose_56 + BatchNormalization_57 + Relu_58.weight] had the following issues when converted to FP16:
[08/10/2023-11:17:25] [W] [TRT]  - Subnormal FP16 values detected. 
[08/10/2023-11:17:25] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.
[08/10/2023-11:17:25] [V] [TRT] --------------- Timing Runner: ConvTranspose_56 + BatchNormalization_57 + Relu_58 (CaskDeconvolutionV2)
[08/10/2023-11:17:25] [V] [TRT] CaskDeconvolutionV2 has no valid tactics for this config, skipping
[08/10/2023-11:17:25] [V] [TRT] =============== Computing costs for 
[08/10/2023-11:17:25] [V] [TRT] *************** Autotuning format combination: Float(16588800,32400,180,1) -> Float(2073600,32400,180,1) ***************
[08/10/2023-11:17:25] [V] [TRT] --------------- Timing Runner: Conv_60 + Relu_61 (CudaDepthwiseConvolution)
[08/10/2023-11:17:25] [V] [TRT] CudaDepthwiseConvolution has no valid tactics for this config, skipping
[08/10/2023-11:17:25] [V] [TRT] --------------- Timing Runner: Conv_60 + Relu_61 (FusedConvActConvolution)
[08/10/2023-11:17:25] [V] [TRT] FusedConvActConvolution has no valid tactics for this config, skipping
[08/10/2023-11:17:25] [V] [TRT] --------------- Timing Runner: Conv_60 + Relu_61 (CudnnConvolution)
[08/10/2023-11:17:25] [V] [TRT] Tactic: 0x0000000000000000 Time: 16.9773
[08/10/2023-11:17:25] [V] [TRT] Tactic: 0x0000000000000001 Time: 3.89072
[08/10/2023-11:17:26] [V] [TRT] Tactic: 0x0000000000000002 Time: 32.7024
[08/10/2023-11:17:26] [V] [TRT] Tactic: 0x0000000000000004 skipped. Scratch requested: 17585799168, available: 4294967296
[08/10/2023-11:17:26] [V] [TRT] Tactic: 0x0000000000000005 Time: 32.756
[08/10/2023-11:17:26] [V] [TRT] Tactic: 0x0000000000000006 Time: 5.92442
[08/10/2023-11:17:26] [V] [TRT] Tactic: 0x0000000000000038 Time: 16.7437
[08/10/2023-11:17:26] [V] [TRT] Tactic: 0x0000000000000039 Time: 3.82551
[08/10/2023-11:17:27] [V] [TRT] Tactic: 0x000000000000003a Time: 36.1446
[08/10/2023-11:17:27] [V] [TRT] Tactic: 0x000000000000003c skipped. Scratch requested: 17585799168, available: 4294967296
[08/10/2023-11:17:27] [V] [TRT] Tactic: 0x000000000000003d Time: 32.3214
[08/10/2023-11:17:27] [V] [TRT] Tactic: 0x000000000000003e Time: 5.94276
[08/10/2023-11:17:27] [V] [TRT] Tactic: 0x0000000000000070 Time: 16.7087
[08/10/2023-11:17:28] [V] [TRT] Tactic: 0x0000000000000071 Time: 10.579
[08/10/2023-11:17:28] [V] [TRT] Tactic: 0x0000000000000072 Time: 36.4468
[08/10/2023-11:17:28] [V] [TRT] Tactic: 0x0000000000000074 skipped. Scratch requested: 17585799168, available: 4294967296
[08/10/2023-11:17:28] [V] [TRT] Tactic: 0x0000000000000075 Time: 32.7278
[08/10/2023-11:17:28] [V] [TRT] Tactic: 0x0000000000000076 Time: 5.97501
[08/10/2023-11:17:28] [V] [TRT] Fastest Tactic: 0x0000000000000039 Time: 3.82551
[08/10/2023-11:17:28] [V] [TRT] Setting workspace to 17585799168enables more tactics for profiling
[08/10/2023-11:17:28] [V] [TRT] --------------- Timing Runner: Conv_60 + Relu_61 (CaskConvolution)
[08/10/2023-11:17:28] [V] [TRT] Conv_60 + Relu_61 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x128x8_stage3_warpsize1x4x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0x01cf8ce2da913006
[08/10/2023-11:17:29] [V] [TRT] Tactic: 0x01cf8ce2da913006 Time: 19.9694
[08/10/2023-11:17:29] [V] [TRT] Conv_60 + Relu_61 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x64x8_stage3_warpsize2x2x1_g1_ffma_aligna4_alignc4 Tactic: 0x12dbf7d94ee3696d
[08/10/2023-11:17:29] [V] [TRT] Tactic: 0x12dbf7d94ee3696d Time: 10.8957
[08/10/2023-11:17:29] [V] [TRT] Conv_60 + Relu_61 Set Tactic Name: ampere_scudnn_128x64_relu_small_nn_v1 Tactic: 0x3f243c490d502deb
[08/10/2023-11:17:29] [V] [TRT] Tactic: 0x3f243c490d502deb Time: 10.8619
[08/10/2023-11:17:29] [V] [TRT] Conv_60 + Relu_61 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0x4727434768e46395
[08/10/2023-11:17:29] [V] [TRT] Tactic: 0x4727434768e46395 Time: 11.7556
[08/10/2023-11:17:29] [V] [TRT] Conv_60 + Relu_61 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize256x128x8_stage3_warpsize4x2x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0x4efce38acc876f5c
[08/10/2023-11:17:29] [V] [TRT] Tactic: 0x4efce38acc876f5c Time: 20.5443
[08/10/2023-11:17:29] [V] [TRT] Conv_60 + Relu_61 Set Tactic Name: ampere_scudnn_128x128_relu_small_nn_v1 Tactic: 0x503619c69ae500ff
[08/10/2023-11:17:29] [V] [TRT] Tactic: 0x503619c69ae500ff Time: 18.9376
[08/10/2023-11:17:29] [V] [TRT] Conv_60 + Relu_61 Set Tactic Name: ampere_scudnn_128x128_relu_xregs_large_nn_v1 Tactic: 0x5403ad713f811a18
[08/10/2023-11:17:29] [V] [TRT] Tactic: 0x5403ad713f811a18 Time: 19.4982
[08/10/2023-11:17:29] [V] [TRT] Conv_60 + Relu_61 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x128x8_stage3_warpsize1x4x1_g1_ffma_aligna4_alignc4 Tactic: 0x5aa723e0481da855
[08/10/2023-11:17:30] [V] [TRT] Tactic: 0x5aa723e0481da855 Time: 20.0991
[08/10/2023-11:17:30] [V] [TRT] Conv_60 + Relu_61 Set Tactic Name: ampere_scudnn_128x64_relu_xregs_large_nn_v1 Tactic: 0x5deb29b7a8e275f7
[08/10/2023-11:17:30] [V] [TRT] Tactic: 0x5deb29b7a8e275f7 Time: 10.2995
[08/10/2023-11:17:30] [V] [TRT] Conv_60 + Relu_61 Set Tactic Name: ampere_scudnn_winograd_128x128_ldg1_ldg4_relu_tile148t_nt_v1 Tactic: 0x94119b4c514b211a
[08/10/2023-11:17:30] [V] [TRT] Tactic: 0x94119b4c514b211a Time: 5.81536
[08/10/2023-11:17:30] [V] [TRT] Conv_60 + Relu_61 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_aligna4_alignc4 Tactic: 0xa31d27de74b895ff
[08/10/2023-11:17:30] [V] [TRT] Tactic: 0xa31d27de74b895ff Time: 11.6154
[08/10/2023-11:17:30] [V] [TRT] Conv_60 + Relu_61 Set Tactic Name: ampere_scudnn_128x32_relu_small_nn_v1 Tactic: 0xa8609adc4e0ceb90
[08/10/2023-11:17:30] [V] [TRT] Tactic: 0xa8609adc4e0ceb90 Time: 12.3553
[08/10/2023-11:17:30] [V] [TRT] Conv_60 + Relu_61 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize256x128x8_stage3_warpsize4x2x1_g1_ffma_aligna4_alignc4 Tactic: 0xbb8c3889c7eacd30
[08/10/2023-11:17:30] [V] [TRT] Tactic: 0xbb8c3889c7eacd30 Time: 20.559
[08/10/2023-11:17:30] [V] [TRT] Conv_60 + Relu_61 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_aligna4_alignc4 Tactic: 0xd828f024626fa982
[08/10/2023-11:17:30] [V] [TRT] Tactic: 0xd828f024626fa982 Time: 16.4905
[08/10/2023-11:17:30] [V] [TRT] Conv_60 + Relu_61 Set Tactic Name: ampere_scudnn_128x128_relu_medium_nn_v1 Tactic: 0xf067e6205da31c2e
[08/10/2023-11:17:30] [V] [TRT] Tactic: 0xf067e6205da31c2e Time: 19.2424
[08/10/2023-11:17:30] [V] [TRT] Conv_60 + Relu_61 Set Tactic Name: ampere_scudnn_128x64_relu_medium_nn_v1 Tactic: 0xf64396b97c889179
[08/10/2023-11:17:31] [V] [TRT] Tactic: 0xf64396b97c889179 Time: 11.0618
[08/10/2023-11:17:31] [V] [TRT] Fastest Tactic: 0x94119b4c514b211a Time: 5.81536
[08/10/2023-11:17:31] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CudnnConvolution Tactic: 0x0000000000000039
[08/10/2023-11:17:31] [V] [TRT] *************** Autotuning format combination: Float(16588800,1,92160,512) -> Float(2073600,1,11520,64) ***************
[08/10/2023-11:17:31] [V] [TRT] --------------- Timing Runner: Conv_60 + Relu_61 (CaskConvolution)
[08/10/2023-11:17:31] [V] [TRT] Conv_60 + Relu_61 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize64x128x32_stage5_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0x112d7e4d280f396d
[08/10/2023-11:17:31] [V] [TRT] Tactic: 0x112d7e4d280f396d Time: 5.23662
[08/10/2023-11:17:31] [V] [TRT] Conv_60 + Relu_61 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x64x32_stage5_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0x13cc2ad8dcd32ba2
[08/10/2023-11:17:31] [V] [TRT] Tactic: 0x13cc2ad8dcd32ba2 Time: 2.83763
[08/10/2023-11:17:31] [V] [TRT] Conv_60 + Relu_61 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_aligna4_alignc4 Tactic: 0x19b688348f983aa0
[08/10/2023-11:17:31] [V] [TRT] Tactic: 0x19b688348f983aa0 Time: 10.9808
[08/10/2023-11:17:31] [V] [TRT] Conv_60 + Relu_61 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x64x8_stage3_warpsize2x2x1_g1_ffma_aligna4_alignc4 Tactic: 0x1da91d865428f237
[08/10/2023-11:17:31] [V] [TRT] Tactic: 0x1da91d865428f237 Time: 8.68331
[08/10/2023-11:17:31] [V] [TRT] Conv_60 + Relu_61 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize256x128x32_stage3_warpsize4x2x1_g1_tensor16x8x8_t1r3s3 Tactic: 0x21246c8544eff903
[08/10/2023-11:17:31] [V] [TRT] Tactic: 0x21246c8544eff903 Time: 3.39931
[08/10/2023-11:17:31] [V] [TRT] Conv_60 + Relu_61 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x128x32_stage4_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0x25b2b9d5c9d5ca0d
[08/10/2023-11:17:31] [V] [TRT] Tactic: 0x25b2b9d5c9d5ca0d Time: 3.53389
[08/10/2023-11:17:31] [V] [TRT] Conv_60 + Relu_61 Set Tactic Name: ampere_scudnn_128x64_sliced1x2_ldg4_relu_exp_small_nhwc_tn_v1 Tactic: 0x27b316f52c109002
[08/10/2023-11:17:31] [V] [TRT] Tactic: 0x27b316f52c109002 Time: 10.5656
[08/10/2023-11:17:31] [V] [TRT] Conv_60 + Relu_61 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage4_warpsize2x2x1_g1_tensor16x8x8_t1r3s3 Tactic: 0x2d80d2b384ab13f1
[08/10/2023-11:17:31] [V] [TRT] Tactic: 0x2d80d2b384ab13f1 Time: 2.93465
[08/10/2023-11:17:31] [V] [TRT] Conv_60 + Relu_61 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize256x128x32_stage3_warpsize4x2x1_g1_tensor16x8x8 Tactic: 0x36303f399d3ee4fd
[08/10/2023-11:17:31] [V] [TRT] Tactic: 0x36303f399d3ee4fd Time: 4.34751
[08/10/2023-11:17:31] [V] [TRT] Conv_60 + Relu_61 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage4_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0x385bfab9332b1413
[08/10/2023-11:17:31] [V] [TRT] Tactic: 0x385bfab9332b1413 Time: 3.14637
[08/10/2023-11:17:31] [V] [TRT] Conv_60 + Relu_61 Set Tactic Name: ampere_scudnn_128x32_sliced1x4_ldg4_relu_exp_medium_nhwc_tn_v1 Tactic: 0x3e191488237fab8f
[08/10/2023-11:17:31] [V] [TRT] Tactic: 0x3e191488237fab8f Time: 10.7952
[08/10/2023-11:17:31] [V] [TRT] Conv_60 + Relu_61 Set Tactic Name: ampere_scudnn_128x32_sliced1x4_ldg4_relu_exp_small_nhwc_tn_v1 Tactic: 0x3e2b881168d9689d
[08/10/2023-11:17:32] [V] [TRT] Tactic: 0x3e2b881168d9689d Time: 10.622
[08/10/2023-11:17:32] [V] [TRT] Conv_60 + Relu_61 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x256x8_stage3_warpsize1x4x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0x3f0c846d6379bc98
[08/10/2023-11:17:32] [V] [TRT] Tactic: 0x3f0c846d6379bc98 Time: 30.9693
[08/10/2023-11:17:32] [V] [TRT] Conv_60 + Relu_61 Set Tactic Name: ampere_scudnn_128x64_sliced1x2_ldg4_relu_exp_medium_nhwc_tn_v1 Tactic: 0x412c44dfeaf9161d
[08/10/2023-11:17:32] [V] [TRT] Tactic: 0x412c44dfeaf9161d Time: 10.6896
[08/10/2023-11:17:32] [V] [TRT] Conv_60 + Relu_61 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x256x32_stage3_warpsize2x4x1_g1_tensor16x8x8 Tactic: 0x482b48242255b8ce
[08/10/2023-11:17:32] [V] [TRT] Tactic: 0x482b48242255b8ce Time: 6.77244
[08/10/2023-11:17:32] [V] [TRT] Conv_60 + Relu_61 Set Tactic Name: ampere_scudnn_128x128_relu_exp_small_nhwc_tn_v1 Tactic: 0x5030121339a48bf3
[08/10/2023-11:17:32] [V] [TRT] Tactic: 0x5030121339a48bf3 Time: 19.052
[08/10/2023-11:17:32] [V] [TRT] Conv_60 + Relu_61 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x256x32_stage3_warpsize2x4x1_g1_tensor16x8x8_t1r3s3 Tactic: 0x614e89f7852edbc3
[08/10/2023-11:17:32] [V] [TRT] Tactic: 0x614e89f7852edbc3 Time: 6.47263
[08/10/2023-11:17:32] [V] [TRT] Conv_60 + Relu_61 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_aligna4_alignc4 Tactic: 0x62835fce994f06dd
[08/10/2023-11:17:32] [V] [TRT] Tactic: 0x62835fce994f06dd Time: 9.51494
[08/10/2023-11:17:32] [V] [TRT] Conv_60 + Relu_61 Set Tactic Name: ampere_scudnn_128x128_ldg4_relu_exp_large_nhwc_tn_v1 Tactic: 0x634e99502974e4da
[08/10/2023-11:17:33] [V] [TRT] Tactic: 0x634e99502974e4da Time: 18.5787
[08/10/2023-11:17:33] [V] [TRT] Conv_60 + Relu_61 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0x65e41d81f093b482
[08/10/2023-11:17:33] [V] [TRT] Tactic: 0x65e41d81f093b482 Time: 4.18458
[08/10/2023-11:17:33] [V] [TRT] Conv_60 + Relu_61 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize64x128x32_stage5_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0x65f71607d20d5438
[08/10/2023-11:17:33] [V] [TRT] Tactic: 0x65f71607d20d5438 Time: 4.70903
[08/10/2023-11:17:33] [V] [TRT] Conv_60 + Relu_61 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x64x32_stage5_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0x6716429226d146f7
[08/10/2023-11:17:33] [V] [TRT] Tactic: 0x6716429226d146f7 Time: 2.44483
[08/10/2023-11:17:33] [V] [TRT] Conv_60 + Relu_61 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize64x32x64_stage5_warpsize2x2x1_g1_tensor16x8x8_t1r3s3 Tactic: 0x679a281f7660c436
[08/10/2023-11:17:33] [V] [TRT] Tactic: 0x679a281f7660c436 Time: 4.27288
[08/10/2023-11:17:33] [V] [TRT] Conv_60 + Relu_61 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize64x32x64_stage5_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0x6e2a3c7a7fc5e4e1
[08/10/2023-11:17:33] [V] [TRT] Tactic: 0x6e2a3c7a7fc5e4e1 Time: 4.23605
[08/10/2023-11:17:33] [V] [TRT] Conv_60 + Relu_61 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x64x32_stage5_warpsize2x2x1_g1_tensor16x8x8_t1r3s3 Tactic: 0x7aa21282feb15f1a
[08/10/2023-11:17:33] [V] [TRT] Tactic: 0x7aa21282feb15f1a Time: 2.48792
[08/10/2023-11:17:33] [V] [TRT] Conv_60 + Relu_61 Set Tactic Name: ampere_scudnn_128x128_ldg4_relu_exp_medium_nhwc_tn_v1 Tactic: 0x7bc32c782b800c48
[08/10/2023-11:17:33] [V] [TRT] Tactic: 0x7bc32c782b800c48 Time: 18.585
[08/10/2023-11:17:33] [V] [TRT] Conv_60 + Relu_61 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize256x64x8_stage3_warpsize2x2x1_g1_ffma_aligna4_alignc4 Tactic: 0x8014228ec08b4d49
[08/10/2023-11:17:33] [V] [TRT] Tactic: 0x8014228ec08b4d49 Time: 8.59243
[08/10/2023-11:17:33] [V] [TRT] Conv_60 + Relu_61 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage4_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0x818413b69874bb35
[08/10/2023-11:17:33] [V] [TRT] Tactic: 0x818413b69874bb35 Time: 2.97048
[08/10/2023-11:17:33] [V] [TRT] Conv_60 + Relu_61 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0x94a7db94ba744c45
[08/10/2023-11:17:33] [V] [TRT] Tactic: 0x94a7db94ba744c45 Time: 9.75479
[08/10/2023-11:17:33] [V] [TRT] Conv_60 + Relu_61 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x256x32_stage3_warpsize2x4x1_g1_tensor16x8x8 Tactic: 0x998c97842e775a17
[08/10/2023-11:17:33] [V] [TRT] Tactic: 0x998c97842e775a17 Time: 6.7141
[08/10/2023-11:17:33] [V] [TRT] Conv_60 + Relu_61 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8_t1r3s3 Tactic: 0x999e005e3b016ea6
[08/10/2023-11:17:33] [V] [TRT] Tactic: 0x999e005e3b016ea6 Time: 3.69253
[08/10/2023-11:17:33] [V] [TRT] Conv_60 + Relu_61 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize256x64x32_stage3_warpsize4x2x1_g1_tensor16x8x8_t1r3s3 Tactic: 0xa9a06d0633580c0c
[08/10/2023-11:17:33] [V] [TRT] Tactic: 0xa9a06d0633580c0c Time: 2.48763
[08/10/2023-11:17:33] [V] [TRT] Conv_60 + Relu_61 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0xb443c221fcb1565b
[08/10/2023-11:17:33] [V] [TRT] Tactic: 0xb443c221fcb1565b Time: 3.78212
[08/10/2023-11:17:33] [V] [TRT] Conv_60 + Relu_61 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage3_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0xb47d7d926de02dee
[08/10/2023-11:17:34] [V] [TRT] Tactic: 0xb47d7d926de02dee Time: 2.80421
[08/10/2023-11:17:34] [V] [TRT] Conv_60 + Relu_61 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize256x64x32_stage3_warpsize4x2x1_g1_tensor16x8x8 Tactic: 0xba0185279ccb2b85
[08/10/2023-11:17:34] [V] [TRT] Tactic: 0xba0185279ccb2b85 Time: 2.57535
[08/10/2023-11:17:34] [V] [TRT] Conv_60 + Relu_61 Set Tactic Name: ampere_scudnn_128x64_sliced1x2_ldg4_relu_exp_large_nhwc_tn_v1 Tactic: 0xbdfdef6b84f7ccc9
[08/10/2023-11:17:34] [V] [TRT] Tactic: 0xbdfdef6b84f7ccc9 Time: 10.5935
[08/10/2023-11:17:34] [V] [TRT] Conv_60 + Relu_61 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage3_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0xc0a715d897e240bb
[08/10/2023-11:17:34] [V] [TRT] Tactic: 0xc0a715d897e240bb Time: 2.5628
[08/10/2023-11:17:34] [V] [TRT] Conv_60 + Relu_61 Set Tactic Name: ampere_scudnn_128x128_relu_exp_large_nhwc_tn_v1 Tactic: 0xca7eeb8d9143d738
[08/10/2023-11:17:34] [V] [TRT] Tactic: 0xca7eeb8d9143d738 Time: 19.3421
[08/10/2023-11:17:34] [V] [TRT] Conv_60 + Relu_61 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize256x64x32_stage3_warpsize4x2x1_g1_tensor16x8x8 Tactic: 0xcedbed6d66c946d0
[08/10/2023-11:17:34] [V] [TRT] Tactic: 0xcedbed6d66c946d0 Time: 2.368
[08/10/2023-11:17:34] [V] [TRT] Conv_60 + Relu_61 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x128x8_stage3_warpsize1x4x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0xd15dd11d64344e83
[08/10/2023-11:17:34] [V] [TRT] Tactic: 0xd15dd11d64344e83 Time: 16.6559
[08/10/2023-11:17:34] [V] [TRT] Conv_60 + Relu_61 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize64x128x32_stage5_warpsize2x2x1_g1_tensor16x8x8_t1r3s3 Tactic: 0xd4428a7355769d7d
[08/10/2023-11:17:34] [V] [TRT] Tactic: 0xd4428a7355769d7d Time: 4.6009
[08/10/2023-11:17:34] [V] [TRT] Conv_60 + Relu_61 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize64x32x64_stage5_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0xd7f5d575d49a4bc7
[08/10/2023-11:17:34] [V] [TRT] Tactic: 0xd7f5d575d49a4bc7 Time: 5.10233
[08/10/2023-11:17:34] [V] [TRT] Conv_60 + Relu_61 Set Tactic Name: ampere_scudnn_128x128_relu_exp_medium_nhwc_tn_v1 Tactic: 0xd9031472c05adf51
[08/10/2023-11:17:34] [V] [TRT] Tactic: 0xd9031472c05adf51 Time: 19.3116
[08/10/2023-11:17:34] [V] [TRT] Conv_60 + Relu_61 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x128x32_stage4_warpsize2x2x1_g1_tensor16x8x8_t1r3s3 Tactic: 0xd920b33c9bd27143
[08/10/2023-11:17:34] [V] [TRT] Tactic: 0xd920b33c9bd27143 Time: 3.3949
[08/10/2023-11:17:34] [V] [TRT] Conv_60 + Relu_61 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize256x128x32_stage3_warpsize4x2x1_g1_tensor16x8x8 Tactic: 0xe797e099911c0624
[08/10/2023-11:17:35] [V] [TRT] Tactic: 0xe797e099911c0624 Time: 3.66419
[08/10/2023-11:17:35] [V] [TRT] Conv_60 + Relu_61 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage3_warpsize2x2x1_g1_tensor16x8x8_t1r3s3 Tactic: 0xeb65d4c1e8fa2294
[08/10/2023-11:17:35] [V] [TRT] Tactic: 0xeb65d4c1e8fa2294 Time: 2.66733
[08/10/2023-11:17:35] [V] [TRT] Conv_60 + Relu_61 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x128x32_stage4_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0xf4156675c5f728d4
[08/10/2023-11:17:35] [V] [TRT] Tactic: 0xf4156675c5f728d4 Time: 5.55245
[08/10/2023-11:17:35] [V] [TRT] Conv_60 + Relu_61 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x64x8_stage3_warpsize2x2x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0xf48db81f02eca9ee
[08/10/2023-11:17:35] [V] [TRT] Tactic: 0xf48db81f02eca9ee Time: 8.59006
[08/10/2023-11:17:35] [V] [TRT] Conv_60 + Relu_61 Set Tactic Name: ampere_scudnn_128x128_ldg4_relu_exp_small_nhwc_tn_v1 Tactic: 0xf90060ce8193b811
[08/10/2023-11:17:35] [V] [TRT] Tactic: 0xf90060ce8193b811 Time: 18.525
[08/10/2023-11:17:35] [V] [TRT] Fastest Tactic: 0xcedbed6d66c946d0 Time: 2.368
[08/10/2023-11:17:35] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0xcedbed6d66c946d0
[08/10/2023-11:17:35] [V] [TRT] *************** Autotuning format combination: Float(4147200,1:4,23040,128) -> Float(518400,1:4,2880,16) ***************
[08/10/2023-11:17:35] [V] [TRT] --------------- Timing Runner: Conv_60 + Relu_61 (CaskConvolution)
[08/10/2023-11:17:35] [V] [TRT] Conv_60 + Relu_61 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize64x128x32_stage5_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0x112d7e4d280f396d
[08/10/2023-11:17:35] [V] [TRT] Tactic: 0x112d7e4d280f396d Time: 5.26137
[08/10/2023-11:17:35] [V] [TRT] Conv_60 + Relu_61 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x64x32_stage5_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0x13cc2ad8dcd32ba2
[08/10/2023-11:17:35] [V] [TRT] Tactic: 0x13cc2ad8dcd32ba2 Time: 2.84066
[08/10/2023-11:17:35] [V] [TRT] Conv_60 + Relu_61 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize256x128x32_stage3_warpsize4x2x1_g1_tensor16x8x8_t1r3s3 Tactic: 0x21246c8544eff903
[08/10/2023-11:17:35] [V] [TRT] Tactic: 0x21246c8544eff903 Time: 3.40383
[08/10/2023-11:17:35] [V] [TRT] Conv_60 + Relu_61 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x128x32_stage4_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0x25b2b9d5c9d5ca0d
[08/10/2023-11:17:35] [V] [TRT] Tactic: 0x25b2b9d5c9d5ca0d Time: 3.53387
[08/10/2023-11:17:35] [V] [TRT] Conv_60 + Relu_61 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage4_warpsize2x2x1_g1_tensor16x8x8_t1r3s3 Tactic: 0x2d80d2b384ab13f1
[08/10/2023-11:17:35] [V] [TRT] Tactic: 0x2d80d2b384ab13f1 Time: 2.84603
[08/10/2023-11:17:35] [V] [TRT] Conv_60 + Relu_61 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize256x128x32_stage3_warpsize4x2x1_g1_tensor16x8x8 Tactic: 0x36303f399d3ee4fd
[08/10/2023-11:17:35] [V] [TRT] Tactic: 0x36303f399d3ee4fd Time: 4.34382
[08/10/2023-11:17:35] [V] [TRT] Conv_60 + Relu_61 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage4_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0x385bfab9332b1413
[08/10/2023-11:17:35] [V] [TRT] Tactic: 0x385bfab9332b1413 Time: 3.14628
[08/10/2023-11:17:35] [V] [TRT] Conv_60 + Relu_61 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x256x32_stage3_warpsize2x4x1_g1_tensor16x8x8 Tactic: 0x482b48242255b8ce
[08/10/2023-11:17:35] [V] [TRT] Tactic: 0x482b48242255b8ce Time: 6.9264
[08/10/2023-11:17:35] [V] [TRT] Conv_60 + Relu_61 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x256x32_stage3_warpsize2x4x1_g1_tensor16x8x8_t1r3s3 Tactic: 0x614e89f7852edbc3
[08/10/2023-11:17:35] [V] [TRT] Tactic: 0x614e89f7852edbc3 Time: 6.48575
[08/10/2023-11:17:35] [V] [TRT] Conv_60 + Relu_61 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0x65e41d81f093b482
[08/10/2023-11:17:35] [V] [TRT] Tactic: 0x65e41d81f093b482 Time: 4.17503
[08/10/2023-11:17:35] [V] [TRT] Conv_60 + Relu_61 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize64x128x32_stage5_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0x65f71607d20d5438
[08/10/2023-11:17:36] [V] [TRT] Tactic: 0x65f71607d20d5438 Time: 4.79811
[08/10/2023-11:17:36] [V] [TRT] Conv_60 + Relu_61 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x64x32_stage5_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0x6716429226d146f7
[08/10/2023-11:17:36] [V] [TRT] Tactic: 0x6716429226d146f7 Time: 2.4503
[08/10/2023-11:17:36] [V] [TRT] Conv_60 + Relu_61 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize64x32x64_stage5_warpsize2x2x1_g1_tensor16x8x8_t1r3s3 Tactic: 0x679a281f7660c436
[08/10/2023-11:17:36] [V] [TRT] Tactic: 0x679a281f7660c436 Time: 4.19542
[08/10/2023-11:17:36] [V] [TRT] Conv_60 + Relu_61 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize64x32x64_stage5_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0x6e2a3c7a7fc5e4e1
[08/10/2023-11:17:36] [V] [TRT] Tactic: 0x6e2a3c7a7fc5e4e1 Time: 4.23324
[08/10/2023-11:17:36] [V] [TRT] Conv_60 + Relu_61 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x64x32_stage5_warpsize2x2x1_g1_tensor16x8x8_t1r3s3 Tactic: 0x7aa21282feb15f1a
[08/10/2023-11:17:36] [V] [TRT] Tactic: 0x7aa21282feb15f1a Time: 2.48149
[08/10/2023-11:17:36] [V] [TRT] Conv_60 + Relu_61 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage4_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0x818413b69874bb35
[08/10/2023-11:17:36] [V] [TRT] Tactic: 0x818413b69874bb35 Time: 2.97222
[08/10/2023-11:17:36] [V] [TRT] Conv_60 + Relu_61 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x256x32_stage3_warpsize2x4x1_g1_tensor16x8x8 Tactic: 0x998c97842e775a17
[08/10/2023-11:17:36] [V] [TRT] Tactic: 0x998c97842e775a17 Time: 6.86499
[08/10/2023-11:17:36] [V] [TRT] Conv_60 + Relu_61 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8_t1r3s3 Tactic: 0x999e005e3b016ea6
[08/10/2023-11:17:36] [V] [TRT] Tactic: 0x999e005e3b016ea6 Time: 3.6999
[08/10/2023-11:17:36] [V] [TRT] Conv_60 + Relu_61 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize256x64x32_stage3_warpsize4x2x1_g1_tensor16x8x8_t1r3s3 Tactic: 0xa9a06d0633580c0c
[08/10/2023-11:17:36] [V] [TRT] Tactic: 0xa9a06d0633580c0c Time: 2.34639
[08/10/2023-11:17:36] [V] [TRT] Conv_60 + Relu_61 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0xb443c221fcb1565b
[08/10/2023-11:17:36] [V] [TRT] Tactic: 0xb443c221fcb1565b Time: 3.77542
[08/10/2023-11:17:36] [V] [TRT] Conv_60 + Relu_61 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage3_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0xb47d7d926de02dee
[08/10/2023-11:17:36] [V] [TRT] Tactic: 0xb47d7d926de02dee Time: 2.65239
[08/10/2023-11:17:36] [V] [TRT] Conv_60 + Relu_61 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize256x64x32_stage3_warpsize4x2x1_g1_tensor16x8x8 Tactic: 0xba0185279ccb2b85
[08/10/2023-11:17:36] [V] [TRT] Tactic: 0xba0185279ccb2b85 Time: 2.69998
[08/10/2023-11:17:36] [V] [TRT] Conv_60 + Relu_61 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage3_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0xc0a715d897e240bb
[08/10/2023-11:17:36] [V] [TRT] Tactic: 0xc0a715d897e240bb Time: 2.49291
[08/10/2023-11:17:36] [V] [TRT] Conv_60 + Relu_61 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize256x64x32_stage3_warpsize4x2x1_g1_tensor16x8x8 Tactic: 0xcedbed6d66c946d0
[08/10/2023-11:17:36] [V] [TRT] Tactic: 0xcedbed6d66c946d0 Time: 2.47152
[08/10/2023-11:17:36] [V] [TRT] Conv_60 + Relu_61 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize64x128x32_stage5_warpsize2x2x1_g1_tensor16x8x8_t1r3s3 Tactic: 0xd4428a7355769d7d
[08/10/2023-11:17:36] [V] [TRT] Tactic: 0xd4428a7355769d7d Time: 4.57806
[08/10/2023-11:17:36] [V] [TRT] Conv_60 + Relu_61 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize64x32x64_stage5_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0xd7f5d575d49a4bc7
[08/10/2023-11:17:36] [V] [TRT] Tactic: 0xd7f5d575d49a4bc7 Time: 5.22813
[08/10/2023-11:17:36] [V] [TRT] Conv_60 + Relu_61 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x128x32_stage4_warpsize2x2x1_g1_tensor16x8x8_t1r3s3 Tactic: 0xd920b33c9bd27143
[08/10/2023-11:17:36] [V] [TRT] Tactic: 0xd920b33c9bd27143 Time: 3.41671
[08/10/2023-11:17:36] [V] [TRT] Conv_60 + Relu_61 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize256x128x32_stage3_warpsize4x2x1_g1_tensor16x8x8 Tactic: 0xe797e099911c0624
[08/10/2023-11:17:36] [V] [TRT] Tactic: 0xe797e099911c0624 Time: 3.61544
[08/10/2023-11:17:36] [V] [TRT] Conv_60 + Relu_61 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage3_warpsize2x2x1_g1_tensor16x8x8_t1r3s3 Tactic: 0xeb65d4c1e8fa2294
[08/10/2023-11:17:36] [V] [TRT] Tactic: 0xeb65d4c1e8fa2294 Time: 2.40573
[08/10/2023-11:17:36] [V] [TRT] Conv_60 + Relu_61 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x128x32_stage4_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0xf4156675c5f728d4
[08/10/2023-11:17:36] [V] [TRT] Tactic: 0xf4156675c5f728d4 Time: 5.56905
[08/10/2023-11:17:36] [V] [TRT] Fastest Tactic: 0xa9a06d0633580c0c Time: 2.34639
[08/10/2023-11:17:36] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0xa9a06d0633580c0c
[08/10/2023-11:17:36] [V] [TRT] *************** Autotuning format combination: Half(16588800,32400,180,1) -> Half(2073600,32400,180,1) ***************
[08/10/2023-11:17:37] [W] [TRT] Weights [name=Conv_60 + Relu_61.weight] had the following issues when converted to FP16:
[08/10/2023-11:17:37] [W] [TRT]  - Subnormal FP16 values detected. 
[08/10/2023-11:17:37] [W] [TRT]  - Values less than smallest positive FP16 Subnormal value detected. Converting to FP16 minimum subnormalized value. 
[08/10/2023-11:17:37] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.
[08/10/2023-11:17:37] [V] [TRT] --------------- Timing Runner: Conv_60 + Relu_61 (CudnnConvolution)
[08/10/2023-11:17:37] [V] [TRT] Tactic: 0x0000000000000000 Time: 25.3739
[08/10/2023-11:17:37] [V] [TRT] Tactic: 0x0000000000000001 Time: 12.6027
[08/10/2023-11:17:37] [V] [TRT] Tactic: 0x0000000000000002 Time: 36.4611
[08/10/2023-11:17:37] [V] [TRT] Tactic: 0x0000000000000004 skipped. Scratch requested: 17585799168, available: 4294967296
[08/10/2023-11:17:38] [V] [TRT] Tactic: 0x0000000000000005 Time: 48.0761
[08/10/2023-11:17:38] [V] [TRT] Tactic: 0x0000000000000006 Time: 6.89842
[08/10/2023-11:17:38] [V] [TRT] Tactic: 0x0000000000000038 Time: 25.3427
[08/10/2023-11:17:38] [V] [TRT] Tactic: 0x000000000000003a Time: 36.4815
[08/10/2023-11:17:38] [V] [TRT] Tactic: 0x000000000000003c skipped. Scratch requested: 17585799168, available: 4294967296
[08/10/2023-11:17:39] [V] [TRT] Tactic: 0x000000000000003d Time: 36.0292
[08/10/2023-11:17:39] [V] [TRT] Tactic: 0x000000000000003e Time: 6.89861
[08/10/2023-11:17:39] [V] [TRT] Fastest Tactic: 0x0000000000000006 Time: 6.89842
[08/10/2023-11:17:39] [V] [TRT] Setting workspace to 17585799168enables more tactics for profiling
[08/10/2023-11:17:39] [W] [TRT] Weights [name=Conv_60 + Relu_61.weight] had the following issues when converted to FP16:
[08/10/2023-11:17:39] [W] [TRT]  - Subnormal FP16 values detected. 
[08/10/2023-11:17:39] [W] [TRT]  - Values less than smallest positive FP16 Subnormal value detected. Converting to FP16 minimum subnormalized value. 
[08/10/2023-11:17:39] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.
[08/10/2023-11:17:39] [V] [TRT] --------------- Timing Runner: Conv_60 + Relu_61 (CaskConvolution)
[08/10/2023-11:17:39] [V] [TRT] CaskConvolution has no valid tactics for this config, skipping
[08/10/2023-11:17:39] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CudnnConvolution Tactic: 0x0000000000000006
[08/10/2023-11:17:39] [V] [TRT] *************** Autotuning format combination: Half(8294400,32400:2,180,1) -> Half(1036800,32400:2,180,1) ***************
[08/10/2023-11:17:39] [W] [TRT] Weights [name=Conv_60 + Relu_61.weight] had the following issues when converted to FP16:
[08/10/2023-11:17:39] [W] [TRT]  - Subnormal FP16 values detected. 
[08/10/2023-11:17:39] [W] [TRT]  - Values less than smallest positive FP16 Subnormal value detected. Converting to FP16 minimum subnormalized value. 
[08/10/2023-11:17:39] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.
[08/10/2023-11:17:39] [V] [TRT] --------------- Timing Runner: Conv_60 + Relu_61 (FusedConvActConvolution)
[08/10/2023-11:17:39] [V] [TRT] FusedConvActConvolution has no valid tactics for this config, skipping
[08/10/2023-11:17:39] [W] [TRT] Weights [name=Conv_60 + Relu_61.weight] had the following issues when converted to FP16:
[08/10/2023-11:17:39] [W] [TRT]  - Subnormal FP16 values detected. 
[08/10/2023-11:17:39] [W] [TRT]  - Values less than smallest positive FP16 Subnormal value detected. Converting to FP16 minimum subnormalized value. 
[08/10/2023-11:17:39] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.
[08/10/2023-11:17:39] [V] [TRT] --------------- Timing Runner: Conv_60 + Relu_61 (CaskConvolution)
[08/10/2023-11:17:39] [V] [TRT] CaskConvolution has no valid tactics for this config, skipping
[08/10/2023-11:17:39] [V] [TRT] *************** Autotuning format combination: Half(2073600,1:8,11520,64) -> Float(2073600,32400,180,1) ***************
[08/10/2023-11:17:39] [W] [TRT] Weights [name=Conv_60 + Relu_61.weight] had the following issues when converted to FP16:
[08/10/2023-11:17:39] [W] [TRT]  - Subnormal FP16 values detected. 
[08/10/2023-11:17:39] [W] [TRT]  - Values less than smallest positive FP16 Subnormal value detected. Converting to FP16 minimum subnormalized value. 
[08/10/2023-11:17:39] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.
[08/10/2023-11:17:39] [V] [TRT] --------------- Timing Runner: Conv_60 + Relu_61 (CaskConvolution)
[08/10/2023-11:17:39] [V] [TRT] CaskConvolution has no valid tactics for this config, skipping
[08/10/2023-11:17:39] [V] [TRT] *************** Autotuning format combination: Half(2073600,1:8,11520,64) -> Half(259200,1:8,1440,8) ***************
[08/10/2023-11:17:39] [W] [TRT] Weights [name=Conv_60 + Relu_61.weight] had the following issues when converted to FP16:
[08/10/2023-11:17:39] [W] [TRT]  - Subnormal FP16 values detected. 
[08/10/2023-11:17:39] [W] [TRT]  - Values less than smallest positive FP16 Subnormal value detected. Converting to FP16 minimum subnormalized value. 
[08/10/2023-11:17:39] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.
[08/10/2023-11:17:39] [V] [TRT] --------------- Timing Runner: Conv_60 + Relu_61 (CudaDepthwiseConvolution)
[08/10/2023-11:17:39] [V] [TRT] CudaDepthwiseConvolution has no valid tactics for this config, skipping
[08/10/2023-11:17:39] [W] [TRT] Weights [name=Conv_60 + Relu_61.weight] had the following issues when converted to FP16:
[08/10/2023-11:17:39] [W] [TRT]  - Subnormal FP16 values detected. 
[08/10/2023-11:17:39] [W] [TRT]  - Values less than smallest positive FP16 Subnormal value detected. Converting to FP16 minimum subnormalized value. 
[08/10/2023-11:17:39] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.
[08/10/2023-11:17:39] [V] [TRT] --------------- Timing Runner: Conv_60 + Relu_61 (CaskConvolution)
[08/10/2023-11:17:39] [V] [TRT] Conv_60 + Relu_61 Set Tactic Name: ampere_h16816cudnn_256x128_ldg8_relu_exp_medium_nhwc_tn_v1 Tactic: 0x03896956a39a1203
[08/10/2023-11:17:39] [V] [TRT] Tactic: 0x03896956a39a1203 Time: 1.8365
[08/10/2023-11:17:39] [V] [TRT] Conv_60 + Relu_61 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x128x32_stage3_warpsize4x2x1_g1_tensor16x8x16_t1r3s3 Tactic: 0x048d6d0400f33439
[08/10/2023-11:17:39] [V] [TRT] Tactic: 0x048d6d0400f33439 Time: 1.75023
[08/10/2023-11:17:39] [V] [TRT] Conv_60 + Relu_61 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x64x64_stage3_warpsize2x2x1_g1_tensor16x8x16_t1r3s3 Tactic: 0x05b6220f243edacd
[08/10/2023-11:17:39] [V] [TRT] Tactic: 0x05b6220f243edacd Time: 1.06615
[08/10/2023-11:17:39] [V] [TRT] Conv_60 + Relu_61 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x64x32_stage3_warpsize4x1x1_g1_tensor16x8x16_t1r3s3_aACCESS Tactic: 0x0866ddee325d07a6
[08/10/2023-11:17:39] [V] [TRT] Tactic: 0x0866ddee325d07a6 Time: 0.977179
[08/10/2023-11:17:39] [V] [TRT] Conv_60 + Relu_61 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x128x32_stage4_warpsize2x2x1_g1_tensor16x8x16 Tactic: 0x0e07ff4f4f7c1ac9
[08/10/2023-11:17:39] [V] [TRT] Tactic: 0x0e07ff4f4f7c1ac9 Time: 1.72911
[08/10/2023-11:17:39] [V] [TRT] Conv_60 + Relu_61 Set Tactic Name: ampere_h16816cudnn_256x128_ldg8_relu_exp_stages_64x3_small_nhwc_tn_v1 Tactic: 0x0e199750c30c6928
[08/10/2023-11:17:39] [V] [TRT] Tactic: 0x0e199750c30c6928 Time: 1.7243
[08/10/2023-11:17:39] [V] [TRT] Conv_60 + Relu_61 Set Tactic Name: ampere_h1688cudnn_128x128_ldg8_relu_exp_medium_nhwc_tn_v1 Tactic: 0x100c1ad308e08d35
[08/10/2023-11:17:39] [V] [TRT] Tactic: 0x100c1ad308e08d35 Time: 2.20114
[08/10/2023-11:17:39] [V] [TRT] Conv_60 + Relu_61 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x128x32_stage5_warpsize2x2x1_g1_tensor16x8x16_t1r3s3 Tactic: 0x17ebf0c9f418f10a
[08/10/2023-11:17:39] [V] [TRT] Tactic: 0x17ebf0c9f418f10a Time: 1.75938
[08/10/2023-11:17:39] [V] [TRT] Conv_60 + Relu_61 Set Tactic Name: ampere_h16816cudnn_128x64_ldg8_relu_exp_stages_64x4_medium_nhwc_tn_v1 Tactic: 0x196cbc423a9bb69e
[08/10/2023-11:17:39] [V] [TRT] Tactic: 0x196cbc423a9bb69e Time: 1.11962
[08/10/2023-11:17:39] [V] [TRT] Conv_60 + Relu_61 Set Tactic Name: ampere_h16816cudnn_128x64_sliced1x2_ldg8_relu_exp_stages_64x3_small_nhwc_tn_v1 Tactic: 0x19822de884f42a6a
[08/10/2023-11:17:39] [V] [TRT] Tactic: 0x19822de884f42a6a Time: 1.02217
[08/10/2023-11:17:39] [V] [TRT] Conv_60 + Relu_61 Set Tactic Name: ampere_h16816cudnn_64x64_sliced1x2_ldg8_relu_exp_stages_64x5_small_nhwc_tn_v1 Tactic: 0x1a5ba808ad4cc5a8
[08/10/2023-11:17:39] [V] [TRT] Tactic: 0x1a5ba808ad4cc5a8 Time: 1.33001
[08/10/2023-11:17:39] [V] [TRT] Conv_60 + Relu_61 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x64x64_stage3_warpsize2x2x1_g1_tensor16x8x16 Tactic: 0x21b295c0c8f6c95a
[08/10/2023-11:17:39] [V] [TRT] Tactic: 0x21b295c0c8f6c95a Time: 1.08083
[08/10/2023-11:17:39] [V] [TRT] Conv_60 + Relu_61 Set Tactic Name: ampere_h16816cudnn_128x128_ldg8_relu_exp_medium_nhwc_tn_v1 Tactic: 0x234580e8a194335c
[08/10/2023-11:17:39] [V] [TRT] Tactic: 0x234580e8a194335c Time: 1.68437
[08/10/2023-11:17:39] [V] [TRT] Conv_60 + Relu_61 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x128x32_stage4_warpsize2x2x1_g1_tensor16x8x16_t1r3s3_aACCESS Tactic: 0x245530c34bd6090f
[08/10/2023-11:17:39] [V] [TRT] Tactic: 0x245530c34bd6090f Time: 1.61083
[08/10/2023-11:17:39] [V] [TRT] Conv_60 + Relu_61 Set Tactic Name: ampere_h16816cudnn_64x64_sliced1x2_ldg8_relu_exp_stages_64x6_small_nhwc_tn_v1 Tactic: 0x24e01e7405cfdfe9
[08/10/2023-11:17:39] [V] [TRT] Tactic: 0x24e01e7405cfdfe9 Time: 1.26959
[08/10/2023-11:17:39] [V] [TRT] Conv_60 + Relu_61 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x64x64_stage3_warpsize2x2x1_g1_tensor16x8x16_t1r3s3_aACCESS Tactic: 0x263a38afd75e3a43
[08/10/2023-11:17:39] [V] [TRT] Tactic: 0x263a38afd75e3a43 Time: 0.965166
[08/10/2023-11:17:39] [V] [TRT] Conv_60 + Relu_61 Set Tactic Name: ampere_h16816cudnn_64x128_sliced1x2_ldg8_relu_exp_stages_64x4_large_nhwc_tn_v1 Tactic: 0x2721a7f18c2700db
[08/10/2023-11:17:39] [V] [TRT] Tactic: 0x2721a7f18c2700db Time: 2.0002
[08/10/2023-11:17:39] [V] [TRT] Conv_60 + Relu_61 Set Tactic Name: ampere_h16816cudnn_128x64_sliced1x2_ldg8_relu_exp_stages_64x3_medium_nhwc_tn_v1 Tactic: 0x29329b5741ea05f2
[08/10/2023-11:17:39] [V] [TRT] Tactic: 0x29329b5741ea05f2 Time: 1.00975
[08/10/2023-11:17:39] [V] [TRT] Conv_60 + Relu_61 Set Tactic Name: ampere_h16816cudnn_128x128_ldg8_relu_exp_large_nhwc_tn_v1 Tactic: 0x2970ae65966d569d
[08/10/2023-11:17:39] [V] [TRT] Tactic: 0x2970ae65966d569d Time: 1.68905
[08/10/2023-11:17:39] [V] [TRT] Conv_60 + Relu_61 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x256x64_stage3_warpsize1x4x1_g1_tensor16x8x16 Tactic: 0x29dc14520e0e4eb6
[08/10/2023-11:17:39] [V] [TRT] Tactic: 0x29dc14520e0e4eb6 Time: 3.31472
[08/10/2023-11:17:39] [V] [TRT] Conv_60 + Relu_61 Set Tactic Name: ampere_h16816cudnn_256x64_ldg8_relu_exp_stages_64x3_large_nhwc_tn_v1 Tactic: 0x2cb4a662694e89d3
[08/10/2023-11:17:39] [V] [TRT] Tactic: 0x2cb4a662694e89d3 Time: 0.944238
[08/10/2023-11:17:39] [V] [TRT] Conv_60 + Relu_61 Set Tactic Name: ampere_h16816cudnn_128x128_ldg8_relu_exp_stages_64x3_medium_nhwc_tn_v1 Tactic: 0x2eaa2202de9404d6
[08/10/2023-11:17:39] [V] [TRT] Tactic: 0x2eaa2202de9404d6 Time: 1.79733
[08/10/2023-11:17:39] [V] [TRT] Conv_60 + Relu_61 Set Tactic Name: ampere_h1688cudnn_256x128_ldg8_relu_exp_medium_nhwc_tn_v1 Tactic: 0x30c0f36d0aeeac6a
[08/10/2023-11:17:39] [V] [TRT] Tactic: 0x30c0f36d0aeeac6a Time: 2.31873
[08/10/2023-11:17:39] [V] [TRT] Conv_60 + Relu_61 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x64x64_stage4_warpsize2x2x1_g1_tensor16x8x16_t1r3s3_aACCESS Tactic: 0x30e8a8d7a953e5e9
[08/10/2023-11:17:39] [V] [TRT] Tactic: 0x30e8a8d7a953e5e9 Time: 1.11877
[08/10/2023-11:17:39] [V] [TRT] Conv_60 + Relu_61 Set Tactic Name: ampere_h16816cudnn_128x64_ldg8_relu_exp_stages_64x4_large_nhwc_tn_v1 Tactic: 0x3197d5044d71e98e
[08/10/2023-11:17:39] [V] [TRT] Tactic: 0x3197d5044d71e98e Time: 1.13001
[08/10/2023-11:17:39] [V] [TRT] Conv_60 + Relu_61 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x64x64_stage4_warpsize2x2x1_g1_tensor16x8x16_t1r3s3 Tactic: 0x31d93dc22d2af081
[08/10/2023-11:17:39] [V] [TRT] Tactic: 0x31d93dc22d2af081 Time: 1.36531
[08/10/2023-11:17:39] [V] [TRT] Conv_60 + Relu_61 Set Tactic Name: ampere_h1688cudnn_128x128_ldg8_relu_exp_small_nhwc_tn_v1 Tactic: 0x3369260c04f9ad73
[08/10/2023-11:17:39] [V] [TRT] Tactic: 0x3369260c04f9ad73 Time: 2.12969
[08/10/2023-11:17:39] [V] [TRT] Conv_60 + Relu_61 Set Tactic Name: ampere_h16816cudnn_64x128_ldg8_relu_exp_stages_64x4_small_nhwc_tn_v1 Tactic: 0x3b5fa0f2a8fc2410
[08/10/2023-11:17:39] [V] [TRT] Tactic: 0x3b5fa0f2a8fc2410 Time: 2.01482
[08/10/2023-11:17:39] [V] [TRT] Conv_60 + Relu_61 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x32x32_stage4_warpsize4x1x1_g1_tensor16x8x16_t1r3s3 Tactic: 0x3e7eb35b91b9fa63
[08/10/2023-11:17:39] [V] [TRT] Tactic: 0x3e7eb35b91b9fa63 Time: 1.69372
[08/10/2023-11:17:39] [V] [TRT] Conv_60 + Relu_61 Set Tactic Name: ampere_h16816cudnn_256x64_ldg8_relu_exp_stages_64x3_medium_nhwc_tn_v1 Tactic: 0x3f031df0e579d52c
[08/10/2023-11:17:39] [V] [TRT] Tactic: 0x3f031df0e579d52c Time: 0.946825
[08/10/2023-11:17:39] [V] [TRT] Conv_60 + Relu_61 Set Tactic Name: ampere_h16816cudnn_256x64_sliced1x2_ldg8_relu_exp_small_nhwc_tn_v1 Tactic: 0x42a5b8709d0039be
[08/10/2023-11:17:39] [V] [TRT] Tactic: 0x42a5b8709d0039be Time: 1.57663
[08/10/2023-11:17:39] [V] [TRT] Conv_60 + Relu_61 Set Tactic Name: ampere_h16816cudnn_256x128_ldg8_relu_exp_small_nhwc_tn_v1 Tactic: 0x463794ee4acffd1f
[08/10/2023-11:17:39] [V] [TRT] Tactic: 0x463794ee4acffd1f Time: 1.82732
[08/10/2023-11:17:39] [V] [TRT] Conv_60 + Relu_61 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x64x64_stage4_warpsize2x2x1_g1_tensor16x8x16 Tactic: 0x4a81ea1e51436a30
[08/10/2023-11:17:39] [V] [TRT] Tactic: 0x4a81ea1e51436a30 Time: 1.40224
[08/10/2023-11:17:39] [V] [TRT] Conv_60 + Relu_61 Set Tactic Name: ampere_h1688cudnn_256x64_sliced1x2_ldg8_relu_exp_small_nhwc_tn_v1 Tactic: 0x4aed1956bd10a795
[08/10/2023-11:17:39] [V] [TRT] Tactic: 0x4aed1956bd10a795 Time: 1.46747
[08/10/2023-11:17:40] [V] [TRT] Conv_60 + Relu_61 Set Tactic Name: ampere_h16816cudnn_64x128_sliced1x2_ldg8_relu_exp_stages_64x3_large_nhwc_tn_v1 Tactic: 0x4fc05923455fc266
[08/10/2023-11:17:40] [V] [TRT] Tactic: 0x4fc05923455fc266 Time: 1.69163
[08/10/2023-11:17:40] [V] [TRT] Conv_60 + Relu_61 Set Tactic Name: ampere_h16816cudnn_256x64_ldg8_relu_exp_large_nhwc_tn_v1 Tactic: 0x5013c38f55afa9ba
[08/10/2023-11:17:40] [V] [TRT] Tactic: 0x5013c38f55afa9ba Time: 1.14711
[08/10/2023-11:17:40] [V] [TRT] Conv_60 + Relu_61 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x64x32_stage3_warpsize4x1x1_g1_tensor16x8x16_t1r3s3 Tactic: 0x529f4431bdae94f5
[08/10/2023-11:17:40] [V] [TRT] Tactic: 0x529f4431bdae94f5 Time: 0.915643
[08/10/2023-11:17:40] [V] [TRT] Conv_60 + Relu_61 Set Tactic Name: ampere_h16816cudnn_64x128_ldg8_relu_exp_stages_64x3_small_nhwc_tn_v1 Tactic: 0x53be5e206184e6ad
[08/10/2023-11:17:40] [V] [TRT] Tactic: 0x53be5e206184e6ad Time: 1.91555
[08/10/2023-11:17:40] [V] [TRT] Conv_60 + Relu_61 Set Tactic Name: ampere_h16816cudnn_64x64_ldg8_relu_exp_stages_64x5_large_nhwc_tn_v1 Tactic: 0x544bff7ff9c5d908
[08/10/2023-11:17:40] [V] [TRT] Tactic: 0x544bff7ff9c5d908 Time: 1.39934
[08/10/2023-11:17:40] [V] [TRT] Conv_60 + Relu_61 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x64x64_stage3_warpsize2x2x1_g1_tensor16x8x16 Tactic: 0x5568fd8a32f4a40f
[08/10/2023-11:17:40] [V] [TRT] Tactic: 0x5568fd8a32f4a40f Time: 1.06352
[08/10/2023-11:17:40] [V] [TRT] Conv_60 + Relu_61 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x128x64_stage3_warpsize2x2x1_g1_tensor16x8x16 Tactic: 0x57c9a5ff682354a6
[08/10/2023-11:17:40] [V] [TRT] Tactic: 0x57c9a5ff682354a6 Time: 1.7063
[08/10/2023-11:17:40] [V] [TRT] Conv_60 + Relu_61 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x32x64_stage5_warpsize2x2x1_g1_tensor16x8x16 Tactic: 0x5820b3dda403c4d0
[08/10/2023-11:17:40] [V] [TRT] Tactic: 0x5820b3dda403c4d0 Time: 2.00644
[08/10/2023-11:17:40] [V] [TRT] Conv_60 + Relu_61 Set Tactic Name: ampere_h16816cudnn_128x128_ldg8_relu_exp_stages_64x3_small_nhwc_tn_v1 Tactic: 0x58816bf2e9c36afb
[08/10/2023-11:17:40] [V] [TRT] Tactic: 0x58816bf2e9c36afb Time: 1.76901
[08/10/2023-11:17:40] [V] [TRT] Conv_60 + Relu_61 Set Tactic Name: ampere_h16816cudnn_128x64_ldg8_relu_exp_stages_64x3_large_nhwc_tn_v1 Tactic: 0x59762bd684092b33
[08/10/2023-11:17:40] [V] [TRT] Tactic: 0x59762bd684092b33 Time: 1.1114
[08/10/2023-11:17:40] [V] [TRT] Conv_60 + Relu_61 Set Tactic Name: ampere_h16816cudnn_128x64_sliced1x2_ldg8_relu_exp_stages_64x4_medium_nhwc_tn_v1 Tactic: 0x59c5c647b4c76593
[08/10/2023-11:17:40] [V] [TRT] Tactic: 0x59c5c647b4c76593 Time: 1.05895
[08/10/2023-11:17:40] [V] [TRT] Conv_60 + Relu_61 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x128x64_stage3_warpsize4x2x1_g1_tensor16x8x16 Tactic: 0x5a55274960724ba8
[08/10/2023-11:17:40] [V] [TRT] Tactic: 0x5a55274960724ba8 Time: 1.73995
[08/10/2023-11:17:40] [V] [TRT] Conv_60 + Relu_61 Set Tactic Name: ampere_h1688cudnn_256x128_ldg8_relu_exp_large_nhwc_tn_v1 Tactic: 0x5c2e1c87d85b06f1
[08/10/2023-11:17:40] [V] [TRT] Tactic: 0x5c2e1c87d85b06f1 Time: 2.30305
[08/10/2023-11:17:40] [V] [TRT] Conv_60 + Relu_61 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x256x64_stage3_warpsize1x4x1_g1_tensor16x8x16 Tactic: 0x5d067c18f40c23e3
[08/10/2023-11:17:40] [V] [TRT] Tactic: 0x5d067c18f40c23e3 Time: 3.45036
[08/10/2023-11:17:40] [V] [TRT] Conv_60 + Relu_61 Set Tactic Name: ampere_h16816cudnn_64x64_ldg8_relu_exp_stages_64x5_medium_nhwc_tn_v1 Tactic: 0x5f31c22ec167f384
[08/10/2023-11:17:40] [V] [TRT] Tactic: 0x5f31c22ec167f384 Time: 1.40779
[08/10/2023-11:17:40] [V] [TRT] Conv_60 + Relu_61 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x128x32_stage4_warpsize2x2x1_g1_tensor16x8x16_t1r3s3 Tactic: 0x60c3421152ef8e10
[08/10/2023-11:17:40] [V] [TRT] Tactic: 0x60c3421152ef8e10 Time: 1.59415
[08/10/2023-11:17:40] [V] [TRT] Conv_60 + Relu_61 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x128x32_stage3_warpsize4x2x1_g1_tensor16x8x16 Tactic: 0x60da8c7151d91e47
[08/10/2023-11:17:40] [V] [TRT] Tactic: 0x60da8c7151d91e47 Time: 1.79554
[08/10/2023-11:17:40] [V] [TRT] Conv_60 + Relu_61 Set Tactic Name: ampere_h1688cudnn_256x64_ldg8_relu_exp_medium_nhwc_tn_v1 Tactic: 0x6426696f872a3b13
[08/10/2023-11:17:40] [V] [TRT] Tactic: 0x6426696f872a3b13 Time: 1.64362
[08/10/2023-11:17:40] [V] [TRT] Conv_60 + Relu_61 Set Tactic Name: ampere_h16816cudnn_128x64_ldg8_relu_exp_stages_64x3_medium_nhwc_tn_v1 Tactic: 0x699be152cfb6d6ff
[08/10/2023-11:17:40] [V] [TRT] Tactic: 0x699be152cfb6d6ff Time: 1.11056
[08/10/2023-11:17:40] [V] [TRT] Conv_60 + Relu_61 Set Tactic Name: ampere_h16816cudnn_64x64_ldg8_relu_exp_stages_64x6_large_nhwc_tn_v1 Tactic: 0x6af049035146c349
[08/10/2023-11:17:40] [V] [TRT] Tactic: 0x6af049035146c349 Time: 1.57286
[08/10/2023-11:17:40] [V] [TRT] Conv_60 + Relu_61 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x256x32_stage3_warpsize2x4x1_g1_tensor16x8x16 Tactic: 0x7005d10718f6c22d
[08/10/2023-11:17:40] [V] [TRT] Tactic: 0x7005d10718f6c22d Time: 3.28847
[08/10/2023-11:17:40] [V] [TRT] Conv_60 + Relu_61 Set Tactic Name: ampere_h16816cudnn_64x64_sliced1x2_ldg8_relu_exp_stages_64x5_medium_nhwc_tn_v1 Tactic: 0x706f08da35c795a5
[08/10/2023-11:17:40] [V] [TRT] Tactic: 0x706f08da35c795a5 Time: 1.33118
[08/10/2023-11:17:40] [V] [TRT] Conv_60 + Relu_61 Set Tactic Name: ampere_h16816cudnn_128x64_sliced1x2_ldg8_relu_exp_stages_64x4_small_nhwc_tn_v1 Tactic: 0x7163d33a4d8ce8d7
[08/10/2023-11:17:40] [V] [TRT] Tactic: 0x7163d33a4d8ce8d7 Time: 1.00222
[08/10/2023-11:17:40] [V] [TRT] Conv_60 + Relu_61 Set Tactic Name: ampere_h1688cudnn_256x64_ldg8_relu_exp_small_nhwc_tn_v1 Tactic: 0x7aad3976677d7155
[08/10/2023-11:17:40] [V] [TRT] Tactic: 0x7aad3976677d7155 Time: 1.53178
[08/10/2023-11:17:40] [V] [TRT] Conv_60 + Relu_61 Set Tactic Name: ampere_h16816cudnn_64x64_sliced1x2_ldg8_relu_exp_stages_64x5_large_nhwc_tn_v1 Tactic: 0x8015519605ab9963
[08/10/2023-11:17:40] [V] [TRT] Tactic: 0x8015519605ab9963 Time: 1.31959
[08/10/2023-11:17:40] [V] [TRT] Conv_60 + Relu_61 Set Tactic Name: ampere_h16816cudnn_256x64_sliced1x2_ldg8_relu_exp_medium_nhwc_tn_v1 Tactic: 0x82f8a2214b0b4178
[08/10/2023-11:17:40] [V] [TRT] Tactic: 0x82f8a2214b0b4178 Time: 1.93013
[08/10/2023-11:17:40] [V] [TRT] Conv_60 + Relu_61 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x32x32_stage4_warpsize4x1x1_g1_tensor16x8x16 Tactic: 0x834e11ecd4ab9454
[08/10/2023-11:17:40] [V] [TRT] Tactic: 0x834e11ecd4ab9454 Time: 1.69948
[08/10/2023-11:17:40] [V] [TRT] Conv_60 + Relu_61 Set Tactic Name: ampere_h16816cudnn_128x64_sliced1x2_ldg8_relu_exp_stages_64x3_large_nhwc_tn_v1 Tactic: 0x83ccd4762c1376a1
[08/10/2023-11:17:40] [V] [TRT] Tactic: 0x83ccd4762c1376a1 Time: 1.01568
[08/10/2023-11:17:40] [V] [TRT] Conv_60 + Relu_61 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x64x64_stage3_warpsize4x1x1_g1_tensor16x8x16_t1r3s3 Tactic: 0x84ee897dd1f4d2aa
[08/10/2023-11:17:40] [V] [TRT] Tactic: 0x84ee897dd1f4d2aa Time: 1.02304
[08/10/2023-11:17:40] [V] [TRT] Conv_60 + Relu_61 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x128x64_stage3_warpsize2x2x1_g1_tensor16x8x16 Tactic: 0x866e7a5f6401b67f
[08/10/2023-11:17:40] [V] [TRT] Tactic: 0x866e7a5f6401b67f Time: 1.75552
[08/10/2023-11:17:40] [V] [TRT] Conv_60 + Relu_61 Set Tactic Name: ampere_h16816cudnn_64x64_ldg8_relu_exp_stages_64x6_medium_nhwc_tn_v1 Tactic: 0x88971eec55aba850
[08/10/2023-11:17:40] [V] [TRT] Tactic: 0x88971eec55aba850 Time: 1.54615
[08/10/2023-11:17:40] [V] [TRT] Conv_60 + Relu_61 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x128x64_stage3_warpsize4x2x1_g1_tensor16x8x16 Tactic: 0x8bf2f8e96c50a971
[08/10/2023-11:17:40] [V] [TRT] Tactic: 0x8bf2f8e96c50a971 Time: 1.73275
[08/10/2023-11:17:40] [V] [TRT] Conv_60 + Relu_61 Set Tactic Name: ampere_h16816cudnn_256x128_ldg8_relu_exp_stages_64x3_large_nhwc_tn_v1 Tactic: 0x94576ece6beb35e3
[08/10/2023-11:17:40] [V] [TRT] Tactic: 0x94576ece6beb35e3 Time: 1.74909
[08/10/2023-11:17:40] [V] [TRT] Conv_60 + Relu_61 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x64x64_stage3_warpsize4x1x1_g1_tensor16x8x16 Tactic: 0x9fff6e65019cc310
[08/10/2023-11:17:40] [V] [TRT] Tactic: 0x9fff6e65019cc310 Time: 1.13904
[08/10/2023-11:17:40] [V] [TRT] Conv_60 + Relu_61 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x32x64_stage5_warpsize2x2x1_g1_tensor16x8x16_t1r3s3 Tactic: 0xa033e20ae9f412b2
[08/10/2023-11:17:40] [V] [TRT] Tactic: 0xa033e20ae9f412b2 Time: 2.27956
[08/10/2023-11:17:40] [V] [TRT] Conv_60 + Relu_61 Set Tactic Name: ampere_h16816cudnn_64x128_ldg8_relu_exp_stages_64x4_large_nhwc_tn_v1 Tactic: 0xa111596c001b78db
[08/10/2023-11:17:40] [V] [TRT] Tactic: 0xa111596c001b78db Time: 2.04823
[08/10/2023-11:17:40] [V] [TRT] Conv_60 + Relu_61 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x256x32_stage3_warpsize2x4x1_g1_tensor16x8x16 Tactic: 0xa1a20ea714d420f4
[08/10/2023-11:17:40] [V] [TRT] Tactic: 0xa1a20ea714d420f4 Time: 3.24052
[08/10/2023-11:17:40] [V] [TRT] Conv_60 + Relu_61 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x32x32_stage4_warpsize4x1x1_g1_tensor16x8x16_t1r3s3 Tactic: 0xa40cb43c296a36a8
[08/10/2023-11:17:40] [V] [TRT] Tactic: 0xa40cb43c296a36a8 Time: 1.58549
[08/10/2023-11:17:40] [V] [TRT] Conv_60 + Relu_61 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x64x32_stage3_warpsize4x1x1_g1_tensor16x8x16 Tactic: 0xa570c55d303796ff
[08/10/2023-11:17:40] [V] [TRT] Tactic: 0xa570c55d303796ff Time: 1.03294
[08/10/2023-11:17:40] [V] [TRT] Conv_60 + Relu_61 Set Tactic Name: ampere_h16816cudnn_64x64_sliced1x2_ldg8_relu_exp_stages_64x6_medium_nhwc_tn_v1 Tactic: 0xa7c9d418a10bce71
[08/10/2023-11:17:40] [V] [TRT] Tactic: 0xa7c9d418a10bce71 Time: 1.43048
[08/10/2023-11:17:40] [V] [TRT] Conv_60 + Relu_61 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x32x32_stage4_warpsize4x1x1_g1_tensor16x8x16 Tactic: 0xa83b68f30462f971
[08/10/2023-11:17:40] [V] [TRT] Tactic: 0xa83b68f30462f971 Time: 1.60353
[08/10/2023-11:17:40] [V] [TRT] Conv_60 + Relu_61 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x256x32_stage3_warpsize2x4x1_g1_tensor16x8x16_t1r3s3 Tactic: 0xa9177bbe4e767df8
[08/10/2023-11:17:40] [V] [TRT] Tactic: 0xa9177bbe4e767df8 Time: 3.23701
[08/10/2023-11:17:40] [V] [TRT] Conv_60 + Relu_61 Set Tactic Name: ampere_h1688cudnn_128x128_ldg8_relu_exp_large_nhwc_tn_v1 Tactic: 0xa927df92ac1ef1b8
[08/10/2023-11:17:40] [V] [TRT] Tactic: 0xa927df92ac1ef1b8 Time: 2.22433
[08/10/2023-11:17:40] [V] [TRT] Conv_60 + Relu_61 Set Tactic Name: ampere_h16816cudnn_128x64_ldg8_relu_exp_stages_64x4_small_nhwc_tn_v1 Tactic: 0xabd92c9ae596b545
[08/10/2023-11:17:41] [V] [TRT] Tactic: 0xabd92c9ae596b545 Time: 1.09776
[08/10/2023-11:17:41] [V] [TRT] Conv_60 + Relu_61 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x128x32_stage5_warpsize2x2x1_g1_tensor16x8x16 Tactic: 0xafd1e8bf6bd3d638
[08/10/2023-11:17:41] [V] [TRT] Tactic: 0xafd1e8bf6bd3d638 Time: 1.82902
[08/10/2023-11:17:41] [V] [TRT] Conv_60 + Relu_61 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x128x32_stage3_warpsize4x2x1_g1_tensor16x8x16 Tactic: 0xb17d53d15dfbfc9e
[08/10/2023-11:17:41] [V] [TRT] Tactic: 0xb17d53d15dfbfc9e Time: 2.09255
[08/10/2023-11:17:41] [V] [TRT] Conv_60 + Relu_61 Set Tactic Name: ampere_h16816cudnn_128x128_ldg8_relu_exp_small_nhwc_tn_v1 Tactic: 0xb33e57fb3e8a0a56
[08/10/2023-11:17:41] [V] [TRT] Tactic: 0xb33e57fb3e8a0a56 Time: 1.6782
[08/10/2023-11:17:41] [V] [TRT] Conv_60 + Relu_61 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x64x32_stage5_warpsize2x2x1_g1_tensor16x8x16_t1r3s3 Tactic: 0xb4bec086187edcfc
[08/10/2023-11:17:41] [V] [TRT] Tactic: 0xb4bec086187edcfc Time: 1.09625
[08/10/2023-11:17:41] [V] [TRT] Conv_60 + Relu_61 Set Tactic Name: ampere_h16816cudnn_256x64_ldg8_relu_exp_stages_64x3_small_nhwc_tn_v1 Tactic: 0xb6fa5ffcc1a9d518
[08/10/2023-11:17:41] [V] [TRT] Tactic: 0xb6fa5ffcc1a9d518 Time: 0.99451
[08/10/2023-11:17:41] [V] [TRT] Conv_60 + Relu_61 Set Tactic Name: ampere_h16816cudnn_64x128_sliced1x2_ldg8_relu_exp_stages_64x3_medium_nhwc_tn_v1 Tactic: 0xb85e52e87caf60a2
[08/10/2023-11:17:41] [V] [TRT] Tactic: 0xb85e52e87caf60a2 Time: 1.72901
[08/10/2023-11:17:41] [V] [TRT] Conv_60 + Relu_61 Set Tactic Name: ampere_h16816cudnn_64x128_ldg8_relu_exp_stages_64x3_medium_nhwc_tn_v1 Tactic: 0xb8d86216e1235cda
[08/10/2023-11:17:41] [V] [TRT] Tactic: 0xb8d86216e1235cda Time: 2.17493
[08/10/2023-11:17:41] [V] [TRT] Conv_60 + Relu_61 Set Tactic Name: ampere_h16816cudnn_256x128_ldg8_relu_exp_stages_64x3_medium_nhwc_tn_v1 Tactic: 0xb9171d70ba2eda4b
[08/10/2023-11:17:41] [V] [TRT] Tactic: 0xb9171d70ba2eda4b Time: 2.24173
[08/10/2023-11:17:41] [V] [TRT] Conv_60 + Relu_61 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x64x32_stage5_warpsize2x2x1_g1_tensor16x8x16 Tactic: 0xbd08239a9317f2fd
[08/10/2023-11:17:41] [V] [TRT] Tactic: 0xbd08239a9317f2fd Time: 1.19624
[08/10/2023-11:17:41] [V] [TRT] Conv_60 + Relu_61 Set Tactic Name: ampere_h16816cudnn_64x128_sliced1x2_ldg8_relu_exp_stages_64x4_small_nhwc_tn_v1 Tactic: 0xbd6f5e6f24c05c10
[08/10/2023-11:17:41] [V] [TRT] Tactic: 0xbd6f5e6f24c05c10 Time: 2.41109
[08/10/2023-11:17:41] [V] [TRT] Conv_60 + Relu_61 Set Tactic Name: ampere_h16816cudnn_64x64_sliced1x2_ldg8_relu_exp_stages_64x6_large_nhwc_tn_v1 Tactic: 0xbeaee7eaad288322
[08/10/2023-11:17:41] [V] [TRT] Tactic: 0xbeaee7eaad288322 Time: 1.87153
[08/10/2023-11:17:41] [V] [TRT] Conv_60 + Relu_61 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x128x64_stage3_warpsize2x2x1_g1_tensor16x8x16_t1r3s3 Tactic: 0xc0a02dc6095497cc
[08/10/2023-11:17:41] [V] [TRT] Tactic: 0xc0a02dc6095497cc Time: 1.89487
[08/10/2023-11:17:41] [V] [TRT] Conv_60 + Relu_61 Set Tactic Name: ampere_h16816cudnn_128x128_ldg8_relu_exp_stages_64x3_large_nhwc_tn_v1 Tactic: 0xc2cf926c41243630
[08/10/2023-11:17:41] [V] [TRT] Tactic: 0xc2cf926c41243630 Time: 2.21373
[08/10/2023-11:17:41] [V] [TRT] Conv_60 + Relu_61 Set Tactic Name: ampere_h16816cudnn_128x64_ldg8_relu_exp_stages_64x3_small_nhwc_tn_v1 Tactic: 0xc338d2482cee77f8
[08/10/2023-11:17:41] [V] [TRT] Tactic: 0xc338d2482cee77f8 Time: 1.30855
[08/10/2023-11:17:41] [V] [TRT] Conv_60 + Relu_61 Set Tactic Name: ampere_h1688cudnn_256x128_ldg8_relu_exp_small_nhwc_tn_v1 Tactic: 0xc660e51970bc5a3a
[08/10/2023-11:17:41] [V] [TRT] Tactic: 0xc660e51970bc5a3a Time: 2.60199
[08/10/2023-11:17:41] [V] [TRT] Conv_60 + Relu_61 Set Tactic Name: ampere_h16816cudnn_64x128_ldg8_relu_exp_stages_64x4_medium_nhwc_tn_v1 Tactic: 0xc82f3f06140e3cbb
[08/10/2023-11:17:41] [V] [TRT] Tactic: 0xc82f3f06140e3cbb Time: 2.50133
[08/10/2023-11:17:41] [V] [TRT] Conv_60 + Relu_61 Set Tactic Name: ampere_h16816cudnn_64x128_sliced1x2_ldg8_relu_exp_stages_64x4_medium_nhwc_tn_v1 Tactic: 0xc8a90ff8898200c3
[08/10/2023-11:17:41] [V] [TRT] Tactic: 0xc8a90ff8898200c3 Time: 2.36644
[08/10/2023-11:17:41] [V] [TRT] Conv_60 + Relu_61 Set Tactic Name: ampere_h1688cudnn_256x64_sliced1x2_ldg8_relu_exp_medium_nhwc_tn_v1 Tactic: 0xc9cc55109bb4de26
[08/10/2023-11:17:41] [V] [TRT] Tactic: 0xc9cc55109bb4de26 Time: 1.96427
[08/10/2023-11:17:41] [V] [TRT] Conv_60 + Relu_61 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x64x32_stage5_warpsize2x2x1_g1_tensor16x8x16 Tactic: 0xc9d24bd069159fa8
[08/10/2023-11:17:41] [V] [TRT] Tactic: 0xc9d24bd069159fa8 Time: 1.08352
[08/10/2023-11:17:41] [V] [TRT] Conv_60 + Relu_61 Set Tactic Name: ampere_h16816cudnn_64x128_ldg8_relu_exp_stages_64x3_large_nhwc_tn_v1 Tactic: 0xc9f0a7bec963ba66
[08/10/2023-11:17:41] [V] [TRT] Tactic: 0xc9f0a7bec963ba66 Time: 1.92507
[08/10/2023-11:17:41] [V] [TRT] Conv_60 + Relu_61 Set Tactic Name: ampere_h16816cudnn_256x64_ldg8_relu_exp_small_nhwc_tn_v1 Tactic: 0xca5d3a11fd48f571
[08/10/2023-11:17:41] [V] [TRT] Tactic: 0xca5d3a11fd48f571 Time: 1.19791
[08/10/2023-11:17:41] [V] [TRT] Conv_60 + Relu_61 Set Tactic Name: ampere_h16816cudnn_64x64_ldg8_relu_exp_stages_64x5_small_nhwc_tn_v1 Tactic: 0xce0506e1512285c3
[08/10/2023-11:17:41] [V] [TRT] Tactic: 0xce0506e1512285c3 Time: 1.40622
[08/10/2023-11:17:41] [V] [TRT] Conv_60 + Relu_61 Set Tactic Name: ampere_h1688cudnn_256x64_sliced1x2_ldg8_relu_exp_large_nhwc_tn_v1 Tactic: 0xd0a3e0c815f7fb5e
[08/10/2023-11:17:41] [V] [TRT] Tactic: 0xd0a3e0c815f7fb5e Time: 1.77886
[08/10/2023-11:17:41] [V] [TRT] Conv_60 + Relu_61 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x64x32_stage3_warpsize4x1x1_g1_tensor16x8x16 Tactic: 0xd1aaad17ca35fbaa
[08/10/2023-11:17:41] [V] [TRT] Tactic: 0xd1aaad17ca35fbaa Time: 0.964914
[08/10/2023-11:17:41] [V] [TRT] Conv_60 + Relu_61 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x128x64_stage3_warpsize4x2x1_g1_tensor16x8x16_t1r3s3 Tactic: 0xd2fca0486ca97266
[08/10/2023-11:17:41] [V] [TRT] Tactic: 0xd2fca0486ca97266 Time: 1.7574
[08/10/2023-11:17:41] [V] [TRT] Conv_60 + Relu_61 Set Tactic Name: ampere_h16816cudnn_64x128_sliced1x2_ldg8_relu_exp_stages_64x3_small_nhwc_tn_v1 Tactic: 0xd58ea0bdedb89ead
[08/10/2023-11:17:41] [V] [TRT] Tactic: 0xd58ea0bdedb89ead Time: 1.7167
[08/10/2023-11:17:41] [V] [TRT] Conv_60 + Relu_61 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x256x64_stage3_warpsize1x4x1_g1_tensor16x8x16_t1r3s3 Tactic: 0xd7c261fa01db2af9
[08/10/2023-11:17:41] [V] [TRT] Tactic: 0xd7c261fa01db2af9 Time: 3.42379
[08/10/2023-11:17:41] [V] [TRT] Conv_60 + Relu_61 Set Tactic Name: ampere_h16816cudnn_256x64_sliced1x2_ldg8_relu_exp_large_nhwc_tn_v1 Tactic: 0xd8eb41ee35e76575
[08/10/2023-11:17:41] [V] [TRT] Tactic: 0xd8eb41ee35e76575 Time: 1.96162
[08/10/2023-11:17:41] [V] [TRT] Conv_60 + Relu_61 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x128x32_stage5_warpsize2x2x1_g1_tensor16x8x16 Tactic: 0xdb0b80f591d1bb6d
[08/10/2023-11:17:41] [V] [TRT] Tactic: 0xdb0b80f591d1bb6d Time: 1.74926
[08/10/2023-11:17:41] [V] [TRT] Conv_60 + Relu_61 Set Tactic Name: ampere_h16816cudnn_256x128_ldg8_relu_exp_large_nhwc_tn_v1 Tactic: 0xdc796d70e228a1d4
[08/10/2023-11:17:41] [V] [TRT] Tactic: 0xdc796d70e228a1d4 Time: 2.01758
[08/10/2023-11:17:41] [V] [TRT] Conv_60 + Relu_61 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x32x32_stage4_warpsize4x1x1_g1_tensor16x8x16 Tactic: 0xdce100b9fe609424
[08/10/2023-11:17:41] [V] [TRT] Tactic: 0xdce100b9fe609424 Time: 1.6078
[08/10/2023-11:17:41] [V] [TRT] Conv_60 + Relu_61 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x128x32_stage4_warpsize2x2x1_g1_tensor16x8x16 Tactic: 0xdfa020ef435ef810
[08/10/2023-11:17:41] [V] [TRT] Tactic: 0xdfa020ef435ef810 Time: 1.63147
[08/10/2023-11:17:41] [V] [TRT] Conv_60 + Relu_61 Set Tactic Name: ampere_h1688cudnn_256x64_ldg8_relu_exp_large_nhwc_tn_v1 Tactic: 0xe0e3c0e8cf9a2d9e
[08/10/2023-11:17:41] [V] [TRT] Tactic: 0xe0e3c0e8cf9a2d9e Time: 1.64137
[08/10/2023-11:17:41] [V] [TRT] Conv_60 + Relu_61 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x32x64_stage5_warpsize2x2x1_g1_tensor16x8x16 Tactic: 0xe1ff5ad20f5c6bf6
[08/10/2023-11:17:41] [V] [TRT] Tactic: 0xe1ff5ad20f5c6bf6 Time: 2.00802
[08/10/2023-11:17:41] [V] [TRT] Conv_60 + Relu_61 Set Tactic Name: ampere_h16816cudnn_256x64_ldg8_relu_exp_medium_nhwc_tn_v1 Tactic: 0xe4711898bd599c36
[08/10/2023-11:17:41] [V] [TRT] Tactic: 0xe4711898bd599c36 Time: 1.1988
[08/10/2023-11:17:41] [V] [TRT] Conv_60 + Relu_61 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x64x64_stage3_warpsize4x1x1_g1_tensor16x8x16 Tactic: 0xeb25062ffb9eae45
[08/10/2023-11:17:41] [V] [TRT] Tactic: 0xeb25062ffb9eae45 Time: 1.03908
[08/10/2023-11:17:41] [V] [TRT] Conv_60 + Relu_61 Set Tactic Name: ampere_h16816cudnn_128x64_sliced1x2_ldg8_relu_exp_stages_64x4_large_nhwc_tn_v1 Tactic: 0xeb2d2aa4e56bb41c
[08/10/2023-11:17:41] [V] [TRT] Tactic: 0xeb2d2aa4e56bb41c Time: 1.07779
[08/10/2023-11:17:41] [V] [TRT] Conv_60 + Relu_61 Set Tactic Name: ampere_h16816cudnn_64x64_ldg8_relu_exp_stages_64x6_small_nhwc_tn_v1 Tactic: 0xf0beb09df9a19f82
[08/10/2023-11:17:41] [V] [TRT] Tactic: 0xf0beb09df9a19f82 Time: 1.40803
[08/10/2023-11:17:41] [V] [TRT] Conv_60 + Relu_61 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x64x64_stage4_warpsize2x2x1_g1_tensor16x8x16 Tactic: 0xf35e0311fa1cc516
[08/10/2023-11:17:41] [V] [TRT] Tactic: 0xf35e0311fa1cc516 Time: 1.36725
[08/10/2023-11:17:41] [V] [TRT] Conv_60 + Relu_61 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x32x32_stage4_warpsize4x1x1_g1_tensor16x8x16 Tactic: 0xf79479a62ea9f901
[08/10/2023-11:17:41] [V] [TRT] Tactic: 0xf79479a62ea9f901 Time: 1.70585
[08/10/2023-11:17:41] [V] [TRT] Fastest Tactic: 0x529f4431bdae94f5 Time: 0.915643
[08/10/2023-11:17:41] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x529f4431bdae94f5
[08/10/2023-11:17:41] [V] [TRT] *************** Autotuning format combination: Half(1036800,1:16,5760,32) -> Half(129600,1:16,720,4) ***************
[08/10/2023-11:17:41] [W] [TRT] Weights [name=Conv_60 + Relu_61.weight] had the following issues when converted to FP16:
[08/10/2023-11:17:41] [W] [TRT]  - Subnormal FP16 values detected. 
[08/10/2023-11:17:41] [W] [TRT]  - Values less than smallest positive FP16 Subnormal value detected. Converting to FP16 minimum subnormalized value. 
[08/10/2023-11:17:41] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.
[08/10/2023-11:17:41] [V] [TRT] --------------- Timing Runner: Conv_60 + Relu_61 (CaskConvolution)
[08/10/2023-11:17:41] [V] [TRT] Conv_60 + Relu_61 Set Tactic Name: ampere_h16816cudnn_256x128_ldg8_relu_exp_medium_nhwc_tn_v1 Tactic: 0x03896956a39a1203
[08/10/2023-11:17:41] [V] [TRT] Tactic: 0x03896956a39a1203 Time: 2.03168
[08/10/2023-11:17:41] [V] [TRT] Conv_60 + Relu_61 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x128x32_stage3_warpsize4x2x1_g1_tensor16x8x16_t1r3s3 Tactic: 0x048d6d0400f33439
[08/10/2023-11:17:42] [V] [TRT] Tactic: 0x048d6d0400f33439 Time: 1.86482
[08/10/2023-11:17:42] [V] [TRT] Conv_60 + Relu_61 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x64x64_stage3_warpsize2x2x1_g1_tensor16x8x16_t1r3s3 Tactic: 0x05b6220f243edacd
[08/10/2023-11:17:42] [V] [TRT] Tactic: 0x05b6220f243edacd Time: 1.09441
[08/10/2023-11:17:42] [V] [TRT] Conv_60 + Relu_61 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x64x32_stage3_warpsize4x1x1_g1_tensor16x8x16_t1r3s3_aACCESS Tactic: 0x0866ddee325d07a6
[08/10/2023-11:17:42] [V] [TRT] Tactic: 0x0866ddee325d07a6 Time: 0.994217
[08/10/2023-11:17:42] [V] [TRT] Conv_60 + Relu_61 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x128x32_stage4_warpsize2x2x1_g1_tensor16x8x16 Tactic: 0x0e07ff4f4f7c1ac9
[08/10/2023-11:17:42] [V] [TRT] Tactic: 0x0e07ff4f4f7c1ac9 Time: 1.73595
[08/10/2023-11:17:42] [V] [TRT] Conv_60 + Relu_61 Set Tactic Name: ampere_h16816cudnn_256x128_ldg8_relu_exp_stages_64x3_small_nhwc_tn_v1 Tactic: 0x0e199750c30c6928
[08/10/2023-11:17:42] [V] [TRT] Tactic: 0x0e199750c30c6928 Time: 1.73145
[08/10/2023-11:17:42] [V] [TRT] Conv_60 + Relu_61 Set Tactic Name: ampere_h1688cudnn_128x128_ldg8_relu_exp_medium_nhwc_tn_v1 Tactic: 0x100c1ad308e08d35
[08/10/2023-11:17:42] [V] [TRT] Tactic: 0x100c1ad308e08d35 Time: 2.20794
[08/10/2023-11:17:42] [V] [TRT] Conv_60 + Relu_61 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x128x32_stage5_warpsize2x2x1_g1_tensor16x8x16_t1r3s3 Tactic: 0x17ebf0c9f418f10a
[08/10/2023-11:17:42] [V] [TRT] Tactic: 0x17ebf0c9f418f10a Time: 1.75943
[08/10/2023-11:17:42] [V] [TRT] Conv_60 + Relu_61 Set Tactic Name: ampere_h16816cudnn_128x64_ldg8_relu_exp_stages_64x4_medium_nhwc_tn_v1 Tactic: 0x196cbc423a9bb69e
[08/10/2023-11:17:42] [V] [TRT] Tactic: 0x196cbc423a9bb69e Time: 1.10082
[08/10/2023-11:17:42] [V] [TRT] Conv_60 + Relu_61 Set Tactic Name: ampere_h16816cudnn_128x64_sliced1x2_ldg8_relu_exp_stages_64x3_small_nhwc_tn_v1 Tactic: 0x19822de884f42a6a
[08/10/2023-11:17:42] [V] [TRT] Tactic: 0x19822de884f42a6a Time: 1.02787
[08/10/2023-11:17:42] [V] [TRT] Conv_60 + Relu_61 Set Tactic Name: ampere_h16816cudnn_64x64_sliced1x2_ldg8_relu_exp_stages_64x5_small_nhwc_tn_v1 Tactic: 0x1a5ba808ad4cc5a8
[08/10/2023-11:17:42] [V] [TRT] Tactic: 0x1a5ba808ad4cc5a8 Time: 1.33968
[08/10/2023-11:17:42] [V] [TRT] Conv_60 + Relu_61 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x64x64_stage3_warpsize2x2x1_g1_tensor16x8x16 Tactic: 0x21b295c0c8f6c95a
[08/10/2023-11:17:42] [V] [TRT] Tactic: 0x21b295c0c8f6c95a Time: 1.33255
[08/10/2023-11:17:42] [V] [TRT] Conv_60 + Relu_61 Set Tactic Name: ampere_h16816cudnn_128x128_ldg8_relu_exp_medium_nhwc_tn_v1 Tactic: 0x234580e8a194335c
[08/10/2023-11:17:42] [V] [TRT] Tactic: 0x234580e8a194335c Time: 1.91865
[08/10/2023-11:17:42] [V] [TRT] Conv_60 + Relu_61 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x128x32_stage4_warpsize2x2x1_g1_tensor16x8x16_t1r3s3_aACCESS Tactic: 0x245530c34bd6090f
[08/10/2023-11:17:42] [V] [TRT] Tactic: 0x245530c34bd6090f Time: 1.82479
[08/10/2023-11:17:42] [V] [TRT] Conv_60 + Relu_61 Set Tactic Name: ampere_h16816cudnn_64x64_sliced1x2_ldg8_relu_exp_stages_64x6_small_nhwc_tn_v1 Tactic: 0x24e01e7405cfdfe9
[08/10/2023-11:17:42] [V] [TRT] Tactic: 0x24e01e7405cfdfe9 Time: 1.46854
[08/10/2023-11:17:42] [V] [TRT] Conv_60 + Relu_61 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x64x64_stage3_warpsize2x2x1_g1_tensor16x8x16_t1r3s3_aACCESS Tactic: 0x263a38afd75e3a43
[08/10/2023-11:17:42] [V] [TRT] Tactic: 0x263a38afd75e3a43 Time: 1.19093
[08/10/2023-11:17:42] [V] [TRT] Conv_60 + Relu_61 Set Tactic Name: ampere_h16816cudnn_64x128_sliced1x2_ldg8_relu_exp_stages_64x4_large_nhwc_tn_v1 Tactic: 0x2721a7f18c2700db
[08/10/2023-11:17:42] [V] [TRT] Tactic: 0x2721a7f18c2700db Time: 2.2133
[08/10/2023-11:17:42] [V] [TRT] Conv_60 + Relu_61 Set Tactic Name: ampere_h16816cudnn_128x64_sliced1x2_ldg8_relu_exp_stages_64x3_medium_nhwc_tn_v1 Tactic: 0x29329b5741ea05f2
[08/10/2023-11:17:42] [V] [TRT] Tactic: 0x29329b5741ea05f2 Time: 1.22821
[08/10/2023-11:17:42] [V] [TRT] Conv_60 + Relu_61 Set Tactic Name: ampere_h16816cudnn_128x128_ldg8_relu_exp_large_nhwc_tn_v1 Tactic: 0x2970ae65966d569d
[08/10/2023-11:17:42] [V] [TRT] Tactic: 0x2970ae65966d569d Time: 1.90588
[08/10/2023-11:17:42] [V] [TRT] Conv_60 + Relu_61 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x256x64_stage3_warpsize1x4x1_g1_tensor16x8x16 Tactic: 0x29dc14520e0e4eb6
[08/10/2023-11:17:42] [V] [TRT] Tactic: 0x29dc14520e0e4eb6 Time: 3.31698
[08/10/2023-11:17:42] [V] [TRT] Conv_60 + Relu_61 Set Tactic Name: ampere_h16816cudnn_256x64_ldg8_relu_exp_stages_64x3_large_nhwc_tn_v1 Tactic: 0x2cb4a662694e89d3
[08/10/2023-11:17:42] [V] [TRT] Tactic: 0x2cb4a662694e89d3 Time: 1.05882
[08/10/2023-11:17:42] [V] [TRT] Conv_60 + Relu_61 Set Tactic Name: ampere_h16816cudnn_128x128_ldg8_relu_exp_stages_64x3_medium_nhwc_tn_v1 Tactic: 0x2eaa2202de9404d6
[08/10/2023-11:17:42] [V] [TRT] Tactic: 0x2eaa2202de9404d6 Time: 1.80546
[08/10/2023-11:17:42] [V] [TRT] Conv_60 + Relu_61 Set Tactic Name: ampere_h1688cudnn_256x128_ldg8_relu_exp_medium_nhwc_tn_v1 Tactic: 0x30c0f36d0aeeac6a
[08/10/2023-11:17:42] [V] [TRT] Tactic: 0x30c0f36d0aeeac6a Time: 2.29967
[08/10/2023-11:17:42] [V] [TRT] Conv_60 + Relu_61 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x64x64_stage4_warpsize2x2x1_g1_tensor16x8x16_t1r3s3_aACCESS Tactic: 0x30e8a8d7a953e5e9
[08/10/2023-11:17:42] [V] [TRT] Tactic: 0x30e8a8d7a953e5e9 Time: 1.11443
[08/10/2023-11:17:42] [V] [TRT] Conv_60 + Relu_61 Set Tactic Name: ampere_h16816cudnn_128x64_ldg8_relu_exp_stages_64x4_large_nhwc_tn_v1 Tactic: 0x3197d5044d71e98e
[08/10/2023-11:17:42] [V] [TRT] Tactic: 0x3197d5044d71e98e Time: 1.13488
[08/10/2023-11:17:42] [V] [TRT] Conv_60 + Relu_61 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x64x64_stage4_warpsize2x2x1_g1_tensor16x8x16_t1r3s3 Tactic: 0x31d93dc22d2af081
[08/10/2023-11:17:42] [V] [TRT] Tactic: 0x31d93dc22d2af081 Time: 1.37134
[08/10/2023-11:17:42] [V] [TRT] Conv_60 + Relu_61 Set Tactic Name: ampere_h1688cudnn_128x128_ldg8_relu_exp_small_nhwc_tn_v1 Tactic: 0x3369260c04f9ad73
[08/10/2023-11:17:42] [V] [TRT] Tactic: 0x3369260c04f9ad73 Time: 2.38905
[08/10/2023-11:17:42] [V] [TRT] Conv_60 + Relu_61 Set Tactic Name: ampere_h16816cudnn_64x128_ldg8_relu_exp_stages_64x4_small_nhwc_tn_v1 Tactic: 0x3b5fa0f2a8fc2410
[08/10/2023-11:17:42] [V] [TRT] Tactic: 0x3b5fa0f2a8fc2410 Time: 2.30303
[08/10/2023-11:17:42] [V] [TRT] Conv_60 + Relu_61 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x32x32_stage4_warpsize4x1x1_g1_tensor16x8x16_t1r3s3 Tactic: 0x3e7eb35b91b9fa63
[08/10/2023-11:17:42] [V] [TRT] Tactic: 0x3e7eb35b91b9fa63 Time: 2.00747
[08/10/2023-11:17:42] [V] [TRT] Conv_60 + Relu_61 Set Tactic Name: ampere_h16816cudnn_256x64_ldg8_relu_exp_stages_64x3_medium_nhwc_tn_v1 Tactic: 0x3f031df0e579d52c
[08/10/2023-11:17:42] [V] [TRT] Tactic: 0x3f031df0e579d52c Time: 1.1711
[08/10/2023-11:17:42] [V] [TRT] Conv_60 + Relu_61 Set Tactic Name: ampere_h16816cudnn_256x64_sliced1x2_ldg8_relu_exp_small_nhwc_tn_v1 Tactic: 0x42a5b8709d0039be
[08/10/2023-11:17:42] [V] [TRT] Tactic: 0x42a5b8709d0039be Time: 2.02321
[08/10/2023-11:17:42] [V] [TRT] Conv_60 + Relu_61 Set Tactic Name: ampere_h16816cudnn_256x128_ldg8_relu_exp_small_nhwc_tn_v1 Tactic: 0x463794ee4acffd1f
[08/10/2023-11:17:42] [V] [TRT] Tactic: 0x463794ee4acffd1f Time: 1.96085
[08/10/2023-11:17:42] [V] [TRT] Conv_60 + Relu_61 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x64x64_stage4_warpsize2x2x1_g1_tensor16x8x16 Tactic: 0x4a81ea1e51436a30
[08/10/2023-11:17:42] [V] [TRT] Tactic: 0x4a81ea1e51436a30 Time: 1.59018
[08/10/2023-11:17:42] [V] [TRT] Conv_60 + Relu_61 Set Tactic Name: ampere_h1688cudnn_256x64_sliced1x2_ldg8_relu_exp_small_nhwc_tn_v1 Tactic: 0x4aed1956bd10a795
[08/10/2023-11:17:42] [V] [TRT] Tactic: 0x4aed1956bd10a795 Time: 1.68901
[08/10/2023-11:17:42] [V] [TRT] Conv_60 + Relu_61 Set Tactic Name: ampere_h16816cudnn_64x128_sliced1x2_ldg8_relu_exp_stages_64x3_large_nhwc_tn_v1 Tactic: 0x4fc05923455fc266
[08/10/2023-11:17:42] [V] [TRT] Tactic: 0x4fc05923455fc266 Time: 1.91787
[08/10/2023-11:17:42] [V] [TRT] Conv_60 + Relu_61 Set Tactic Name: ampere_h16816cudnn_256x64_ldg8_relu_exp_large_nhwc_tn_v1 Tactic: 0x5013c38f55afa9ba
[08/10/2023-11:17:42] [V] [TRT] Tactic: 0x5013c38f55afa9ba Time: 1.17743
[08/10/2023-11:17:42] [V] [TRT] Conv_60 + Relu_61 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x64x32_stage3_warpsize4x1x1_g1_tensor16x8x16_t1r3s3 Tactic: 0x529f4431bdae94f5
[08/10/2023-11:17:42] [V] [TRT] Tactic: 0x529f4431bdae94f5 Time: 1.1827
[08/10/2023-11:17:42] [V] [TRT] Conv_60 + Relu_61 Set Tactic Name: ampere_h16816cudnn_64x128_ldg8_relu_exp_stages_64x3_small_nhwc_tn_v1 Tactic: 0x53be5e206184e6ad
[08/10/2023-11:17:42] [V] [TRT] Tactic: 0x53be5e206184e6ad Time: 2.38206
[08/10/2023-11:17:42] [V] [TRT] Conv_60 + Relu_61 Set Tactic Name: ampere_h16816cudnn_64x64_ldg8_relu_exp_stages_64x5_large_nhwc_tn_v1 Tactic: 0x544bff7ff9c5d908
[08/10/2023-11:17:42] [V] [TRT] Tactic: 0x544bff7ff9c5d908 Time: 1.41571
[08/10/2023-11:17:42] [V] [TRT] Conv_60 + Relu_61 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x64x64_stage3_warpsize2x2x1_g1_tensor16x8x16 Tactic: 0x5568fd8a32f4a40f
[08/10/2023-11:17:42] [V] [TRT] Tactic: 0x5568fd8a32f4a40f Time: 1.30299
[08/10/2023-11:17:42] [V] [TRT] Conv_60 + Relu_61 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x128x64_stage3_warpsize2x2x1_g1_tensor16x8x16 Tactic: 0x57c9a5ff682354a6
[08/10/2023-11:17:42] [V] [TRT] Tactic: 0x57c9a5ff682354a6 Time: 2.31548
[08/10/2023-11:17:42] [V] [TRT] Conv_60 + Relu_61 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x32x64_stage5_warpsize2x2x1_g1_tensor16x8x16 Tactic: 0x5820b3dda403c4d0
[08/10/2023-11:17:42] [V] [TRT] Tactic: 0x5820b3dda403c4d0 Time: 2.30518
[08/10/2023-11:17:42] [V] [TRT] Conv_60 + Relu_61 Set Tactic Name: ampere_h16816cudnn_128x128_ldg8_relu_exp_stages_64x3_small_nhwc_tn_v1 Tactic: 0x58816bf2e9c36afb
[08/10/2023-11:17:42] [V] [TRT] Tactic: 0x58816bf2e9c36afb Time: 2.5373
[08/10/2023-11:17:42] [V] [TRT] Conv_60 + Relu_61 Set Tactic Name: ampere_h16816cudnn_128x64_ldg8_relu_exp_stages_64x3_large_nhwc_tn_v1 Tactic: 0x59762bd684092b33
[08/10/2023-11:17:43] [V] [TRT] Tactic: 0x59762bd684092b33 Time: 1.78896
[08/10/2023-11:17:43] [V] [TRT] Conv_60 + Relu_61 Set Tactic Name: ampere_h16816cudnn_128x64_sliced1x2_ldg8_relu_exp_stages_64x4_medium_nhwc_tn_v1 Tactic: 0x59c5c647b4c76593
[08/10/2023-11:17:43] [V] [TRT] Tactic: 0x59c5c647b4c76593 Time: 1.36216
[08/10/2023-11:17:43] [V] [TRT] Conv_60 + Relu_61 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x128x64_stage3_warpsize4x2x1_g1_tensor16x8x16 Tactic: 0x5a55274960724ba8
[08/10/2023-11:17:43] [V] [TRT] Tactic: 0x5a55274960724ba8 Time: 2.63588
[08/10/2023-11:17:43] [V] [TRT] Conv_60 + Relu_61 Set Tactic Name: ampere_h1688cudnn_256x128_ldg8_relu_exp_large_nhwc_tn_v1 Tactic: 0x5c2e1c87d85b06f1
[08/10/2023-11:17:43] [V] [TRT] Tactic: 0x5c2e1c87d85b06f1 Time: 2.72083
[08/10/2023-11:17:43] [V] [TRT] Conv_60 + Relu_61 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x256x64_stage3_warpsize1x4x1_g1_tensor16x8x16 Tactic: 0x5d067c18f40c23e3
[08/10/2023-11:17:43] [V] [TRT] Tactic: 0x5d067c18f40c23e3 Time: 4.59031
[08/10/2023-11:17:43] [V] [TRT] Conv_60 + Relu_61 Set Tactic Name: ampere_h16816cudnn_64x64_ldg8_relu_exp_stages_64x5_medium_nhwc_tn_v1 Tactic: 0x5f31c22ec167f384
[08/10/2023-11:17:43] [V] [TRT] Tactic: 0x5f31c22ec167f384 Time: 1.90671
[08/10/2023-11:17:43] [V] [TRT] Conv_60 + Relu_61 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x128x32_stage4_warpsize2x2x1_g1_tensor16x8x16_t1r3s3 Tactic: 0x60c3421152ef8e10
[08/10/2023-11:17:43] [V] [TRT] Tactic: 0x60c3421152ef8e10 Time: 2.02686
[08/10/2023-11:17:43] [V] [TRT] Conv_60 + Relu_61 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x128x32_stage3_warpsize4x2x1_g1_tensor16x8x16 Tactic: 0x60da8c7151d91e47
[08/10/2023-11:17:43] [V] [TRT] Tactic: 0x60da8c7151d91e47 Time: 2.03054
[08/10/2023-11:17:43] [V] [TRT] Conv_60 + Relu_61 Set Tactic Name: ampere_h1688cudnn_256x64_ldg8_relu_exp_medium_nhwc_tn_v1 Tactic: 0x6426696f872a3b13
[08/10/2023-11:17:43] [V] [TRT] Tactic: 0x6426696f872a3b13 Time: 2.05076
[08/10/2023-11:17:43] [V] [TRT] Conv_60 + Relu_61 Set Tactic Name: ampere_h16816cudnn_128x64_ldg8_relu_exp_stages_64x3_medium_nhwc_tn_v1 Tactic: 0x699be152cfb6d6ff
[08/10/2023-11:17:43] [V] [TRT] Tactic: 0x699be152cfb6d6ff Time: 1.45126
[08/10/2023-11:17:43] [V] [TRT] Conv_60 + Relu_61 Set Tactic Name: ampere_h16816cudnn_64x64_ldg8_relu_exp_stages_64x6_large_nhwc_tn_v1 Tactic: 0x6af049035146c349
[08/10/2023-11:17:43] [V] [TRT] Tactic: 0x6af049035146c349 Time: 2.14
[08/10/2023-11:17:43] [V] [TRT] Conv_60 + Relu_61 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x256x32_stage3_warpsize2x4x1_g1_tensor16x8x16 Tactic: 0x7005d10718f6c22d
[08/10/2023-11:17:43] [V] [TRT] Tactic: 0x7005d10718f6c22d Time: 3.29044
[08/10/2023-11:17:43] [V] [TRT] Conv_60 + Relu_61 Set Tactic Name: ampere_h16816cudnn_64x64_sliced1x2_ldg8_relu_exp_stages_64x5_medium_nhwc_tn_v1 Tactic: 0x706f08da35c795a5
[08/10/2023-11:17:43] [V] [TRT] Tactic: 0x706f08da35c795a5 Time: 1.55106
[08/10/2023-11:17:43] [V] [TRT] Conv_60 + Relu_61 Set Tactic Name: ampere_h16816cudnn_128x64_sliced1x2_ldg8_relu_exp_stages_64x4_small_nhwc_tn_v1 Tactic: 0x7163d33a4d8ce8d7
[08/10/2023-11:17:43] [V] [TRT] Tactic: 0x7163d33a4d8ce8d7 Time: 1.00006
[08/10/2023-11:17:43] [V] [TRT] Conv_60 + Relu_61 Set Tactic Name: ampere_h1688cudnn_256x64_ldg8_relu_exp_small_nhwc_tn_v1 Tactic: 0x7aad3976677d7155
[08/10/2023-11:17:43] [V] [TRT] Tactic: 0x7aad3976677d7155 Time: 2.37109
[08/10/2023-11:17:43] [V] [TRT] Conv_60 + Relu_61 Set Tactic Name: ampere_h16816cudnn_64x64_sliced1x2_ldg8_relu_exp_stages_64x5_large_nhwc_tn_v1 Tactic: 0x8015519605ab9963
[08/10/2023-11:17:43] [V] [TRT] Tactic: 0x8015519605ab9963 Time: 1.45577
[08/10/2023-11:17:43] [V] [TRT] Conv_60 + Relu_61 Set Tactic Name: ampere_h16816cudnn_256x64_sliced1x2_ldg8_relu_exp_medium_nhwc_tn_v1 Tactic: 0x82f8a2214b0b4178
[08/10/2023-11:17:43] [V] [TRT] Tactic: 0x82f8a2214b0b4178 Time: 2.57145
[08/10/2023-11:17:43] [V] [TRT] Conv_60 + Relu_61 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x32x32_stage4_warpsize4x1x1_g1_tensor16x8x16 Tactic: 0x834e11ecd4ab9454
[08/10/2023-11:17:43] [V] [TRT] Tactic: 0x834e11ecd4ab9454 Time: 2.12279
[08/10/2023-11:17:43] [V] [TRT] Conv_60 + Relu_61 Set Tactic Name: ampere_h16816cudnn_128x64_sliced1x2_ldg8_relu_exp_stages_64x3_large_nhwc_tn_v1 Tactic: 0x83ccd4762c1376a1
[08/10/2023-11:17:43] [V] [TRT] Tactic: 0x83ccd4762c1376a1 Time: 1.01971
[08/10/2023-11:17:43] [V] [TRT] Conv_60 + Relu_61 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x64x64_stage3_warpsize4x1x1_g1_tensor16x8x16_t1r3s3 Tactic: 0x84ee897dd1f4d2aa
[08/10/2023-11:17:43] [V] [TRT] Tactic: 0x84ee897dd1f4d2aa Time: 1.20771
[08/10/2023-11:17:43] [V] [TRT] Conv_60 + Relu_61 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x128x64_stage3_warpsize2x2x1_g1_tensor16x8x16 Tactic: 0x866e7a5f6401b67f
[08/10/2023-11:17:43] [V] [TRT] Tactic: 0x866e7a5f6401b67f Time: 2.30662
[08/10/2023-11:17:43] [V] [TRT] Conv_60 + Relu_61 Set Tactic Name: ampere_h16816cudnn_64x64_ldg8_relu_exp_stages_64x6_medium_nhwc_tn_v1 Tactic: 0x88971eec55aba850
[08/10/2023-11:17:43] [V] [TRT] Tactic: 0x88971eec55aba850 Time: 1.74784
[08/10/2023-11:17:43] [V] [TRT] Conv_60 + Relu_61 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x128x64_stage3_warpsize4x2x1_g1_tensor16x8x16 Tactic: 0x8bf2f8e96c50a971
[08/10/2023-11:17:43] [V] [TRT] Tactic: 0x8bf2f8e96c50a971 Time: 1.98089
[08/10/2023-11:17:43] [V] [TRT] Conv_60 + Relu_61 Set Tactic Name: ampere_h16816cudnn_256x128_ldg8_relu_exp_stages_64x3_large_nhwc_tn_v1 Tactic: 0x94576ece6beb35e3
[08/10/2023-11:17:43] [V] [TRT] Tactic: 0x94576ece6beb35e3 Time: 1.74861
[08/10/2023-11:17:43] [V] [TRT] Conv_60 + Relu_61 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x64x64_stage3_warpsize4x1x1_g1_tensor16x8x16 Tactic: 0x9fff6e65019cc310
[08/10/2023-11:17:43] [V] [TRT] Tactic: 0x9fff6e65019cc310 Time: 1.50468
[08/10/2023-11:17:43] [V] [TRT] Conv_60 + Relu_61 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x32x64_stage5_warpsize2x2x1_g1_tensor16x8x16_t1r3s3 Tactic: 0xa033e20ae9f412b2
[08/10/2023-11:17:43] [V] [TRT] Tactic: 0xa033e20ae9f412b2 Time: 2.59837
[08/10/2023-11:17:43] [V] [TRT] Conv_60 + Relu_61 Set Tactic Name: ampere_h16816cudnn_64x128_ldg8_relu_exp_stages_64x4_large_nhwc_tn_v1 Tactic: 0xa111596c001b78db
[08/10/2023-11:17:43] [V] [TRT] Tactic: 0xa111596c001b78db Time: 2.7326
[08/10/2023-11:17:43] [V] [TRT] Conv_60 + Relu_61 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x256x32_stage3_warpsize2x4x1_g1_tensor16x8x16 Tactic: 0xa1a20ea714d420f4
[08/10/2023-11:17:43] [V] [TRT] Tactic: 0xa1a20ea714d420f4 Time: 3.69953
[08/10/2023-11:17:43] [V] [TRT] Conv_60 + Relu_61 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x32x32_stage4_warpsize4x1x1_g1_tensor16x8x16_t1r3s3 Tactic: 0xa40cb43c296a36a8
[08/10/2023-11:17:43] [V] [TRT] Tactic: 0xa40cb43c296a36a8 Time: 1.5435
[08/10/2023-11:17:43] [V] [TRT] Conv_60 + Relu_61 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x64x32_stage3_warpsize4x1x1_g1_tensor16x8x16 Tactic: 0xa570c55d303796ff
[08/10/2023-11:17:43] [V] [TRT] Tactic: 0xa570c55d303796ff Time: 1.01125
[08/10/2023-11:17:43] [V] [TRT] Conv_60 + Relu_61 Set Tactic Name: ampere_h16816cudnn_64x64_sliced1x2_ldg8_relu_exp_stages_64x6_medium_nhwc_tn_v1 Tactic: 0xa7c9d418a10bce71
[08/10/2023-11:17:43] [V] [TRT] Tactic: 0xa7c9d418a10bce71 Time: 1.41766
[08/10/2023-11:17:43] [V] [TRT] Conv_60 + Relu_61 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x32x32_stage4_warpsize4x1x1_g1_tensor16x8x16 Tactic: 0xa83b68f30462f971
[08/10/2023-11:17:43] [V] [TRT] Tactic: 0xa83b68f30462f971 Time: 1.59776
[08/10/2023-11:17:43] [V] [TRT] Conv_60 + Relu_61 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x256x32_stage3_warpsize2x4x1_g1_tensor16x8x16_t1r3s3 Tactic: 0xa9177bbe4e767df8
[08/10/2023-11:17:43] [V] [TRT] Tactic: 0xa9177bbe4e767df8 Time: 3.30596
[08/10/2023-11:17:43] [V] [TRT] Conv_60 + Relu_61 Set Tactic Name: ampere_h1688cudnn_128x128_ldg8_relu_exp_large_nhwc_tn_v1 Tactic: 0xa927df92ac1ef1b8
[08/10/2023-11:17:43] [V] [TRT] Tactic: 0xa927df92ac1ef1b8 Time: 2.2675
[08/10/2023-11:17:43] [V] [TRT] Conv_60 + Relu_61 Set Tactic Name: ampere_h16816cudnn_128x64_ldg8_relu_exp_stages_64x4_small_nhwc_tn_v1 Tactic: 0xabd92c9ae596b545
[08/10/2023-11:17:43] [V] [TRT] Tactic: 0xabd92c9ae596b545 Time: 1.1008
[08/10/2023-11:17:43] [V] [TRT] Conv_60 + Relu_61 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x128x32_stage5_warpsize2x2x1_g1_tensor16x8x16 Tactic: 0xafd1e8bf6bd3d638
[08/10/2023-11:17:43] [V] [TRT] Tactic: 0xafd1e8bf6bd3d638 Time: 1.8323
[08/10/2023-11:17:43] [V] [TRT] Conv_60 + Relu_61 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x128x32_stage3_warpsize4x2x1_g1_tensor16x8x16 Tactic: 0xb17d53d15dfbfc9e
[08/10/2023-11:17:44] [V] [TRT] Tactic: 0xb17d53d15dfbfc9e Time: 1.91119
[08/10/2023-11:17:44] [V] [TRT] Conv_60 + Relu_61 Set Tactic Name: ampere_h16816cudnn_128x128_ldg8_relu_exp_small_nhwc_tn_v1 Tactic: 0xb33e57fb3e8a0a56
[08/10/2023-11:17:44] [V] [TRT] Tactic: 0xb33e57fb3e8a0a56 Time: 1.68175
[08/10/2023-11:17:44] [V] [TRT] Conv_60 + Relu_61 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x64x32_stage5_warpsize2x2x1_g1_tensor16x8x16_t1r3s3 Tactic: 0xb4bec086187edcfc
[08/10/2023-11:17:44] [V] [TRT] Tactic: 0xb4bec086187edcfc Time: 1.06736
[08/10/2023-11:17:44] [V] [TRT] Conv_60 + Relu_61 Set Tactic Name: ampere_h16816cudnn_256x64_ldg8_relu_exp_stages_64x3_small_nhwc_tn_v1 Tactic: 0xb6fa5ffcc1a9d518
[08/10/2023-11:17:44] [V] [TRT] Tactic: 0xb6fa5ffcc1a9d518 Time: 0.97861
[08/10/2023-11:17:44] [V] [TRT] Conv_60 + Relu_61 Set Tactic Name: ampere_h16816cudnn_64x128_sliced1x2_ldg8_relu_exp_stages_64x3_medium_nhwc_tn_v1 Tactic: 0xb85e52e87caf60a2
[08/10/2023-11:17:44] [V] [TRT] Tactic: 0xb85e52e87caf60a2 Time: 1.70243
[08/10/2023-11:17:44] [V] [TRT] Conv_60 + Relu_61 Set Tactic Name: ampere_h16816cudnn_64x128_ldg8_relu_exp_stages_64x3_medium_nhwc_tn_v1 Tactic: 0xb8d86216e1235cda
[08/10/2023-11:17:44] [V] [TRT] Tactic: 0xb8d86216e1235cda Time: 1.92295
[08/10/2023-11:17:44] [V] [TRT] Conv_60 + Relu_61 Set Tactic Name: ampere_h16816cudnn_256x128_ldg8_relu_exp_stages_64x3_medium_nhwc_tn_v1 Tactic: 0xb9171d70ba2eda4b
[08/10/2023-11:17:44] [V] [TRT] Tactic: 0xb9171d70ba2eda4b Time: 1.78123
[08/10/2023-11:17:44] [V] [TRT] Conv_60 + Relu_61 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x64x32_stage5_warpsize2x2x1_g1_tensor16x8x16 Tactic: 0xbd08239a9317f2fd
[08/10/2023-11:17:44] [V] [TRT] Tactic: 0xbd08239a9317f2fd Time: 1.12374
[08/10/2023-11:17:44] [V] [TRT] Conv_60 + Relu_61 Set Tactic Name: ampere_h16816cudnn_64x128_sliced1x2_ldg8_relu_exp_stages_64x4_small_nhwc_tn_v1 Tactic: 0xbd6f5e6f24c05c10
[08/10/2023-11:17:44] [V] [TRT] Tactic: 0xbd6f5e6f24c05c10 Time: 1.92844
[08/10/2023-11:17:44] [V] [TRT] Conv_60 + Relu_61 Set Tactic Name: ampere_h16816cudnn_64x64_sliced1x2_ldg8_relu_exp_stages_64x6_large_nhwc_tn_v1 Tactic: 0xbeaee7eaad288322
[08/10/2023-11:17:44] [V] [TRT] Tactic: 0xbeaee7eaad288322 Time: 1.40714
[08/10/2023-11:17:44] [V] [TRT] Conv_60 + Relu_61 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x128x64_stage3_warpsize2x2x1_g1_tensor16x8x16_t1r3s3 Tactic: 0xc0a02dc6095497cc
[08/10/2023-11:17:44] [V] [TRT] Tactic: 0xc0a02dc6095497cc Time: 1.70325
[08/10/2023-11:17:44] [V] [TRT] Conv_60 + Relu_61 Set Tactic Name: ampere_h16816cudnn_128x128_ldg8_relu_exp_stages_64x3_large_nhwc_tn_v1 Tactic: 0xc2cf926c41243630
[08/10/2023-11:17:44] [V] [TRT] Tactic: 0xc2cf926c41243630 Time: 1.75994
[08/10/2023-11:17:44] [V] [TRT] Conv_60 + Relu_61 Set Tactic Name: ampere_h16816cudnn_128x64_ldg8_relu_exp_stages_64x3_small_nhwc_tn_v1 Tactic: 0xc338d2482cee77f8
[08/10/2023-11:17:44] [V] [TRT] Tactic: 0xc338d2482cee77f8 Time: 1.07756
[08/10/2023-11:17:44] [V] [TRT] Conv_60 + Relu_61 Set Tactic Name: ampere_h1688cudnn_256x128_ldg8_relu_exp_small_nhwc_tn_v1 Tactic: 0xc660e51970bc5a3a
[08/10/2023-11:17:44] [V] [TRT] Tactic: 0xc660e51970bc5a3a Time: 2.11024
[08/10/2023-11:17:44] [V] [TRT] Conv_60 + Relu_61 Set Tactic Name: ampere_h16816cudnn_64x128_ldg8_relu_exp_stages_64x4_medium_nhwc_tn_v1 Tactic: 0xc82f3f06140e3cbb
[08/10/2023-11:17:44] [V] [TRT] Tactic: 0xc82f3f06140e3cbb Time: 2.0781
[08/10/2023-11:17:44] [V] [TRT] Conv_60 + Relu_61 Set Tactic Name: ampere_h16816cudnn_64x128_sliced1x2_ldg8_relu_exp_stages_64x4_medium_nhwc_tn_v1 Tactic: 0xc8a90ff8898200c3
[08/10/2023-11:17:44] [V] [TRT] Tactic: 0xc8a90ff8898200c3 Time: 1.93899
[08/10/2023-11:17:44] [V] [TRT] Conv_60 + Relu_61 Set Tactic Name: ampere_h1688cudnn_256x64_sliced1x2_ldg8_relu_exp_medium_nhwc_tn_v1 Tactic: 0xc9cc55109bb4de26
[08/10/2023-11:17:44] [V] [TRT] Tactic: 0xc9cc55109bb4de26 Time: 1.76004
[08/10/2023-11:17:44] [V] [TRT] Conv_60 + Relu_61 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x64x32_stage5_warpsize2x2x1_g1_tensor16x8x16 Tactic: 0xc9d24bd069159fa8
[08/10/2023-11:17:44] [V] [TRT] Tactic: 0xc9d24bd069159fa8 Time: 1.08659
[08/10/2023-11:17:44] [V] [TRT] Conv_60 + Relu_61 Set Tactic Name: ampere_h16816cudnn_64x128_ldg8_relu_exp_stages_64x3_large_nhwc_tn_v1 Tactic: 0xc9f0a7bec963ba66
[08/10/2023-11:17:44] [V] [TRT] Tactic: 0xc9f0a7bec963ba66 Time: 1.93
[08/10/2023-11:17:44] [V] [TRT] Conv_60 + Relu_61 Set Tactic Name: ampere_h16816cudnn_256x64_ldg8_relu_exp_small_nhwc_tn_v1 Tactic: 0xca5d3a11fd48f571
[08/10/2023-11:17:44] [V] [TRT] Tactic: 0xca5d3a11fd48f571 Time: 1.21237
[08/10/2023-11:17:44] [V] [TRT] Conv_60 + Relu_61 Set Tactic Name: ampere_h16816cudnn_64x64_ldg8_relu_exp_stages_64x5_small_nhwc_tn_v1 Tactic: 0xce0506e1512285c3
[08/10/2023-11:17:44] [V] [TRT] Tactic: 0xce0506e1512285c3 Time: 1.40405
[08/10/2023-11:17:44] [V] [TRT] Conv_60 + Relu_61 Set Tactic Name: ampere_h1688cudnn_256x64_sliced1x2_ldg8_relu_exp_large_nhwc_tn_v1 Tactic: 0xd0a3e0c815f7fb5e
[08/10/2023-11:17:44] [V] [TRT] Tactic: 0xd0a3e0c815f7fb5e Time: 1.77254
[08/10/2023-11:17:44] [V] [TRT] Conv_60 + Relu_61 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x64x32_stage3_warpsize4x1x1_g1_tensor16x8x16 Tactic: 0xd1aaad17ca35fbaa
[08/10/2023-11:17:44] [V] [TRT] Tactic: 0xd1aaad17ca35fbaa Time: 0.94219
[08/10/2023-11:17:44] [V] [TRT] Conv_60 + Relu_61 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x128x64_stage3_warpsize4x2x1_g1_tensor16x8x16_t1r3s3 Tactic: 0xd2fca0486ca97266
[08/10/2023-11:17:44] [V] [TRT] Tactic: 0xd2fca0486ca97266 Time: 1.71857
[08/10/2023-11:17:44] [V] [TRT] Conv_60 + Relu_61 Set Tactic Name: ampere_h16816cudnn_64x128_sliced1x2_ldg8_relu_exp_stages_64x3_small_nhwc_tn_v1 Tactic: 0xd58ea0bdedb89ead
[08/10/2023-11:17:44] [V] [TRT] Tactic: 0xd58ea0bdedb89ead Time: 1.75561
[08/10/2023-11:17:44] [V] [TRT] Conv_60 + Relu_61 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x256x64_stage3_warpsize1x4x1_g1_tensor16x8x16_t1r3s3 Tactic: 0xd7c261fa01db2af9
[08/10/2023-11:17:44] [V] [TRT] Tactic: 0xd7c261fa01db2af9 Time: 3.49089
[08/10/2023-11:17:44] [V] [TRT] Conv_60 + Relu_61 Set Tactic Name: ampere_h16816cudnn_256x64_sliced1x2_ldg8_relu_exp_large_nhwc_tn_v1 Tactic: 0xd8eb41ee35e76575
[08/10/2023-11:17:44] [V] [TRT] Tactic: 0xd8eb41ee35e76575 Time: 1.96722
[08/10/2023-11:17:44] [V] [TRT] Conv_60 + Relu_61 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x128x32_stage5_warpsize2x2x1_g1_tensor16x8x16 Tactic: 0xdb0b80f591d1bb6d
[08/10/2023-11:17:44] [V] [TRT] Tactic: 0xdb0b80f591d1bb6d Time: 1.75124
[08/10/2023-11:17:44] [V] [TRT] Conv_60 + Relu_61 Set Tactic Name: ampere_h16816cudnn_256x128_ldg8_relu_exp_large_nhwc_tn_v1 Tactic: 0xdc796d70e228a1d4
[08/10/2023-11:17:44] [V] [TRT] Tactic: 0xdc796d70e228a1d4 Time: 2.03763
[08/10/2023-11:17:44] [V] [TRT] Conv_60 + Relu_61 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x32x32_stage4_warpsize4x1x1_g1_tensor16x8x16 Tactic: 0xdce100b9fe609424
[08/10/2023-11:17:44] [V] [TRT] Tactic: 0xdce100b9fe609424 Time: 1.56608
[08/10/2023-11:17:44] [V] [TRT] Conv_60 + Relu_61 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x128x32_stage4_warpsize2x2x1_g1_tensor16x8x16 Tactic: 0xdfa020ef435ef810
[08/10/2023-11:17:44] [V] [TRT] Tactic: 0xdfa020ef435ef810 Time: 1.59177
[08/10/2023-11:17:44] [V] [TRT] Conv_60 + Relu_61 Set Tactic Name: ampere_h1688cudnn_256x64_ldg8_relu_exp_large_nhwc_tn_v1 Tactic: 0xe0e3c0e8cf9a2d9e
[08/10/2023-11:17:44] [V] [TRT] Tactic: 0xe0e3c0e8cf9a2d9e Time: 1.63767
[08/10/2023-11:17:44] [V] [TRT] Conv_60 + Relu_61 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x32x64_stage5_warpsize2x2x1_g1_tensor16x8x16 Tactic: 0xe1ff5ad20f5c6bf6
[08/10/2023-11:17:44] [V] [TRT] Tactic: 0xe1ff5ad20f5c6bf6 Time: 2.05358
[08/10/2023-11:17:44] [V] [TRT] Conv_60 + Relu_61 Set Tactic Name: ampere_h16816cudnn_256x64_ldg8_relu_exp_medium_nhwc_tn_v1 Tactic: 0xe4711898bd599c36
[08/10/2023-11:17:44] [V] [TRT] Tactic: 0xe4711898bd599c36 Time: 1.17004
[08/10/2023-11:17:44] [V] [TRT] Conv_60 + Relu_61 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x64x64_stage3_warpsize4x1x1_g1_tensor16x8x16 Tactic: 0xeb25062ffb9eae45
[08/10/2023-11:17:44] [V] [TRT] Tactic: 0xeb25062ffb9eae45 Time: 1.09109
[08/10/2023-11:17:44] [V] [TRT] Conv_60 + Relu_61 Set Tactic Name: ampere_h16816cudnn_128x64_sliced1x2_ldg8_relu_exp_stages_64x4_large_nhwc_tn_v1 Tactic: 0xeb2d2aa4e56bb41c
[08/10/2023-11:17:44] [V] [TRT] Tactic: 0xeb2d2aa4e56bb41c Time: 1.07758
[08/10/2023-11:17:44] [V] [TRT] Conv_60 + Relu_61 Set Tactic Name: ampere_h16816cudnn_64x64_ldg8_relu_exp_stages_64x6_small_nhwc_tn_v1 Tactic: 0xf0beb09df9a19f82
[08/10/2023-11:17:44] [V] [TRT] Tactic: 0xf0beb09df9a19f82 Time: 1.41569
[08/10/2023-11:17:44] [V] [TRT] Conv_60 + Relu_61 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x64x64_stage4_warpsize2x2x1_g1_tensor16x8x16 Tactic: 0xf35e0311fa1cc516
[08/10/2023-11:17:44] [V] [TRT] Tactic: 0xf35e0311fa1cc516 Time: 1.36626
[08/10/2023-11:17:44] [V] [TRT] Conv_60 + Relu_61 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x32x32_stage4_warpsize4x1x1_g1_tensor16x8x16 Tactic: 0xf79479a62ea9f901
[08/10/2023-11:17:44] [V] [TRT] Tactic: 0xf79479a62ea9f901 Time: 1.70565
[08/10/2023-11:17:44] [V] [TRT] Fastest Tactic: 0xd1aaad17ca35fbaa Time: 0.94219
[08/10/2023-11:17:44] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0xd1aaad17ca35fbaa
[08/10/2023-11:17:44] [V] [TRT] =============== Computing costs for 
[08/10/2023-11:17:44] [V] [TRT] *************** Autotuning format combination: Float(2073600,32400,180,1) -> Float(16588800,32400,180,1) ***************
[08/10/2023-11:17:44] [V] [TRT] --------------- Timing Runner: Conv_62 + Relu_63 || Conv_65 + Relu_66 || Conv_68 + Relu_69 || Conv_71 + Relu_72 || Conv_74 + Relu_75 || Conv_77 + Relu_78 || Conv_80 + Relu_81 || Conv_83 + Relu_84 (CudaDepthwiseConvolution)
[08/10/2023-11:17:44] [V] [TRT] CudaDepthwiseConvolution has no valid tactics for this config, skipping
[08/10/2023-11:17:44] [V] [TRT] --------------- Timing Runner: Conv_62 + Relu_63 || Conv_65 + Relu_66 || Conv_68 + Relu_69 || Conv_71 + Relu_72 || Conv_74 + Relu_75 || Conv_77 + Relu_78 || Conv_80 + Relu_81 || Conv_83 + Relu_84 (FusedConvActConvolution)
[08/10/2023-11:17:44] [V] [TRT] FusedConvActConvolution has no valid tactics for this config, skipping
[08/10/2023-11:17:44] [V] [TRT] --------------- Timing Runner: Conv_62 + Relu_63 || Conv_65 + Relu_66 || Conv_68 + Relu_69 || Conv_71 + Relu_72 || Conv_74 + Relu_75 || Conv_77 + Relu_78 || Conv_80 + Relu_81 || Conv_83 + Relu_84 (CudnnConvolution)
[08/10/2023-11:17:45] [V] [TRT] Tactic: 0x0000000000000000 Time: 12.7565
[08/10/2023-11:17:45] [V] [TRT] Tactic: 0x0000000000000001 Time: 5.0149
[08/10/2023-11:17:45] [V] [TRT] Tactic: 0x0000000000000002 Time: 18.7712
[08/10/2023-11:17:45] [V] [TRT] Tactic: 0x0000000000000004 skipped. Scratch requested: 17349083136, available: 4294967296
[08/10/2023-11:17:45] [V] [TRT] Tactic: 0x0000000000000005 Time: 39.0424
[08/10/2023-11:17:45] [V] [TRT] Tactic: 0x0000000000000006 Time: 8.89978
[08/10/2023-11:17:45] [V] [TRT] Tactic: 0x0000000000000038 Time: 12.8671
[08/10/2023-11:17:46] [V] [TRT] Tactic: 0x0000000000000039 Time: 4.07307
[08/10/2023-11:17:46] [V] [TRT] Tactic: 0x000000000000003a Time: 17.8822
[08/10/2023-11:17:46] [V] [TRT] Tactic: 0x000000000000003c skipped. Scratch requested: 17349083136, available: 4294967296
[08/10/2023-11:17:46] [V] [TRT] Tactic: 0x000000000000003d Time: 34.048
[08/10/2023-11:17:46] [V] [TRT] Tactic: 0x000000000000003e Time: 8.53909
[08/10/2023-11:17:46] [V] [TRT] Tactic: 0x0000000000000070 Time: 12.0423
[08/10/2023-11:17:46] [V] [TRT] Tactic: 0x0000000000000071 Time: 12.3261
[08/10/2023-11:17:47] [V] [TRT] Tactic: 0x0000000000000072 Time: 17.8497
[08/10/2023-11:17:47] [V] [TRT] Tactic: 0x0000000000000074 skipped. Scratch requested: 17349083136, available: 4294967296
[08/10/2023-11:17:47] [V] [TRT] Tactic: 0x0000000000000075 Time: 34.5682
[08/10/2023-11:17:47] [V] [TRT] Tactic: 0x0000000000000076 Time: 8.4213
[08/10/2023-11:17:47] [V] [TRT] Fastest Tactic: 0x0000000000000039 Time: 4.07307
[08/10/2023-11:17:47] [V] [TRT] Setting workspace to 17349083136enables more tactics for profiling
[08/10/2023-11:17:47] [V] [TRT] --------------- Timing Runner: Conv_62 + Relu_63 || Conv_65 + Relu_66 || Conv_68 + Relu_69 || Conv_71 + Relu_72 || Conv_74 + Relu_75 || Conv_77 + Relu_78 || Conv_80 + Relu_81 || Conv_83 + Relu_84 (CaskConvolution)
[08/10/2023-11:17:47] [V] [TRT] Conv_62 + Relu_63 || Conv_65 + Relu_66 || Conv_68 + Relu_69 || Conv_71 + Relu_72 || Conv_74 + Relu_75 || Conv_77 + Relu_78 || Conv_80 + Relu_81 || Conv_83 + Relu_84 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x128x8_stage3_warpsize1x4x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0x01cf8ce2da913006
[08/10/2023-11:17:47] [V] [TRT] Tactic: 0x01cf8ce2da913006 Time: 10.6311
[08/10/2023-11:17:47] [V] [TRT] Conv_62 + Relu_63 || Conv_65 + Relu_66 || Conv_68 + Relu_69 || Conv_71 + Relu_72 || Conv_74 + Relu_75 || Conv_77 + Relu_78 || Conv_80 + Relu_81 || Conv_83 + Relu_84 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x64x8_stage3_warpsize2x2x1_g1_ffma_aligna4_alignc4 Tactic: 0x12dbf7d94ee3696d
[08/10/2023-11:17:47] [V] [TRT] Tactic: 0x12dbf7d94ee3696d Time: 11.2415
[08/10/2023-11:17:47] [V] [TRT] Conv_62 + Relu_63 || Conv_65 + Relu_66 || Conv_68 + Relu_69 || Conv_71 + Relu_72 || Conv_74 + Relu_75 || Conv_77 + Relu_78 || Conv_80 + Relu_81 || Conv_83 + Relu_84 Set Tactic Name: ampere_scudnn_128x64_relu_small_nn_v1 Tactic: 0x3f243c490d502deb
[08/10/2023-11:17:47] [V] [TRT] Tactic: 0x3f243c490d502deb Time: 10.5731
[08/10/2023-11:17:47] [V] [TRT] Conv_62 + Relu_63 || Conv_65 + Relu_66 || Conv_68 + Relu_69 || Conv_71 + Relu_72 || Conv_74 + Relu_75 || Conv_77 + Relu_78 || Conv_80 + Relu_81 || Conv_83 + Relu_84 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0x4727434768e46395
[08/10/2023-11:17:47] [V] [TRT] Tactic: 0x4727434768e46395 Time: 12.2287
[08/10/2023-11:17:47] [V] [TRT] Conv_62 + Relu_63 || Conv_65 + Relu_66 || Conv_68 + Relu_69 || Conv_71 + Relu_72 || Conv_74 + Relu_75 || Conv_77 + Relu_78 || Conv_80 + Relu_81 || Conv_83 + Relu_84 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize256x128x8_stage3_warpsize4x2x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0x4efce38acc876f5c
[08/10/2023-11:17:48] [V] [TRT] Tactic: 0x4efce38acc876f5c Time: 10.3873
[08/10/2023-11:17:48] [V] [TRT] Conv_62 + Relu_63 || Conv_65 + Relu_66 || Conv_68 + Relu_69 || Conv_71 + Relu_72 || Conv_74 + Relu_75 || Conv_77 + Relu_78 || Conv_80 + Relu_81 || Conv_83 + Relu_84 Set Tactic Name: ampere_scudnn_128x128_relu_small_nn_v1 Tactic: 0x503619c69ae500ff
[08/10/2023-11:17:48] [V] [TRT] Tactic: 0x503619c69ae500ff Time: 9.30926
[08/10/2023-11:17:48] [V] [TRT] Conv_62 + Relu_63 || Conv_65 + Relu_66 || Conv_68 + Relu_69 || Conv_71 + Relu_72 || Conv_74 + Relu_75 || Conv_77 + Relu_78 || Conv_80 + Relu_81 || Conv_83 + Relu_84 Set Tactic Name: ampere_scudnn_128x128_relu_xregs_large_nn_v1 Tactic: 0x5403ad713f811a18
[08/10/2023-11:17:48] [V] [TRT] Tactic: 0x5403ad713f811a18 Time: 10.207
[08/10/2023-11:17:48] [V] [TRT] Conv_62 + Relu_63 || Conv_65 + Relu_66 || Conv_68 + Relu_69 || Conv_71 + Relu_72 || Conv_74 + Relu_75 || Conv_77 + Relu_78 || Conv_80 + Relu_81 || Conv_83 + Relu_84 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x128x8_stage3_warpsize1x4x1_g1_ffma_aligna4_alignc4 Tactic: 0x5aa723e0481da855
[08/10/2023-11:17:48] [V] [TRT] Tactic: 0x5aa723e0481da855 Time: 10.5626
[08/10/2023-11:17:48] [V] [TRT] Conv_62 + Relu_63 || Conv_65 + Relu_66 || Conv_68 + Relu_69 || Conv_71 + Relu_72 || Conv_74 + Relu_75 || Conv_77 + Relu_78 || Conv_80 + Relu_81 || Conv_83 + Relu_84 Set Tactic Name: ampere_scudnn_128x64_relu_xregs_large_nn_v1 Tactic: 0x5deb29b7a8e275f7
[08/10/2023-11:17:48] [V] [TRT] Tactic: 0x5deb29b7a8e275f7 Time: 10.1575
[08/10/2023-11:17:48] [V] [TRT] Conv_62 + Relu_63 || Conv_65 + Relu_66 || Conv_68 + Relu_69 || Conv_71 + Relu_72 || Conv_74 + Relu_75 || Conv_77 + Relu_78 || Conv_80 + Relu_81 || Conv_83 + Relu_84 Set Tactic Name: ampere_scudnn_winograd_128x128_ldg1_ldg4_relu_tile148t_nt_v1 Tactic: 0x94119b4c514b211a
[08/10/2023-11:17:48] [V] [TRT] Tactic: 0x94119b4c514b211a Time: 7.35134
[08/10/2023-11:17:48] [V] [TRT] Conv_62 + Relu_63 || Conv_65 + Relu_66 || Conv_68 + Relu_69 || Conv_71 + Relu_72 || Conv_74 + Relu_75 || Conv_77 + Relu_78 || Conv_80 + Relu_81 || Conv_83 + Relu_84 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_aligna4_alignc4 Tactic: 0xa31d27de74b895ff
[08/10/2023-11:17:48] [V] [TRT] Tactic: 0xa31d27de74b895ff Time: 12.2065
[08/10/2023-11:17:48] [V] [TRT] Conv_62 + Relu_63 || Conv_65 + Relu_66 || Conv_68 + Relu_69 || Conv_71 + Relu_72 || Conv_74 + Relu_75 || Conv_77 + Relu_78 || Conv_80 + Relu_81 || Conv_83 + Relu_84 Set Tactic Name: ampere_scudnn_128x32_relu_small_nn_v1 Tactic: 0xa8609adc4e0ceb90
[08/10/2023-11:17:48] [V] [TRT] Tactic: 0xa8609adc4e0ceb90 Time: 11.8698
[08/10/2023-11:17:48] [V] [TRT] Conv_62 + Relu_63 || Conv_65 + Relu_66 || Conv_68 + Relu_69 || Conv_71 + Relu_72 || Conv_74 + Relu_75 || Conv_77 + Relu_78 || Conv_80 + Relu_81 || Conv_83 + Relu_84 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize256x128x8_stage3_warpsize4x2x1_g1_ffma_aligna4_alignc4 Tactic: 0xbb8c3889c7eacd30
[08/10/2023-11:17:48] [V] [TRT] Tactic: 0xbb8c3889c7eacd30 Time: 12.2083
[08/10/2023-11:17:48] [V] [TRT] Conv_62 + Relu_63 || Conv_65 + Relu_66 || Conv_68 + Relu_69 || Conv_71 + Relu_72 || Conv_74 + Relu_75 || Conv_77 + Relu_78 || Conv_80 + Relu_81 || Conv_83 + Relu_84 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_aligna4_alignc4 Tactic: 0xd828f024626fa982
[08/10/2023-11:17:49] [V] [TRT] Tactic: 0xd828f024626fa982 Time: 16.9622
[08/10/2023-11:17:49] [V] [TRT] Conv_62 + Relu_63 || Conv_65 + Relu_66 || Conv_68 + Relu_69 || Conv_71 + Relu_72 || Conv_74 + Relu_75 || Conv_77 + Relu_78 || Conv_80 + Relu_81 || Conv_83 + Relu_84 Set Tactic Name: ampere_scudnn_128x128_relu_medium_nn_v1 Tactic: 0xf067e6205da31c2e
[08/10/2023-11:17:49] [V] [TRT] Tactic: 0xf067e6205da31c2e Time: 9.45304
[08/10/2023-11:17:49] [V] [TRT] Conv_62 + Relu_63 || Conv_65 + Relu_66 || Conv_68 + Relu_69 || Conv_71 + Relu_72 || Conv_74 + Relu_75 || Conv_77 + Relu_78 || Conv_80 + Relu_81 || Conv_83 + Relu_84 Set Tactic Name: ampere_scudnn_128x64_relu_medium_nn_v1 Tactic: 0xf64396b97c889179
[08/10/2023-11:17:49] [V] [TRT] Tactic: 0xf64396b97c889179 Time: 10.787
[08/10/2023-11:17:49] [V] [TRT] Fastest Tactic: 0x94119b4c514b211a Time: 7.35134
[08/10/2023-11:17:49] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CudnnConvolution Tactic: 0x0000000000000039
[08/10/2023-11:17:49] [V] [TRT] *************** Autotuning format combination: Float(2073600,1,11520,64) -> Float(16588800,1,92160,512) ***************
[08/10/2023-11:17:49] [V] [TRT] --------------- Timing Runner: Conv_62 + Relu_63 || Conv_65 + Relu_66 || Conv_68 + Relu_69 || Conv_71 + Relu_72 || Conv_74 + Relu_75 || Conv_77 + Relu_78 || Conv_80 + Relu_81 || Conv_83 + Relu_84 (CaskConvolution)
[08/10/2023-11:17:49] [V] [TRT] Conv_62 + Relu_63 || Conv_65 + Relu_66 || Conv_68 + Relu_69 || Conv_71 + Relu_72 || Conv_74 + Relu_75 || Conv_77 + Relu_78 || Conv_80 + Relu_81 || Conv_83 + Relu_84 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize64x128x32_stage5_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0x112d7e4d280f396d
[08/10/2023-11:17:49] [V] [TRT] Tactic: 0x112d7e4d280f396d Time: 3.21947
[08/10/2023-11:17:49] [V] [TRT] Conv_62 + Relu_63 || Conv_65 + Relu_66 || Conv_68 + Relu_69 || Conv_71 + Relu_72 || Conv_74 + Relu_75 || Conv_77 + Relu_78 || Conv_80 + Relu_81 || Conv_83 + Relu_84 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x64x32_stage5_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0x13cc2ad8dcd32ba2
[08/10/2023-11:17:49] [V] [TRT] Tactic: 0x13cc2ad8dcd32ba2 Time: 3.41354
[08/10/2023-11:17:49] [V] [TRT] Conv_62 + Relu_63 || Conv_65 + Relu_66 || Conv_68 + Relu_69 || Conv_71 + Relu_72 || Conv_74 + Relu_75 || Conv_77 + Relu_78 || Conv_80 + Relu_81 || Conv_83 + Relu_84 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_aligna4_alignc4 Tactic: 0x19b688348f983aa0
[08/10/2023-11:17:49] [V] [TRT] Tactic: 0x19b688348f983aa0 Time: 11.2339
[08/10/2023-11:17:49] [V] [TRT] Conv_62 + Relu_63 || Conv_65 + Relu_66 || Conv_68 + Relu_69 || Conv_71 + Relu_72 || Conv_74 + Relu_75 || Conv_77 + Relu_78 || Conv_80 + Relu_81 || Conv_83 + Relu_84 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x64x8_stage3_warpsize2x2x1_g1_ffma_aligna4_alignc4 Tactic: 0x1da91d865428f237
[08/10/2023-11:17:49] [V] [TRT] Tactic: 0x1da91d865428f237 Time: 8.82542
[08/10/2023-11:17:49] [V] [TRT] Conv_62 + Relu_63 || Conv_65 + Relu_66 || Conv_68 + Relu_69 || Conv_71 + Relu_72 || Conv_74 + Relu_75 || Conv_77 + Relu_78 || Conv_80 + Relu_81 || Conv_83 + Relu_84 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize256x128x32_stage3_warpsize4x2x1_g1_tensor16x8x8_t1r3s3 Tactic: 0x21246c8544eff903
[08/10/2023-11:17:49] [V] [TRT] Tactic: 0x21246c8544eff903 Time: 2.07152
[08/10/2023-11:17:49] [V] [TRT] Conv_62 + Relu_63 || Conv_65 + Relu_66 || Conv_68 + Relu_69 || Conv_71 + Relu_72 || Conv_74 + Relu_75 || Conv_77 + Relu_78 || Conv_80 + Relu_81 || Conv_83 + Relu_84 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x128x32_stage4_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0x25b2b9d5c9d5ca0d
[08/10/2023-11:17:49] [V] [TRT] Tactic: 0x25b2b9d5c9d5ca0d Time: 2.25
[08/10/2023-11:17:49] [V] [TRT] Conv_62 + Relu_63 || Conv_65 + Relu_66 || Conv_68 + Relu_69 || Conv_71 + Relu_72 || Conv_74 + Relu_75 || Conv_77 + Relu_78 || Conv_80 + Relu_81 || Conv_83 + Relu_84 Set Tactic Name: ampere_scudnn_128x64_sliced1x2_ldg4_relu_exp_small_nhwc_tn_v1 Tactic: 0x27b316f52c109002
[08/10/2023-11:17:49] [V] [TRT] Tactic: 0x27b316f52c109002 Time: 10.628
[08/10/2023-11:17:49] [V] [TRT] Conv_62 + Relu_63 || Conv_65 + Relu_66 || Conv_68 + Relu_69 || Conv_71 + Relu_72 || Conv_74 + Relu_75 || Conv_77 + Relu_78 || Conv_80 + Relu_81 || Conv_83 + Relu_84 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage4_warpsize2x2x1_g1_tensor16x8x8_t1r3s3 Tactic: 0x2d80d2b384ab13f1
[08/10/2023-11:17:49] [V] [TRT] Tactic: 0x2d80d2b384ab13f1 Time: 3.94105
[08/10/2023-11:17:49] [V] [TRT] Conv_62 + Relu_63 || Conv_65 + Relu_66 || Conv_68 + Relu_69 || Conv_71 + Relu_72 || Conv_74 + Relu_75 || Conv_77 + Relu_78 || Conv_80 + Relu_81 || Conv_83 + Relu_84 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize256x128x32_stage3_warpsize4x2x1_g1_tensor16x8x8 Tactic: 0x36303f399d3ee4fd
[08/10/2023-11:17:49] [V] [TRT] Tactic: 0x36303f399d3ee4fd Time: 2.50097
[08/10/2023-11:17:49] [V] [TRT] Conv_62 + Relu_63 || Conv_65 + Relu_66 || Conv_68 + Relu_69 || Conv_71 + Relu_72 || Conv_74 + Relu_75 || Conv_77 + Relu_78 || Conv_80 + Relu_81 || Conv_83 + Relu_84 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage4_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0x385bfab9332b1413
[08/10/2023-11:17:49] [V] [TRT] Tactic: 0x385bfab9332b1413 Time: 4.14248
[08/10/2023-11:17:49] [V] [TRT] Conv_62 + Relu_63 || Conv_65 + Relu_66 || Conv_68 + Relu_69 || Conv_71 + Relu_72 || Conv_74 + Relu_75 || Conv_77 + Relu_78 || Conv_80 + Relu_81 || Conv_83 + Relu_84 Set Tactic Name: ampere_scudnn_128x32_sliced1x4_ldg4_relu_exp_medium_nhwc_tn_v1 Tactic: 0x3e191488237fab8f
[08/10/2023-11:17:50] [V] [TRT] Tactic: 0x3e191488237fab8f Time: 11.0447
[08/10/2023-11:17:50] [V] [TRT] Conv_62 + Relu_63 || Conv_65 + Relu_66 || Conv_68 + Relu_69 || Conv_71 + Relu_72 || Conv_74 + Relu_75 || Conv_77 + Relu_78 || Conv_80 + Relu_81 || Conv_83 + Relu_84 Set Tactic Name: ampere_scudnn_128x32_sliced1x4_ldg4_relu_exp_small_nhwc_tn_v1 Tactic: 0x3e2b881168d9689d
[08/10/2023-11:17:50] [V] [TRT] Tactic: 0x3e2b881168d9689d Time: 11.0988
[08/10/2023-11:17:50] [V] [TRT] Conv_62 + Relu_63 || Conv_65 + Relu_66 || Conv_68 + Relu_69 || Conv_71 + Relu_72 || Conv_74 + Relu_75 || Conv_77 + Relu_78 || Conv_80 + Relu_81 || Conv_83 + Relu_84 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x256x8_stage3_warpsize1x4x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0x3f0c846d6379bc98
[08/10/2023-11:17:50] [V] [TRT] Tactic: 0x3f0c846d6379bc98 Time: 9.2195
[08/10/2023-11:17:50] [V] [TRT] Conv_62 + Relu_63 || Conv_65 + Relu_66 || Conv_68 + Relu_69 || Conv_71 + Relu_72 || Conv_74 + Relu_75 || Conv_77 + Relu_78 || Conv_80 + Relu_81 || Conv_83 + Relu_84 Set Tactic Name: ampere_scudnn_128x64_sliced1x2_ldg4_relu_exp_medium_nhwc_tn_v1 Tactic: 0x412c44dfeaf9161d
[08/10/2023-11:17:50] [V] [TRT] Tactic: 0x412c44dfeaf9161d Time: 10.6039
[08/10/2023-11:17:50] [V] [TRT] Conv_62 + Relu_63 || Conv_65 + Relu_66 || Conv_68 + Relu_69 || Conv_71 + Relu_72 || Conv_74 + Relu_75 || Conv_77 + Relu_78 || Conv_80 + Relu_81 || Conv_83 + Relu_84 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x256x32_stage3_warpsize2x4x1_g1_tensor16x8x8 Tactic: 0x482b48242255b8ce
[08/10/2023-11:17:50] [V] [TRT] Tactic: 0x482b48242255b8ce Time: 2.03089
[08/10/2023-11:17:50] [V] [TRT] Conv_62 + Relu_63 || Conv_65 + Relu_66 || Conv_68 + Relu_69 || Conv_71 + Relu_72 || Conv_74 + Relu_75 || Conv_77 + Relu_78 || Conv_80 + Relu_81 || Conv_83 + Relu_84 Set Tactic Name: ampere_scudnn_128x128_relu_exp_small_nhwc_tn_v1 Tactic: 0x5030121339a48bf3
[08/10/2023-11:17:50] [V] [TRT] Tactic: 0x5030121339a48bf3 Time: 9.26291
[08/10/2023-11:17:50] [V] [TRT] Conv_62 + Relu_63 || Conv_65 + Relu_66 || Conv_68 + Relu_69 || Conv_71 + Relu_72 || Conv_74 + Relu_75 || Conv_77 + Relu_78 || Conv_80 + Relu_81 || Conv_83 + Relu_84 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x256x32_stage3_warpsize2x4x1_g1_tensor16x8x8_t1r3s3 Tactic: 0x614e89f7852edbc3
[08/10/2023-11:17:50] [V] [TRT] Tactic: 0x614e89f7852edbc3 Time: 2.104
[08/10/2023-11:17:50] [V] [TRT] Conv_62 + Relu_63 || Conv_65 + Relu_66 || Conv_68 + Relu_69 || Conv_71 + Relu_72 || Conv_74 + Relu_75 || Conv_77 + Relu_78 || Conv_80 + Relu_81 || Conv_83 + Relu_84 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_aligna4_alignc4 Tactic: 0x62835fce994f06dd
[08/10/2023-11:17:50] [V] [TRT] Tactic: 0x62835fce994f06dd Time: 9.62377
[08/10/2023-11:17:50] [V] [TRT] Conv_62 + Relu_63 || Conv_65 + Relu_66 || Conv_68 + Relu_69 || Conv_71 + Relu_72 || Conv_74 + Relu_75 || Conv_77 + Relu_78 || Conv_80 + Relu_81 || Conv_83 + Relu_84 Set Tactic Name: ampere_scudnn_128x128_ldg4_relu_exp_large_nhwc_tn_v1 Tactic: 0x634e99502974e4da
[08/10/2023-11:17:50] [V] [TRT] Tactic: 0x634e99502974e4da Time: 9.22586
[08/10/2023-11:17:50] [V] [TRT] Conv_62 + Relu_63 || Conv_65 + Relu_66 || Conv_68 + Relu_69 || Conv_71 + Relu_72 || Conv_74 + Relu_75 || Conv_77 + Relu_78 || Conv_80 + Relu_81 || Conv_83 + Relu_84 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0x65e41d81f093b482
[08/10/2023-11:17:50] [V] [TRT] Tactic: 0x65e41d81f093b482 Time: 2.51284
[08/10/2023-11:17:50] [V] [TRT] Conv_62 + Relu_63 || Conv_65 + Relu_66 || Conv_68 + Relu_69 || Conv_71 + Relu_72 || Conv_74 + Relu_75 || Conv_77 + Relu_78 || Conv_80 + Relu_81 || Conv_83 + Relu_84 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize64x128x32_stage5_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0x65f71607d20d5438
[08/10/2023-11:17:50] [V] [TRT] Tactic: 0x65f71607d20d5438 Time: 3.1282
[08/10/2023-11:17:50] [V] [TRT] Conv_62 + Relu_63 || Conv_65 + Relu_66 || Conv_68 + Relu_69 || Conv_71 + Relu_72 || Conv_74 + Relu_75 || Conv_77 + Relu_78 || Conv_80 + Relu_81 || Conv_83 + Relu_84 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x64x32_stage5_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0x6716429226d146f7
[08/10/2023-11:17:50] [V] [TRT] Tactic: 0x6716429226d146f7 Time: 3.94215
[08/10/2023-11:17:50] [V] [TRT] Conv_62 + Relu_63 || Conv_65 + Relu_66 || Conv_68 + Relu_69 || Conv_71 + Relu_72 || Conv_74 + Relu_75 || Conv_77 + Relu_78 || Conv_80 + Relu_81 || Conv_83 + Relu_84 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize64x32x64_stage5_warpsize2x2x1_g1_tensor16x8x8_t1r3s3 Tactic: 0x679a281f7660c436
[08/10/2023-11:17:50] [V] [TRT] Tactic: 0x679a281f7660c436 Time: 6.28908
[08/10/2023-11:17:50] [V] [TRT] Conv_62 + Relu_63 || Conv_65 + Relu_66 || Conv_68 + Relu_69 || Conv_71 + Relu_72 || Conv_74 + Relu_75 || Conv_77 + Relu_78 || Conv_80 + Relu_81 || Conv_83 + Relu_84 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize64x32x64_stage5_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0x6e2a3c7a7fc5e4e1
[08/10/2023-11:17:51] [V] [TRT] Tactic: 0x6e2a3c7a7fc5e4e1 Time: 6.70229
[08/10/2023-11:17:51] [V] [TRT] Conv_62 + Relu_63 || Conv_65 + Relu_66 || Conv_68 + Relu_69 || Conv_71 + Relu_72 || Conv_74 + Relu_75 || Conv_77 + Relu_78 || Conv_80 + Relu_81 || Conv_83 + Relu_84 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x64x32_stage5_warpsize2x2x1_g1_tensor16x8x8_t1r3s3 Tactic: 0x7aa21282feb15f1a
[08/10/2023-11:17:51] [V] [TRT] Tactic: 0x7aa21282feb15f1a Time: 3.08436
[08/10/2023-11:17:51] [V] [TRT] Conv_62 + Relu_63 || Conv_65 + Relu_66 || Conv_68 + Relu_69 || Conv_71 + Relu_72 || Conv_74 + Relu_75 || Conv_77 + Relu_78 || Conv_80 + Relu_81 || Conv_83 + Relu_84 Set Tactic Name: ampere_scudnn_128x128_ldg4_relu_exp_medium_nhwc_tn_v1 Tactic: 0x7bc32c782b800c48
[08/10/2023-11:17:51] [V] [TRT] Tactic: 0x7bc32c782b800c48 Time: 9.05967
[08/10/2023-11:17:51] [V] [TRT] Conv_62 + Relu_63 || Conv_65 + Relu_66 || Conv_68 + Relu_69 || Conv_71 + Relu_72 || Conv_74 + Relu_75 || Conv_77 + Relu_78 || Conv_80 + Relu_81 || Conv_83 + Relu_84 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize256x64x8_stage3_warpsize2x2x1_g1_ffma_aligna4_alignc4 Tactic: 0x8014228ec08b4d49
[08/10/2023-11:17:51] [V] [TRT] Tactic: 0x8014228ec08b4d49 Time: 10.0899
[08/10/2023-11:17:51] [V] [TRT] Conv_62 + Relu_63 || Conv_65 + Relu_66 || Conv_68 + Relu_69 || Conv_71 + Relu_72 || Conv_74 + Relu_75 || Conv_77 + Relu_78 || Conv_80 + Relu_81 || Conv_83 + Relu_84 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage4_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0x818413b69874bb35
[08/10/2023-11:17:51] [V] [TRT] Tactic: 0x818413b69874bb35 Time: 4.19229
[08/10/2023-11:17:51] [V] [TRT] Conv_62 + Relu_63 || Conv_65 + Relu_66 || Conv_68 + Relu_69 || Conv_71 + Relu_72 || Conv_74 + Relu_75 || Conv_77 + Relu_78 || Conv_80 + Relu_81 || Conv_83 + Relu_84 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0x94a7db94ba744c45
[08/10/2023-11:17:51] [V] [TRT] Tactic: 0x94a7db94ba744c45 Time: 9.74639
[08/10/2023-11:17:51] [V] [TRT] Conv_62 + Relu_63 || Conv_65 + Relu_66 || Conv_68 + Relu_69 || Conv_71 + Relu_72 || Conv_74 + Relu_75 || Conv_77 + Relu_78 || Conv_80 + Relu_81 || Conv_83 + Relu_84 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x256x32_stage3_warpsize2x4x1_g1_tensor16x8x8 Tactic: 0x998c97842e775a17
[08/10/2023-11:17:51] [V] [TRT] Tactic: 0x998c97842e775a17 Time: 2.12067
[08/10/2023-11:17:51] [V] [TRT] Conv_62 + Relu_63 || Conv_65 + Relu_66 || Conv_68 + Relu_69 || Conv_71 + Relu_72 || Conv_74 + Relu_75 || Conv_77 + Relu_78 || Conv_80 + Relu_81 || Conv_83 + Relu_84 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8_t1r3s3 Tactic: 0x999e005e3b016ea6
[08/10/2023-11:17:51] [V] [TRT] Tactic: 0x999e005e3b016ea6 Time: 2.10784
[08/10/2023-11:17:51] [V] [TRT] Conv_62 + Relu_63 || Conv_65 + Relu_66 || Conv_68 + Relu_69 || Conv_71 + Relu_72 || Conv_74 + Relu_75 || Conv_77 + Relu_78 || Conv_80 + Relu_81 || Conv_83 + Relu_84 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize256x64x32_stage3_warpsize4x2x1_g1_tensor16x8x8_t1r3s3 Tactic: 0xa9a06d0633580c0c
[08/10/2023-11:17:51] [V] [TRT] Tactic: 0xa9a06d0633580c0c Time: 2.5068
[08/10/2023-11:17:51] [V] [TRT] Conv_62 + Relu_63 || Conv_65 + Relu_66 || Conv_68 + Relu_69 || Conv_71 + Relu_72 || Conv_74 + Relu_75 || Conv_77 + Relu_78 || Conv_80 + Relu_81 || Conv_83 + Relu_84 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0xb443c221fcb1565b
[08/10/2023-11:17:51] [V] [TRT] Tactic: 0xb443c221fcb1565b Time: 2.16955
[08/10/2023-11:17:51] [V] [TRT] Conv_62 + Relu_63 || Conv_65 + Relu_66 || Conv_68 + Relu_69 || Conv_71 + Relu_72 || Conv_74 + Relu_75 || Conv_77 + Relu_78 || Conv_80 + Relu_81 || Conv_83 + Relu_84 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage3_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0xb47d7d926de02dee
[08/10/2023-11:17:51] [V] [TRT] Tactic: 0xb47d7d926de02dee Time: 3.1592
[08/10/2023-11:17:51] [V] [TRT] Conv_62 + Relu_63 || Conv_65 + Relu_66 || Conv_68 + Relu_69 || Conv_71 + Relu_72 || Conv_74 + Relu_75 || Conv_77 + Relu_78 || Conv_80 + Relu_81 || Conv_83 + Relu_84 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize256x64x32_stage3_warpsize4x2x1_g1_tensor16x8x8 Tactic: 0xba0185279ccb2b85
[08/10/2023-11:17:51] [V] [TRT] Tactic: 0xba0185279ccb2b85 Time: 2.73229
[08/10/2023-11:17:51] [V] [TRT] Conv_62 + Relu_63 || Conv_65 + Relu_66 || Conv_68 + Relu_69 || Conv_71 + Relu_72 || Conv_74 + Relu_75 || Conv_77 + Relu_78 || Conv_80 + Relu_81 || Conv_83 + Relu_84 Set Tactic Name: ampere_scudnn_128x64_sliced1x2_ldg4_relu_exp_large_nhwc_tn_v1 Tactic: 0xbdfdef6b84f7ccc9
[08/10/2023-11:17:51] [V] [TRT] Tactic: 0xbdfdef6b84f7ccc9 Time: 10.5004
[08/10/2023-11:17:51] [V] [TRT] Conv_62 + Relu_63 || Conv_65 + Relu_66 || Conv_68 + Relu_69 || Conv_71 + Relu_72 || Conv_74 + Relu_75 || Conv_77 + Relu_78 || Conv_80 + Relu_81 || Conv_83 + Relu_84 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage3_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0xc0a715d897e240bb
[08/10/2023-11:17:51] [V] [TRT] Tactic: 0xc0a715d897e240bb Time: 3.23244
[08/10/2023-11:17:51] [V] [TRT] Conv_62 + Relu_63 || Conv_65 + Relu_66 || Conv_68 + Relu_69 || Conv_71 + Relu_72 || Conv_74 + Relu_75 || Conv_77 + Relu_78 || Conv_80 + Relu_81 || Conv_83 + Relu_84 Set Tactic Name: ampere_scudnn_128x128_relu_exp_large_nhwc_tn_v1 Tactic: 0xca7eeb8d9143d738
[08/10/2023-11:17:51] [V] [TRT] Tactic: 0xca7eeb8d9143d738 Time: 9.41549
[08/10/2023-11:17:51] [V] [TRT] Conv_62 + Relu_63 || Conv_65 + Relu_66 || Conv_68 + Relu_69 || Conv_71 + Relu_72 || Conv_74 + Relu_75 || Conv_77 + Relu_78 || Conv_80 + Relu_81 || Conv_83 + Relu_84 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize256x64x32_stage3_warpsize4x2x1_g1_tensor16x8x8 Tactic: 0xcedbed6d66c946d0
[08/10/2023-11:17:52] [V] [TRT] Tactic: 0xcedbed6d66c946d0 Time: 2.6365
[08/10/2023-11:17:52] [V] [TRT] Conv_62 + Relu_63 || Conv_65 + Relu_66 || Conv_68 + Relu_69 || Conv_71 + Relu_72 || Conv_74 + Relu_75 || Conv_77 + Relu_78 || Conv_80 + Relu_81 || Conv_83 + Relu_84 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x128x8_stage3_warpsize1x4x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0xd15dd11d64344e83
[08/10/2023-11:17:52] [V] [TRT] Tactic: 0xd15dd11d64344e83 Time: 8.71384
[08/10/2023-11:17:52] [V] [TRT] Conv_62 + Relu_63 || Conv_65 + Relu_66 || Conv_68 + Relu_69 || Conv_71 + Relu_72 || Conv_74 + Relu_75 || Conv_77 + Relu_78 || Conv_80 + Relu_81 || Conv_83 + Relu_84 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize64x128x32_stage5_warpsize2x2x1_g1_tensor16x8x8_t1r3s3 Tactic: 0xd4428a7355769d7d
[08/10/2023-11:17:52] [V] [TRT] Tactic: 0xd4428a7355769d7d Time: 2.81578
[08/10/2023-11:17:52] [V] [TRT] Conv_62 + Relu_63 || Conv_65 + Relu_66 || Conv_68 + Relu_69 || Conv_71 + Relu_72 || Conv_74 + Relu_75 || Conv_77 + Relu_78 || Conv_80 + Relu_81 || Conv_83 + Relu_84 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize64x32x64_stage5_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0xd7f5d575d49a4bc7
[08/10/2023-11:17:52] [V] [TRT] Tactic: 0xd7f5d575d49a4bc7 Time: 7.15104
[08/10/2023-11:17:52] [V] [TRT] Conv_62 + Relu_63 || Conv_65 + Relu_66 || Conv_68 + Relu_69 || Conv_71 + Relu_72 || Conv_74 + Relu_75 || Conv_77 + Relu_78 || Conv_80 + Relu_81 || Conv_83 + Relu_84 Set Tactic Name: ampere_scudnn_128x128_relu_exp_medium_nhwc_tn_v1 Tactic: 0xd9031472c05adf51
[08/10/2023-11:17:52] [V] [TRT] Tactic: 0xd9031472c05adf51 Time: 9.39501
[08/10/2023-11:17:52] [V] [TRT] Conv_62 + Relu_63 || Conv_65 + Relu_66 || Conv_68 + Relu_69 || Conv_71 + Relu_72 || Conv_74 + Relu_75 || Conv_77 + Relu_78 || Conv_80 + Relu_81 || Conv_83 + Relu_84 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x128x32_stage4_warpsize2x2x1_g1_tensor16x8x8_t1r3s3 Tactic: 0xd920b33c9bd27143
[08/10/2023-11:17:52] [V] [TRT] Tactic: 0xd920b33c9bd27143 Time: 2.12487
[08/10/2023-11:17:52] [V] [TRT] Conv_62 + Relu_63 || Conv_65 + Relu_66 || Conv_68 + Relu_69 || Conv_71 + Relu_72 || Conv_74 + Relu_75 || Conv_77 + Relu_78 || Conv_80 + Relu_81 || Conv_83 + Relu_84 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize256x128x32_stage3_warpsize4x2x1_g1_tensor16x8x8 Tactic: 0xe797e099911c0624
[08/10/2023-11:17:52] [V] [TRT] Tactic: 0xe797e099911c0624 Time: 2.17442
[08/10/2023-11:17:52] [V] [TRT] Conv_62 + Relu_63 || Conv_65 + Relu_66 || Conv_68 + Relu_69 || Conv_71 + Relu_72 || Conv_74 + Relu_75 || Conv_77 + Relu_78 || Conv_80 + Relu_81 || Conv_83 + Relu_84 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage3_warpsize2x2x1_g1_tensor16x8x8_t1r3s3 Tactic: 0xeb65d4c1e8fa2294
[08/10/2023-11:17:52] [V] [TRT] Tactic: 0xeb65d4c1e8fa2294 Time: 3.16954
[08/10/2023-11:17:52] [V] [TRT] Conv_62 + Relu_63 || Conv_65 + Relu_66 || Conv_68 + Relu_69 || Conv_71 + Relu_72 || Conv_74 + Relu_75 || Conv_77 + Relu_78 || Conv_80 + Relu_81 || Conv_83 + Relu_84 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x128x32_stage4_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0xf4156675c5f728d4
[08/10/2023-11:17:52] [V] [TRT] Tactic: 0xf4156675c5f728d4 Time: 3.15432
[08/10/2023-11:17:52] [V] [TRT] Conv_62 + Relu_63 || Conv_65 + Relu_66 || Conv_68 + Relu_69 || Conv_71 + Relu_72 || Conv_74 + Relu_75 || Conv_77 + Relu_78 || Conv_80 + Relu_81 || Conv_83 + Relu_84 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x64x8_stage3_warpsize2x2x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0xf48db81f02eca9ee
[08/10/2023-11:17:52] [V] [TRT] Tactic: 0xf48db81f02eca9ee Time: 8.81832
[08/10/2023-11:17:52] [V] [TRT] Conv_62 + Relu_63 || Conv_65 + Relu_66 || Conv_68 + Relu_69 || Conv_71 + Relu_72 || Conv_74 + Relu_75 || Conv_77 + Relu_78 || Conv_80 + Relu_81 || Conv_83 + Relu_84 Set Tactic Name: ampere_scudnn_128x128_ldg4_relu_exp_small_nhwc_tn_v1 Tactic: 0xf90060ce8193b811
[08/10/2023-11:17:52] [V] [TRT] Tactic: 0xf90060ce8193b811 Time: 9.64798
[08/10/2023-11:17:52] [V] [TRT] Fastest Tactic: 0x482b48242255b8ce Time: 2.03089
[08/10/2023-11:17:52] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x482b48242255b8ce
[08/10/2023-11:17:52] [V] [TRT] *************** Autotuning format combination: Float(518400,1:4,2880,16) -> Float(4147200,1:4,23040,128) ***************
[08/10/2023-11:17:52] [V] [TRT] --------------- Timing Runner: Conv_62 + Relu_63 || Conv_65 + Relu_66 || Conv_68 + Relu_69 || Conv_71 + Relu_72 || Conv_74 + Relu_75 || Conv_77 + Relu_78 || Conv_80 + Relu_81 || Conv_83 + Relu_84 (CaskConvolution)
[08/10/2023-11:17:52] [V] [TRT] Conv_62 + Relu_63 || Conv_65 + Relu_66 || Conv_68 + Relu_69 || Conv_71 + Relu_72 || Conv_74 + Relu_75 || Conv_77 + Relu_78 || Conv_80 + Relu_81 || Conv_83 + Relu_84 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize64x128x32_stage5_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0x112d7e4d280f396d
[08/10/2023-11:17:52] [V] [TRT] Tactic: 0x112d7e4d280f396d Time: 3.22934
[08/10/2023-11:17:52] [V] [TRT] Conv_62 + Relu_63 || Conv_65 + Relu_66 || Conv_68 + Relu_69 || Conv_71 + Relu_72 || Conv_74 + Relu_75 || Conv_77 + Relu_78 || Conv_80 + Relu_81 || Conv_83 + Relu_84 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x64x32_stage5_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0x13cc2ad8dcd32ba2
[08/10/2023-11:17:52] [V] [TRT] Tactic: 0x13cc2ad8dcd32ba2 Time: 3.39034
[08/10/2023-11:17:52] [V] [TRT] Conv_62 + Relu_63 || Conv_65 + Relu_66 || Conv_68 + Relu_69 || Conv_71 + Relu_72 || Conv_74 + Relu_75 || Conv_77 + Relu_78 || Conv_80 + Relu_81 || Conv_83 + Relu_84 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize256x128x32_stage3_warpsize4x2x1_g1_tensor16x8x8_t1r3s3 Tactic: 0x21246c8544eff903
[08/10/2023-11:17:52] [V] [TRT] Tactic: 0x21246c8544eff903 Time: 2.05705
[08/10/2023-11:17:52] [V] [TRT] Conv_62 + Relu_63 || Conv_65 + Relu_66 || Conv_68 + Relu_69 || Conv_71 + Relu_72 || Conv_74 + Relu_75 || Conv_77 + Relu_78 || Conv_80 + Relu_81 || Conv_83 + Relu_84 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x128x32_stage4_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0x25b2b9d5c9d5ca0d
[08/10/2023-11:17:52] [V] [TRT] Tactic: 0x25b2b9d5c9d5ca0d Time: 2.30964
[08/10/2023-11:17:52] [V] [TRT] Conv_62 + Relu_63 || Conv_65 + Relu_66 || Conv_68 + Relu_69 || Conv_71 + Relu_72 || Conv_74 + Relu_75 || Conv_77 + Relu_78 || Conv_80 + Relu_81 || Conv_83 + Relu_84 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage4_warpsize2x2x1_g1_tensor16x8x8_t1r3s3 Tactic: 0x2d80d2b384ab13f1
[08/10/2023-11:17:52] [V] [TRT] Tactic: 0x2d80d2b384ab13f1 Time: 4.14598
[08/10/2023-11:17:52] [V] [TRT] Conv_62 + Relu_63 || Conv_65 + Relu_66 || Conv_68 + Relu_69 || Conv_71 + Relu_72 || Conv_74 + Relu_75 || Conv_77 + Relu_78 || Conv_80 + Relu_81 || Conv_83 + Relu_84 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize256x128x32_stage3_warpsize4x2x1_g1_tensor16x8x8 Tactic: 0x36303f399d3ee4fd
[08/10/2023-11:17:52] [V] [TRT] Tactic: 0x36303f399d3ee4fd Time: 2.95419
[08/10/2023-11:17:53] [V] [TRT] Conv_62 + Relu_63 || Conv_65 + Relu_66 || Conv_68 + Relu_69 || Conv_71 + Relu_72 || Conv_74 + Relu_75 || Conv_77 + Relu_78 || Conv_80 + Relu_81 || Conv_83 + Relu_84 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage4_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0x385bfab9332b1413
[08/10/2023-11:17:53] [V] [TRT] Tactic: 0x385bfab9332b1413 Time: 5.22837
[08/10/2023-11:17:53] [V] [TRT] Conv_62 + Relu_63 || Conv_65 + Relu_66 || Conv_68 + Relu_69 || Conv_71 + Relu_72 || Conv_74 + Relu_75 || Conv_77 + Relu_78 || Conv_80 + Relu_81 || Conv_83 + Relu_84 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x256x32_stage3_warpsize2x4x1_g1_tensor16x8x8 Tactic: 0x482b48242255b8ce
[08/10/2023-11:17:53] [V] [TRT] Tactic: 0x482b48242255b8ce Time: 2.53365
[08/10/2023-11:17:53] [V] [TRT] Conv_62 + Relu_63 || Conv_65 + Relu_66 || Conv_68 + Relu_69 || Conv_71 + Relu_72 || Conv_74 + Relu_75 || Conv_77 + Relu_78 || Conv_80 + Relu_81 || Conv_83 + Relu_84 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x256x32_stage3_warpsize2x4x1_g1_tensor16x8x8_t1r3s3 Tactic: 0x614e89f7852edbc3
[08/10/2023-11:17:53] [V] [TRT] Tactic: 0x614e89f7852edbc3 Time: 2.02701
[08/10/2023-11:17:53] [V] [TRT] Conv_62 + Relu_63 || Conv_65 + Relu_66 || Conv_68 + Relu_69 || Conv_71 + Relu_72 || Conv_74 + Relu_75 || Conv_77 + Relu_78 || Conv_80 + Relu_81 || Conv_83 + Relu_84 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0x65e41d81f093b482
[08/10/2023-11:17:53] [V] [TRT] Tactic: 0x65e41d81f093b482 Time: 2.28233
[08/10/2023-11:17:53] [V] [TRT] Conv_62 + Relu_63 || Conv_65 + Relu_66 || Conv_68 + Relu_69 || Conv_71 + Relu_72 || Conv_74 + Relu_75 || Conv_77 + Relu_78 || Conv_80 + Relu_81 || Conv_83 + Relu_84 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize64x128x32_stage5_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0x65f71607d20d5438
[08/10/2023-11:17:53] [V] [TRT] Tactic: 0x65f71607d20d5438 Time: 3.13259
[08/10/2023-11:17:53] [V] [TRT] Conv_62 + Relu_63 || Conv_65 + Relu_66 || Conv_68 + Relu_69 || Conv_71 + Relu_72 || Conv_74 + Relu_75 || Conv_77 + Relu_78 || Conv_80 + Relu_81 || Conv_83 + Relu_84 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x64x32_stage5_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0x6716429226d146f7
[08/10/2023-11:17:53] [V] [TRT] Tactic: 0x6716429226d146f7 Time: 3.7796
[08/10/2023-11:17:53] [V] [TRT] Conv_62 + Relu_63 || Conv_65 + Relu_66 || Conv_68 + Relu_69 || Conv_71 + Relu_72 || Conv_74 + Relu_75 || Conv_77 + Relu_78 || Conv_80 + Relu_81 || Conv_83 + Relu_84 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize64x32x64_stage5_warpsize2x2x1_g1_tensor16x8x8_t1r3s3 Tactic: 0x679a281f7660c436
[08/10/2023-11:17:53] [V] [TRT] Tactic: 0x679a281f7660c436 Time: 7.21753
[08/10/2023-11:17:53] [V] [TRT] Conv_62 + Relu_63 || Conv_65 + Relu_66 || Conv_68 + Relu_69 || Conv_71 + Relu_72 || Conv_74 + Relu_75 || Conv_77 + Relu_78 || Conv_80 + Relu_81 || Conv_83 + Relu_84 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize64x32x64_stage5_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0x6e2a3c7a7fc5e4e1
[08/10/2023-11:17:53] [V] [TRT] Tactic: 0x6e2a3c7a7fc5e4e1 Time: 6.60232
[08/10/2023-11:17:53] [V] [TRT] Conv_62 + Relu_63 || Conv_65 + Relu_66 || Conv_68 + Relu_69 || Conv_71 + Relu_72 || Conv_74 + Relu_75 || Conv_77 + Relu_78 || Conv_80 + Relu_81 || Conv_83 + Relu_84 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x64x32_stage5_warpsize2x2x1_g1_tensor16x8x8_t1r3s3 Tactic: 0x7aa21282feb15f1a
[08/10/2023-11:17:53] [V] [TRT] Tactic: 0x7aa21282feb15f1a Time: 3.08531
[08/10/2023-11:17:53] [V] [TRT] Conv_62 + Relu_63 || Conv_65 + Relu_66 || Conv_68 + Relu_69 || Conv_71 + Relu_72 || Conv_74 + Relu_75 || Conv_77 + Relu_78 || Conv_80 + Relu_81 || Conv_83 + Relu_84 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage4_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0x818413b69874bb35
[08/10/2023-11:17:53] [V] [TRT] Tactic: 0x818413b69874bb35 Time: 4.2072
[08/10/2023-11:17:53] [V] [TRT] Conv_62 + Relu_63 || Conv_65 + Relu_66 || Conv_68 + Relu_69 || Conv_71 + Relu_72 || Conv_74 + Relu_75 || Conv_77 + Relu_78 || Conv_80 + Relu_81 || Conv_83 + Relu_84 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x256x32_stage3_warpsize2x4x1_g1_tensor16x8x8 Tactic: 0x998c97842e775a17
[08/10/2023-11:17:53] [V] [TRT] Tactic: 0x998c97842e775a17 Time: 2.18006
[08/10/2023-11:17:53] [V] [TRT] Conv_62 + Relu_63 || Conv_65 + Relu_66 || Conv_68 + Relu_69 || Conv_71 + Relu_72 || Conv_74 + Relu_75 || Conv_77 + Relu_78 || Conv_80 + Relu_81 || Conv_83 + Relu_84 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8_t1r3s3 Tactic: 0x999e005e3b016ea6
[08/10/2023-11:17:53] [V] [TRT] Tactic: 0x999e005e3b016ea6 Time: 2.10835
[08/10/2023-11:17:53] [V] [TRT] Conv_62 + Relu_63 || Conv_65 + Relu_66 || Conv_68 + Relu_69 || Conv_71 + Relu_72 || Conv_74 + Relu_75 || Conv_77 + Relu_78 || Conv_80 + Relu_81 || Conv_83 + Relu_84 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize256x64x32_stage3_warpsize4x2x1_g1_tensor16x8x8_t1r3s3 Tactic: 0xa9a06d0633580c0c
[08/10/2023-11:17:53] [V] [TRT] Tactic: 0xa9a06d0633580c0c Time: 2.45683
[08/10/2023-11:17:53] [V] [TRT] Conv_62 + Relu_63 || Conv_65 + Relu_66 || Conv_68 + Relu_69 || Conv_71 + Relu_72 || Conv_74 + Relu_75 || Conv_77 + Relu_78 || Conv_80 + Relu_81 || Conv_83 + Relu_84 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0xb443c221fcb1565b
[08/10/2023-11:17:53] [V] [TRT] Tactic: 0xb443c221fcb1565b Time: 2.16402
[08/10/2023-11:17:53] [V] [TRT] Conv_62 + Relu_63 || Conv_65 + Relu_66 || Conv_68 + Relu_69 || Conv_71 + Relu_72 || Conv_74 + Relu_75 || Conv_77 + Relu_78 || Conv_80 + Relu_81 || Conv_83 + Relu_84 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage3_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0xb47d7d926de02dee
[08/10/2023-11:17:53] [V] [TRT] Tactic: 0xb47d7d926de02dee Time: 3.15941
[08/10/2023-11:17:53] [V] [TRT] Conv_62 + Relu_63 || Conv_65 + Relu_66 || Conv_68 + Relu_69 || Conv_71 + Relu_72 || Conv_74 + Relu_75 || Conv_77 + Relu_78 || Conv_80 + Relu_81 || Conv_83 + Relu_84 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize256x64x32_stage3_warpsize4x2x1_g1_tensor16x8x8 Tactic: 0xba0185279ccb2b85
[08/10/2023-11:17:53] [V] [TRT] Tactic: 0xba0185279ccb2b85 Time: 2.75083
[08/10/2023-11:17:53] [V] [TRT] Conv_62 + Relu_63 || Conv_65 + Relu_66 || Conv_68 + Relu_69 || Conv_71 + Relu_72 || Conv_74 + Relu_75 || Conv_77 + Relu_78 || Conv_80 + Relu_81 || Conv_83 + Relu_84 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage3_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0xc0a715d897e240bb
[08/10/2023-11:17:53] [V] [TRT] Tactic: 0xc0a715d897e240bb Time: 3.26591
[08/10/2023-11:17:53] [V] [TRT] Conv_62 + Relu_63 || Conv_65 + Relu_66 || Conv_68 + Relu_69 || Conv_71 + Relu_72 || Conv_74 + Relu_75 || Conv_77 + Relu_78 || Conv_80 + Relu_81 || Conv_83 + Relu_84 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize256x64x32_stage3_warpsize4x2x1_g1_tensor16x8x8 Tactic: 0xcedbed6d66c946d0
[08/10/2023-11:17:53] [V] [TRT] Tactic: 0xcedbed6d66c946d0 Time: 2.57451
[08/10/2023-11:17:53] [V] [TRT] Conv_62 + Relu_63 || Conv_65 + Relu_66 || Conv_68 + Relu_69 || Conv_71 + Relu_72 || Conv_74 + Relu_75 || Conv_77 + Relu_78 || Conv_80 + Relu_81 || Conv_83 + Relu_84 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize64x128x32_stage5_warpsize2x2x1_g1_tensor16x8x8_t1r3s3 Tactic: 0xd4428a7355769d7d
[08/10/2023-11:17:53] [V] [TRT] Tactic: 0xd4428a7355769d7d Time: 2.82223
[08/10/2023-11:17:53] [V] [TRT] Conv_62 + Relu_63 || Conv_65 + Relu_66 || Conv_68 + Relu_69 || Conv_71 + Relu_72 || Conv_74 + Relu_75 || Conv_77 + Relu_78 || Conv_80 + Relu_81 || Conv_83 + Relu_84 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize64x32x64_stage5_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0xd7f5d575d49a4bc7
[08/10/2023-11:17:54] [V] [TRT] Tactic: 0xd7f5d575d49a4bc7 Time: 7.09529
[08/10/2023-11:17:54] [V] [TRT] Conv_62 + Relu_63 || Conv_65 + Relu_66 || Conv_68 + Relu_69 || Conv_71 + Relu_72 || Conv_74 + Relu_75 || Conv_77 + Relu_78 || Conv_80 + Relu_81 || Conv_83 + Relu_84 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x128x32_stage4_warpsize2x2x1_g1_tensor16x8x8_t1r3s3 Tactic: 0xd920b33c9bd27143
[08/10/2023-11:17:54] [V] [TRT] Tactic: 0xd920b33c9bd27143 Time: 2.14091
[08/10/2023-11:17:54] [V] [TRT] Conv_62 + Relu_63 || Conv_65 + Relu_66 || Conv_68 + Relu_69 || Conv_71 + Relu_72 || Conv_74 + Relu_75 || Conv_77 + Relu_78 || Conv_80 + Relu_81 || Conv_83 + Relu_84 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize256x128x32_stage3_warpsize4x2x1_g1_tensor16x8x8 Tactic: 0xe797e099911c0624
[08/10/2023-11:17:54] [V] [TRT] Tactic: 0xe797e099911c0624 Time: 2.17855
[08/10/2023-11:17:54] [V] [TRT] Conv_62 + Relu_63 || Conv_65 + Relu_66 || Conv_68 + Relu_69 || Conv_71 + Relu_72 || Conv_74 + Relu_75 || Conv_77 + Relu_78 || Conv_80 + Relu_81 || Conv_83 + Relu_84 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage3_warpsize2x2x1_g1_tensor16x8x8_t1r3s3 Tactic: 0xeb65d4c1e8fa2294
[08/10/2023-11:17:54] [V] [TRT] Tactic: 0xeb65d4c1e8fa2294 Time: 3.10216
[08/10/2023-11:17:54] [V] [TRT] Conv_62 + Relu_63 || Conv_65 + Relu_66 || Conv_68 + Relu_69 || Conv_71 + Relu_72 || Conv_74 + Relu_75 || Conv_77 + Relu_78 || Conv_80 + Relu_81 || Conv_83 + Relu_84 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x128x32_stage4_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0xf4156675c5f728d4
[08/10/2023-11:17:54] [V] [TRT] Tactic: 0xf4156675c5f728d4 Time: 3.15205
[08/10/2023-11:17:54] [V] [TRT] Fastest Tactic: 0x614e89f7852edbc3 Time: 2.02701
[08/10/2023-11:17:54] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x614e89f7852edbc3
[08/10/2023-11:17:54] [V] [TRT] *************** Autotuning format combination: Half(2073600,32400,180,1) -> Half(16588800,32400,180,1) ***************
[08/10/2023-11:17:54] [W] [TRT] Weights [name=Conv_62 + Relu_63 || Conv_65 + Relu_66 || Conv_68 + Relu_69 || Conv_71 + Relu_72 || Conv_74 + Relu_75 || Conv_77 + Relu_78 || Conv_80 + Relu_81 || Conv_83 + Relu_84.weight] had the following issues when converted to FP16:
[08/10/2023-11:17:54] [W] [TRT]  - Subnormal FP16 values detected. 
[08/10/2023-11:17:54] [W] [TRT]  - Values less than smallest positive FP16 Subnormal value detected. Converting to FP16 minimum subnormalized value. 
[08/10/2023-11:17:54] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.
[08/10/2023-11:17:54] [W] [TRT] Weights [name=Conv_62 + Relu_63 || Conv_65 + Relu_66 || Conv_68 + Relu_69 || Conv_71 + Relu_72 || Conv_74 + Relu_75 || Conv_77 + Relu_78 || Conv_80 + Relu_81 || Conv_83 + Relu_84.bias] had the following issues when converted to FP16:
[08/10/2023-11:17:54] [W] [TRT]  - Subnormal FP16 values detected. 
[08/10/2023-11:17:54] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.
[08/10/2023-11:17:54] [V] [TRT] --------------- Timing Runner: Conv_62 + Relu_63 || Conv_65 + Relu_66 || Conv_68 + Relu_69 || Conv_71 + Relu_72 || Conv_74 + Relu_75 || Conv_77 + Relu_78 || Conv_80 + Relu_81 || Conv_83 + Relu_84 (CudnnConvolution)
[08/10/2023-11:17:54] [V] [TRT] Tactic: 0x0000000000000000 Time: 15.0059
[08/10/2023-11:17:54] [V] [TRT] Tactic: 0x0000000000000001 Time: 12.7314
[08/10/2023-11:17:54] [V] [TRT] Tactic: 0x0000000000000002 Time: 14.8614
[08/10/2023-11:17:54] [V] [TRT] Tactic: 0x0000000000000004 skipped. Scratch requested: 17349083136, available: 4294967296
[08/10/2023-11:17:55] [V] [TRT] Tactic: 0x0000000000000005 Time: 57.9218
[08/10/2023-11:17:55] [V] [TRT] Tactic: 0x0000000000000006 Time: 10.1312
[08/10/2023-11:17:55] [V] [TRT] Tactic: 0x0000000000000038 Time: 14.9297
[08/10/2023-11:17:55] [V] [TRT] Tactic: 0x000000000000003a Time: 14.7964
[08/10/2023-11:17:55] [V] [TRT] Tactic: 0x000000000000003c skipped. Scratch requested: 17349083136, available: 4294967296
[08/10/2023-11:17:55] [V] [TRT] Tactic: 0x000000000000003d Time: 37.8286
[08/10/2023-11:17:55] [V] [TRT] Tactic: 0x000000000000003e Time: 9.25242
[08/10/2023-11:17:55] [V] [TRT] Fastest Tactic: 0x000000000000003e Time: 9.25242
[08/10/2023-11:17:55] [V] [TRT] Setting workspace to 17349083136enables more tactics for profiling
[08/10/2023-11:17:55] [W] [TRT] Weights [name=Conv_62 + Relu_63 || Conv_65 + Relu_66 || Conv_68 + Relu_69 || Conv_71 + Relu_72 || Conv_74 + Relu_75 || Conv_77 + Relu_78 || Conv_80 + Relu_81 || Conv_83 + Relu_84.weight] had the following issues when converted to FP16:
[08/10/2023-11:17:55] [W] [TRT]  - Subnormal FP16 values detected. 
[08/10/2023-11:17:55] [W] [TRT]  - Values less than smallest positive FP16 Subnormal value detected. Converting to FP16 minimum subnormalized value. 
[08/10/2023-11:17:55] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.
[08/10/2023-11:17:55] [W] [TRT] Weights [name=Conv_62 + Relu_63 || Conv_65 + Relu_66 || Conv_68 + Relu_69 || Conv_71 + Relu_72 || Conv_74 + Relu_75 || Conv_77 + Relu_78 || Conv_80 + Relu_81 || Conv_83 + Relu_84.bias] had the following issues when converted to FP16:
[08/10/2023-11:17:55] [W] [TRT]  - Subnormal FP16 values detected. 
[08/10/2023-11:17:55] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.
[08/10/2023-11:17:55] [V] [TRT] --------------- Timing Runner: Conv_62 + Relu_63 || Conv_65 + Relu_66 || Conv_68 + Relu_69 || Conv_71 + Relu_72 || Conv_74 + Relu_75 || Conv_77 + Relu_78 || Conv_80 + Relu_81 || Conv_83 + Relu_84 (CaskConvolution)
[08/10/2023-11:17:55] [V] [TRT] CaskConvolution has no valid tactics for this config, skipping
[08/10/2023-11:17:55] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CudnnConvolution Tactic: 0x000000000000003e
[08/10/2023-11:17:55] [V] [TRT] *************** Autotuning format combination: Half(1036800,32400:2,180,1) -> Half(8294400,32400:2,180,1) ***************
[08/10/2023-11:17:55] [W] [TRT] Weights [name=Conv_62 + Relu_63 || Conv_65 + Relu_66 || Conv_68 + Relu_69 || Conv_71 + Relu_72 || Conv_74 + Relu_75 || Conv_77 + Relu_78 || Conv_80 + Relu_81 || Conv_83 + Relu_84.weight] had the following issues when converted to FP16:
[08/10/2023-11:17:55] [W] [TRT]  - Subnormal FP16 values detected. 
[08/10/2023-11:17:55] [W] [TRT]  - Values less than smallest positive FP16 Subnormal value detected. Converting to FP16 minimum subnormalized value. 
[08/10/2023-11:17:55] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.
[08/10/2023-11:17:55] [W] [TRT] Weights [name=Conv_62 + Relu_63 || Conv_65 + Relu_66 || Conv_68 + Relu_69 || Conv_71 + Relu_72 || Conv_74 + Relu_75 || Conv_77 + Relu_78 || Conv_80 + Relu_81 || Conv_83 + Relu_84.bias] had the following issues when converted to FP16:
[08/10/2023-11:17:55] [W] [TRT]  - Subnormal FP16 values detected. 
[08/10/2023-11:17:55] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.
[08/10/2023-11:17:55] [V] [TRT] --------------- Timing Runner: Conv_62 + Relu_63 || Conv_65 + Relu_66 || Conv_68 + Relu_69 || Conv_71 + Relu_72 || Conv_74 + Relu_75 || Conv_77 + Relu_78 || Conv_80 + Relu_81 || Conv_83 + Relu_84 (FusedConvActConvolution)
[08/10/2023-11:17:55] [V] [TRT] FusedConvActConvolution has no valid tactics for this config, skipping
[08/10/2023-11:17:55] [W] [TRT] Weights [name=Conv_62 + Relu_63 || Conv_65 + Relu_66 || Conv_68 + Relu_69 || Conv_71 + Relu_72 || Conv_74 + Relu_75 || Conv_77 + Relu_78 || Conv_80 + Relu_81 || Conv_83 + Relu_84.weight] had the following issues when converted to FP16:
[08/10/2023-11:17:55] [W] [TRT]  - Subnormal FP16 values detected. 
[08/10/2023-11:17:55] [W] [TRT]  - Values less than smallest positive FP16 Subnormal value detected. Converting to FP16 minimum subnormalized value. 
[08/10/2023-11:17:55] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.
[08/10/2023-11:17:55] [W] [TRT] Weights [name=Conv_62 + Relu_63 || Conv_65 + Relu_66 || Conv_68 + Relu_69 || Conv_71 + Relu_72 || Conv_74 + Relu_75 || Conv_77 + Relu_78 || Conv_80 + Relu_81 || Conv_83 + Relu_84.bias] had the following issues when converted to FP16:
[08/10/2023-11:17:55] [W] [TRT]  - Subnormal FP16 values detected. 
[08/10/2023-11:17:55] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.
[08/10/2023-11:17:55] [V] [TRT] --------------- Timing Runner: Conv_62 + Relu_63 || Conv_65 + Relu_66 || Conv_68 + Relu_69 || Conv_71 + Relu_72 || Conv_74 + Relu_75 || Conv_77 + Relu_78 || Conv_80 + Relu_81 || Conv_83 + Relu_84 (CaskConvolution)
[08/10/2023-11:17:55] [V] [TRT] Conv_62 + Relu_63 || Conv_65 + Relu_66 || Conv_68 + Relu_69 || Conv_71 + Relu_72 || Conv_74 + Relu_75 || Conv_77 + Relu_78 || Conv_80 + Relu_81 || Conv_83 + Relu_84 Set Tactic Name: ampere_fp16x2_hcudnn_fp16x2_128x32_relu_medium_nn_v1 Tactic: 0x1e7896ba71ef1635
[08/10/2023-11:17:56] [V] [TRT] Tactic: 0x1e7896ba71ef1635 Time: 6.44912
[08/10/2023-11:17:56] [V] [TRT] Conv_62 + Relu_63 || Conv_65 + Relu_66 || Conv_68 + Relu_69 || Conv_71 + Relu_72 || Conv_74 + Relu_75 || Conv_77 + Relu_78 || Conv_80 + Relu_81 || Conv_83 + Relu_84 Set Tactic Name: ampere_fp16x2_hcudnn_fp16x2_128x64_relu_large_nn_v1 Tactic: 0x2f735ffbb05a30fd
[08/10/2023-11:17:56] [V] [TRT] Tactic: 0x2f735ffbb05a30fd Time: 6.58048
[08/10/2023-11:17:56] [V] [TRT] Conv_62 + Relu_63 || Conv_65 + Relu_66 || Conv_68 + Relu_69 || Conv_71 + Relu_72 || Conv_74 + Relu_75 || Conv_77 + Relu_78 || Conv_80 + Relu_81 || Conv_83 + Relu_84 Set Tactic Name: ampere_fp16x2_hcudnn_fp16x2_128x128_relu_medium_nn_v1 Tactic: 0x360278e347d63410
[08/10/2023-11:17:56] [V] [TRT] Tactic: 0x360278e347d63410 Time: 7.63445
[08/10/2023-11:17:56] [V] [TRT] Conv_62 + Relu_63 || Conv_65 + Relu_66 || Conv_68 + Relu_69 || Conv_71 + Relu_72 || Conv_74 + Relu_75 || Conv_77 + Relu_78 || Conv_80 + Relu_81 || Conv_83 + Relu_84 Set Tactic Name: ampere_fp16x2_hcudnn_fp16x2_128x64_relu_medium_nn_v1 Tactic: 0x4cfee77ea8c324db
[08/10/2023-11:17:56] [V] [TRT] Tactic: 0x4cfee77ea8c324db Time: 7.00401
[08/10/2023-11:17:56] [V] [TRT] Conv_62 + Relu_63 || Conv_65 + Relu_66 || Conv_68 + Relu_69 || Conv_71 + Relu_72 || Conv_74 + Relu_75 || Conv_77 + Relu_78 || Conv_80 + Relu_81 || Conv_83 + Relu_84 Set Tactic Name: ampere_fp16x2_hcudnn_fp16x2_128x64_relu_small_nn_v1 Tactic: 0x540fde3a7bee53dc
[08/10/2023-11:17:56] [V] [TRT] Tactic: 0x540fde3a7bee53dc Time: 6.80725
[08/10/2023-11:17:56] [V] [TRT] Conv_62 + Relu_63 || Conv_65 + Relu_66 || Conv_68 + Relu_69 || Conv_71 + Relu_72 || Conv_74 + Relu_75 || Conv_77 + Relu_78 || Conv_80 + Relu_81 || Conv_83 + Relu_84 Set Tactic Name: ampere_fp16x2_hcudnn_fp16x2_128x128_relu_large_nn_v1 Tactic: 0x91f7e9c0851ad67c
[08/10/2023-11:17:56] [V] [TRT] Tactic: 0x91f7e9c0851ad67c Time: 8.13851
[08/10/2023-11:17:56] [V] [TRT] Conv_62 + Relu_63 || Conv_65 + Relu_66 || Conv_68 + Relu_69 || Conv_71 + Relu_72 || Conv_74 + Relu_75 || Conv_77 + Relu_78 || Conv_80 + Relu_81 || Conv_83 + Relu_84 Set Tactic Name: ampere_fp16x2_hcudnn_fp16x2_128x32_relu_large_nn_v1 Tactic: 0xb837f96ef306f686
[08/10/2023-11:17:56] [V] [TRT] Tactic: 0xb837f96ef306f686 Time: 7.40345
[08/10/2023-11:17:56] [V] [TRT] Conv_62 + Relu_63 || Conv_65 + Relu_66 || Conv_68 + Relu_69 || Conv_71 + Relu_72 || Conv_74 + Relu_75 || Conv_77 + Relu_78 || Conv_80 + Relu_81 || Conv_83 + Relu_84 Set Tactic Name: ampere_fp16x2_hcudnn_fp16x2_128x32_relu_small_nn_v1 Tactic: 0xc34b78af38b295a7
[08/10/2023-11:17:56] [V] [TRT] Tactic: 0xc34b78af38b295a7 Time: 7.27944
[08/10/2023-11:17:56] [V] [TRT] Conv_62 + Relu_63 || Conv_65 + Relu_66 || Conv_68 + Relu_69 || Conv_71 + Relu_72 || Conv_74 + Relu_75 || Conv_77 + Relu_78 || Conv_80 + Relu_81 || Conv_83 + Relu_84 Set Tactic Name: ampere_fp16x2_hcudnn_winograd_fp16x2_128x128_ldg1_ldg4_relu_tile148t_nt_v1 Tactic: 0xc754debea88ae0b7
[08/10/2023-11:17:56] [V] [TRT] Tactic: 0xc754debea88ae0b7 Time: 4.04186
[08/10/2023-11:17:56] [V] [TRT] Conv_62 + Relu_63 || Conv_65 + Relu_66 || Conv_68 + Relu_69 || Conv_71 + Relu_72 || Conv_74 + Relu_75 || Conv_77 + Relu_78 || Conv_80 + Relu_81 || Conv_83 + Relu_84 Set Tactic Name: ampere_fp16x2_hcudnn_fp16x2_128x128_relu_small_nn_v1 Tactic: 0xea8b68014eaeb55d
[08/10/2023-11:17:56] [V] [TRT] Tactic: 0xea8b68014eaeb55d Time: 7.2281
[08/10/2023-11:17:56] [V] [TRT] Fastest Tactic: 0xc754debea88ae0b7 Time: 4.04186
[08/10/2023-11:17:56] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0xc754debea88ae0b7
[08/10/2023-11:17:56] [V] [TRT] *************** Autotuning format combination: Half(259200,1:8,1440,8) -> Float(16588800,32400,180,1) ***************
[08/10/2023-11:17:56] [W] [TRT] Weights [name=Conv_62 + Relu_63 || Conv_65 + Relu_66 || Conv_68 + Relu_69 || Conv_71 + Relu_72 || Conv_74 + Relu_75 || Conv_77 + Relu_78 || Conv_80 + Relu_81 || Conv_83 + Relu_84.weight] had the following issues when converted to FP16:
[08/10/2023-11:17:56] [W] [TRT]  - Subnormal FP16 values detected. 
[08/10/2023-11:17:56] [W] [TRT]  - Values less than smallest positive FP16 Subnormal value detected. Converting to FP16 minimum subnormalized value. 
[08/10/2023-11:17:56] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.
[08/10/2023-11:17:56] [W] [TRT] Weights [name=Conv_62 + Relu_63 || Conv_65 + Relu_66 || Conv_68 + Relu_69 || Conv_71 + Relu_72 || Conv_74 + Relu_75 || Conv_77 + Relu_78 || Conv_80 + Relu_81 || Conv_83 + Relu_84.bias] had the following issues when converted to FP16:
[08/10/2023-11:17:56] [W] [TRT]  - Subnormal FP16 values detected. 
[08/10/2023-11:17:56] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.
[08/10/2023-11:17:56] [V] [TRT] --------------- Timing Runner: Conv_62 + Relu_63 || Conv_65 + Relu_66 || Conv_68 + Relu_69 || Conv_71 + Relu_72 || Conv_74 + Relu_75 || Conv_77 + Relu_78 || Conv_80 + Relu_81 || Conv_83 + Relu_84 (CaskConvolution)
[08/10/2023-11:17:56] [V] [TRT] CaskConvolution has no valid tactics for this config, skipping
[08/10/2023-11:17:56] [V] [TRT] *************** Autotuning format combination: Half(259200,1:8,1440,8) -> Half(2073600,1:8,11520,64) ***************
[08/10/2023-11:17:56] [W] [TRT] Weights [name=Conv_62 + Relu_63 || Conv_65 + Relu_66 || Conv_68 + Relu_69 || Conv_71 + Relu_72 || Conv_74 + Relu_75 || Conv_77 + Relu_78 || Conv_80 + Relu_81 || Conv_83 + Relu_84.weight] had the following issues when converted to FP16:
[08/10/2023-11:17:56] [W] [TRT]  - Subnormal FP16 values detected. 
[08/10/2023-11:17:56] [W] [TRT]  - Values less than smallest positive FP16 Subnormal value detected. Converting to FP16 minimum subnormalized value. 
[08/10/2023-11:17:56] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.
[08/10/2023-11:17:56] [W] [TRT] Weights [name=Conv_62 + Relu_63 || Conv_65 + Relu_66 || Conv_68 + Relu_69 || Conv_71 + Relu_72 || Conv_74 + Relu_75 || Conv_77 + Relu_78 || Conv_80 + Relu_81 || Conv_83 + Relu_84.bias] had the following issues when converted to FP16:
[08/10/2023-11:17:56] [W] [TRT]  - Subnormal FP16 values detected. 
[08/10/2023-11:17:56] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.
[08/10/2023-11:17:56] [V] [TRT] --------------- Timing Runner: Conv_62 + Relu_63 || Conv_65 + Relu_66 || Conv_68 + Relu_69 || Conv_71 + Relu_72 || Conv_74 + Relu_75 || Conv_77 + Relu_78 || Conv_80 + Relu_81 || Conv_83 + Relu_84 (CudaDepthwiseConvolution)
[08/10/2023-11:17:56] [V] [TRT] CudaDepthwiseConvolution has no valid tactics for this config, skipping
[08/10/2023-11:17:56] [W] [TRT] Weights [name=Conv_62 + Relu_63 || Conv_65 + Relu_66 || Conv_68 + Relu_69 || Conv_71 + Relu_72 || Conv_74 + Relu_75 || Conv_77 + Relu_78 || Conv_80 + Relu_81 || Conv_83 + Relu_84.weight] had the following issues when converted to FP16:
[08/10/2023-11:17:56] [W] [TRT]  - Subnormal FP16 values detected. 
[08/10/2023-11:17:56] [W] [TRT]  - Values less than smallest positive FP16 Subnormal value detected. Converting to FP16 minimum subnormalized value. 
[08/10/2023-11:17:56] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.
[08/10/2023-11:17:56] [W] [TRT] Weights [name=Conv_62 + Relu_63 || Conv_65 + Relu_66 || Conv_68 + Relu_69 || Conv_71 + Relu_72 || Conv_74 + Relu_75 || Conv_77 + Relu_78 || Conv_80 + Relu_81 || Conv_83 + Relu_84.bias] had the following issues when converted to FP16:
[08/10/2023-11:17:56] [W] [TRT]  - Subnormal FP16 values detected. 
[08/10/2023-11:17:56] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.
[08/10/2023-11:17:56] [V] [TRT] --------------- Timing Runner: Conv_62 + Relu_63 || Conv_65 + Relu_66 || Conv_68 + Relu_69 || Conv_71 + Relu_72 || Conv_74 + Relu_75 || Conv_77 + Relu_78 || Conv_80 + Relu_81 || Conv_83 + Relu_84 (CaskConvolution)
[08/10/2023-11:17:56] [V] [TRT] Conv_62 + Relu_63 || Conv_65 + Relu_66 || Conv_68 + Relu_69 || Conv_71 + Relu_72 || Conv_74 + Relu_75 || Conv_77 + Relu_78 || Conv_80 + Relu_81 || Conv_83 + Relu_84 Set Tactic Name: ampere_h16816cudnn_256x128_ldg8_relu_exp_medium_nhwc_tn_v1 Tactic: 0x03896956a39a1203
[08/10/2023-11:17:56] [V] [TRT] Tactic: 0x03896956a39a1203 Time: 1.55562
[08/10/2023-11:17:56] [V] [TRT] Conv_62 + Relu_63 || Conv_65 + Relu_66 || Conv_68 + Relu_69 || Conv_71 + Relu_72 || Conv_74 + Relu_75 || Conv_77 + Relu_78 || Conv_80 + Relu_81 || Conv_83 + Relu_84 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x128x32_stage3_warpsize4x2x1_g1_tensor16x8x16_t1r3s3 Tactic: 0x048d6d0400f33439
[08/10/2023-11:17:56] [V] [TRT] Tactic: 0x048d6d0400f33439 Time: 1.35184
[08/10/2023-11:17:56] [V] [TRT] Conv_62 + Relu_63 || Conv_65 + Relu_66 || Conv_68 + Relu_69 || Conv_71 + Relu_72 || Conv_74 + Relu_75 || Conv_77 + Relu_78 || Conv_80 + Relu_81 || Conv_83 + Relu_84 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x64x64_stage3_warpsize2x2x1_g1_tensor16x8x16_t1r3s3 Tactic: 0x05b6220f243edacd
[08/10/2023-11:17:56] [V] [TRT] Tactic: 0x05b6220f243edacd Time: 1.64772
[08/10/2023-11:17:56] [V] [TRT] Conv_62 + Relu_63 || Conv_65 + Relu_66 || Conv_68 + Relu_69 || Conv_71 + Relu_72 || Conv_74 + Relu_75 || Conv_77 + Relu_78 || Conv_80 + Relu_81 || Conv_83 + Relu_84 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x64x32_stage3_warpsize4x1x1_g1_tensor16x8x16_t1r3s3_aACCESS Tactic: 0x0866ddee325d07a6
[08/10/2023-11:17:56] [V] [TRT] Tactic: 0x0866ddee325d07a6 Time: 1.49738
[08/10/2023-11:17:56] [V] [TRT] Conv_62 + Relu_63 || Conv_65 + Relu_66 || Conv_68 + Relu_69 || Conv_71 + Relu_72 || Conv_74 + Relu_75 || Conv_77 + Relu_78 || Conv_80 + Relu_81 || Conv_83 + Relu_84 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x128x32_stage4_warpsize2x2x1_g1_tensor16x8x16 Tactic: 0x0e07ff4f4f7c1ac9
[08/10/2023-11:17:56] [V] [TRT] Tactic: 0x0e07ff4f4f7c1ac9 Time: 1.29762
[08/10/2023-11:17:56] [V] [TRT] Conv_62 + Relu_63 || Conv_65 + Relu_66 || Conv_68 + Relu_69 || Conv_71 + Relu_72 || Conv_74 + Relu_75 || Conv_77 + Relu_78 || Conv_80 + Relu_81 || Conv_83 + Relu_84 Set Tactic Name: ampere_h16816cudnn_256x128_ldg8_relu_exp_stages_64x3_small_nhwc_tn_v1 Tactic: 0x0e199750c30c6928
[08/10/2023-11:17:56] [V] [TRT] Tactic: 0x0e199750c30c6928 Time: 1.40656
[08/10/2023-11:17:56] [V] [TRT] Conv_62 + Relu_63 || Conv_65 + Relu_66 || Conv_68 + Relu_69 || Conv_71 + Relu_72 || Conv_74 + Relu_75 || Conv_77 + Relu_78 || Conv_80 + Relu_81 || Conv_83 + Relu_84 Set Tactic Name: ampere_h1688cudnn_128x128_ldg8_relu_exp_medium_nhwc_tn_v1 Tactic: 0x100c1ad308e08d35
[08/10/2023-11:17:56] [V] [TRT] Tactic: 0x100c1ad308e08d35 Time: 1.65237
[08/10/2023-11:17:56] [V] [TRT] Conv_62 + Relu_63 || Conv_65 + Relu_66 || Conv_68 + Relu_69 || Conv_71 + Relu_72 || Conv_74 + Relu_75 || Conv_77 + Relu_78 || Conv_80 + Relu_81 || Conv_83 + Relu_84 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x128x32_stage5_warpsize2x2x1_g1_tensor16x8x16_t1r3s3 Tactic: 0x17ebf0c9f418f10a
[08/10/2023-11:17:56] [V] [TRT] Tactic: 0x17ebf0c9f418f10a Time: 1.3467
[08/10/2023-11:17:56] [V] [TRT] Conv_62 + Relu_63 || Conv_65 + Relu_66 || Conv_68 + Relu_69 || Conv_71 + Relu_72 || Conv_74 + Relu_75 || Conv_77 + Relu_78 || Conv_80 + Relu_81 || Conv_83 + Relu_84 Set Tactic Name: ampere_h16816cudnn_128x64_ldg8_relu_exp_stages_64x4_medium_nhwc_tn_v1 Tactic: 0x196cbc423a9bb69e
[08/10/2023-11:17:56] [V] [TRT] Tactic: 0x196cbc423a9bb69e Time: 2.03504
[08/10/2023-11:17:56] [V] [TRT] Conv_62 + Relu_63 || Conv_65 + Relu_66 || Conv_68 + Relu_69 || Conv_71 + Relu_72 || Conv_74 + Relu_75 || Conv_77 + Relu_78 || Conv_80 + Relu_81 || Conv_83 + Relu_84 Set Tactic Name: ampere_h16816cudnn_128x64_sliced1x2_ldg8_relu_exp_stages_64x3_small_nhwc_tn_v1 Tactic: 0x19822de884f42a6a
[08/10/2023-11:17:56] [V] [TRT] Tactic: 0x19822de884f42a6a Time: 1.60619
[08/10/2023-11:17:56] [V] [TRT] Conv_62 + Relu_63 || Conv_65 + Relu_66 || Conv_68 + Relu_69 || Conv_71 + Relu_72 || Conv_74 + Relu_75 || Conv_77 + Relu_78 || Conv_80 + Relu_81 || Conv_83 + Relu_84 Set Tactic Name: ampere_h16816cudnn_64x64_sliced1x2_ldg8_relu_exp_stages_64x5_small_nhwc_tn_v1 Tactic: 0x1a5ba808ad4cc5a8
[08/10/2023-11:17:56] [V] [TRT] Tactic: 0x1a5ba808ad4cc5a8 Time: 2.06128
[08/10/2023-11:17:56] [V] [TRT] Conv_62 + Relu_63 || Conv_65 + Relu_66 || Conv_68 + Relu_69 || Conv_71 + Relu_72 || Conv_74 + Relu_75 || Conv_77 + Relu_78 || Conv_80 + Relu_81 || Conv_83 + Relu_84 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x64x64_stage3_warpsize2x2x1_g1_tensor16x8x16 Tactic: 0x21b295c0c8f6c95a
[08/10/2023-11:17:56] [V] [TRT] Tactic: 0x21b295c0c8f6c95a Time: 1.19028
[08/10/2023-11:17:56] [V] [TRT] Conv_62 + Relu_63 || Conv_65 + Relu_66 || Conv_68 + Relu_69 || Conv_71 + Relu_72 || Conv_74 + Relu_75 || Conv_77 + Relu_78 || Conv_80 + Relu_81 || Conv_83 + Relu_84 Set Tactic Name: ampere_h16816cudnn_128x128_ldg8_relu_exp_medium_nhwc_tn_v1 Tactic: 0x234580e8a194335c
[08/10/2023-11:17:56] [V] [TRT] Tactic: 0x234580e8a194335c Time: 1.04731
[08/10/2023-11:17:56] [V] [TRT] Conv_62 + Relu_63 || Conv_65 + Relu_66 || Conv_68 + Relu_69 || Conv_71 + Relu_72 || Conv_74 + Relu_75 || Conv_77 + Relu_78 || Conv_80 + Relu_81 || Conv_83 + Relu_84 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x128x32_stage4_warpsize2x2x1_g1_tensor16x8x16_t1r3s3_aACCESS Tactic: 0x245530c34bd6090f
[08/10/2023-11:17:56] [V] [TRT] Tactic: 0x245530c34bd6090f Time: 0.972599
[08/10/2023-11:17:56] [V] [TRT] Conv_62 + Relu_63 || Conv_65 + Relu_66 || Conv_68 + Relu_69 || Conv_71 + Relu_72 || Conv_74 + Relu_75 || Conv_77 + Relu_78 || Conv_80 + Relu_81 || Conv_83 + Relu_84 Set Tactic Name: ampere_h16816cudnn_64x64_sliced1x2_ldg8_relu_exp_stages_64x6_small_nhwc_tn_v1 Tactic: 0x24e01e7405cfdfe9
[08/10/2023-11:17:57] [V] [TRT] Tactic: 0x24e01e7405cfdfe9 Time: 2.79705
[08/10/2023-11:17:57] [V] [TRT] Conv_62 + Relu_63 || Conv_65 + Relu_66 || Conv_68 + Relu_69 || Conv_71 + Relu_72 || Conv_74 + Relu_75 || Conv_77 + Relu_78 || Conv_80 + Relu_81 || Conv_83 + Relu_84 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x64x64_stage3_warpsize2x2x1_g1_tensor16x8x16_t1r3s3_aACCESS Tactic: 0x263a38afd75e3a43
[08/10/2023-11:17:57] [V] [TRT] Tactic: 0x263a38afd75e3a43 Time: 1.14421
[08/10/2023-11:17:57] [V] [TRT] Conv_62 + Relu_63 || Conv_65 + Relu_66 || Conv_68 + Relu_69 || Conv_71 + Relu_72 || Conv_74 + Relu_75 || Conv_77 + Relu_78 || Conv_80 + Relu_81 || Conv_83 + Relu_84 Set Tactic Name: ampere_h16816cudnn_64x128_sliced1x2_ldg8_relu_exp_stages_64x4_large_nhwc_tn_v1 Tactic: 0x2721a7f18c2700db
[08/10/2023-11:17:57] [V] [TRT] Tactic: 0x2721a7f18c2700db Time: 1.90899
[08/10/2023-11:17:57] [V] [TRT] Conv_62 + Relu_63 || Conv_65 + Relu_66 || Conv_68 + Relu_69 || Conv_71 + Relu_72 || Conv_74 + Relu_75 || Conv_77 + Relu_78 || Conv_80 + Relu_81 || Conv_83 + Relu_84 Set Tactic Name: ampere_h16816cudnn_128x64_sliced1x2_ldg8_relu_exp_stages_64x3_medium_nhwc_tn_v1 Tactic: 0x29329b5741ea05f2
[08/10/2023-11:17:57] [V] [TRT] Tactic: 0x29329b5741ea05f2 Time: 1.31559
[08/10/2023-11:17:57] [V] [TRT] Conv_62 + Relu_63 || Conv_65 + Relu_66 || Conv_68 + Relu_69 || Conv_71 + Relu_72 || Conv_74 + Relu_75 || Conv_77 + Relu_78 || Conv_80 + Relu_81 || Conv_83 + Relu_84 Set Tactic Name: ampere_h16816cudnn_128x128_ldg8_relu_exp_large_nhwc_tn_v1 Tactic: 0x2970ae65966d569d
[08/10/2023-11:17:57] [V] [TRT] Tactic: 0x2970ae65966d569d Time: 1.02992
[08/10/2023-11:17:57] [V] [TRT] Conv_62 + Relu_63 || Conv_65 + Relu_66 || Conv_68 + Relu_69 || Conv_71 + Relu_72 || Conv_74 + Relu_75 || Conv_77 + Relu_78 || Conv_80 + Relu_81 || Conv_83 + Relu_84 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x256x64_stage3_warpsize1x4x1_g1_tensor16x8x16 Tactic: 0x29dc14520e0e4eb6
[08/10/2023-11:17:57] [V] [TRT] Tactic: 0x29dc14520e0e4eb6 Time: 1.23281
[08/10/2023-11:17:57] [V] [TRT] Conv_62 + Relu_63 || Conv_65 + Relu_66 || Conv_68 + Relu_69 || Conv_71 + Relu_72 || Conv_74 + Relu_75 || Conv_77 + Relu_78 || Conv_80 + Relu_81 || Conv_83 + Relu_84 Set Tactic Name: ampere_h16816cudnn_256x64_ldg8_relu_exp_stages_64x3_large_nhwc_tn_v1 Tactic: 0x2cb4a662694e89d3
[08/10/2023-11:17:57] [V] [TRT] Tactic: 0x2cb4a662694e89d3 Time: 1.44113
[08/10/2023-11:17:57] [V] [TRT] Conv_62 + Relu_63 || Conv_65 + Relu_66 || Conv_68 + Relu_69 || Conv_71 + Relu_72 || Conv_74 + Relu_75 || Conv_77 + Relu_78 || Conv_80 + Relu_81 || Conv_83 + Relu_84 Set Tactic Name: ampere_h16816cudnn_128x128_ldg8_relu_exp_stages_64x3_medium_nhwc_tn_v1 Tactic: 0x2eaa2202de9404d6
[08/10/2023-11:17:57] [V] [TRT] Tactic: 0x2eaa2202de9404d6 Time: 1.38615
[08/10/2023-11:17:57] [V] [TRT] Conv_62 + Relu_63 || Conv_65 + Relu_66 || Conv_68 + Relu_69 || Conv_71 + Relu_72 || Conv_74 + Relu_75 || Conv_77 + Relu_78 || Conv_80 + Relu_81 || Conv_83 + Relu_84 Set Tactic Name: ampere_h1688cudnn_256x128_ldg8_relu_exp_medium_nhwc_tn_v1 Tactic: 0x30c0f36d0aeeac6a
[08/10/2023-11:17:57] [V] [TRT] Tactic: 0x30c0f36d0aeeac6a Time: 1.43296
[08/10/2023-11:17:57] [V] [TRT] Conv_62 + Relu_63 || Conv_65 + Relu_66 || Conv_68 + Relu_69 || Conv_71 + Relu_72 || Conv_74 + Relu_75 || Conv_77 + Relu_78 || Conv_80 + Relu_81 || Conv_83 + Relu_84 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x64x64_stage4_warpsize2x2x1_g1_tensor16x8x16_t1r3s3_aACCESS Tactic: 0x30e8a8d7a953e5e9
[08/10/2023-11:17:57] [V] [TRT] Tactic: 0x30e8a8d7a953e5e9 Time: 1.7618
[08/10/2023-11:17:57] [V] [TRT] Conv_62 + Relu_63 || Conv_65 + Relu_66 || Conv_68 + Relu_69 || Conv_71 + Relu_72 || Conv_74 + Relu_75 || Conv_77 + Relu_78 || Conv_80 + Relu_81 || Conv_83 + Relu_84 Set Tactic Name: ampere_h16816cudnn_128x64_ldg8_relu_exp_stages_64x4_large_nhwc_tn_v1 Tactic: 0x3197d5044d71e98e
[08/10/2023-11:17:57] [V] [TRT] Tactic: 0x3197d5044d71e98e Time: 2.35887
[08/10/2023-11:17:57] [V] [TRT] Conv_62 + Relu_63 || Conv_65 + Relu_66 || Conv_68 + Relu_69 || Conv_71 + Relu_72 || Conv_74 + Relu_75 || Conv_77 + Relu_78 || Conv_80 + Relu_81 || Conv_83 + Relu_84 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x64x64_stage4_warpsize2x2x1_g1_tensor16x8x16_t1r3s3 Tactic: 0x31d93dc22d2af081
[08/10/2023-11:17:57] [V] [TRT] Tactic: 0x31d93dc22d2af081 Time: 1.86423
[08/10/2023-11:17:57] [V] [TRT] Conv_62 + Relu_63 || Conv_65 + Relu_66 || Conv_68 + Relu_69 || Conv_71 + Relu_72 || Conv_74 + Relu_75 || Conv_77 + Relu_78 || Conv_80 + Relu_81 || Conv_83 + Relu_84 Set Tactic Name: ampere_h1688cudnn_128x128_ldg8_relu_exp_small_nhwc_tn_v1 Tactic: 0x3369260c04f9ad73
[08/10/2023-11:17:57] [V] [TRT] Tactic: 0x3369260c04f9ad73 Time: 1.64112
[08/10/2023-11:17:57] [V] [TRT] Conv_62 + Relu_63 || Conv_65 + Relu_66 || Conv_68 + Relu_69 || Conv_71 + Relu_72 || Conv_74 + Relu_75 || Conv_77 + Relu_78 || Conv_80 + Relu_81 || Conv_83 + Relu_84 Set Tactic Name: ampere_h16816cudnn_64x128_ldg8_relu_exp_stages_64x4_small_nhwc_tn_v1 Tactic: 0x3b5fa0f2a8fc2410
[08/10/2023-11:17:57] [V] [TRT] Tactic: 0x3b5fa0f2a8fc2410 Time: 1.97649
[08/10/2023-11:17:57] [V] [TRT] Conv_62 + Relu_63 || Conv_65 + Relu_66 || Conv_68 + Relu_69 || Conv_71 + Relu_72 || Conv_74 + Relu_75 || Conv_77 + Relu_78 || Conv_80 + Relu_81 || Conv_83 + Relu_84 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x32x32_stage4_warpsize4x1x1_g1_tensor16x8x16_t1r3s3 Tactic: 0x3e7eb35b91b9fa63
[08/10/2023-11:17:57] [V] [TRT] Tactic: 0x3e7eb35b91b9fa63 Time: 2.49954
[08/10/2023-11:17:57] [V] [TRT] Conv_62 + Relu_63 || Conv_65 + Relu_66 || Conv_68 + Relu_69 || Conv_71 + Relu_72 || Conv_74 + Relu_75 || Conv_77 + Relu_78 || Conv_80 + Relu_81 || Conv_83 + Relu_84 Set Tactic Name: ampere_h16816cudnn_256x64_ldg8_relu_exp_stages_64x3_medium_nhwc_tn_v1 Tactic: 0x3f031df0e579d52c
[08/10/2023-11:17:57] [V] [TRT] Tactic: 0x3f031df0e579d52c Time: 1.94628
[08/10/2023-11:17:57] [V] [TRT] Conv_62 + Relu_63 || Conv_65 + Relu_66 || Conv_68 + Relu_69 || Conv_71 + Relu_72 || Conv_74 + Relu_75 || Conv_77 + Relu_78 || Conv_80 + Relu_81 || Conv_83 + Relu_84 Set Tactic Name: ampere_h16816cudnn_256x64_sliced1x2_ldg8_relu_exp_small_nhwc_tn_v1 Tactic: 0x42a5b8709d0039be
[08/10/2023-11:17:57] [V] [TRT] Tactic: 0x42a5b8709d0039be Time: 2.03698
[08/10/2023-11:17:57] [V] [TRT] Conv_62 + Relu_63 || Conv_65 + Relu_66 || Conv_68 + Relu_69 || Conv_71 + Relu_72 || Conv_74 + Relu_75 || Conv_77 + Relu_78 || Conv_80 + Relu_81 || Conv_83 + Relu_84 Set Tactic Name: ampere_h16816cudnn_256x128_ldg8_relu_exp_small_nhwc_tn_v1 Tactic: 0x463794ee4acffd1f
[08/10/2023-11:17:57] [V] [TRT] Tactic: 0x463794ee4acffd1f Time: 1.53262
[08/10/2023-11:17:57] [V] [TRT] Conv_62 + Relu_63 || Conv_65 + Relu_66 || Conv_68 + Relu_69 || Conv_71 + Relu_72 || Conv_74 + Relu_75 || Conv_77 + Relu_78 || Conv_80 + Relu_81 || Conv_83 + Relu_84 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x64x64_stage4_warpsize2x2x1_g1_tensor16x8x16 Tactic: 0x4a81ea1e51436a30
[08/10/2023-11:17:57] [V] [TRT] Tactic: 0x4a81ea1e51436a30 Time: 1.9147
[08/10/2023-11:17:57] [V] [TRT] Conv_62 + Relu_63 || Conv_65 + Relu_66 || Conv_68 + Relu_69 || Conv_71 + Relu_72 || Conv_74 + Relu_75 || Conv_77 + Relu_78 || Conv_80 + Relu_81 || Conv_83 + Relu_84 Set Tactic Name: ampere_h1688cudnn_256x64_sliced1x2_ldg8_relu_exp_small_nhwc_tn_v1 Tactic: 0x4aed1956bd10a795
[08/10/2023-11:17:57] [V] [TRT] Tactic: 0x4aed1956bd10a795 Time: 1.73969
[08/10/2023-11:17:57] [V] [TRT] Conv_62 + Relu_63 || Conv_65 + Relu_66 || Conv_68 + Relu_69 || Conv_71 + Relu_72 || Conv_74 + Relu_75 || Conv_77 + Relu_78 || Conv_80 + Relu_81 || Conv_83 + Relu_84 Set Tactic Name: ampere_h16816cudnn_64x128_sliced1x2_ldg8_relu_exp_stages_64x3_large_nhwc_tn_v1 Tactic: 0x4fc05923455fc266
[08/10/2023-11:17:57] [V] [TRT] Tactic: 0x4fc05923455fc266 Time: 1.91392
[08/10/2023-11:17:57] [V] [TRT] Conv_62 + Relu_63 || Conv_65 + Relu_66 || Conv_68 + Relu_69 || Conv_71 + Relu_72 || Conv_74 + Relu_75 || Conv_77 + Relu_78 || Conv_80 + Relu_81 || Conv_83 + Relu_84 Set Tactic Name: ampere_h16816cudnn_256x64_ldg8_relu_exp_large_nhwc_tn_v1 Tactic: 0x5013c38f55afa9ba
[08/10/2023-11:17:57] [V] [TRT] Tactic: 0x5013c38f55afa9ba Time: 1.81505
[08/10/2023-11:17:57] [V] [TRT] Conv_62 + Relu_63 || Conv_65 + Relu_66 || Conv_68 + Relu_69 || Conv_71 + Relu_72 || Conv_74 + Relu_75 || Conv_77 + Relu_78 || Conv_80 + Relu_81 || Conv_83 + Relu_84 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x64x32_stage3_warpsize4x1x1_g1_tensor16x8x16_t1r3s3 Tactic: 0x529f4431bdae94f5
[08/10/2023-11:17:57] [V] [TRT] Tactic: 0x529f4431bdae94f5 Time: 1.18958
[08/10/2023-11:17:57] [V] [TRT] Conv_62 + Relu_63 || Conv_65 + Relu_66 || Conv_68 + Relu_69 || Conv_71 + Relu_72 || Conv_74 + Relu_75 || Conv_77 + Relu_78 || Conv_80 + Relu_81 || Conv_83 + Relu_84 Set Tactic Name: ampere_h16816cudnn_64x128_ldg8_relu_exp_stages_64x3_small_nhwc_tn_v1 Tactic: 0x53be5e206184e6ad
[08/10/2023-11:17:57] [V] [TRT] Tactic: 0x53be5e206184e6ad Time: 1.31989
[08/10/2023-11:17:57] [V] [TRT] Conv_62 + Relu_63 || Conv_65 + Relu_66 || Conv_68 + Relu_69 || Conv_71 + Relu_72 || Conv_74 + Relu_75 || Conv_77 + Relu_78 || Conv_80 + Relu_81 || Conv_83 + Relu_84 Set Tactic Name: ampere_h16816cudnn_64x64_ldg8_relu_exp_stages_64x5_large_nhwc_tn_v1 Tactic: 0x544bff7ff9c5d908
[08/10/2023-11:17:57] [V] [TRT] Tactic: 0x544bff7ff9c5d908 Time: 2.1258
[08/10/2023-11:17:57] [V] [TRT] Conv_62 + Relu_63 || Conv_65 + Relu_66 || Conv_68 + Relu_69 || Conv_71 + Relu_72 || Conv_74 + Relu_75 || Conv_77 + Relu_78 || Conv_80 + Relu_81 || Conv_83 + Relu_84 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x64x64_stage3_warpsize2x2x1_g1_tensor16x8x16 Tactic: 0x5568fd8a32f4a40f
[08/10/2023-11:17:57] [V] [TRT] Tactic: 0x5568fd8a32f4a40f Time: 1.57888
[08/10/2023-11:17:57] [V] [TRT] Conv_62 + Relu_63 || Conv_65 + Relu_66 || Conv_68 + Relu_69 || Conv_71 + Relu_72 || Conv_74 + Relu_75 || Conv_77 + Relu_78 || Conv_80 + Relu_81 || Conv_83 + Relu_84 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x128x64_stage3_warpsize2x2x1_g1_tensor16x8x16 Tactic: 0x57c9a5ff682354a6
[08/10/2023-11:17:57] [V] [TRT] Tactic: 0x57c9a5ff682354a6 Time: 1.46479
[08/10/2023-11:17:57] [V] [TRT] Conv_62 + Relu_63 || Conv_65 + Relu_66 || Conv_68 + Relu_69 || Conv_71 + Relu_72 || Conv_74 + Relu_75 || Conv_77 + Relu_78 || Conv_80 + Relu_81 || Conv_83 + Relu_84 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x32x64_stage5_warpsize2x2x1_g1_tensor16x8x16 Tactic: 0x5820b3dda403c4d0
[08/10/2023-11:17:57] [V] [TRT] Tactic: 0x5820b3dda403c4d0 Time: 2.49095
[08/10/2023-11:17:57] [V] [TRT] Conv_62 + Relu_63 || Conv_65 + Relu_66 || Conv_68 + Relu_69 || Conv_71 + Relu_72 || Conv_74 + Relu_75 || Conv_77 + Relu_78 || Conv_80 + Relu_81 || Conv_83 + Relu_84 Set Tactic Name: ampere_h16816cudnn_128x128_ldg8_relu_exp_stages_64x3_small_nhwc_tn_v1 Tactic: 0x58816bf2e9c36afb
[08/10/2023-11:17:57] [V] [TRT] Tactic: 0x58816bf2e9c36afb Time: 1.47771
[08/10/2023-11:17:57] [V] [TRT] Conv_62 + Relu_63 || Conv_65 + Relu_66 || Conv_68 + Relu_69 || Conv_71 + Relu_72 || Conv_74 + Relu_75 || Conv_77 + Relu_78 || Conv_80 + Relu_81 || Conv_83 + Relu_84 Set Tactic Name: ampere_h16816cudnn_128x64_ldg8_relu_exp_stages_64x3_large_nhwc_tn_v1 Tactic: 0x59762bd684092b33
[08/10/2023-11:17:57] [V] [TRT] Tactic: 0x59762bd684092b33 Time: 1.37051
[08/10/2023-11:17:57] [V] [TRT] Conv_62 + Relu_63 || Conv_65 + Relu_66 || Conv_68 + Relu_69 || Conv_71 + Relu_72 || Conv_74 + Relu_75 || Conv_77 + Relu_78 || Conv_80 + Relu_81 || Conv_83 + Relu_84 Set Tactic Name: ampere_h16816cudnn_128x64_sliced1x2_ldg8_relu_exp_stages_64x4_medium_nhwc_tn_v1 Tactic: 0x59c5c647b4c76593
[08/10/2023-11:17:57] [V] [TRT] Tactic: 0x59c5c647b4c76593 Time: 1.91831
[08/10/2023-11:17:57] [V] [TRT] Conv_62 + Relu_63 || Conv_65 + Relu_66 || Conv_68 + Relu_69 || Conv_71 + Relu_72 || Conv_74 + Relu_75 || Conv_77 + Relu_78 || Conv_80 + Relu_81 || Conv_83 + Relu_84 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x128x64_stage3_warpsize4x2x1_g1_tensor16x8x16 Tactic: 0x5a55274960724ba8
[08/10/2023-11:17:57] [V] [TRT] Tactic: 0x5a55274960724ba8 Time: 1.09799
[08/10/2023-11:17:57] [V] [TRT] Conv_62 + Relu_63 || Conv_65 + Relu_66 || Conv_68 + Relu_69 || Conv_71 + Relu_72 || Conv_74 + Relu_75 || Conv_77 + Relu_78 || Conv_80 + Relu_81 || Conv_83 + Relu_84 Set Tactic Name: ampere_h1688cudnn_256x128_ldg8_relu_exp_large_nhwc_tn_v1 Tactic: 0x5c2e1c87d85b06f1
[08/10/2023-11:17:57] [V] [TRT] Tactic: 0x5c2e1c87d85b06f1 Time: 1.41724
[08/10/2023-11:17:57] [V] [TRT] Conv_62 + Relu_63 || Conv_65 + Relu_66 || Conv_68 + Relu_69 || Conv_71 + Relu_72 || Conv_74 + Relu_75 || Conv_77 + Relu_78 || Conv_80 + Relu_81 || Conv_83 + Relu_84 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x256x64_stage3_warpsize1x4x1_g1_tensor16x8x16 Tactic: 0x5d067c18f40c23e3
[08/10/2023-11:17:57] [V] [TRT] Tactic: 0x5d067c18f40c23e3 Time: 2.06464
[08/10/2023-11:17:57] [V] [TRT] Conv_62 + Relu_63 || Conv_65 + Relu_66 || Conv_68 + Relu_69 || Conv_71 + Relu_72 || Conv_74 + Relu_75 || Conv_77 + Relu_78 || Conv_80 + Relu_81 || Conv_83 + Relu_84 Set Tactic Name: ampere_h16816cudnn_64x64_ldg8_relu_exp_stages_64x5_medium_nhwc_tn_v1 Tactic: 0x5f31c22ec167f384
[08/10/2023-11:17:57] [V] [TRT] Tactic: 0x5f31c22ec167f384 Time: 2.13541
[08/10/2023-11:17:57] [V] [TRT] Conv_62 + Relu_63 || Conv_65 + Relu_66 || Conv_68 + Relu_69 || Conv_71 + Relu_72 || Conv_74 + Relu_75 || Conv_77 + Relu_78 || Conv_80 + Relu_81 || Conv_83 + Relu_84 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x128x32_stage4_warpsize2x2x1_g1_tensor16x8x16_t1r3s3 Tactic: 0x60c3421152ef8e10
[08/10/2023-11:17:57] [V] [TRT] Tactic: 0x60c3421152ef8e10 Time: 1.35904
[08/10/2023-11:17:57] [V] [TRT] Conv_62 + Relu_63 || Conv_65 + Relu_66 || Conv_68 + Relu_69 || Conv_71 + Relu_72 || Conv_74 + Relu_75 || Conv_77 + Relu_78 || Conv_80 + Relu_81 || Conv_83 + Relu_84 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x128x32_stage3_warpsize4x2x1_g1_tensor16x8x16 Tactic: 0x60da8c7151d91e47
[08/10/2023-11:17:57] [V] [TRT] Tactic: 0x60da8c7151d91e47 Time: 1.18572
[08/10/2023-11:17:57] [V] [TRT] Conv_62 + Relu_63 || Conv_65 + Relu_66 || Conv_68 + Relu_69 || Conv_71 + Relu_72 || Conv_74 + Relu_75 || Conv_77 + Relu_78 || Conv_80 + Relu_81 || Conv_83 + Relu_84 Set Tactic Name: ampere_h1688cudnn_256x64_ldg8_relu_exp_medium_nhwc_tn_v1 Tactic: 0x6426696f872a3b13
[08/10/2023-11:17:57] [V] [TRT] Tactic: 0x6426696f872a3b13 Time: 1.81229
[08/10/2023-11:17:57] [V] [TRT] Conv_62 + Relu_63 || Conv_65 + Relu_66 || Conv_68 + Relu_69 || Conv_71 + Relu_72 || Conv_74 + Relu_75 || Conv_77 + Relu_78 || Conv_80 + Relu_81 || Conv_83 + Relu_84 Set Tactic Name: ampere_h16816cudnn_128x64_ldg8_relu_exp_stages_64x3_medium_nhwc_tn_v1 Tactic: 0x699be152cfb6d6ff
[08/10/2023-11:17:57] [V] [TRT] Tactic: 0x699be152cfb6d6ff Time: 1.37619
[08/10/2023-11:17:57] [V] [TRT] Conv_62 + Relu_63 || Conv_65 + Relu_66 || Conv_68 + Relu_69 || Conv_71 + Relu_72 || Conv_74 + Relu_75 || Conv_77 + Relu_78 || Conv_80 + Relu_81 || Conv_83 + Relu_84 Set Tactic Name: ampere_h16816cudnn_64x64_ldg8_relu_exp_stages_64x6_large_nhwc_tn_v1 Tactic: 0x6af049035146c349
[08/10/2023-11:17:58] [V] [TRT] Tactic: 0x6af049035146c349 Time: 4.60745
[08/10/2023-11:17:58] [V] [TRT] Conv_62 + Relu_63 || Conv_65 + Relu_66 || Conv_68 + Relu_69 || Conv_71 + Relu_72 || Conv_74 + Relu_75 || Conv_77 + Relu_78 || Conv_80 + Relu_81 || Conv_83 + Relu_84 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x256x32_stage3_warpsize2x4x1_g1_tensor16x8x16 Tactic: 0x7005d10718f6c22d
[08/10/2023-11:17:58] [V] [TRT] Tactic: 0x7005d10718f6c22d Time: 1.59704
[08/10/2023-11:17:58] [V] [TRT] Conv_62 + Relu_63 || Conv_65 + Relu_66 || Conv_68 + Relu_69 || Conv_71 + Relu_72 || Conv_74 + Relu_75 || Conv_77 + Relu_78 || Conv_80 + Relu_81 || Conv_83 + Relu_84 Set Tactic Name: ampere_h16816cudnn_64x64_sliced1x2_ldg8_relu_exp_stages_64x5_medium_nhwc_tn_v1 Tactic: 0x706f08da35c795a5
[08/10/2023-11:17:58] [V] [TRT] Tactic: 0x706f08da35c795a5 Time: 2.26889
[08/10/2023-11:17:58] [V] [TRT] Conv_62 + Relu_63 || Conv_65 + Relu_66 || Conv_68 + Relu_69 || Conv_71 + Relu_72 || Conv_74 + Relu_75 || Conv_77 + Relu_78 || Conv_80 + Relu_81 || Conv_83 + Relu_84 Set Tactic Name: ampere_h16816cudnn_128x64_sliced1x2_ldg8_relu_exp_stages_64x4_small_nhwc_tn_v1 Tactic: 0x7163d33a4d8ce8d7
[08/10/2023-11:17:58] [V] [TRT] Tactic: 0x7163d33a4d8ce8d7 Time: 2.38308
[08/10/2023-11:17:58] [V] [TRT] Conv_62 + Relu_63 || Conv_65 + Relu_66 || Conv_68 + Relu_69 || Conv_71 + Relu_72 || Conv_74 + Relu_75 || Conv_77 + Relu_78 || Conv_80 + Relu_81 || Conv_83 + Relu_84 Set Tactic Name: ampere_h1688cudnn_256x64_ldg8_relu_exp_small_nhwc_tn_v1 Tactic: 0x7aad3976677d7155
[08/10/2023-11:17:58] [V] [TRT] Tactic: 0x7aad3976677d7155 Time: 1.46955
[08/10/2023-11:17:58] [V] [TRT] Conv_62 + Relu_63 || Conv_65 + Relu_66 || Conv_68 + Relu_69 || Conv_71 + Relu_72 || Conv_74 + Relu_75 || Conv_77 + Relu_78 || Conv_80 + Relu_81 || Conv_83 + Relu_84 Set Tactic Name: ampere_h16816cudnn_64x64_sliced1x2_ldg8_relu_exp_stages_64x5_large_nhwc_tn_v1 Tactic: 0x8015519605ab9963
[08/10/2023-11:17:58] [V] [TRT] Tactic: 0x8015519605ab9963 Time: 2.08122
[08/10/2023-11:17:58] [V] [TRT] Conv_62 + Relu_63 || Conv_65 + Relu_66 || Conv_68 + Relu_69 || Conv_71 + Relu_72 || Conv_74 + Relu_75 || Conv_77 + Relu_78 || Conv_80 + Relu_81 || Conv_83 + Relu_84 Set Tactic Name: ampere_h16816cudnn_256x64_sliced1x2_ldg8_relu_exp_medium_nhwc_tn_v1 Tactic: 0x82f8a2214b0b4178
[08/10/2023-11:17:58] [V] [TRT] Tactic: 0x82f8a2214b0b4178 Time: 1.79275
[08/10/2023-11:17:58] [V] [TRT] Conv_62 + Relu_63 || Conv_65 + Relu_66 || Conv_68 + Relu_69 || Conv_71 + Relu_72 || Conv_74 + Relu_75 || Conv_77 + Relu_78 || Conv_80 + Relu_81 || Conv_83 + Relu_84 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x32x32_stage4_warpsize4x1x1_g1_tensor16x8x16 Tactic: 0x834e11ecd4ab9454
[08/10/2023-11:17:58] [V] [TRT] Tactic: 0x834e11ecd4ab9454 Time: 1.803
[08/10/2023-11:17:58] [V] [TRT] Conv_62 + Relu_63 || Conv_65 + Relu_66 || Conv_68 + Relu_69 || Conv_71 + Relu_72 || Conv_74 + Relu_75 || Conv_77 + Relu_78 || Conv_80 + Relu_81 || Conv_83 + Relu_84 Set Tactic Name: ampere_h16816cudnn_128x64_sliced1x2_ldg8_relu_exp_stages_64x3_large_nhwc_tn_v1 Tactic: 0x83ccd4762c1376a1
[08/10/2023-11:17:58] [V] [TRT] Tactic: 0x83ccd4762c1376a1 Time: 1.33118
[08/10/2023-11:17:58] [V] [TRT] Conv_62 + Relu_63 || Conv_65 + Relu_66 || Conv_68 + Relu_69 || Conv_71 + Relu_72 || Conv_74 + Relu_75 || Conv_77 + Relu_78 || Conv_80 + Relu_81 || Conv_83 + Relu_84 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x64x64_stage3_warpsize4x1x1_g1_tensor16x8x16_t1r3s3 Tactic: 0x84ee897dd1f4d2aa
[08/10/2023-11:17:58] [V] [TRT] Tactic: 0x84ee897dd1f4d2aa Time: 1.29381
[08/10/2023-11:17:58] [V] [TRT] Conv_62 + Relu_63 || Conv_65 + Relu_66 || Conv_68 + Relu_69 || Conv_71 + Relu_72 || Conv_74 + Relu_75 || Conv_77 + Relu_78 || Conv_80 + Relu_81 || Conv_83 + Relu_84 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x128x64_stage3_warpsize2x2x1_g1_tensor16x8x16 Tactic: 0x866e7a5f6401b67f
[08/10/2023-11:17:58] [V] [TRT] Tactic: 0x866e7a5f6401b67f Time: 1.46906
[08/10/2023-11:17:58] [V] [TRT] Conv_62 + Relu_63 || Conv_65 + Relu_66 || Conv_68 + Relu_69 || Conv_71 + Relu_72 || Conv_74 + Relu_75 || Conv_77 + Relu_78 || Conv_80 + Relu_81 || Conv_83 + Relu_84 Set Tactic Name: ampere_h16816cudnn_64x64_ldg8_relu_exp_stages_64x6_medium_nhwc_tn_v1 Tactic: 0x88971eec55aba850
[08/10/2023-11:17:58] [V] [TRT] Tactic: 0x88971eec55aba850 Time: 2.83907
[08/10/2023-11:17:58] [V] [TRT] Conv_62 + Relu_63 || Conv_65 + Relu_66 || Conv_68 + Relu_69 || Conv_71 + Relu_72 || Conv_74 + Relu_75 || Conv_77 + Relu_78 || Conv_80 + Relu_81 || Conv_83 + Relu_84 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x128x64_stage3_warpsize4x2x1_g1_tensor16x8x16 Tactic: 0x8bf2f8e96c50a971
[08/10/2023-11:17:58] [V] [TRT] Tactic: 0x8bf2f8e96c50a971 Time: 1.0654
[08/10/2023-11:17:58] [V] [TRT] Conv_62 + Relu_63 || Conv_65 + Relu_66 || Conv_68 + Relu_69 || Conv_71 + Relu_72 || Conv_74 + Relu_75 || Conv_77 + Relu_78 || Conv_80 + Relu_81 || Conv_83 + Relu_84 Set Tactic Name: ampere_h16816cudnn_256x128_ldg8_relu_exp_stages_64x3_large_nhwc_tn_v1 Tactic: 0x94576ece6beb35e3
[08/10/2023-11:17:58] [V] [TRT] Tactic: 0x94576ece6beb35e3 Time: 1.45813
[08/10/2023-11:17:58] [V] [TRT] Conv_62 + Relu_63 || Conv_65 + Relu_66 || Conv_68 + Relu_69 || Conv_71 + Relu_72 || Conv_74 + Relu_75 || Conv_77 + Relu_78 || Conv_80 + Relu_81 || Conv_83 + Relu_84 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x64x64_stage3_warpsize4x1x1_g1_tensor16x8x16 Tactic: 0x9fff6e65019cc310
[08/10/2023-11:17:58] [V] [TRT] Tactic: 0x9fff6e65019cc310 Time: 1.55191
[08/10/2023-11:17:58] [V] [TRT] Conv_62 + Relu_63 || Conv_65 + Relu_66 || Conv_68 + Relu_69 || Conv_71 + Relu_72 || Conv_74 + Relu_75 || Conv_77 + Relu_78 || Conv_80 + Relu_81 || Conv_83 + Relu_84 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x32x64_stage5_warpsize2x2x1_g1_tensor16x8x16_t1r3s3 Tactic: 0xa033e20ae9f412b2
[08/10/2023-11:17:58] [V] [TRT] Tactic: 0xa033e20ae9f412b2 Time: 2.78654
[08/10/2023-11:17:58] [V] [TRT] Conv_62 + Relu_63 || Conv_65 + Relu_66 || Conv_68 + Relu_69 || Conv_71 + Relu_72 || Conv_74 + Relu_75 || Conv_77 + Relu_78 || Conv_80 + Relu_81 || Conv_83 + Relu_84 Set Tactic Name: ampere_h16816cudnn_64x128_ldg8_relu_exp_stages_64x4_large_nhwc_tn_v1 Tactic: 0xa111596c001b78db
[08/10/2023-11:17:58] [V] [TRT] Tactic: 0xa111596c001b78db Time: 2.25603
[08/10/2023-11:17:58] [V] [TRT] Conv_62 + Relu_63 || Conv_65 + Relu_66 || Conv_68 + Relu_69 || Conv_71 + Relu_72 || Conv_74 + Relu_75 || Conv_77 + Relu_78 || Conv_80 + Relu_81 || Conv_83 + Relu_84 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x256x32_stage3_warpsize2x4x1_g1_tensor16x8x16 Tactic: 0xa1a20ea714d420f4
[08/10/2023-11:17:58] [V] [TRT] Tactic: 0xa1a20ea714d420f4 Time: 1.27111
[08/10/2023-11:17:58] [V] [TRT] Conv_62 + Relu_63 || Conv_65 + Relu_66 || Conv_68 + Relu_69 || Conv_71 + Relu_72 || Conv_74 + Relu_75 || Conv_77 + Relu_78 || Conv_80 + Relu_81 || Conv_83 + Relu_84 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x32x32_stage4_warpsize4x1x1_g1_tensor16x8x16_t1r3s3 Tactic: 0xa40cb43c296a36a8
[08/10/2023-11:17:58] [V] [TRT] Tactic: 0xa40cb43c296a36a8 Time: 2.11678
[08/10/2023-11:17:58] [V] [TRT] Conv_62 + Relu_63 || Conv_65 + Relu_66 || Conv_68 + Relu_69 || Conv_71 + Relu_72 || Conv_74 + Relu_75 || Conv_77 + Relu_78 || Conv_80 + Relu_81 || Conv_83 + Relu_84 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x64x32_stage3_warpsize4x1x1_g1_tensor16x8x16 Tactic: 0xa570c55d303796ff
[08/10/2023-11:17:58] [V] [TRT] Tactic: 0xa570c55d303796ff Time: 1.05953
[08/10/2023-11:17:58] [V] [TRT] Conv_62 + Relu_63 || Conv_65 + Relu_66 || Conv_68 + Relu_69 || Conv_71 + Relu_72 || Conv_74 + Relu_75 || Conv_77 + Relu_78 || Conv_80 + Relu_81 || Conv_83 + Relu_84 Set Tactic Name: ampere_h16816cudnn_64x64_sliced1x2_ldg8_relu_exp_stages_64x6_medium_nhwc_tn_v1 Tactic: 0xa7c9d418a10bce71
[08/10/2023-11:17:58] [V] [TRT] Tactic: 0xa7c9d418a10bce71 Time: 2.99508
[08/10/2023-11:17:58] [V] [TRT] Conv_62 + Relu_63 || Conv_65 + Relu_66 || Conv_68 + Relu_69 || Conv_71 + Relu_72 || Conv_74 + Relu_75 || Conv_77 + Relu_78 || Conv_80 + Relu_81 || Conv_83 + Relu_84 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x32x32_stage4_warpsize4x1x1_g1_tensor16x8x16 Tactic: 0xa83b68f30462f971
[08/10/2023-11:17:58] [V] [TRT] Tactic: 0xa83b68f30462f971 Time: 1.64389
[08/10/2023-11:17:58] [V] [TRT] Conv_62 + Relu_63 || Conv_65 + Relu_66 || Conv_68 + Relu_69 || Conv_71 + Relu_72 || Conv_74 + Relu_75 || Conv_77 + Relu_78 || Conv_80 + Relu_81 || Conv_83 + Relu_84 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x256x32_stage3_warpsize2x4x1_g1_tensor16x8x16_t1r3s3 Tactic: 0xa9177bbe4e767df8
[08/10/2023-11:17:58] [V] [TRT] Tactic: 0xa9177bbe4e767df8 Time: 1.03472
[08/10/2023-11:17:58] [V] [TRT] Conv_62 + Relu_63 || Conv_65 + Relu_66 || Conv_68 + Relu_69 || Conv_71 + Relu_72 || Conv_74 + Relu_75 || Conv_77 + Relu_78 || Conv_80 + Relu_81 || Conv_83 + Relu_84 Set Tactic Name: ampere_h1688cudnn_128x128_ldg8_relu_exp_large_nhwc_tn_v1 Tactic: 0xa927df92ac1ef1b8
[08/10/2023-11:17:58] [V] [TRT] Tactic: 0xa927df92ac1ef1b8 Time: 1.33185
[08/10/2023-11:17:58] [V] [TRT] Conv_62 + Relu_63 || Conv_65 + Relu_66 || Conv_68 + Relu_69 || Conv_71 + Relu_72 || Conv_74 + Relu_75 || Conv_77 + Relu_78 || Conv_80 + Relu_81 || Conv_83 + Relu_84 Set Tactic Name: ampere_h16816cudnn_128x64_ldg8_relu_exp_stages_64x4_small_nhwc_tn_v1 Tactic: 0xabd92c9ae596b545
[08/10/2023-11:17:58] [V] [TRT] Tactic: 0xabd92c9ae596b545 Time: 1.84799
[08/10/2023-11:17:58] [V] [TRT] Conv_62 + Relu_63 || Conv_65 + Relu_66 || Conv_68 + Relu_69 || Conv_71 + Relu_72 || Conv_74 + Relu_75 || Conv_77 + Relu_78 || Conv_80 + Relu_81 || Conv_83 + Relu_84 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x128x32_stage5_warpsize2x2x1_g1_tensor16x8x16 Tactic: 0xafd1e8bf6bd3d638
[08/10/2023-11:17:58] [V] [TRT] Tactic: 0xafd1e8bf6bd3d638 Time: 1.21092
[08/10/2023-11:17:58] [V] [TRT] Conv_62 + Relu_63 || Conv_65 + Relu_66 || Conv_68 + Relu_69 || Conv_71 + Relu_72 || Conv_74 + Relu_75 || Conv_77 + Relu_78 || Conv_80 + Relu_81 || Conv_83 + Relu_84 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x128x32_stage3_warpsize4x2x1_g1_tensor16x8x16 Tactic: 0xb17d53d15dfbfc9e
[08/10/2023-11:17:58] [V] [TRT] Tactic: 0xb17d53d15dfbfc9e Time: 1.08187
[08/10/2023-11:17:58] [V] [TRT] Conv_62 + Relu_63 || Conv_65 + Relu_66 || Conv_68 + Relu_69 || Conv_71 + Relu_72 || Conv_74 + Relu_75 || Conv_77 + Relu_78 || Conv_80 + Relu_81 || Conv_83 + Relu_84 Set Tactic Name: ampere_h16816cudnn_128x128_ldg8_relu_exp_small_nhwc_tn_v1 Tactic: 0xb33e57fb3e8a0a56
[08/10/2023-11:17:58] [V] [TRT] Tactic: 0xb33e57fb3e8a0a56 Time: 1.03064
[08/10/2023-11:17:58] [V] [TRT] Conv_62 + Relu_63 || Conv_65 + Relu_66 || Conv_68 + Relu_69 || Conv_71 + Relu_72 || Conv_74 + Relu_75 || Conv_77 + Relu_78 || Conv_80 + Relu_81 || Conv_83 + Relu_84 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x64x32_stage5_warpsize2x2x1_g1_tensor16x8x16_t1r3s3 Tactic: 0xb4bec086187edcfc
[08/10/2023-11:17:58] [V] [TRT] Tactic: 0xb4bec086187edcfc Time: 1.39032
[08/10/2023-11:17:58] [V] [TRT] Conv_62 + Relu_63 || Conv_65 + Relu_66 || Conv_68 + Relu_69 || Conv_71 + Relu_72 || Conv_74 + Relu_75 || Conv_77 + Relu_78 || Conv_80 + Relu_81 || Conv_83 + Relu_84 Set Tactic Name: ampere_h16816cudnn_256x64_ldg8_relu_exp_stages_64x3_small_nhwc_tn_v1 Tactic: 0xb6fa5ffcc1a9d518
[08/10/2023-11:17:58] [V] [TRT] Tactic: 0xb6fa5ffcc1a9d518 Time: 1.42021
[08/10/2023-11:17:58] [V] [TRT] Conv_62 + Relu_63 || Conv_65 + Relu_66 || Conv_68 + Relu_69 || Conv_71 + Relu_72 || Conv_74 + Relu_75 || Conv_77 + Relu_78 || Conv_80 + Relu_81 || Conv_83 + Relu_84 Set Tactic Name: ampere_h16816cudnn_64x128_sliced1x2_ldg8_relu_exp_stages_64x3_medium_nhwc_tn_v1 Tactic: 0xb85e52e87caf60a2
[08/10/2023-11:17:58] [V] [TRT] Tactic: 0xb85e52e87caf60a2 Time: 1.32393
[08/10/2023-11:17:58] [V] [TRT] Conv_62 + Relu_63 || Conv_65 + Relu_66 || Conv_68 + Relu_69 || Conv_71 + Relu_72 || Conv_74 + Relu_75 || Conv_77 + Relu_78 || Conv_80 + Relu_81 || Conv_83 + Relu_84 Set Tactic Name: ampere_h16816cudnn_64x128_ldg8_relu_exp_stages_64x3_medium_nhwc_tn_v1 Tactic: 0xb8d86216e1235cda
[08/10/2023-11:17:58] [V] [TRT] Tactic: 0xb8d86216e1235cda Time: 1.32359
[08/10/2023-11:17:58] [V] [TRT] Conv_62 + Relu_63 || Conv_65 + Relu_66 || Conv_68 + Relu_69 || Conv_71 + Relu_72 || Conv_74 + Relu_75 || Conv_77 + Relu_78 || Conv_80 + Relu_81 || Conv_83 + Relu_84 Set Tactic Name: ampere_h16816cudnn_256x128_ldg8_relu_exp_stages_64x3_medium_nhwc_tn_v1 Tactic: 0xb9171d70ba2eda4b
[08/10/2023-11:17:58] [V] [TRT] Tactic: 0xb9171d70ba2eda4b Time: 1.21214
[08/10/2023-11:17:58] [V] [TRT] Conv_62 + Relu_63 || Conv_65 + Relu_66 || Conv_68 + Relu_69 || Conv_71 + Relu_72 || Conv_74 + Relu_75 || Conv_77 + Relu_78 || Conv_80 + Relu_81 || Conv_83 + Relu_84 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x64x32_stage5_warpsize2x2x1_g1_tensor16x8x16 Tactic: 0xbd08239a9317f2fd
[08/10/2023-11:17:58] [V] [TRT] Tactic: 0xbd08239a9317f2fd Time: 1.20294
[08/10/2023-11:17:58] [V] [TRT] Conv_62 + Relu_63 || Conv_65 + Relu_66 || Conv_68 + Relu_69 || Conv_71 + Relu_72 || Conv_74 + Relu_75 || Conv_77 + Relu_78 || Conv_80 + Relu_81 || Conv_83 + Relu_84 Set Tactic Name: ampere_h16816cudnn_64x128_sliced1x2_ldg8_relu_exp_stages_64x4_small_nhwc_tn_v1 Tactic: 0xbd6f5e6f24c05c10
[08/10/2023-11:17:58] [V] [TRT] Tactic: 0xbd6f5e6f24c05c10 Time: 1.83882
[08/10/2023-11:17:58] [V] [TRT] Conv_62 + Relu_63 || Conv_65 + Relu_66 || Conv_68 + Relu_69 || Conv_71 + Relu_72 || Conv_74 + Relu_75 || Conv_77 + Relu_78 || Conv_80 + Relu_81 || Conv_83 + Relu_84 Set Tactic Name: ampere_h16816cudnn_64x64_sliced1x2_ldg8_relu_exp_stages_64x6_large_nhwc_tn_v1 Tactic: 0xbeaee7eaad288322
[08/10/2023-11:17:58] [V] [TRT] Tactic: 0xbeaee7eaad288322 Time: 3.09272
[08/10/2023-11:17:58] [V] [TRT] Conv_62 + Relu_63 || Conv_65 + Relu_66 || Conv_68 + Relu_69 || Conv_71 + Relu_72 || Conv_74 + Relu_75 || Conv_77 + Relu_78 || Conv_80 + Relu_81 || Conv_83 + Relu_84 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x128x64_stage3_warpsize2x2x1_g1_tensor16x8x16_t1r3s3 Tactic: 0xc0a02dc6095497cc
[08/10/2023-11:17:58] [V] [TRT] Tactic: 0xc0a02dc6095497cc Time: 1.22574
[08/10/2023-11:17:58] [V] [TRT] Conv_62 + Relu_63 || Conv_65 + Relu_66 || Conv_68 + Relu_69 || Conv_71 + Relu_72 || Conv_74 + Relu_75 || Conv_77 + Relu_78 || Conv_80 + Relu_81 || Conv_83 + Relu_84 Set Tactic Name: ampere_h16816cudnn_128x128_ldg8_relu_exp_stages_64x3_large_nhwc_tn_v1 Tactic: 0xc2cf926c41243630
[08/10/2023-11:17:58] [V] [TRT] Tactic: 0xc2cf926c41243630 Time: 1.36059
[08/10/2023-11:17:58] [V] [TRT] Conv_62 + Relu_63 || Conv_65 + Relu_66 || Conv_68 + Relu_69 || Conv_71 + Relu_72 || Conv_74 + Relu_75 || Conv_77 + Relu_78 || Conv_80 + Relu_81 || Conv_83 + Relu_84 Set Tactic Name: ampere_h16816cudnn_128x64_ldg8_relu_exp_stages_64x3_small_nhwc_tn_v1 Tactic: 0xc338d2482cee77f8
[08/10/2023-11:17:58] [V] [TRT] Tactic: 0xc338d2482cee77f8 Time: 1.33605
[08/10/2023-11:17:58] [V] [TRT] Conv_62 + Relu_63 || Conv_65 + Relu_66 || Conv_68 + Relu_69 || Conv_71 + Relu_72 || Conv_74 + Relu_75 || Conv_77 + Relu_78 || Conv_80 + Relu_81 || Conv_83 + Relu_84 Set Tactic Name: ampere_h1688cudnn_256x128_ldg8_relu_exp_small_nhwc_tn_v1 Tactic: 0xc660e51970bc5a3a
[08/10/2023-11:17:58] [V] [TRT] Tactic: 0xc660e51970bc5a3a Time: 1.33191
[08/10/2023-11:17:59] [V] [TRT] Conv_62 + Relu_63 || Conv_65 + Relu_66 || Conv_68 + Relu_69 || Conv_71 + Relu_72 || Conv_74 + Relu_75 || Conv_77 + Relu_78 || Conv_80 + Relu_81 || Conv_83 + Relu_84 Set Tactic Name: ampere_h16816cudnn_64x128_ldg8_relu_exp_stages_64x4_medium_nhwc_tn_v1 Tactic: 0xc82f3f06140e3cbb
[08/10/2023-11:17:59] [V] [TRT] Tactic: 0xc82f3f06140e3cbb Time: 1.75808
[08/10/2023-11:17:59] [V] [TRT] Conv_62 + Relu_63 || Conv_65 + Relu_66 || Conv_68 + Relu_69 || Conv_71 + Relu_72 || Conv_74 + Relu_75 || Conv_77 + Relu_78 || Conv_80 + Relu_81 || Conv_83 + Relu_84 Set Tactic Name: ampere_h16816cudnn_64x128_sliced1x2_ldg8_relu_exp_stages_64x4_medium_nhwc_tn_v1 Tactic: 0xc8a90ff8898200c3
[08/10/2023-11:17:59] [V] [TRT] Tactic: 0xc8a90ff8898200c3 Time: 1.84527
[08/10/2023-11:17:59] [V] [TRT] Conv_62 + Relu_63 || Conv_65 + Relu_66 || Conv_68 + Relu_69 || Conv_71 + Relu_72 || Conv_74 + Relu_75 || Conv_77 + Relu_78 || Conv_80 + Relu_81 || Conv_83 + Relu_84 Set Tactic Name: ampere_h1688cudnn_256x64_sliced1x2_ldg8_relu_exp_medium_nhwc_tn_v1 Tactic: 0xc9cc55109bb4de26
[08/10/2023-11:17:59] [V] [TRT] Tactic: 0xc9cc55109bb4de26 Time: 1.85686
[08/10/2023-11:17:59] [V] [TRT] Conv_62 + Relu_63 || Conv_65 + Relu_66 || Conv_68 + Relu_69 || Conv_71 + Relu_72 || Conv_74 + Relu_75 || Conv_77 + Relu_78 || Conv_80 + Relu_81 || Conv_83 + Relu_84 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x64x32_stage5_warpsize2x2x1_g1_tensor16x8x16 Tactic: 0xc9d24bd069159fa8
[08/10/2023-11:17:59] [V] [TRT] Tactic: 0xc9d24bd069159fa8 Time: 1.32638
[08/10/2023-11:17:59] [V] [TRT] Conv_62 + Relu_63 || Conv_65 + Relu_66 || Conv_68 + Relu_69 || Conv_71 + Relu_72 || Conv_74 + Relu_75 || Conv_77 + Relu_78 || Conv_80 + Relu_81 || Conv_83 + Relu_84 Set Tactic Name: ampere_h16816cudnn_64x128_ldg8_relu_exp_stages_64x3_large_nhwc_tn_v1 Tactic: 0xc9f0a7bec963ba66
[08/10/2023-11:17:59] [V] [TRT] Tactic: 0xc9f0a7bec963ba66 Time: 1.32499
[08/10/2023-11:17:59] [V] [TRT] Conv_62 + Relu_63 || Conv_65 + Relu_66 || Conv_68 + Relu_69 || Conv_71 + Relu_72 || Conv_74 + Relu_75 || Conv_77 + Relu_78 || Conv_80 + Relu_81 || Conv_83 + Relu_84 Set Tactic Name: ampere_h16816cudnn_256x64_ldg8_relu_exp_small_nhwc_tn_v1 Tactic: 0xca5d3a11fd48f571
[08/10/2023-11:17:59] [V] [TRT] Tactic: 0xca5d3a11fd48f571 Time: 1.18548
[08/10/2023-11:17:59] [V] [TRT] Conv_62 + Relu_63 || Conv_65 + Relu_66 || Conv_68 + Relu_69 || Conv_71 + Relu_72 || Conv_74 + Relu_75 || Conv_77 + Relu_78 || Conv_80 + Relu_81 || Conv_83 + Relu_84 Set Tactic Name: ampere_h16816cudnn_64x64_ldg8_relu_exp_stages_64x5_small_nhwc_tn_v1 Tactic: 0xce0506e1512285c3
[08/10/2023-11:17:59] [V] [TRT] Tactic: 0xce0506e1512285c3 Time: 2.17577
[08/10/2023-11:17:59] [V] [TRT] Conv_62 + Relu_63 || Conv_65 + Relu_66 || Conv_68 + Relu_69 || Conv_71 + Relu_72 || Conv_74 + Relu_75 || Conv_77 + Relu_78 || Conv_80 + Relu_81 || Conv_83 + Relu_84 Set Tactic Name: ampere_h1688cudnn_256x64_sliced1x2_ldg8_relu_exp_large_nhwc_tn_v1 Tactic: 0xd0a3e0c815f7fb5e
[08/10/2023-11:17:59] [V] [TRT] Tactic: 0xd0a3e0c815f7fb5e Time: 1.89995
[08/10/2023-11:17:59] [V] [TRT] Conv_62 + Relu_63 || Conv_65 + Relu_66 || Conv_68 + Relu_69 || Conv_71 + Relu_72 || Conv_74 + Relu_75 || Conv_77 + Relu_78 || Conv_80 + Relu_81 || Conv_83 + Relu_84 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x64x32_stage3_warpsize4x1x1_g1_tensor16x8x16 Tactic: 0xd1aaad17ca35fbaa
[08/10/2023-11:17:59] [V] [TRT] Tactic: 0xd1aaad17ca35fbaa Time: 0.992832
[08/10/2023-11:17:59] [V] [TRT] Conv_62 + Relu_63 || Conv_65 + Relu_66 || Conv_68 + Relu_69 || Conv_71 + Relu_72 || Conv_74 + Relu_75 || Conv_77 + Relu_78 || Conv_80 + Relu_81 || Conv_83 + Relu_84 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x128x64_stage3_warpsize4x2x1_g1_tensor16x8x16_t1r3s3 Tactic: 0xd2fca0486ca97266
[08/10/2023-11:17:59] [V] [TRT] Tactic: 0xd2fca0486ca97266 Time: 1.09626
[08/10/2023-11:17:59] [V] [TRT] Conv_62 + Relu_63 || Conv_65 + Relu_66 || Conv_68 + Relu_69 || Conv_71 + Relu_72 || Conv_74 + Relu_75 || Conv_77 + Relu_78 || Conv_80 + Relu_81 || Conv_83 + Relu_84 Set Tactic Name: ampere_h16816cudnn_64x128_sliced1x2_ldg8_relu_exp_stages_64x3_small_nhwc_tn_v1 Tactic: 0xd58ea0bdedb89ead
[08/10/2023-11:17:59] [V] [TRT] Tactic: 0xd58ea0bdedb89ead Time: 1.31251
[08/10/2023-11:17:59] [V] [TRT] Conv_62 + Relu_63 || Conv_65 + Relu_66 || Conv_68 + Relu_69 || Conv_71 + Relu_72 || Conv_74 + Relu_75 || Conv_77 + Relu_78 || Conv_80 + Relu_81 || Conv_83 + Relu_84 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x256x64_stage3_warpsize1x4x1_g1_tensor16x8x16_t1r3s3 Tactic: 0xd7c261fa01db2af9
[08/10/2023-11:17:59] [V] [TRT] Tactic: 0xd7c261fa01db2af9 Time: 1.19753
[08/10/2023-11:17:59] [V] [TRT] Conv_62 + Relu_63 || Conv_65 + Relu_66 || Conv_68 + Relu_69 || Conv_71 + Relu_72 || Conv_74 + Relu_75 || Conv_77 + Relu_78 || Conv_80 + Relu_81 || Conv_83 + Relu_84 Set Tactic Name: ampere_h16816cudnn_256x64_sliced1x2_ldg8_relu_exp_large_nhwc_tn_v1 Tactic: 0xd8eb41ee35e76575
[08/10/2023-11:17:59] [V] [TRT] Tactic: 0xd8eb41ee35e76575 Time: 1.79271
[08/10/2023-11:17:59] [V] [TRT] Conv_62 + Relu_63 || Conv_65 + Relu_66 || Conv_68 + Relu_69 || Conv_71 + Relu_72 || Conv_74 + Relu_75 || Conv_77 + Relu_78 || Conv_80 + Relu_81 || Conv_83 + Relu_84 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x128x32_stage5_warpsize2x2x1_g1_tensor16x8x16 Tactic: 0xdb0b80f591d1bb6d
[08/10/2023-11:17:59] [V] [TRT] Tactic: 0xdb0b80f591d1bb6d Time: 1.17926
[08/10/2023-11:17:59] [V] [TRT] Conv_62 + Relu_63 || Conv_65 + Relu_66 || Conv_68 + Relu_69 || Conv_71 + Relu_72 || Conv_74 + Relu_75 || Conv_77 + Relu_78 || Conv_80 + Relu_81 || Conv_83 + Relu_84 Set Tactic Name: ampere_h16816cudnn_256x128_ldg8_relu_exp_large_nhwc_tn_v1 Tactic: 0xdc796d70e228a1d4
[08/10/2023-11:17:59] [V] [TRT] Tactic: 0xdc796d70e228a1d4 Time: 1.23881
[08/10/2023-11:17:59] [V] [TRT] Conv_62 + Relu_63 || Conv_65 + Relu_66 || Conv_68 + Relu_69 || Conv_71 + Relu_72 || Conv_74 + Relu_75 || Conv_77 + Relu_78 || Conv_80 + Relu_81 || Conv_83 + Relu_84 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x32x32_stage4_warpsize4x1x1_g1_tensor16x8x16 Tactic: 0xdce100b9fe609424
[08/10/2023-11:17:59] [V] [TRT] Tactic: 0xdce100b9fe609424 Time: 1.72963
[08/10/2023-11:17:59] [V] [TRT] Conv_62 + Relu_63 || Conv_65 + Relu_66 || Conv_68 + Relu_69 || Conv_71 + Relu_72 || Conv_74 + Relu_75 || Conv_77 + Relu_78 || Conv_80 + Relu_81 || Conv_83 + Relu_84 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x128x32_stage4_warpsize2x2x1_g1_tensor16x8x16 Tactic: 0xdfa020ef435ef810
[08/10/2023-11:17:59] [V] [TRT] Tactic: 0xdfa020ef435ef810 Time: 0.954597
[08/10/2023-11:17:59] [V] [TRT] Conv_62 + Relu_63 || Conv_65 + Relu_66 || Conv_68 + Relu_69 || Conv_71 + Relu_72 || Conv_74 + Relu_75 || Conv_77 + Relu_78 || Conv_80 + Relu_81 || Conv_83 + Relu_84 Set Tactic Name: ampere_h1688cudnn_256x64_ldg8_relu_exp_large_nhwc_tn_v1 Tactic: 0xe0e3c0e8cf9a2d9e
[08/10/2023-11:17:59] [V] [TRT] Tactic: 0xe0e3c0e8cf9a2d9e Time: 2.11585
[08/10/2023-11:17:59] [V] [TRT] Conv_62 + Relu_63 || Conv_65 + Relu_66 || Conv_68 + Relu_69 || Conv_71 + Relu_72 || Conv_74 + Relu_75 || Conv_77 + Relu_78 || Conv_80 + Relu_81 || Conv_83 + Relu_84 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x32x64_stage5_warpsize2x2x1_g1_tensor16x8x16 Tactic: 0xe1ff5ad20f5c6bf6
[08/10/2023-11:17:59] [V] [TRT] Tactic: 0xe1ff5ad20f5c6bf6 Time: 2.91205
[08/10/2023-11:17:59] [V] [TRT] Conv_62 + Relu_63 || Conv_65 + Relu_66 || Conv_68 + Relu_69 || Conv_71 + Relu_72 || Conv_74 + Relu_75 || Conv_77 + Relu_78 || Conv_80 + Relu_81 || Conv_83 + Relu_84 Set Tactic Name: ampere_h16816cudnn_256x64_ldg8_relu_exp_medium_nhwc_tn_v1 Tactic: 0xe4711898bd599c36
[08/10/2023-11:17:59] [V] [TRT] Tactic: 0xe4711898bd599c36 Time: 1.4336
[08/10/2023-11:17:59] [V] [TRT] Conv_62 + Relu_63 || Conv_65 + Relu_66 || Conv_68 + Relu_69 || Conv_71 + Relu_72 || Conv_74 + Relu_75 || Conv_77 + Relu_78 || Conv_80 + Relu_81 || Conv_83 + Relu_84 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x64x64_stage3_warpsize4x1x1_g1_tensor16x8x16 Tactic: 0xeb25062ffb9eae45
[08/10/2023-11:17:59] [V] [TRT] Tactic: 0xeb25062ffb9eae45 Time: 1.32019
[08/10/2023-11:17:59] [V] [TRT] Conv_62 + Relu_63 || Conv_65 + Relu_66 || Conv_68 + Relu_69 || Conv_71 + Relu_72 || Conv_74 + Relu_75 || Conv_77 + Relu_78 || Conv_80 + Relu_81 || Conv_83 + Relu_84 Set Tactic Name: ampere_h16816cudnn_128x64_sliced1x2_ldg8_relu_exp_stages_64x4_large_nhwc_tn_v1 Tactic: 0xeb2d2aa4e56bb41c
[08/10/2023-11:17:59] [V] [TRT] Tactic: 0xeb2d2aa4e56bb41c Time: 1.97953
[08/10/2023-11:17:59] [V] [TRT] Conv_62 + Relu_63 || Conv_65 + Relu_66 || Conv_68 + Relu_69 || Conv_71 + Relu_72 || Conv_74 + Relu_75 || Conv_77 + Relu_78 || Conv_80 + Relu_81 || Conv_83 + Relu_84 Set Tactic Name: ampere_h16816cudnn_64x64_ldg8_relu_exp_stages_64x6_small_nhwc_tn_v1 Tactic: 0xf0beb09df9a19f82
[08/10/2023-11:17:59] [V] [TRT] Tactic: 0xf0beb09df9a19f82 Time: 2.67829
[08/10/2023-11:17:59] [V] [TRT] Conv_62 + Relu_63 || Conv_65 + Relu_66 || Conv_68 + Relu_69 || Conv_71 + Relu_72 || Conv_74 + Relu_75 || Conv_77 + Relu_78 || Conv_80 + Relu_81 || Conv_83 + Relu_84 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x64x64_stage4_warpsize2x2x1_g1_tensor16x8x16 Tactic: 0xf35e0311fa1cc516
[08/10/2023-11:17:59] [V] [TRT] Tactic: 0xf35e0311fa1cc516 Time: 1.55101
[08/10/2023-11:17:59] [V] [TRT] Conv_62 + Relu_63 || Conv_65 + Relu_66 || Conv_68 + Relu_69 || Conv_71 + Relu_72 || Conv_74 + Relu_75 || Conv_77 + Relu_78 || Conv_80 + Relu_81 || Conv_83 + Relu_84 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x32x32_stage4_warpsize4x1x1_g1_tensor16x8x16 Tactic: 0xf79479a62ea9f901
[08/10/2023-11:17:59] [V] [TRT] Tactic: 0xf79479a62ea9f901 Time: 1.83774
[08/10/2023-11:17:59] [V] [TRT] Fastest Tactic: 0xdfa020ef435ef810 Time: 0.954597
[08/10/2023-11:17:59] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0xdfa020ef435ef810
[08/10/2023-11:17:59] [V] [TRT] *************** Autotuning format combination: Half(129600,1:16,720,4) -> Half(1036800,1:16,5760,32) ***************
[08/10/2023-11:17:59] [W] [TRT] Weights [name=Conv_62 + Relu_63 || Conv_65 + Relu_66 || Conv_68 + Relu_69 || Conv_71 + Relu_72 || Conv_74 + Relu_75 || Conv_77 + Relu_78 || Conv_80 + Relu_81 || Conv_83 + Relu_84.weight] had the following issues when converted to FP16:
[08/10/2023-11:17:59] [W] [TRT]  - Subnormal FP16 values detected. 
[08/10/2023-11:17:59] [W] [TRT]  - Values less than smallest positive FP16 Subnormal value detected. Converting to FP16 minimum subnormalized value. 
[08/10/2023-11:17:59] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.
[08/10/2023-11:17:59] [W] [TRT] Weights [name=Conv_62 + Relu_63 || Conv_65 + Relu_66 || Conv_68 + Relu_69 || Conv_71 + Relu_72 || Conv_74 + Relu_75 || Conv_77 + Relu_78 || Conv_80 + Relu_81 || Conv_83 + Relu_84.bias] had the following issues when converted to FP16:
[08/10/2023-11:17:59] [W] [TRT]  - Subnormal FP16 values detected. 
[08/10/2023-11:17:59] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.
[08/10/2023-11:17:59] [V] [TRT] --------------- Timing Runner: Conv_62 + Relu_63 || Conv_65 + Relu_66 || Conv_68 + Relu_69 || Conv_71 + Relu_72 || Conv_74 + Relu_75 || Conv_77 + Relu_78 || Conv_80 + Relu_81 || Conv_83 + Relu_84 (CaskConvolution)
[08/10/2023-11:17:59] [V] [TRT] Conv_62 + Relu_63 || Conv_65 + Relu_66 || Conv_68 + Relu_69 || Conv_71 + Relu_72 || Conv_74 + Relu_75 || Conv_77 + Relu_78 || Conv_80 + Relu_81 || Conv_83 + Relu_84 Set Tactic Name: ampere_h16816cudnn_256x128_ldg8_relu_exp_medium_nhwc_tn_v1 Tactic: 0x03896956a39a1203
[08/10/2023-11:17:59] [V] [TRT] Tactic: 0x03896956a39a1203 Time: 1.56676
[08/10/2023-11:17:59] [V] [TRT] Conv_62 + Relu_63 || Conv_65 + Relu_66 || Conv_68 + Relu_69 || Conv_71 + Relu_72 || Conv_74 + Relu_75 || Conv_77 + Relu_78 || Conv_80 + Relu_81 || Conv_83 + Relu_84 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x128x32_stage3_warpsize4x2x1_g1_tensor16x8x16_t1r3s3 Tactic: 0x048d6d0400f33439
[08/10/2023-11:17:59] [V] [TRT] Tactic: 0x048d6d0400f33439 Time: 1.04687
[08/10/2023-11:17:59] [V] [TRT] Conv_62 + Relu_63 || Conv_65 + Relu_66 || Conv_68 + Relu_69 || Conv_71 + Relu_72 || Conv_74 + Relu_75 || Conv_77 + Relu_78 || Conv_80 + Relu_81 || Conv_83 + Relu_84 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x64x64_stage3_warpsize2x2x1_g1_tensor16x8x16_t1r3s3 Tactic: 0x05b6220f243edacd
[08/10/2023-11:17:59] [V] [TRT] Tactic: 0x05b6220f243edacd Time: 1.18097
[08/10/2023-11:17:59] [V] [TRT] Conv_62 + Relu_63 || Conv_65 + Relu_66 || Conv_68 + Relu_69 || Conv_71 + Relu_72 || Conv_74 + Relu_75 || Conv_77 + Relu_78 || Conv_80 + Relu_81 || Conv_83 + Relu_84 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x64x32_stage3_warpsize4x1x1_g1_tensor16x8x16_t1r3s3_aACCESS Tactic: 0x0866ddee325d07a6
[08/10/2023-11:17:59] [V] [TRT] Tactic: 0x0866ddee325d07a6 Time: 1.36737
[08/10/2023-11:17:59] [V] [TRT] Conv_62 + Relu_63 || Conv_65 + Relu_66 || Conv_68 + Relu_69 || Conv_71 + Relu_72 || Conv_74 + Relu_75 || Conv_77 + Relu_78 || Conv_80 + Relu_81 || Conv_83 + Relu_84 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x128x32_stage4_warpsize2x2x1_g1_tensor16x8x16 Tactic: 0x0e07ff4f4f7c1ac9
[08/10/2023-11:17:59] [V] [TRT] Tactic: 0x0e07ff4f4f7c1ac9 Time: 1.01625
[08/10/2023-11:17:59] [V] [TRT] Conv_62 + Relu_63 || Conv_65 + Relu_66 || Conv_68 + Relu_69 || Conv_71 + Relu_72 || Conv_74 + Relu_75 || Conv_77 + Relu_78 || Conv_80 + Relu_81 || Conv_83 + Relu_84 Set Tactic Name: ampere_h16816cudnn_256x128_ldg8_relu_exp_stages_64x3_small_nhwc_tn_v1 Tactic: 0x0e199750c30c6928
[08/10/2023-11:17:59] [V] [TRT] Tactic: 0x0e199750c30c6928 Time: 1.70187
[08/10/2023-11:17:59] [V] [TRT] Conv_62 + Relu_63 || Conv_65 + Relu_66 || Conv_68 + Relu_69 || Conv_71 + Relu_72 || Conv_74 + Relu_75 || Conv_77 + Relu_78 || Conv_80 + Relu_81 || Conv_83 + Relu_84 Set Tactic Name: ampere_h1688cudnn_128x128_ldg8_relu_exp_medium_nhwc_tn_v1 Tactic: 0x100c1ad308e08d35
[08/10/2023-11:17:59] [V] [TRT] Tactic: 0x100c1ad308e08d35 Time: 1.63101
[08/10/2023-11:17:59] [V] [TRT] Conv_62 + Relu_63 || Conv_65 + Relu_66 || Conv_68 + Relu_69 || Conv_71 + Relu_72 || Conv_74 + Relu_75 || Conv_77 + Relu_78 || Conv_80 + Relu_81 || Conv_83 + Relu_84 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x128x32_stage5_warpsize2x2x1_g1_tensor16x8x16_t1r3s3 Tactic: 0x17ebf0c9f418f10a
[08/10/2023-11:17:59] [V] [TRT] Tactic: 0x17ebf0c9f418f10a Time: 1.46186
[08/10/2023-11:17:59] [V] [TRT] Conv_62 + Relu_63 || Conv_65 + Relu_66 || Conv_68 + Relu_69 || Conv_71 + Relu_72 || Conv_74 + Relu_75 || Conv_77 + Relu_78 || Conv_80 + Relu_81 || Conv_83 + Relu_84 Set Tactic Name: ampere_h16816cudnn_128x64_ldg8_relu_exp_stages_64x4_medium_nhwc_tn_v1 Tactic: 0x196cbc423a9bb69e
[08/10/2023-11:17:59] [V] [TRT] Tactic: 0x196cbc423a9bb69e Time: 2.4536
[08/10/2023-11:17:59] [V] [TRT] Conv_62 + Relu_63 || Conv_65 + Relu_66 || Conv_68 + Relu_69 || Conv_71 + Relu_72 || Conv_74 + Relu_75 || Conv_77 + Relu_78 || Conv_80 + Relu_81 || Conv_83 + Relu_84 Set Tactic Name: ampere_h16816cudnn_128x64_sliced1x2_ldg8_relu_exp_stages_64x3_small_nhwc_tn_v1 Tactic: 0x19822de884f42a6a
[08/10/2023-11:17:59] [V] [TRT] Tactic: 0x19822de884f42a6a Time: 1.52375
[08/10/2023-11:17:59] [V] [TRT] Conv_62 + Relu_63 || Conv_65 + Relu_66 || Conv_68 + Relu_69 || Conv_71 + Relu_72 || Conv_74 + Relu_75 || Conv_77 + Relu_78 || Conv_80 + Relu_81 || Conv_83 + Relu_84 Set Tactic Name: ampere_h16816cudnn_64x64_sliced1x2_ldg8_relu_exp_stages_64x5_small_nhwc_tn_v1 Tactic: 0x1a5ba808ad4cc5a8
[08/10/2023-11:17:59] [V] [TRT] Tactic: 0x1a5ba808ad4cc5a8 Time: 2.64804
[08/10/2023-11:17:59] [V] [TRT] Conv_62 + Relu_63 || Conv_65 + Relu_66 || Conv_68 + Relu_69 || Conv_71 + Relu_72 || Conv_74 + Relu_75 || Conv_77 + Relu_78 || Conv_80 + Relu_81 || Conv_83 + Relu_84 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x64x64_stage3_warpsize2x2x1_g1_tensor16x8x16 Tactic: 0x21b295c0c8f6c95a
[08/10/2023-11:17:59] [V] [TRT] Tactic: 0x21b295c0c8f6c95a Time: 1.59296
[08/10/2023-11:17:59] [V] [TRT] Conv_62 + Relu_63 || Conv_65 + Relu_66 || Conv_68 + Relu_69 || Conv_71 + Relu_72 || Conv_74 + Relu_75 || Conv_77 + Relu_78 || Conv_80 + Relu_81 || Conv_83 + Relu_84 Set Tactic Name: ampere_h16816cudnn_128x128_ldg8_relu_exp_medium_nhwc_tn_v1 Tactic: 0x234580e8a194335c
[08/10/2023-11:17:59] [V] [TRT] Tactic: 0x234580e8a194335c Time: 1.22287
[08/10/2023-11:17:59] [V] [TRT] Conv_62 + Relu_63 || Conv_65 + Relu_66 || Conv_68 + Relu_69 || Conv_71 + Relu_72 || Conv_74 + Relu_75 || Conv_77 + Relu_78 || Conv_80 + Relu_81 || Conv_83 + Relu_84 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x128x32_stage4_warpsize2x2x1_g1_tensor16x8x16_t1r3s3_aACCESS Tactic: 0x245530c34bd6090f
[08/10/2023-11:17:59] [V] [TRT] Tactic: 0x245530c34bd6090f Time: 1.20072
[08/10/2023-11:17:59] [V] [TRT] Conv_62 + Relu_63 || Conv_65 + Relu_66 || Conv_68 + Relu_69 || Conv_71 + Relu_72 || Conv_74 + Relu_75 || Conv_77 + Relu_78 || Conv_80 + Relu_81 || Conv_83 + Relu_84 Set Tactic Name: ampere_h16816cudnn_64x64_sliced1x2_ldg8_relu_exp_stages_64x6_small_nhwc_tn_v1 Tactic: 0x24e01e7405cfdfe9
[08/10/2023-11:17:59] [V] [TRT] Tactic: 0x24e01e7405cfdfe9 Time: 2.79307
[08/10/2023-11:17:59] [V] [TRT] Conv_62 + Relu_63 || Conv_65 + Relu_66 || Conv_68 + Relu_69 || Conv_71 + Relu_72 || Conv_74 + Relu_75 || Conv_77 + Relu_78 || Conv_80 + Relu_81 || Conv_83 + Relu_84 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x64x64_stage3_warpsize2x2x1_g1_tensor16x8x16_t1r3s3_aACCESS Tactic: 0x263a38afd75e3a43
[08/10/2023-11:17:59] [V] [TRT] Tactic: 0x263a38afd75e3a43 Time: 1.13998
[08/10/2023-11:18:00] [V] [TRT] Conv_62 + Relu_63 || Conv_65 + Relu_66 || Conv_68 + Relu_69 || Conv_71 + Relu_72 || Conv_74 + Relu_75 || Conv_77 + Relu_78 || Conv_80 + Relu_81 || Conv_83 + Relu_84 Set Tactic Name: ampere_h16816cudnn_64x128_sliced1x2_ldg8_relu_exp_stages_64x4_large_nhwc_tn_v1 Tactic: 0x2721a7f18c2700db
[08/10/2023-11:18:00] [V] [TRT] Tactic: 0x2721a7f18c2700db Time: 2.10429
[08/10/2023-11:18:00] [V] [TRT] Conv_62 + Relu_63 || Conv_65 + Relu_66 || Conv_68 + Relu_69 || Conv_71 + Relu_72 || Conv_74 + Relu_75 || Conv_77 + Relu_78 || Conv_80 + Relu_81 || Conv_83 + Relu_84 Set Tactic Name: ampere_h16816cudnn_128x64_sliced1x2_ldg8_relu_exp_stages_64x3_medium_nhwc_tn_v1 Tactic: 0x29329b5741ea05f2
[08/10/2023-11:18:00] [V] [TRT] Tactic: 0x29329b5741ea05f2 Time: 1.34818
[08/10/2023-11:18:00] [V] [TRT] Conv_62 + Relu_63 || Conv_65 + Relu_66 || Conv_68 + Relu_69 || Conv_71 + Relu_72 || Conv_74 + Relu_75 || Conv_77 + Relu_78 || Conv_80 + Relu_81 || Conv_83 + Relu_84 Set Tactic Name: ampere_h16816cudnn_128x128_ldg8_relu_exp_large_nhwc_tn_v1 Tactic: 0x2970ae65966d569d
[08/10/2023-11:18:00] [V] [TRT] Tactic: 0x2970ae65966d569d Time: 1.05757
[08/10/2023-11:18:00] [V] [TRT] Conv_62 + Relu_63 || Conv_65 + Relu_66 || Conv_68 + Relu_69 || Conv_71 + Relu_72 || Conv_74 + Relu_75 || Conv_77 + Relu_78 || Conv_80 + Relu_81 || Conv_83 + Relu_84 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x256x64_stage3_warpsize1x4x1_g1_tensor16x8x16 Tactic: 0x29dc14520e0e4eb6
[08/10/2023-11:18:00] [V] [TRT] Tactic: 0x29dc14520e0e4eb6 Time: 1.26225
[08/10/2023-11:18:00] [V] [TRT] Conv_62 + Relu_63 || Conv_65 + Relu_66 || Conv_68 + Relu_69 || Conv_71 + Relu_72 || Conv_74 + Relu_75 || Conv_77 + Relu_78 || Conv_80 + Relu_81 || Conv_83 + Relu_84 Set Tactic Name: ampere_h16816cudnn_256x64_ldg8_relu_exp_stages_64x3_large_nhwc_tn_v1 Tactic: 0x2cb4a662694e89d3
[08/10/2023-11:18:00] [V] [TRT] Tactic: 0x2cb4a662694e89d3 Time: 1.43944
[08/10/2023-11:18:00] [V] [TRT] Conv_62 + Relu_63 || Conv_65 + Relu_66 || Conv_68 + Relu_69 || Conv_71 + Relu_72 || Conv_74 + Relu_75 || Conv_77 + Relu_78 || Conv_80 + Relu_81 || Conv_83 + Relu_84 Set Tactic Name: ampere_h16816cudnn_128x128_ldg8_relu_exp_stages_64x3_medium_nhwc_tn_v1 Tactic: 0x2eaa2202de9404d6
[08/10/2023-11:18:00] [V] [TRT] Tactic: 0x2eaa2202de9404d6 Time: 1.36395
[08/10/2023-11:18:00] [V] [TRT] Conv_62 + Relu_63 || Conv_65 + Relu_66 || Conv_68 + Relu_69 || Conv_71 + Relu_72 || Conv_74 + Relu_75 || Conv_77 + Relu_78 || Conv_80 + Relu_81 || Conv_83 + Relu_84 Set Tactic Name: ampere_h1688cudnn_256x128_ldg8_relu_exp_medium_nhwc_tn_v1 Tactic: 0x30c0f36d0aeeac6a
[08/10/2023-11:18:00] [V] [TRT] Tactic: 0x30c0f36d0aeeac6a Time: 1.40843
[08/10/2023-11:18:00] [V] [TRT] Conv_62 + Relu_63 || Conv_65 + Relu_66 || Conv_68 + Relu_69 || Conv_71 + Relu_72 || Conv_74 + Relu_75 || Conv_77 + Relu_78 || Conv_80 + Relu_81 || Conv_83 + Relu_84 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x64x64_stage4_warpsize2x2x1_g1_tensor16x8x16_t1r3s3_aACCESS Tactic: 0x30e8a8d7a953e5e9
[08/10/2023-11:18:00] [V] [TRT] Tactic: 0x30e8a8d7a953e5e9 Time: 1.42811
[08/10/2023-11:18:00] [V] [TRT] Conv_62 + Relu_63 || Conv_65 + Relu_66 || Conv_68 + Relu_69 || Conv_71 + Relu_72 || Conv_74 + Relu_75 || Conv_77 + Relu_78 || Conv_80 + Relu_81 || Conv_83 + Relu_84 Set Tactic Name: ampere_h16816cudnn_128x64_ldg8_relu_exp_stages_64x4_large_nhwc_tn_v1 Tactic: 0x3197d5044d71e98e
[08/10/2023-11:18:00] [V] [TRT] Tactic: 0x3197d5044d71e98e Time: 1.92224
[08/10/2023-11:18:00] [V] [TRT] Conv_62 + Relu_63 || Conv_65 + Relu_66 || Conv_68 + Relu_69 || Conv_71 + Relu_72 || Conv_74 + Relu_75 || Conv_77 + Relu_78 || Conv_80 + Relu_81 || Conv_83 + Relu_84 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x64x64_stage4_warpsize2x2x1_g1_tensor16x8x16_t1r3s3 Tactic: 0x31d93dc22d2af081
[08/10/2023-11:18:00] [V] [TRT] Tactic: 0x31d93dc22d2af081 Time: 1.50909
[08/10/2023-11:18:00] [V] [TRT] Conv_62 + Relu_63 || Conv_65 + Relu_66 || Conv_68 + Relu_69 || Conv_71 + Relu_72 || Conv_74 + Relu_75 || Conv_77 + Relu_78 || Conv_80 + Relu_81 || Conv_83 + Relu_84 Set Tactic Name: ampere_h1688cudnn_128x128_ldg8_relu_exp_small_nhwc_tn_v1 Tactic: 0x3369260c04f9ad73
[08/10/2023-11:18:00] [V] [TRT] Tactic: 0x3369260c04f9ad73 Time: 1.61632
[08/10/2023-11:18:00] [V] [TRT] Conv_62 + Relu_63 || Conv_65 + Relu_66 || Conv_68 + Relu_69 || Conv_71 + Relu_72 || Conv_74 + Relu_75 || Conv_77 + Relu_78 || Conv_80 + Relu_81 || Conv_83 + Relu_84 Set Tactic Name: ampere_h16816cudnn_64x128_ldg8_relu_exp_stages_64x4_small_nhwc_tn_v1 Tactic: 0x3b5fa0f2a8fc2410
[08/10/2023-11:18:00] [V] [TRT] Tactic: 0x3b5fa0f2a8fc2410 Time: 1.76631
[08/10/2023-11:18:00] [V] [TRT] Conv_62 + Relu_63 || Conv_65 + Relu_66 || Conv_68 + Relu_69 || Conv_71 + Relu_72 || Conv_74 + Relu_75 || Conv_77 + Relu_78 || Conv_80 + Relu_81 || Conv_83 + Relu_84 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x32x32_stage4_warpsize4x1x1_g1_tensor16x8x16_t1r3s3 Tactic: 0x3e7eb35b91b9fa63
[08/10/2023-11:18:00] [V] [TRT] Tactic: 0x3e7eb35b91b9fa63 Time: 1.80025
[08/10/2023-11:18:00] [V] [TRT] Conv_62 + Relu_63 || Conv_65 + Relu_66 || Conv_68 + Relu_69 || Conv_71 + Relu_72 || Conv_74 + Relu_75 || Conv_77 + Relu_78 || Conv_80 + Relu_81 || Conv_83 + Relu_84 Set Tactic Name: ampere_h16816cudnn_256x64_ldg8_relu_exp_stages_64x3_medium_nhwc_tn_v1 Tactic: 0x3f031df0e579d52c
[08/10/2023-11:18:00] [V] [TRT] Tactic: 0x3f031df0e579d52c Time: 1.43951
[08/10/2023-11:18:00] [V] [TRT] Conv_62 + Relu_63 || Conv_65 + Relu_66 || Conv_68 + Relu_69 || Conv_71 + Relu_72 || Conv_74 + Relu_75 || Conv_77 + Relu_78 || Conv_80 + Relu_81 || Conv_83 + Relu_84 Set Tactic Name: ampere_h16816cudnn_256x64_sliced1x2_ldg8_relu_exp_small_nhwc_tn_v1 Tactic: 0x42a5b8709d0039be
[08/10/2023-11:18:00] [V] [TRT] Tactic: 0x42a5b8709d0039be Time: 1.60299
[08/10/2023-11:18:00] [V] [TRT] Conv_62 + Relu_63 || Conv_65 + Relu_66 || Conv_68 + Relu_69 || Conv_71 + Relu_72 || Conv_74 + Relu_75 || Conv_77 + Relu_78 || Conv_80 + Relu_81 || Conv_83 + Relu_84 Set Tactic Name: ampere_h16816cudnn_256x128_ldg8_relu_exp_small_nhwc_tn_v1 Tactic: 0x463794ee4acffd1f
[08/10/2023-11:18:00] [V] [TRT] Tactic: 0x463794ee4acffd1f Time: 1.20069
[08/10/2023-11:18:00] [V] [TRT] Conv_62 + Relu_63 || Conv_65 + Relu_66 || Conv_68 + Relu_69 || Conv_71 + Relu_72 || Conv_74 + Relu_75 || Conv_77 + Relu_78 || Conv_80 + Relu_81 || Conv_83 + Relu_84 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x64x64_stage4_warpsize2x2x1_g1_tensor16x8x16 Tactic: 0x4a81ea1e51436a30
[08/10/2023-11:18:00] [V] [TRT] Tactic: 0x4a81ea1e51436a30 Time: 1.54618
[08/10/2023-11:18:00] [V] [TRT] Conv_62 + Relu_63 || Conv_65 + Relu_66 || Conv_68 + Relu_69 || Conv_71 + Relu_72 || Conv_74 + Relu_75 || Conv_77 + Relu_78 || Conv_80 + Relu_81 || Conv_83 + Relu_84 Set Tactic Name: ampere_h1688cudnn_256x64_sliced1x2_ldg8_relu_exp_small_nhwc_tn_v1 Tactic: 0x4aed1956bd10a795
[08/10/2023-11:18:00] [V] [TRT] Tactic: 0x4aed1956bd10a795 Time: 1.73337
[08/10/2023-11:18:00] [V] [TRT] Conv_62 + Relu_63 || Conv_65 + Relu_66 || Conv_68 + Relu_69 || Conv_71 + Relu_72 || Conv_74 + Relu_75 || Conv_77 + Relu_78 || Conv_80 + Relu_81 || Conv_83 + Relu_84 Set Tactic Name: ampere_h16816cudnn_64x128_sliced1x2_ldg8_relu_exp_stages_64x3_large_nhwc_tn_v1 Tactic: 0x4fc05923455fc266
[08/10/2023-11:18:00] [V] [TRT] Tactic: 0x4fc05923455fc266 Time: 1.31773
[08/10/2023-11:18:00] [V] [TRT] Conv_62 + Relu_63 || Conv_65 + Relu_66 || Conv_68 + Relu_69 || Conv_71 + Relu_72 || Conv_74 + Relu_75 || Conv_77 + Relu_78 || Conv_80 + Relu_81 || Conv_83 + Relu_84 Set Tactic Name: ampere_h16816cudnn_256x64_ldg8_relu_exp_large_nhwc_tn_v1 Tactic: 0x5013c38f55afa9ba
[08/10/2023-11:18:00] [V] [TRT] Tactic: 0x5013c38f55afa9ba Time: 1.24453
[08/10/2023-11:18:00] [V] [TRT] Conv_62 + Relu_63 || Conv_65 + Relu_66 || Conv_68 + Relu_69 || Conv_71 + Relu_72 || Conv_74 + Relu_75 || Conv_77 + Relu_78 || Conv_80 + Relu_81 || Conv_83 + Relu_84 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x64x32_stage3_warpsize4x1x1_g1_tensor16x8x16_t1r3s3 Tactic: 0x529f4431bdae94f5
[08/10/2023-11:18:00] [V] [TRT] Tactic: 0x529f4431bdae94f5 Time: 0.990354
[08/10/2023-11:18:00] [V] [TRT] Conv_62 + Relu_63 || Conv_65 + Relu_66 || Conv_68 + Relu_69 || Conv_71 + Relu_72 || Conv_74 + Relu_75 || Conv_77 + Relu_78 || Conv_80 + Relu_81 || Conv_83 + Relu_84 Set Tactic Name: ampere_h16816cudnn_64x128_ldg8_relu_exp_stages_64x3_small_nhwc_tn_v1 Tactic: 0x53be5e206184e6ad
[08/10/2023-11:18:00] [V] [TRT] Tactic: 0x53be5e206184e6ad Time: 1.34971
[08/10/2023-11:18:00] [V] [TRT] Conv_62 + Relu_63 || Conv_65 + Relu_66 || Conv_68 + Relu_69 || Conv_71 + Relu_72 || Conv_74 + Relu_75 || Conv_77 + Relu_78 || Conv_80 + Relu_81 || Conv_83 + Relu_84 Set Tactic Name: ampere_h16816cudnn_64x64_ldg8_relu_exp_stages_64x5_large_nhwc_tn_v1 Tactic: 0x544bff7ff9c5d908
[08/10/2023-11:18:00] [V] [TRT] Tactic: 0x544bff7ff9c5d908 Time: 2.17298
[08/10/2023-11:18:00] [V] [TRT] Conv_62 + Relu_63 || Conv_65 + Relu_66 || Conv_68 + Relu_69 || Conv_71 + Relu_72 || Conv_74 + Relu_75 || Conv_77 + Relu_78 || Conv_80 + Relu_81 || Conv_83 + Relu_84 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x64x64_stage3_warpsize2x2x1_g1_tensor16x8x16 Tactic: 0x5568fd8a32f4a40f
[08/10/2023-11:18:00] [V] [TRT] Tactic: 0x5568fd8a32f4a40f Time: 1.17967
[08/10/2023-11:18:00] [V] [TRT] Conv_62 + Relu_63 || Conv_65 + Relu_66 || Conv_68 + Relu_69 || Conv_71 + Relu_72 || Conv_74 + Relu_75 || Conv_77 + Relu_78 || Conv_80 + Relu_81 || Conv_83 + Relu_84 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x128x64_stage3_warpsize2x2x1_g1_tensor16x8x16 Tactic: 0x57c9a5ff682354a6
[08/10/2023-11:18:00] [V] [TRT] Tactic: 0x57c9a5ff682354a6 Time: 1.22279
[08/10/2023-11:18:00] [V] [TRT] Conv_62 + Relu_63 || Conv_65 + Relu_66 || Conv_68 + Relu_69 || Conv_71 + Relu_72 || Conv_74 + Relu_75 || Conv_77 + Relu_78 || Conv_80 + Relu_81 || Conv_83 + Relu_84 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x32x64_stage5_warpsize2x2x1_g1_tensor16x8x16 Tactic: 0x5820b3dda403c4d0
[08/10/2023-11:18:00] [V] [TRT] Tactic: 0x5820b3dda403c4d0 Time: 3.01791
[08/10/2023-11:18:00] [V] [TRT] Conv_62 + Relu_63 || Conv_65 + Relu_66 || Conv_68 + Relu_69 || Conv_71 + Relu_72 || Conv_74 + Relu_75 || Conv_77 + Relu_78 || Conv_80 + Relu_81 || Conv_83 + Relu_84 Set Tactic Name: ampere_h16816cudnn_128x128_ldg8_relu_exp_stages_64x3_small_nhwc_tn_v1 Tactic: 0x58816bf2e9c36afb
[08/10/2023-11:18:00] [V] [TRT] Tactic: 0x58816bf2e9c36afb Time: 1.35185
[08/10/2023-11:18:00] [V] [TRT] Conv_62 + Relu_63 || Conv_65 + Relu_66 || Conv_68 + Relu_69 || Conv_71 + Relu_72 || Conv_74 + Relu_75 || Conv_77 + Relu_78 || Conv_80 + Relu_81 || Conv_83 + Relu_84 Set Tactic Name: ampere_h16816cudnn_128x64_ldg8_relu_exp_stages_64x3_large_nhwc_tn_v1 Tactic: 0x59762bd684092b33
[08/10/2023-11:18:00] [V] [TRT] Tactic: 0x59762bd684092b33 Time: 1.34162
[08/10/2023-11:18:00] [V] [TRT] Conv_62 + Relu_63 || Conv_65 + Relu_66 || Conv_68 + Relu_69 || Conv_71 + Relu_72 || Conv_74 + Relu_75 || Conv_77 + Relu_78 || Conv_80 + Relu_81 || Conv_83 + Relu_84 Set Tactic Name: ampere_h16816cudnn_128x64_sliced1x2_ldg8_relu_exp_stages_64x4_medium_nhwc_tn_v1 Tactic: 0x59c5c647b4c76593
[08/10/2023-11:18:00] [V] [TRT] Tactic: 0x59c5c647b4c76593 Time: 1.91467
[08/10/2023-11:18:00] [V] [TRT] Conv_62 + Relu_63 || Conv_65 + Relu_66 || Conv_68 + Relu_69 || Conv_71 + Relu_72 || Conv_74 + Relu_75 || Conv_77 + Relu_78 || Conv_80 + Relu_81 || Conv_83 + Relu_84 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x128x64_stage3_warpsize4x2x1_g1_tensor16x8x16 Tactic: 0x5a55274960724ba8
[08/10/2023-11:18:00] [V] [TRT] Tactic: 0x5a55274960724ba8 Time: 1.11013
[08/10/2023-11:18:00] [V] [TRT] Conv_62 + Relu_63 || Conv_65 + Relu_66 || Conv_68 + Relu_69 || Conv_71 + Relu_72 || Conv_74 + Relu_75 || Conv_77 + Relu_78 || Conv_80 + Relu_81 || Conv_83 + Relu_84 Set Tactic Name: ampere_h1688cudnn_256x128_ldg8_relu_exp_large_nhwc_tn_v1 Tactic: 0x5c2e1c87d85b06f1
[08/10/2023-11:18:00] [V] [TRT] Tactic: 0x5c2e1c87d85b06f1 Time: 1.44042
[08/10/2023-11:18:00] [V] [TRT] Conv_62 + Relu_63 || Conv_65 + Relu_66 || Conv_68 + Relu_69 || Conv_71 + Relu_72 || Conv_74 + Relu_75 || Conv_77 + Relu_78 || Conv_80 + Relu_81 || Conv_83 + Relu_84 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x256x64_stage3_warpsize1x4x1_g1_tensor16x8x16 Tactic: 0x5d067c18f40c23e3
[08/10/2023-11:18:00] [V] [TRT] Tactic: 0x5d067c18f40c23e3 Time: 1.21852
[08/10/2023-11:18:00] [V] [TRT] Conv_62 + Relu_63 || Conv_65 + Relu_66 || Conv_68 + Relu_69 || Conv_71 + Relu_72 || Conv_74 + Relu_75 || Conv_77 + Relu_78 || Conv_80 + Relu_81 || Conv_83 + Relu_84 Set Tactic Name: ampere_h16816cudnn_64x64_ldg8_relu_exp_stages_64x5_medium_nhwc_tn_v1 Tactic: 0x5f31c22ec167f384
[08/10/2023-11:18:00] [V] [TRT] Tactic: 0x5f31c22ec167f384 Time: 2.13286
[08/10/2023-11:18:00] [V] [TRT] Conv_62 + Relu_63 || Conv_65 + Relu_66 || Conv_68 + Relu_69 || Conv_71 + Relu_72 || Conv_74 + Relu_75 || Conv_77 + Relu_78 || Conv_80 + Relu_81 || Conv_83 + Relu_84 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x128x32_stage4_warpsize2x2x1_g1_tensor16x8x16_t1r3s3 Tactic: 0x60c3421152ef8e10
[08/10/2023-11:18:00] [V] [TRT] Tactic: 0x60c3421152ef8e10 Time: 0.935072
[08/10/2023-11:18:00] [V] [TRT] Conv_62 + Relu_63 || Conv_65 + Relu_66 || Conv_68 + Relu_69 || Conv_71 + Relu_72 || Conv_74 + Relu_75 || Conv_77 + Relu_78 || Conv_80 + Relu_81 || Conv_83 + Relu_84 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x128x32_stage3_warpsize4x2x1_g1_tensor16x8x16 Tactic: 0x60da8c7151d91e47
[08/10/2023-11:18:00] [V] [TRT] Tactic: 0x60da8c7151d91e47 Time: 1.05515
[08/10/2023-11:18:00] [V] [TRT] Conv_62 + Relu_63 || Conv_65 + Relu_66 || Conv_68 + Relu_69 || Conv_71 + Relu_72 || Conv_74 + Relu_75 || Conv_77 + Relu_78 || Conv_80 + Relu_81 || Conv_83 + Relu_84 Set Tactic Name: ampere_h1688cudnn_256x64_ldg8_relu_exp_medium_nhwc_tn_v1 Tactic: 0x6426696f872a3b13
[08/10/2023-11:18:00] [V] [TRT] Tactic: 0x6426696f872a3b13 Time: 1.57796
[08/10/2023-11:18:00] [V] [TRT] Conv_62 + Relu_63 || Conv_65 + Relu_66 || Conv_68 + Relu_69 || Conv_71 + Relu_72 || Conv_74 + Relu_75 || Conv_77 + Relu_78 || Conv_80 + Relu_81 || Conv_83 + Relu_84 Set Tactic Name: ampere_h16816cudnn_128x64_ldg8_relu_exp_stages_64x3_medium_nhwc_tn_v1 Tactic: 0x699be152cfb6d6ff
[08/10/2023-11:18:00] [V] [TRT] Tactic: 0x699be152cfb6d6ff Time: 1.34935
[08/10/2023-11:18:00] [V] [TRT] Conv_62 + Relu_63 || Conv_65 + Relu_66 || Conv_68 + Relu_69 || Conv_71 + Relu_72 || Conv_74 + Relu_75 || Conv_77 + Relu_78 || Conv_80 + Relu_81 || Conv_83 + Relu_84 Set Tactic Name: ampere_h16816cudnn_64x64_ldg8_relu_exp_stages_64x6_large_nhwc_tn_v1 Tactic: 0x6af049035146c349
[08/10/2023-11:18:00] [V] [TRT] Tactic: 0x6af049035146c349 Time: 2.89882
[08/10/2023-11:18:00] [V] [TRT] Conv_62 + Relu_63 || Conv_65 + Relu_66 || Conv_68 + Relu_69 || Conv_71 + Relu_72 || Conv_74 + Relu_75 || Conv_77 + Relu_78 || Conv_80 + Relu_81 || Conv_83 + Relu_84 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x256x32_stage3_warpsize2x4x1_g1_tensor16x8x16 Tactic: 0x7005d10718f6c22d
[08/10/2023-11:18:00] [V] [TRT] Tactic: 0x7005d10718f6c22d Time: 1.04431
[08/10/2023-11:18:00] [V] [TRT] Conv_62 + Relu_63 || Conv_65 + Relu_66 || Conv_68 + Relu_69 || Conv_71 + Relu_72 || Conv_74 + Relu_75 || Conv_77 + Relu_78 || Conv_80 + Relu_81 || Conv_83 + Relu_84 Set Tactic Name: ampere_h16816cudnn_64x64_sliced1x2_ldg8_relu_exp_stages_64x5_medium_nhwc_tn_v1 Tactic: 0x706f08da35c795a5
[08/10/2023-11:18:00] [V] [TRT] Tactic: 0x706f08da35c795a5 Time: 2.06463
[08/10/2023-11:18:00] [V] [TRT] Conv_62 + Relu_63 || Conv_65 + Relu_66 || Conv_68 + Relu_69 || Conv_71 + Relu_72 || Conv_74 + Relu_75 || Conv_77 + Relu_78 || Conv_80 + Relu_81 || Conv_83 + Relu_84 Set Tactic Name: ampere_h16816cudnn_128x64_sliced1x2_ldg8_relu_exp_stages_64x4_small_nhwc_tn_v1 Tactic: 0x7163d33a4d8ce8d7
[08/10/2023-11:18:00] [V] [TRT] Tactic: 0x7163d33a4d8ce8d7 Time: 1.91372
[08/10/2023-11:18:00] [V] [TRT] Conv_62 + Relu_63 || Conv_65 + Relu_66 || Conv_68 + Relu_69 || Conv_71 + Relu_72 || Conv_74 + Relu_75 || Conv_77 + Relu_78 || Conv_80 + Relu_81 || Conv_83 + Relu_84 Set Tactic Name: ampere_h1688cudnn_256x64_ldg8_relu_exp_small_nhwc_tn_v1 Tactic: 0x7aad3976677d7155
[08/10/2023-11:18:00] [V] [TRT] Tactic: 0x7aad3976677d7155 Time: 2.02138
[08/10/2023-11:18:00] [V] [TRT] Conv_62 + Relu_63 || Conv_65 + Relu_66 || Conv_68 + Relu_69 || Conv_71 + Relu_72 || Conv_74 + Relu_75 || Conv_77 + Relu_78 || Conv_80 + Relu_81 || Conv_83 + Relu_84 Set Tactic Name: ampere_h16816cudnn_64x64_sliced1x2_ldg8_relu_exp_stages_64x5_large_nhwc_tn_v1 Tactic: 0x8015519605ab9963
[08/10/2023-11:18:01] [V] [TRT] Tactic: 0x8015519605ab9963 Time: 2.07228
[08/10/2023-11:18:01] [V] [TRT] Conv_62 + Relu_63 || Conv_65 + Relu_66 || Conv_68 + Relu_69 || Conv_71 + Relu_72 || Conv_74 + Relu_75 || Conv_77 + Relu_78 || Conv_80 + Relu_81 || Conv_83 + Relu_84 Set Tactic Name: ampere_h16816cudnn_256x64_sliced1x2_ldg8_relu_exp_medium_nhwc_tn_v1 Tactic: 0x82f8a2214b0b4178
[08/10/2023-11:18:01] [V] [TRT] Tactic: 0x82f8a2214b0b4178 Time: 1.77311
[08/10/2023-11:18:01] [V] [TRT] Conv_62 + Relu_63 || Conv_65 + Relu_66 || Conv_68 + Relu_69 || Conv_71 + Relu_72 || Conv_74 + Relu_75 || Conv_77 + Relu_78 || Conv_80 + Relu_81 || Conv_83 + Relu_84 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x32x32_stage4_warpsize4x1x1_g1_tensor16x8x16 Tactic: 0x834e11ecd4ab9454
[08/10/2023-11:18:01] [V] [TRT] Tactic: 0x834e11ecd4ab9454 Time: 2.06082
[08/10/2023-11:18:01] [V] [TRT] Conv_62 + Relu_63 || Conv_65 + Relu_66 || Conv_68 + Relu_69 || Conv_71 + Relu_72 || Conv_74 + Relu_75 || Conv_77 + Relu_78 || Conv_80 + Relu_81 || Conv_83 + Relu_84 Set Tactic Name: ampere_h16816cudnn_128x64_sliced1x2_ldg8_relu_exp_stages_64x3_large_nhwc_tn_v1 Tactic: 0x83ccd4762c1376a1
[08/10/2023-11:18:01] [V] [TRT] Tactic: 0x83ccd4762c1376a1 Time: 1.41502
[08/10/2023-11:18:01] [V] [TRT] Conv_62 + Relu_63 || Conv_65 + Relu_66 || Conv_68 + Relu_69 || Conv_71 + Relu_72 || Conv_74 + Relu_75 || Conv_77 + Relu_78 || Conv_80 + Relu_81 || Conv_83 + Relu_84 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x64x64_stage3_warpsize4x1x1_g1_tensor16x8x16_t1r3s3 Tactic: 0x84ee897dd1f4d2aa
[08/10/2023-11:18:01] [V] [TRT] Tactic: 0x84ee897dd1f4d2aa Time: 1.60514
[08/10/2023-11:18:01] [V] [TRT] Conv_62 + Relu_63 || Conv_65 + Relu_66 || Conv_68 + Relu_69 || Conv_71 + Relu_72 || Conv_74 + Relu_75 || Conv_77 + Relu_78 || Conv_80 + Relu_81 || Conv_83 + Relu_84 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x128x64_stage3_warpsize2x2x1_g1_tensor16x8x16 Tactic: 0x866e7a5f6401b67f
[08/10/2023-11:18:01] [V] [TRT] Tactic: 0x866e7a5f6401b67f Time: 1.48646
[08/10/2023-11:18:01] [V] [TRT] Conv_62 + Relu_63 || Conv_65 + Relu_66 || Conv_68 + Relu_69 || Conv_71 + Relu_72 || Conv_74 + Relu_75 || Conv_77 + Relu_78 || Conv_80 + Relu_81 || Conv_83 + Relu_84 Set Tactic Name: ampere_h16816cudnn_64x64_ldg8_relu_exp_stages_64x6_medium_nhwc_tn_v1 Tactic: 0x88971eec55aba850
[08/10/2023-11:18:01] [V] [TRT] Tactic: 0x88971eec55aba850 Time: 3.16879
[08/10/2023-11:18:01] [V] [TRT] Conv_62 + Relu_63 || Conv_65 + Relu_66 || Conv_68 + Relu_69 || Conv_71 + Relu_72 || Conv_74 + Relu_75 || Conv_77 + Relu_78 || Conv_80 + Relu_81 || Conv_83 + Relu_84 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x128x64_stage3_warpsize4x2x1_g1_tensor16x8x16 Tactic: 0x8bf2f8e96c50a971
[08/10/2023-11:18:01] [V] [TRT] Tactic: 0x8bf2f8e96c50a971 Time: 1.33749
[08/10/2023-11:18:01] [V] [TRT] Conv_62 + Relu_63 || Conv_65 + Relu_66 || Conv_68 + Relu_69 || Conv_71 + Relu_72 || Conv_74 + Relu_75 || Conv_77 + Relu_78 || Conv_80 + Relu_81 || Conv_83 + Relu_84 Set Tactic Name: ampere_h16816cudnn_256x128_ldg8_relu_exp_stages_64x3_large_nhwc_tn_v1 Tactic: 0x94576ece6beb35e3
[08/10/2023-11:18:01] [V] [TRT] Tactic: 0x94576ece6beb35e3 Time: 1.21784
[08/10/2023-11:18:01] [V] [TRT] Conv_62 + Relu_63 || Conv_65 + Relu_66 || Conv_68 + Relu_69 || Conv_71 + Relu_72 || Conv_74 + Relu_75 || Conv_77 + Relu_78 || Conv_80 + Relu_81 || Conv_83 + Relu_84 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x64x64_stage3_warpsize4x1x1_g1_tensor16x8x16 Tactic: 0x9fff6e65019cc310
[08/10/2023-11:18:01] [V] [TRT] Tactic: 0x9fff6e65019cc310 Time: 1.50903
[08/10/2023-11:18:01] [V] [TRT] Conv_62 + Relu_63 || Conv_65 + Relu_66 || Conv_68 + Relu_69 || Conv_71 + Relu_72 || Conv_74 + Relu_75 || Conv_77 + Relu_78 || Conv_80 + Relu_81 || Conv_83 + Relu_84 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x32x64_stage5_warpsize2x2x1_g1_tensor16x8x16_t1r3s3 Tactic: 0xa033e20ae9f412b2
[08/10/2023-11:18:01] [V] [TRT] Tactic: 0xa033e20ae9f412b2 Time: 2.68588
[08/10/2023-11:18:01] [V] [TRT] Conv_62 + Relu_63 || Conv_65 + Relu_66 || Conv_68 + Relu_69 || Conv_71 + Relu_72 || Conv_74 + Relu_75 || Conv_77 + Relu_78 || Conv_80 + Relu_81 || Conv_83 + Relu_84 Set Tactic Name: ampere_h16816cudnn_64x128_ldg8_relu_exp_stages_64x4_large_nhwc_tn_v1 Tactic: 0xa111596c001b78db
[08/10/2023-11:18:01] [V] [TRT] Tactic: 0xa111596c001b78db Time: 1.95599
[08/10/2023-11:18:01] [V] [TRT] Conv_62 + Relu_63 || Conv_65 + Relu_66 || Conv_68 + Relu_69 || Conv_71 + Relu_72 || Conv_74 + Relu_75 || Conv_77 + Relu_78 || Conv_80 + Relu_81 || Conv_83 + Relu_84 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x256x32_stage3_warpsize2x4x1_g1_tensor16x8x16 Tactic: 0xa1a20ea714d420f4
[08/10/2023-11:18:01] [V] [TRT] Tactic: 0xa1a20ea714d420f4 Time: 1.64592
[08/10/2023-11:18:01] [V] [TRT] Conv_62 + Relu_63 || Conv_65 + Relu_66 || Conv_68 + Relu_69 || Conv_71 + Relu_72 || Conv_74 + Relu_75 || Conv_77 + Relu_78 || Conv_80 + Relu_81 || Conv_83 + Relu_84 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x32x32_stage4_warpsize4x1x1_g1_tensor16x8x16_t1r3s3 Tactic: 0xa40cb43c296a36a8
[08/10/2023-11:18:01] [V] [TRT] Tactic: 0xa40cb43c296a36a8 Time: 2.02595
[08/10/2023-11:18:01] [V] [TRT] Conv_62 + Relu_63 || Conv_65 + Relu_66 || Conv_68 + Relu_69 || Conv_71 + Relu_72 || Conv_74 + Relu_75 || Conv_77 + Relu_78 || Conv_80 + Relu_81 || Conv_83 + Relu_84 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x64x32_stage3_warpsize4x1x1_g1_tensor16x8x16 Tactic: 0xa570c55d303796ff
[08/10/2023-11:18:01] [V] [TRT] Tactic: 0xa570c55d303796ff Time: 1.28541
[08/10/2023-11:18:01] [V] [TRT] Conv_62 + Relu_63 || Conv_65 + Relu_66 || Conv_68 + Relu_69 || Conv_71 + Relu_72 || Conv_74 + Relu_75 || Conv_77 + Relu_78 || Conv_80 + Relu_81 || Conv_83 + Relu_84 Set Tactic Name: ampere_h16816cudnn_64x64_sliced1x2_ldg8_relu_exp_stages_64x6_medium_nhwc_tn_v1 Tactic: 0xa7c9d418a10bce71
[08/10/2023-11:18:01] [V] [TRT] Tactic: 0xa7c9d418a10bce71 Time: 3.48363
[08/10/2023-11:18:01] [V] [TRT] Conv_62 + Relu_63 || Conv_65 + Relu_66 || Conv_68 + Relu_69 || Conv_71 + Relu_72 || Conv_74 + Relu_75 || Conv_77 + Relu_78 || Conv_80 + Relu_81 || Conv_83 + Relu_84 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x32x32_stage4_warpsize4x1x1_g1_tensor16x8x16 Tactic: 0xa83b68f30462f971
[08/10/2023-11:18:01] [V] [TRT] Tactic: 0xa83b68f30462f971 Time: 1.91335
[08/10/2023-11:18:01] [V] [TRT] Conv_62 + Relu_63 || Conv_65 + Relu_66 || Conv_68 + Relu_69 || Conv_71 + Relu_72 || Conv_74 + Relu_75 || Conv_77 + Relu_78 || Conv_80 + Relu_81 || Conv_83 + Relu_84 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x256x32_stage3_warpsize2x4x1_g1_tensor16x8x16_t1r3s3 Tactic: 0xa9177bbe4e767df8
[08/10/2023-11:18:01] [V] [TRT] Tactic: 0xa9177bbe4e767df8 Time: 1.03885
[08/10/2023-11:18:01] [V] [TRT] Conv_62 + Relu_63 || Conv_65 + Relu_66 || Conv_68 + Relu_69 || Conv_71 + Relu_72 || Conv_74 + Relu_75 || Conv_77 + Relu_78 || Conv_80 + Relu_81 || Conv_83 + Relu_84 Set Tactic Name: ampere_h1688cudnn_128x128_ldg8_relu_exp_large_nhwc_tn_v1 Tactic: 0xa927df92ac1ef1b8
[08/10/2023-11:18:01] [V] [TRT] Tactic: 0xa927df92ac1ef1b8 Time: 1.33981
[08/10/2023-11:18:01] [V] [TRT] Conv_62 + Relu_63 || Conv_65 + Relu_66 || Conv_68 + Relu_69 || Conv_71 + Relu_72 || Conv_74 + Relu_75 || Conv_77 + Relu_78 || Conv_80 + Relu_81 || Conv_83 + Relu_84 Set Tactic Name: ampere_h16816cudnn_128x64_ldg8_relu_exp_stages_64x4_small_nhwc_tn_v1 Tactic: 0xabd92c9ae596b545
[08/10/2023-11:18:01] [V] [TRT] Tactic: 0xabd92c9ae596b545 Time: 2.06592
[08/10/2023-11:18:01] [V] [TRT] Conv_62 + Relu_63 || Conv_65 + Relu_66 || Conv_68 + Relu_69 || Conv_71 + Relu_72 || Conv_74 + Relu_75 || Conv_77 + Relu_78 || Conv_80 + Relu_81 || Conv_83 + Relu_84 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x128x32_stage5_warpsize2x2x1_g1_tensor16x8x16 Tactic: 0xafd1e8bf6bd3d638
[08/10/2023-11:18:01] [V] [TRT] Tactic: 0xafd1e8bf6bd3d638 Time: 1.2092
[08/10/2023-11:18:01] [V] [TRT] Conv_62 + Relu_63 || Conv_65 + Relu_66 || Conv_68 + Relu_69 || Conv_71 + Relu_72 || Conv_74 + Relu_75 || Conv_77 + Relu_78 || Conv_80 + Relu_81 || Conv_83 + Relu_84 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x128x32_stage3_warpsize4x2x1_g1_tensor16x8x16 Tactic: 0xb17d53d15dfbfc9e
[08/10/2023-11:18:01] [V] [TRT] Tactic: 0xb17d53d15dfbfc9e Time: 1.28413
[08/10/2023-11:18:01] [V] [TRT] Conv_62 + Relu_63 || Conv_65 + Relu_66 || Conv_68 + Relu_69 || Conv_71 + Relu_72 || Conv_74 + Relu_75 || Conv_77 + Relu_78 || Conv_80 + Relu_81 || Conv_83 + Relu_84 Set Tactic Name: ampere_h16816cudnn_128x128_ldg8_relu_exp_small_nhwc_tn_v1 Tactic: 0xb33e57fb3e8a0a56
[08/10/2023-11:18:01] [V] [TRT] Tactic: 0xb33e57fb3e8a0a56 Time: 1.08588
[08/10/2023-11:18:01] [V] [TRT] Conv_62 + Relu_63 || Conv_65 + Relu_66 || Conv_68 + Relu_69 || Conv_71 + Relu_72 || Conv_74 + Relu_75 || Conv_77 + Relu_78 || Conv_80 + Relu_81 || Conv_83 + Relu_84 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x64x32_stage5_warpsize2x2x1_g1_tensor16x8x16_t1r3s3 Tactic: 0xb4bec086187edcfc
[08/10/2023-11:18:01] [V] [TRT] Tactic: 0xb4bec086187edcfc Time: 1.50544
[08/10/2023-11:18:01] [V] [TRT] Conv_62 + Relu_63 || Conv_65 + Relu_66 || Conv_68 + Relu_69 || Conv_71 + Relu_72 || Conv_74 + Relu_75 || Conv_77 + Relu_78 || Conv_80 + Relu_81 || Conv_83 + Relu_84 Set Tactic Name: ampere_h16816cudnn_256x64_ldg8_relu_exp_stages_64x3_small_nhwc_tn_v1 Tactic: 0xb6fa5ffcc1a9d518
[08/10/2023-11:18:01] [V] [TRT] Tactic: 0xb6fa5ffcc1a9d518 Time: 1.44349
[08/10/2023-11:18:01] [V] [TRT] Conv_62 + Relu_63 || Conv_65 + Relu_66 || Conv_68 + Relu_69 || Conv_71 + Relu_72 || Conv_74 + Relu_75 || Conv_77 + Relu_78 || Conv_80 + Relu_81 || Conv_83 + Relu_84 Set Tactic Name: ampere_h16816cudnn_64x128_sliced1x2_ldg8_relu_exp_stages_64x3_medium_nhwc_tn_v1 Tactic: 0xb85e52e87caf60a2
[08/10/2023-11:18:01] [V] [TRT] Tactic: 0xb85e52e87caf60a2 Time: 1.54386
[08/10/2023-11:18:01] [V] [TRT] Conv_62 + Relu_63 || Conv_65 + Relu_66 || Conv_68 + Relu_69 || Conv_71 + Relu_72 || Conv_74 + Relu_75 || Conv_77 + Relu_78 || Conv_80 + Relu_81 || Conv_83 + Relu_84 Set Tactic Name: ampere_h16816cudnn_64x128_ldg8_relu_exp_stages_64x3_medium_nhwc_tn_v1 Tactic: 0xb8d86216e1235cda
[08/10/2023-11:18:01] [V] [TRT] Tactic: 0xb8d86216e1235cda Time: 1.3257
[08/10/2023-11:18:01] [V] [TRT] Conv_62 + Relu_63 || Conv_65 + Relu_66 || Conv_68 + Relu_69 || Conv_71 + Relu_72 || Conv_74 + Relu_75 || Conv_77 + Relu_78 || Conv_80 + Relu_81 || Conv_83 + Relu_84 Set Tactic Name: ampere_h16816cudnn_256x128_ldg8_relu_exp_stages_64x3_medium_nhwc_tn_v1 Tactic: 0xb9171d70ba2eda4b
[08/10/2023-11:18:01] [V] [TRT] Tactic: 0xb9171d70ba2eda4b Time: 1.20783
[08/10/2023-11:18:01] [V] [TRT] Conv_62 + Relu_63 || Conv_65 + Relu_66 || Conv_68 + Relu_69 || Conv_71 + Relu_72 || Conv_74 + Relu_75 || Conv_77 + Relu_78 || Conv_80 + Relu_81 || Conv_83 + Relu_84 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x64x32_stage5_warpsize2x2x1_g1_tensor16x8x16 Tactic: 0xbd08239a9317f2fd
[08/10/2023-11:18:01] [V] [TRT] Tactic: 0xbd08239a9317f2fd Time: 1.20534
[08/10/2023-11:18:01] [V] [TRT] Conv_62 + Relu_63 || Conv_65 + Relu_66 || Conv_68 + Relu_69 || Conv_71 + Relu_72 || Conv_74 + Relu_75 || Conv_77 + Relu_78 || Conv_80 + Relu_81 || Conv_83 + Relu_84 Set Tactic Name: ampere_h16816cudnn_64x128_sliced1x2_ldg8_relu_exp_stages_64x4_small_nhwc_tn_v1 Tactic: 0xbd6f5e6f24c05c10
[08/10/2023-11:18:01] [V] [TRT] Tactic: 0xbd6f5e6f24c05c10 Time: 1.84373
[08/10/2023-11:18:01] [V] [TRT] Conv_62 + Relu_63 || Conv_65 + Relu_66 || Conv_68 + Relu_69 || Conv_71 + Relu_72 || Conv_74 + Relu_75 || Conv_77 + Relu_78 || Conv_80 + Relu_81 || Conv_83 + Relu_84 Set Tactic Name: ampere_h16816cudnn_64x64_sliced1x2_ldg8_relu_exp_stages_64x6_large_nhwc_tn_v1 Tactic: 0xbeaee7eaad288322
[08/10/2023-11:18:01] [V] [TRT] Tactic: 0xbeaee7eaad288322 Time: 3.04273
[08/10/2023-11:18:01] [V] [TRT] Conv_62 + Relu_63 || Conv_65 + Relu_66 || Conv_68 + Relu_69 || Conv_71 + Relu_72 || Conv_74 + Relu_75 || Conv_77 + Relu_78 || Conv_80 + Relu_81 || Conv_83 + Relu_84 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x128x64_stage3_warpsize2x2x1_g1_tensor16x8x16_t1r3s3 Tactic: 0xc0a02dc6095497cc
[08/10/2023-11:18:01] [V] [TRT] Tactic: 0xc0a02dc6095497cc Time: 1.19988
[08/10/2023-11:18:01] [V] [TRT] Conv_62 + Relu_63 || Conv_65 + Relu_66 || Conv_68 + Relu_69 || Conv_71 + Relu_72 || Conv_74 + Relu_75 || Conv_77 + Relu_78 || Conv_80 + Relu_81 || Conv_83 + Relu_84 Set Tactic Name: ampere_h16816cudnn_128x128_ldg8_relu_exp_stages_64x3_large_nhwc_tn_v1 Tactic: 0xc2cf926c41243630
[08/10/2023-11:18:01] [V] [TRT] Tactic: 0xc2cf926c41243630 Time: 1.37194
[08/10/2023-11:18:01] [V] [TRT] Conv_62 + Relu_63 || Conv_65 + Relu_66 || Conv_68 + Relu_69 || Conv_71 + Relu_72 || Conv_74 + Relu_75 || Conv_77 + Relu_78 || Conv_80 + Relu_81 || Conv_83 + Relu_84 Set Tactic Name: ampere_h16816cudnn_128x64_ldg8_relu_exp_stages_64x3_small_nhwc_tn_v1 Tactic: 0xc338d2482cee77f8
[08/10/2023-11:18:01] [V] [TRT] Tactic: 0xc338d2482cee77f8 Time: 1.33281
[08/10/2023-11:18:01] [V] [TRT] Conv_62 + Relu_63 || Conv_65 + Relu_66 || Conv_68 + Relu_69 || Conv_71 + Relu_72 || Conv_74 + Relu_75 || Conv_77 + Relu_78 || Conv_80 + Relu_81 || Conv_83 + Relu_84 Set Tactic Name: ampere_h1688cudnn_256x128_ldg8_relu_exp_small_nhwc_tn_v1 Tactic: 0xc660e51970bc5a3a
[08/10/2023-11:18:01] [V] [TRT] Tactic: 0xc660e51970bc5a3a Time: 1.36074
[08/10/2023-11:18:01] [V] [TRT] Conv_62 + Relu_63 || Conv_65 + Relu_66 || Conv_68 + Relu_69 || Conv_71 + Relu_72 || Conv_74 + Relu_75 || Conv_77 + Relu_78 || Conv_80 + Relu_81 || Conv_83 + Relu_84 Set Tactic Name: ampere_h16816cudnn_64x128_ldg8_relu_exp_stages_64x4_medium_nhwc_tn_v1 Tactic: 0xc82f3f06140e3cbb
[08/10/2023-11:18:01] [V] [TRT] Tactic: 0xc82f3f06140e3cbb Time: 1.785
[08/10/2023-11:18:01] [V] [TRT] Conv_62 + Relu_63 || Conv_65 + Relu_66 || Conv_68 + Relu_69 || Conv_71 + Relu_72 || Conv_74 + Relu_75 || Conv_77 + Relu_78 || Conv_80 + Relu_81 || Conv_83 + Relu_84 Set Tactic Name: ampere_h16816cudnn_64x128_sliced1x2_ldg8_relu_exp_stages_64x4_medium_nhwc_tn_v1 Tactic: 0xc8a90ff8898200c3
[08/10/2023-11:18:01] [V] [TRT] Tactic: 0xc8a90ff8898200c3 Time: 1.85299
[08/10/2023-11:18:01] [V] [TRT] Conv_62 + Relu_63 || Conv_65 + Relu_66 || Conv_68 + Relu_69 || Conv_71 + Relu_72 || Conv_74 + Relu_75 || Conv_77 + Relu_78 || Conv_80 + Relu_81 || Conv_83 + Relu_84 Set Tactic Name: ampere_h1688cudnn_256x64_sliced1x2_ldg8_relu_exp_medium_nhwc_tn_v1 Tactic: 0xc9cc55109bb4de26
[08/10/2023-11:18:01] [V] [TRT] Tactic: 0xc9cc55109bb4de26 Time: 1.86147
[08/10/2023-11:18:01] [V] [TRT] Conv_62 + Relu_63 || Conv_65 + Relu_66 || Conv_68 + Relu_69 || Conv_71 + Relu_72 || Conv_74 + Relu_75 || Conv_77 + Relu_78 || Conv_80 + Relu_81 || Conv_83 + Relu_84 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x64x32_stage5_warpsize2x2x1_g1_tensor16x8x16 Tactic: 0xc9d24bd069159fa8
[08/10/2023-11:18:01] [V] [TRT] Tactic: 0xc9d24bd069159fa8 Time: 1.32546
[08/10/2023-11:18:01] [V] [TRT] Conv_62 + Relu_63 || Conv_65 + Relu_66 || Conv_68 + Relu_69 || Conv_71 + Relu_72 || Conv_74 + Relu_75 || Conv_77 + Relu_78 || Conv_80 + Relu_81 || Conv_83 + Relu_84 Set Tactic Name: ampere_h16816cudnn_64x128_ldg8_relu_exp_stages_64x3_large_nhwc_tn_v1 Tactic: 0xc9f0a7bec963ba66
[08/10/2023-11:18:01] [V] [TRT] Tactic: 0xc9f0a7bec963ba66 Time: 1.3238
[08/10/2023-11:18:01] [V] [TRT] Conv_62 + Relu_63 || Conv_65 + Relu_66 || Conv_68 + Relu_69 || Conv_71 + Relu_72 || Conv_74 + Relu_75 || Conv_77 + Relu_78 || Conv_80 + Relu_81 || Conv_83 + Relu_84 Set Tactic Name: ampere_h16816cudnn_256x64_ldg8_relu_exp_small_nhwc_tn_v1 Tactic: 0xca5d3a11fd48f571
[08/10/2023-11:18:01] [V] [TRT] Tactic: 0xca5d3a11fd48f571 Time: 1.18277
[08/10/2023-11:18:01] [V] [TRT] Conv_62 + Relu_63 || Conv_65 + Relu_66 || Conv_68 + Relu_69 || Conv_71 + Relu_72 || Conv_74 + Relu_75 || Conv_77 + Relu_78 || Conv_80 + Relu_81 || Conv_83 + Relu_84 Set Tactic Name: ampere_h16816cudnn_64x64_ldg8_relu_exp_stages_64x5_small_nhwc_tn_v1 Tactic: 0xce0506e1512285c3
[08/10/2023-11:18:02] [V] [TRT] Tactic: 0xce0506e1512285c3 Time: 2.7888
[08/10/2023-11:18:02] [V] [TRT] Conv_62 + Relu_63 || Conv_65 + Relu_66 || Conv_68 + Relu_69 || Conv_71 + Relu_72 || Conv_74 + Relu_75 || Conv_77 + Relu_78 || Conv_80 + Relu_81 || Conv_83 + Relu_84 Set Tactic Name: ampere_h1688cudnn_256x64_sliced1x2_ldg8_relu_exp_large_nhwc_tn_v1 Tactic: 0xd0a3e0c815f7fb5e
[08/10/2023-11:18:02] [V] [TRT] Tactic: 0xd0a3e0c815f7fb5e Time: 1.91874
[08/10/2023-11:18:02] [V] [TRT] Conv_62 + Relu_63 || Conv_65 + Relu_66 || Conv_68 + Relu_69 || Conv_71 + Relu_72 || Conv_74 + Relu_75 || Conv_77 + Relu_78 || Conv_80 + Relu_81 || Conv_83 + Relu_84 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x64x32_stage3_warpsize4x1x1_g1_tensor16x8x16 Tactic: 0xd1aaad17ca35fbaa
[08/10/2023-11:18:02] [V] [TRT] Tactic: 0xd1aaad17ca35fbaa Time: 1.01875
[08/10/2023-11:18:02] [V] [TRT] Conv_62 + Relu_63 || Conv_65 + Relu_66 || Conv_68 + Relu_69 || Conv_71 + Relu_72 || Conv_74 + Relu_75 || Conv_77 + Relu_78 || Conv_80 + Relu_81 || Conv_83 + Relu_84 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x128x64_stage3_warpsize4x2x1_g1_tensor16x8x16_t1r3s3 Tactic: 0xd2fca0486ca97266
[08/10/2023-11:18:02] [V] [TRT] Tactic: 0xd2fca0486ca97266 Time: 1.10851
[08/10/2023-11:18:02] [V] [TRT] Conv_62 + Relu_63 || Conv_65 + Relu_66 || Conv_68 + Relu_69 || Conv_71 + Relu_72 || Conv_74 + Relu_75 || Conv_77 + Relu_78 || Conv_80 + Relu_81 || Conv_83 + Relu_84 Set Tactic Name: ampere_h16816cudnn_64x128_sliced1x2_ldg8_relu_exp_stages_64x3_small_nhwc_tn_v1 Tactic: 0xd58ea0bdedb89ead
[08/10/2023-11:18:02] [V] [TRT] Tactic: 0xd58ea0bdedb89ead Time: 1.31745
[08/10/2023-11:18:02] [V] [TRT] Conv_62 + Relu_63 || Conv_65 + Relu_66 || Conv_68 + Relu_69 || Conv_71 + Relu_72 || Conv_74 + Relu_75 || Conv_77 + Relu_78 || Conv_80 + Relu_81 || Conv_83 + Relu_84 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x256x64_stage3_warpsize1x4x1_g1_tensor16x8x16_t1r3s3 Tactic: 0xd7c261fa01db2af9
[08/10/2023-11:18:02] [V] [TRT] Tactic: 0xd7c261fa01db2af9 Time: 1.19988
[08/10/2023-11:18:02] [V] [TRT] Conv_62 + Relu_63 || Conv_65 + Relu_66 || Conv_68 + Relu_69 || Conv_71 + Relu_72 || Conv_74 + Relu_75 || Conv_77 + Relu_78 || Conv_80 + Relu_81 || Conv_83 + Relu_84 Set Tactic Name: ampere_h16816cudnn_256x64_sliced1x2_ldg8_relu_exp_large_nhwc_tn_v1 Tactic: 0xd8eb41ee35e76575
[08/10/2023-11:18:02] [V] [TRT] Tactic: 0xd8eb41ee35e76575 Time: 2.22348
[08/10/2023-11:18:02] [V] [TRT] Conv_62 + Relu_63 || Conv_65 + Relu_66 || Conv_68 + Relu_69 || Conv_71 + Relu_72 || Conv_74 + Relu_75 || Conv_77 + Relu_78 || Conv_80 + Relu_81 || Conv_83 + Relu_84 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x128x32_stage5_warpsize2x2x1_g1_tensor16x8x16 Tactic: 0xdb0b80f591d1bb6d
[08/10/2023-11:18:02] [V] [TRT] Tactic: 0xdb0b80f591d1bb6d Time: 1.18131
[08/10/2023-11:18:02] [V] [TRT] Conv_62 + Relu_63 || Conv_65 + Relu_66 || Conv_68 + Relu_69 || Conv_71 + Relu_72 || Conv_74 + Relu_75 || Conv_77 + Relu_78 || Conv_80 + Relu_81 || Conv_83 + Relu_84 Set Tactic Name: ampere_h16816cudnn_256x128_ldg8_relu_exp_large_nhwc_tn_v1 Tactic: 0xdc796d70e228a1d4
[08/10/2023-11:18:02] [V] [TRT] Tactic: 0xdc796d70e228a1d4 Time: 1.54893
[08/10/2023-11:18:02] [V] [TRT] Conv_62 + Relu_63 || Conv_65 + Relu_66 || Conv_68 + Relu_69 || Conv_71 + Relu_72 || Conv_74 + Relu_75 || Conv_77 + Relu_78 || Conv_80 + Relu_81 || Conv_83 + Relu_84 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x32x32_stage4_warpsize4x1x1_g1_tensor16x8x16 Tactic: 0xdce100b9fe609424
[08/10/2023-11:18:02] [V] [TRT] Tactic: 0xdce100b9fe609424 Time: 1.68824
[08/10/2023-11:18:02] [V] [TRT] Conv_62 + Relu_63 || Conv_65 + Relu_66 || Conv_68 + Relu_69 || Conv_71 + Relu_72 || Conv_74 + Relu_75 || Conv_77 + Relu_78 || Conv_80 + Relu_81 || Conv_83 + Relu_84 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x128x32_stage4_warpsize2x2x1_g1_tensor16x8x16 Tactic: 0xdfa020ef435ef810
[08/10/2023-11:18:02] [V] [TRT] Tactic: 0xdfa020ef435ef810 Time: 1.42165
[08/10/2023-11:18:02] [V] [TRT] Conv_62 + Relu_63 || Conv_65 + Relu_66 || Conv_68 + Relu_69 || Conv_71 + Relu_72 || Conv_74 + Relu_75 || Conv_77 + Relu_78 || Conv_80 + Relu_81 || Conv_83 + Relu_84 Set Tactic Name: ampere_h1688cudnn_256x64_ldg8_relu_exp_large_nhwc_tn_v1 Tactic: 0xe0e3c0e8cf9a2d9e
[08/10/2023-11:18:02] [V] [TRT] Tactic: 0xe0e3c0e8cf9a2d9e Time: 1.8344
[08/10/2023-11:18:02] [V] [TRT] Conv_62 + Relu_63 || Conv_65 + Relu_66 || Conv_68 + Relu_69 || Conv_71 + Relu_72 || Conv_74 + Relu_75 || Conv_77 + Relu_78 || Conv_80 + Relu_81 || Conv_83 + Relu_84 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x32x64_stage5_warpsize2x2x1_g1_tensor16x8x16 Tactic: 0xe1ff5ad20f5c6bf6
[08/10/2023-11:18:02] [V] [TRT] Tactic: 0xe1ff5ad20f5c6bf6 Time: 3.09319
[08/10/2023-11:18:02] [V] [TRT] Conv_62 + Relu_63 || Conv_65 + Relu_66 || Conv_68 + Relu_69 || Conv_71 + Relu_72 || Conv_74 + Relu_75 || Conv_77 + Relu_78 || Conv_80 + Relu_81 || Conv_83 + Relu_84 Set Tactic Name: ampere_h16816cudnn_256x64_ldg8_relu_exp_medium_nhwc_tn_v1 Tactic: 0xe4711898bd599c36
[08/10/2023-11:18:02] [V] [TRT] Tactic: 0xe4711898bd599c36 Time: 1.79411
[08/10/2023-11:18:02] [V] [TRT] Conv_62 + Relu_63 || Conv_65 + Relu_66 || Conv_68 + Relu_69 || Conv_71 + Relu_72 || Conv_74 + Relu_75 || Conv_77 + Relu_78 || Conv_80 + Relu_81 || Conv_83 + Relu_84 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x64x64_stage3_warpsize4x1x1_g1_tensor16x8x16 Tactic: 0xeb25062ffb9eae45
[08/10/2023-11:18:02] [V] [TRT] Tactic: 0xeb25062ffb9eae45 Time: 1.60644
[08/10/2023-11:18:02] [V] [TRT] Conv_62 + Relu_63 || Conv_65 + Relu_66 || Conv_68 + Relu_69 || Conv_71 + Relu_72 || Conv_74 + Relu_75 || Conv_77 + Relu_78 || Conv_80 + Relu_81 || Conv_83 + Relu_84 Set Tactic Name: ampere_h16816cudnn_128x64_sliced1x2_ldg8_relu_exp_stages_64x4_large_nhwc_tn_v1 Tactic: 0xeb2d2aa4e56bb41c
[08/10/2023-11:18:02] [V] [TRT] Tactic: 0xeb2d2aa4e56bb41c Time: 2.15312
[08/10/2023-11:18:02] [V] [TRT] Conv_62 + Relu_63 || Conv_65 + Relu_66 || Conv_68 + Relu_69 || Conv_71 + Relu_72 || Conv_74 + Relu_75 || Conv_77 + Relu_78 || Conv_80 + Relu_81 || Conv_83 + Relu_84 Set Tactic Name: ampere_h16816cudnn_64x64_ldg8_relu_exp_stages_64x6_small_nhwc_tn_v1 Tactic: 0xf0beb09df9a19f82
[08/10/2023-11:18:02] [V] [TRT] Tactic: 0xf0beb09df9a19f82 Time: 3.27478
[08/10/2023-11:18:02] [V] [TRT] Conv_62 + Relu_63 || Conv_65 + Relu_66 || Conv_68 + Relu_69 || Conv_71 + Relu_72 || Conv_74 + Relu_75 || Conv_77 + Relu_78 || Conv_80 + Relu_81 || Conv_83 + Relu_84 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x64x64_stage4_warpsize2x2x1_g1_tensor16x8x16 Tactic: 0xf35e0311fa1cc516
[08/10/2023-11:18:02] [V] [TRT] Tactic: 0xf35e0311fa1cc516 Time: 2.06853
[08/10/2023-11:18:02] [V] [TRT] Conv_62 + Relu_63 || Conv_65 + Relu_66 || Conv_68 + Relu_69 || Conv_71 + Relu_72 || Conv_74 + Relu_75 || Conv_77 + Relu_78 || Conv_80 + Relu_81 || Conv_83 + Relu_84 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x32x32_stage4_warpsize4x1x1_g1_tensor16x8x16 Tactic: 0xf79479a62ea9f901
[08/10/2023-11:18:02] [V] [TRT] Tactic: 0xf79479a62ea9f901 Time: 2.33267
[08/10/2023-11:18:02] [V] [TRT] Fastest Tactic: 0x60c3421152ef8e10 Time: 0.935072
[08/10/2023-11:18:02] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x60c3421152ef8e10
[08/10/2023-11:18:02] [V] [TRT] =============== Computing costs for 
[08/10/2023-11:18:02] [V] [TRT] *************** Autotuning format combination: Float(16588800,32400,180,1) -> Float(64800,32400,180,1) ***************
[08/10/2023-11:18:02] [V] [TRT] --------------- Timing Runner: Conv_64 (CudaDepthwiseConvolution)
[08/10/2023-11:18:02] [V] [TRT] CudaDepthwiseConvolution has no valid tactics for this config, skipping
[08/10/2023-11:18:02] [V] [TRT] --------------- Timing Runner: Conv_64 (FusedConvActConvolution)
[08/10/2023-11:18:02] [V] [TRT] Tactic: 0x000000000007ffff Time: 0.887625
[08/10/2023-11:18:02] [V] [TRT] Tactic: 0x00000000000affff Time: 1.33939
[08/10/2023-11:18:02] [V] [TRT] Tactic: 0x00000000000effff Time: 1.75399
[08/10/2023-11:18:02] [V] [TRT] Tactic: 0x00000000000fffff Time: 0.735886
[08/10/2023-11:18:02] [V] [TRT] Tactic: 0x00000000001effff Time: 1.64608
[08/10/2023-11:18:02] [V] [TRT] Tactic: 0x000000000027ffff Time: 0.890757
[08/10/2023-11:18:02] [V] [TRT] Tactic: 0x000000000062ffff Time: 0.945394
[08/10/2023-11:18:02] [V] [TRT] Tactic: 0x00000000006effff Time: 1.49525
[08/10/2023-11:18:02] [V] [TRT] Tactic: 0x000000000086ffff Time: 0.993189
[08/10/2023-11:18:02] [V] [TRT] Tactic: 0x000000000089ffff Time: 0.654811
[08/10/2023-11:18:02] [V] [TRT] Tactic: 0x00000000009fffff Time: 0.648859
[08/10/2023-11:18:02] [V] [TRT] Tactic: 0x0000000000a2ffff Time: 0.689061
[08/10/2023-11:18:02] [V] [TRT] Fastest Tactic: 0x00000000009fffff Time: 0.648859
[08/10/2023-11:18:02] [V] [TRT] --------------- Timing Runner: Conv_64 (CudnnConvolution)
[08/10/2023-11:18:02] [V] [TRT] Tactic: 0x0000000000000000 Time: 1.02609
[08/10/2023-11:18:02] [V] [TRT] Tactic: 0x0000000000000001 Time: 0.486519
[08/10/2023-11:18:02] [V] [TRT] Tactic: 0x0000000000000002 Time: 1.0227
[08/10/2023-11:18:02] [V] [TRT] Tactic: 0x0000000000000004 Time: 4.07822
[08/10/2023-11:18:03] [V] [TRT] Tactic: 0x0000000000000005 Time: 1.06257
[08/10/2023-11:18:03] [V] [TRT] Tactic: 0x0000000000000006 Time: 0.437445
[08/10/2023-11:18:03] [V] [TRT] Tactic: 0x0000000000000038 Time: 1.02164
[08/10/2023-11:18:03] [V] [TRT] Tactic: 0x0000000000000039 Time: 0.485984
[08/10/2023-11:18:03] [V] [TRT] Tactic: 0x000000000000003a Time: 1.02278
[08/10/2023-11:18:03] [V] [TRT] Tactic: 0x000000000000003c Time: 3.98916
[08/10/2023-11:18:03] [V] [TRT] Tactic: 0x000000000000003d Time: 1.07573
[08/10/2023-11:18:03] [V] [TRT] Tactic: 0x000000000000003e Time: 0.436507
[08/10/2023-11:18:03] [V] [TRT] Tactic: 0x0000000000000070 Time: 1.02142
[08/10/2023-11:18:03] [V] [TRT] Tactic: 0x0000000000000071 Time: 1.0228
[08/10/2023-11:18:03] [V] [TRT] Tactic: 0x0000000000000072 Time: 1.02418
[08/10/2023-11:18:03] [V] [TRT] Tactic: 0x0000000000000074 Time: 4.04867
[08/10/2023-11:18:03] [V] [TRT] Tactic: 0x0000000000000075 Time: 1.05795
[08/10/2023-11:18:03] [V] [TRT] Tactic: 0x0000000000000076 Time: 0.43728
[08/10/2023-11:18:03] [V] [TRT] Fastest Tactic: 0x000000000000003e Time: 0.436507
[08/10/2023-11:18:03] [V] [TRT] --------------- Timing Runner: Conv_64 (CaskConvolution)
[08/10/2023-11:18:03] [V] [TRT] Conv_64 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x128x8_stage3_warpsize1x4x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0x01cf8ce2da913006
[08/10/2023-11:18:03] [V] [TRT] Tactic: 0x01cf8ce2da913006 Time: 2.7245
[08/10/2023-11:18:03] [V] [TRT] Conv_64 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x64x8_stage3_warpsize2x2x1_g1_ffma_aligna4_alignc4 Tactic: 0x12dbf7d94ee3696d
[08/10/2023-11:18:03] [V] [TRT] Tactic: 0x12dbf7d94ee3696d Time: 1.47401
[08/10/2023-11:18:03] [V] [TRT] Conv_64 Set Tactic Name: ampere_scudnn_128x64_relu_small_nn_v1 Tactic: 0x3f243c490d502deb
[08/10/2023-11:18:03] [V] [TRT] Tactic: 0x3f243c490d502deb Time: 1.38691
[08/10/2023-11:18:03] [V] [TRT] Conv_64 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0x4727434768e46395
[08/10/2023-11:18:03] [V] [TRT] Tactic: 0x4727434768e46395 Time: 1.55599
[08/10/2023-11:18:03] [V] [TRT] Conv_64 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize256x128x8_stage3_warpsize4x2x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0x4efce38acc876f5c
[08/10/2023-11:18:03] [V] [TRT] Tactic: 0x4efce38acc876f5c Time: 2.78755
[08/10/2023-11:18:03] [V] [TRT] Conv_64 Set Tactic Name: ampere_scudnn_128x128_relu_small_nn_v1 Tactic: 0x503619c69ae500ff
[08/10/2023-11:18:03] [V] [TRT] Tactic: 0x503619c69ae500ff Time: 2.42998
[08/10/2023-11:18:03] [V] [TRT] Conv_64 Set Tactic Name: ampere_scudnn_128x128_relu_xregs_large_nn_v1 Tactic: 0x5403ad713f811a18
[08/10/2023-11:18:03] [V] [TRT] Tactic: 0x5403ad713f811a18 Time: 2.62567
[08/10/2023-11:18:03] [V] [TRT] Conv_64 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x128x8_stage3_warpsize1x4x1_g1_ffma_aligna4_alignc4 Tactic: 0x5aa723e0481da855
[08/10/2023-11:18:03] [V] [TRT] Tactic: 0x5aa723e0481da855 Time: 2.73073
[08/10/2023-11:18:03] [V] [TRT] Conv_64 Set Tactic Name: ampere_scudnn_128x64_relu_xregs_large_nn_v1 Tactic: 0x5deb29b7a8e275f7
[08/10/2023-11:18:03] [V] [TRT] Tactic: 0x5deb29b7a8e275f7 Time: 1.33419
[08/10/2023-11:18:03] [V] [TRT] Conv_64 Set Tactic Name: ampere_scudnn_winograd_128x128_ldg1_ldg4_relu_tile148t_nt_v1 Tactic: 0x94119b4c514b211a
[08/10/2023-11:18:03] [V] [TRT] Tactic: 0x94119b4c514b211a Time: 0.422656
[08/10/2023-11:18:03] [V] [TRT] Conv_64 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_aligna4_alignc4 Tactic: 0xa31d27de74b895ff
[08/10/2023-11:18:03] [V] [TRT] Tactic: 0xa31d27de74b895ff Time: 1.58438
[08/10/2023-11:18:03] [V] [TRT] Conv_64 Set Tactic Name: ampere_scudnn_128x32_relu_small_nn_v1 Tactic: 0xa8609adc4e0ceb90
[08/10/2023-11:18:03] [V] [TRT] Tactic: 0xa8609adc4e0ceb90 Time: 0.815552
[08/10/2023-11:18:03] [V] [TRT] Conv_64 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize256x128x8_stage3_warpsize4x2x1_g1_ffma_aligna4_alignc4 Tactic: 0xbb8c3889c7eacd30
[08/10/2023-11:18:03] [V] [TRT] Tactic: 0xbb8c3889c7eacd30 Time: 2.75205
[08/10/2023-11:18:03] [V] [TRT] Conv_64 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_aligna4_alignc4 Tactic: 0xd828f024626fa982
[08/10/2023-11:18:03] [V] [TRT] Tactic: 0xd828f024626fa982 Time: 2.1574
[08/10/2023-11:18:03] [V] [TRT] Conv_64 Set Tactic Name: ampere_scudnn_128x128_relu_medium_nn_v1 Tactic: 0xf067e6205da31c2e
[08/10/2023-11:18:04] [V] [TRT] Tactic: 0xf067e6205da31c2e Time: 2.46324
[08/10/2023-11:18:04] [V] [TRT] Conv_64 Set Tactic Name: ampere_scudnn_128x64_relu_medium_nn_v1 Tactic: 0xf64396b97c889179
[08/10/2023-11:18:04] [V] [TRT] Tactic: 0xf64396b97c889179 Time: 1.93274
[08/10/2023-11:18:04] [V] [TRT] Fastest Tactic: 0x94119b4c514b211a Time: 0.422656
[08/10/2023-11:18:04] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x94119b4c514b211a
[08/10/2023-11:18:04] [V] [TRT] *************** Autotuning format combination: Float(16588800,1,92160,512) -> Float(64800,1,360,2) ***************
[08/10/2023-11:18:04] [V] [TRT] --------------- Timing Runner: Conv_64 (CaskConvolution)
[08/10/2023-11:18:04] [V] [TRT] Conv_64 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_aligna4_alignc4 Tactic: 0x19b688348f983aa0
[08/10/2023-11:18:04] [V] [TRT] Tactic: 0x19b688348f983aa0 Time: 1.63408
[08/10/2023-11:18:04] [V] [TRT] Conv_64 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x64x8_stage3_warpsize2x2x1_g1_ffma_aligna4_alignc4 Tactic: 0x1da91d865428f237
[08/10/2023-11:18:04] [V] [TRT] Tactic: 0x1da91d865428f237 Time: 1.51914
[08/10/2023-11:18:04] [V] [TRT] Conv_64 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x256x8_stage3_warpsize1x4x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0x3f0c846d6379bc98
[08/10/2023-11:18:04] [V] [TRT] Tactic: 0x3f0c846d6379bc98 Time: 4.28563
[08/10/2023-11:18:04] [V] [TRT] Conv_64 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_aligna4_alignc4 Tactic: 0x62835fce994f06dd
[08/10/2023-11:18:04] [V] [TRT] Tactic: 0x62835fce994f06dd Time: 1.43391
[08/10/2023-11:18:04] [V] [TRT] Conv_64 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize256x64x8_stage3_warpsize2x2x1_g1_ffma_aligna4_alignc4 Tactic: 0x8014228ec08b4d49
[08/10/2023-11:18:04] [V] [TRT] Tactic: 0x8014228ec08b4d49 Time: 2.32096
[08/10/2023-11:18:04] [V] [TRT] Conv_64 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0x94a7db94ba744c45
[08/10/2023-11:18:04] [V] [TRT] Tactic: 0x94a7db94ba744c45 Time: 1.464
[08/10/2023-11:18:04] [V] [TRT] Conv_64 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x128x8_stage3_warpsize1x4x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0xd15dd11d64344e83
[08/10/2023-11:18:04] [V] [TRT] Tactic: 0xd15dd11d64344e83 Time: 2.87163
[08/10/2023-11:18:04] [V] [TRT] Conv_64 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x64x8_stage3_warpsize2x2x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0xf48db81f02eca9ee
[08/10/2023-11:18:04] [V] [TRT] Tactic: 0xf48db81f02eca9ee Time: 1.581
[08/10/2023-11:18:04] [V] [TRT] Fastest Tactic: 0x62835fce994f06dd Time: 1.43391
[08/10/2023-11:18:04] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x62835fce994f06dd
[08/10/2023-11:18:04] [V] [TRT] *************** Autotuning format combination: Float(4147200,1:4,23040,128) -> Float(32400,1:4,180,1) ***************
[08/10/2023-11:18:04] [V] [TRT] --------------- Timing Runner: Conv_64 (CaskConvolution)
[08/10/2023-11:18:04] [V] [TRT] Conv_64 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize64x128x32_stage5_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0x112d7e4d280f396d
[08/10/2023-11:18:04] [V] [TRT] Tactic: 0x112d7e4d280f396d Time: 1.22731
[08/10/2023-11:18:04] [V] [TRT] Conv_64 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x64x32_stage5_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0x13cc2ad8dcd32ba2
[08/10/2023-11:18:04] [V] [TRT] Tactic: 0x13cc2ad8dcd32ba2 Time: 0.658071
[08/10/2023-11:18:04] [V] [TRT] Conv_64 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize256x128x32_stage3_warpsize4x2x1_g1_tensor16x8x8_t1r3s3 Tactic: 0x21246c8544eff903
[08/10/2023-11:18:04] [V] [TRT] Tactic: 0x21246c8544eff903 Time: 0.744617
[08/10/2023-11:18:04] [V] [TRT] Conv_64 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x128x32_stage4_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0x25b2b9d5c9d5ca0d
[08/10/2023-11:18:04] [V] [TRT] Tactic: 0x25b2b9d5c9d5ca0d Time: 0.586039
[08/10/2023-11:18:04] [V] [TRT] Conv_64 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage4_warpsize2x2x1_g1_tensor16x8x8_t1r3s3 Tactic: 0x2d80d2b384ab13f1
[08/10/2023-11:18:04] [V] [TRT] Tactic: 0x2d80d2b384ab13f1 Time: 0.512192
[08/10/2023-11:18:04] [V] [TRT] Conv_64 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize256x128x32_stage3_warpsize4x2x1_g1_tensor16x8x8 Tactic: 0x36303f399d3ee4fd
[08/10/2023-11:18:04] [V] [TRT] Tactic: 0x36303f399d3ee4fd Time: 0.635675
[08/10/2023-11:18:04] [V] [TRT] Conv_64 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage4_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0x385bfab9332b1413
[08/10/2023-11:18:04] [V] [TRT] Tactic: 0x385bfab9332b1413 Time: 0.573303
[08/10/2023-11:18:04] [V] [TRT] Conv_64 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x256x32_stage3_warpsize2x4x1_g1_tensor16x8x8 Tactic: 0x482b48242255b8ce
[08/10/2023-11:18:04] [V] [TRT] Tactic: 0x482b48242255b8ce Time: 1.02459
[08/10/2023-11:18:04] [V] [TRT] Conv_64 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x256x32_stage3_warpsize2x4x1_g1_tensor16x8x8_t1r3s3 Tactic: 0x614e89f7852edbc3
[08/10/2023-11:18:04] [V] [TRT] Tactic: 0x614e89f7852edbc3 Time: 0.979534
[08/10/2023-11:18:04] [V] [TRT] Conv_64 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0x65e41d81f093b482
[08/10/2023-11:18:04] [V] [TRT] Tactic: 0x65e41d81f093b482 Time: 0.597262
[08/10/2023-11:18:04] [V] [TRT] Conv_64 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize64x128x32_stage5_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0x65f71607d20d5438
[08/10/2023-11:18:04] [V] [TRT] Tactic: 0x65f71607d20d5438 Time: 0.794318
[08/10/2023-11:18:04] [V] [TRT] Conv_64 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x64x32_stage5_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0x6716429226d146f7
[08/10/2023-11:18:04] [V] [TRT] Tactic: 0x6716429226d146f7 Time: 0.42117
[08/10/2023-11:18:04] [V] [TRT] Conv_64 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize64x32x64_stage5_warpsize2x2x1_g1_tensor16x8x8_t1r3s3 Tactic: 0x679a281f7660c436
[08/10/2023-11:18:04] [V] [TRT] Tactic: 0x679a281f7660c436 Time: 0.397417
[08/10/2023-11:18:04] [V] [TRT] Conv_64 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize64x32x64_stage5_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0x6e2a3c7a7fc5e4e1
[08/10/2023-11:18:04] [V] [TRT] Tactic: 0x6e2a3c7a7fc5e4e1 Time: 0.423319
[08/10/2023-11:18:04] [V] [TRT] Conv_64 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x64x32_stage5_warpsize2x2x1_g1_tensor16x8x8_t1r3s3 Tactic: 0x7aa21282feb15f1a
[08/10/2023-11:18:04] [V] [TRT] Tactic: 0x7aa21282feb15f1a Time: 0.419973
[08/10/2023-11:18:04] [V] [TRT] Conv_64 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage4_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0x818413b69874bb35
[08/10/2023-11:18:04] [V] [TRT] Tactic: 0x818413b69874bb35 Time: 0.569353
[08/10/2023-11:18:04] [V] [TRT] Conv_64 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x256x32_stage3_warpsize2x4x1_g1_tensor16x8x8 Tactic: 0x998c97842e775a17
[08/10/2023-11:18:04] [V] [TRT] Tactic: 0x998c97842e775a17 Time: 1.03422
[08/10/2023-11:18:04] [V] [TRT] Conv_64 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8_t1r3s3 Tactic: 0x999e005e3b016ea6
[08/10/2023-11:18:04] [V] [TRT] Tactic: 0x999e005e3b016ea6 Time: 0.537285
[08/10/2023-11:18:04] [V] [TRT] Conv_64 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize256x64x32_stage3_warpsize4x2x1_g1_tensor16x8x8_t1r3s3 Tactic: 0xa9a06d0633580c0c
[08/10/2023-11:18:04] [V] [TRT] Tactic: 0xa9a06d0633580c0c Time: 0.338258
[08/10/2023-11:18:04] [V] [TRT] Conv_64 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0xb443c221fcb1565b
[08/10/2023-11:18:04] [V] [TRT] Tactic: 0xb443c221fcb1565b Time: 0.555182
[08/10/2023-11:18:04] [V] [TRT] Conv_64 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage3_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0xb47d7d926de02dee
[08/10/2023-11:18:04] [V] [TRT] Tactic: 0xb47d7d926de02dee Time: 0.425184
[08/10/2023-11:18:04] [V] [TRT] Conv_64 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize256x64x32_stage3_warpsize4x2x1_g1_tensor16x8x8 Tactic: 0xba0185279ccb2b85
[08/10/2023-11:18:04] [V] [TRT] Tactic: 0xba0185279ccb2b85 Time: 0.376613
[08/10/2023-11:18:04] [V] [TRT] Conv_64 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage3_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0xc0a715d897e240bb
[08/10/2023-11:18:04] [V] [TRT] Tactic: 0xc0a715d897e240bb Time: 0.423977
[08/10/2023-11:18:04] [V] [TRT] Conv_64 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize256x64x32_stage3_warpsize4x2x1_g1_tensor16x8x8 Tactic: 0xcedbed6d66c946d0
[08/10/2023-11:18:04] [V] [TRT] Tactic: 0xcedbed6d66c946d0 Time: 0.352837
[08/10/2023-11:18:04] [V] [TRT] Conv_64 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize64x128x32_stage5_warpsize2x2x1_g1_tensor16x8x8_t1r3s3 Tactic: 0xd4428a7355769d7d
[08/10/2023-11:18:04] [V] [TRT] Tactic: 0xd4428a7355769d7d Time: 0.764718
[08/10/2023-11:18:04] [V] [TRT] Conv_64 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize64x32x64_stage5_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0xd7f5d575d49a4bc7
[08/10/2023-11:18:04] [V] [TRT] Tactic: 0xd7f5d575d49a4bc7 Time: 0.477312
[08/10/2023-11:18:04] [V] [TRT] Conv_64 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x128x32_stage4_warpsize2x2x1_g1_tensor16x8x8_t1r3s3 Tactic: 0xd920b33c9bd27143
[08/10/2023-11:18:05] [V] [TRT] Tactic: 0xd920b33c9bd27143 Time: 1.07217
[08/10/2023-11:18:05] [V] [TRT] Conv_64 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize256x128x32_stage3_warpsize4x2x1_g1_tensor16x8x8 Tactic: 0xe797e099911c0624
[08/10/2023-11:18:05] [V] [TRT] Tactic: 0xe797e099911c0624 Time: 0.749111
[08/10/2023-11:18:05] [V] [TRT] Conv_64 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage3_warpsize2x2x1_g1_tensor16x8x8_t1r3s3 Tactic: 0xeb65d4c1e8fa2294
[08/10/2023-11:18:05] [V] [TRT] Tactic: 0xeb65d4c1e8fa2294 Time: 0.404498
[08/10/2023-11:18:05] [V] [TRT] Conv_64 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x128x32_stage4_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0xf4156675c5f728d4
[08/10/2023-11:18:05] [V] [TRT] Tactic: 0xf4156675c5f728d4 Time: 1.1619
[08/10/2023-11:18:05] [V] [TRT] Fastest Tactic: 0xa9a06d0633580c0c Time: 0.338258
[08/10/2023-11:18:05] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0xa9a06d0633580c0c
[08/10/2023-11:18:05] [V] [TRT] *************** Autotuning format combination: Half(16588800,32400,180,1) -> Half(64800,32400,180,1) ***************
[08/10/2023-11:18:05] [V] [TRT] --------------- Timing Runner: Conv_64 (CudnnConvolution)
[08/10/2023-11:18:05] [V] [TRT] Tactic: 0x0000000000000000 Time: 3.67528
[08/10/2023-11:18:05] [V] [TRT] Tactic: 0x0000000000000001 Time: 1.16008
[08/10/2023-11:18:05] [V] [TRT] Tactic: 0x0000000000000002 Time: 3.58691
[08/10/2023-11:18:05] [V] [TRT] Tactic: 0x0000000000000004 Time: 5.15971
[08/10/2023-11:18:05] [V] [TRT] Tactic: 0x0000000000000005 Time: 1.33434
[08/10/2023-11:18:05] [V] [TRT] Tactic: 0x0000000000000006 Time: 5.12247
[08/10/2023-11:18:05] [V] [TRT] Tactic: 0x0000000000000038 Time: 3.13051
[08/10/2023-11:18:05] [V] [TRT] Tactic: 0x000000000000003a Time: 3.57537
[08/10/2023-11:18:05] [V] [TRT] Tactic: 0x000000000000003c Time: 4.41266
[08/10/2023-11:18:05] [V] [TRT] Tactic: 0x000000000000003d Time: 1.50005
[08/10/2023-11:18:05] [V] [TRT] Tactic: 0x000000000000003e Time: 6.30256
[08/10/2023-11:18:05] [V] [TRT] Fastest Tactic: 0x0000000000000001 Time: 1.16008
[08/10/2023-11:18:05] [V] [TRT] --------------- Timing Runner: Conv_64 (CaskConvolution)
[08/10/2023-11:18:05] [V] [TRT] CaskConvolution has no valid tactics for this config, skipping
[08/10/2023-11:18:05] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CudnnConvolution Tactic: 0x0000000000000001
[08/10/2023-11:18:05] [V] [TRT] *************** Autotuning format combination: Half(8294400,32400:2,180,1) -> Half(32400,32400:2,180,1) ***************
[08/10/2023-11:18:05] [V] [TRT] --------------- Timing Runner: Conv_64 (FusedConvActConvolution)
[08/10/2023-11:18:05] [V] [TRT] FusedConvActConvolution has no valid tactics for this config, skipping
[08/10/2023-11:18:05] [V] [TRT] --------------- Timing Runner: Conv_64 (CaskConvolution)
[08/10/2023-11:18:05] [V] [TRT] Conv_64 Set Tactic Name: ampere_fp16x2_hcudnn_fp16x2_128x32_relu_medium_nn_v1 Tactic: 0x1e7896ba71ef1635
[08/10/2023-11:18:05] [V] [TRT] Tactic: 0x1e7896ba71ef1635 Time: 0.702418
[08/10/2023-11:18:05] [V] [TRT] Conv_64 Set Tactic Name: ampere_fp16x2_hcudnn_fp16x2_128x64_relu_large_nn_v1 Tactic: 0x2f735ffbb05a30fd
[08/10/2023-11:18:05] [V] [TRT] Tactic: 0x2f735ffbb05a30fd Time: 1.77194
[08/10/2023-11:18:05] [V] [TRT] Conv_64 Set Tactic Name: ampere_fp16x2_hcudnn_fp16x2_128x128_relu_medium_nn_v1 Tactic: 0x360278e347d63410
[08/10/2023-11:18:05] [V] [TRT] Tactic: 0x360278e347d63410 Time: 2.53877
[08/10/2023-11:18:05] [V] [TRT] Conv_64 Set Tactic Name: ampere_fp16x2_hcudnn_fp16x2_128x64_relu_medium_nn_v1 Tactic: 0x4cfee77ea8c324db
[08/10/2023-11:18:05] [V] [TRT] Tactic: 0x4cfee77ea8c324db Time: 0.944969
[08/10/2023-11:18:05] [V] [TRT] Conv_64 Set Tactic Name: ampere_fp16x2_hcudnn_fp16x2_128x64_relu_small_nn_v1 Tactic: 0x540fde3a7bee53dc
[08/10/2023-11:18:05] [V] [TRT] Tactic: 0x540fde3a7bee53dc Time: 1.73161
[08/10/2023-11:18:05] [V] [TRT] Conv_64 Set Tactic Name: ampere_fp16x2_hcudnn_fp16x2_128x128_relu_large_nn_v1 Tactic: 0x91f7e9c0851ad67c
[08/10/2023-11:18:05] [V] [TRT] Tactic: 0x91f7e9c0851ad67c Time: 1.93538
[08/10/2023-11:18:05] [V] [TRT] Conv_64 Set Tactic Name: ampere_fp16x2_hcudnn_fp16x2_128x32_relu_large_nn_v1 Tactic: 0xb837f96ef306f686
[08/10/2023-11:18:05] [V] [TRT] Tactic: 0xb837f96ef306f686 Time: 0.464599
[08/10/2023-11:18:05] [V] [TRT] Conv_64 Set Tactic Name: ampere_fp16x2_hcudnn_fp16x2_128x32_relu_small_nn_v1 Tactic: 0xc34b78af38b295a7
[08/10/2023-11:18:05] [V] [TRT] Tactic: 0xc34b78af38b295a7 Time: 0.555122
[08/10/2023-11:18:05] [V] [TRT] Conv_64 Set Tactic Name: ampere_fp16x2_hcudnn_winograd_fp16x2_128x128_ldg1_ldg4_relu_tile148t_nt_v1 Tactic: 0xc754debea88ae0b7
[08/10/2023-11:18:05] [V] [TRT] Tactic: 0xc754debea88ae0b7 Time: 0.244768
[08/10/2023-11:18:05] [V] [TRT] Conv_64 Set Tactic Name: ampere_fp16x2_hcudnn_fp16x2_128x128_relu_small_nn_v1 Tactic: 0xea8b68014eaeb55d
[08/10/2023-11:18:05] [V] [TRT] Tactic: 0xea8b68014eaeb55d Time: 2.81957
[08/10/2023-11:18:05] [V] [TRT] Fastest Tactic: 0xc754debea88ae0b7 Time: 0.244768
[08/10/2023-11:18:05] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0xc754debea88ae0b7
[08/10/2023-11:18:05] [V] [TRT] *************** Autotuning format combination: Half(2073600,1:8,11520,64) -> Float(64800,32400,180,1) ***************
[08/10/2023-11:18:05] [V] [TRT] --------------- Timing Runner: Conv_64 (CaskConvolution)
[08/10/2023-11:18:05] [V] [TRT] CaskConvolution has no valid tactics for this config, skipping
[08/10/2023-11:18:05] [V] [TRT] *************** Autotuning format combination: Half(2073600,1:8,11520,64) -> Half(32400,1:8,180,1) ***************
[08/10/2023-11:18:05] [V] [TRT] --------------- Timing Runner: Conv_64 (CudaDepthwiseConvolution)
[08/10/2023-11:18:05] [V] [TRT] CudaDepthwiseConvolution has no valid tactics for this config, skipping
[08/10/2023-11:18:05] [V] [TRT] --------------- Timing Runner: Conv_64 (CaskConvolution)
[08/10/2023-11:18:05] [V] [TRT] Conv_64 Set Tactic Name: ampere_h16816cudnn_256x128_ldg8_relu_exp_medium_nhwc_tn_v1 Tactic: 0x03896956a39a1203
[08/10/2023-11:18:05] [V] [TRT] Tactic: 0x03896956a39a1203 Time: 0.541531
[08/10/2023-11:18:05] [V] [TRT] Conv_64 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x128x32_stage3_warpsize4x2x1_g1_tensor16x8x16_t1r3s3 Tactic: 0x048d6d0400f33439
[08/10/2023-11:18:05] [V] [TRT] Tactic: 0x048d6d0400f33439 Time: 0.282313
[08/10/2023-11:18:05] [V] [TRT] Conv_64 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x64x64_stage3_warpsize2x2x1_g1_tensor16x8x16_t1r3s3 Tactic: 0x05b6220f243edacd
[08/10/2023-11:18:05] [V] [TRT] Tactic: 0x05b6220f243edacd Time: 0.150811
[08/10/2023-11:18:05] [V] [TRT] Conv_64 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x64x32_stage3_warpsize4x1x1_g1_tensor16x8x16_t1r3s3_aACCESS Tactic: 0x0866ddee325d07a6
[08/10/2023-11:18:05] [V] [TRT] Tactic: 0x0866ddee325d07a6 Time: 0.162277
[08/10/2023-11:18:05] [V] [TRT] Conv_64 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x128x32_stage4_warpsize2x2x1_g1_tensor16x8x16 Tactic: 0x0e07ff4f4f7c1ac9
[08/10/2023-11:18:05] [V] [TRT] Tactic: 0x0e07ff4f4f7c1ac9 Time: 0.275904
[08/10/2023-11:18:05] [V] [TRT] Conv_64 Set Tactic Name: ampere_h16816cudnn_256x128_ldg8_relu_exp_stages_64x3_small_nhwc_tn_v1 Tactic: 0x0e199750c30c6928
[08/10/2023-11:18:05] [V] [TRT] Tactic: 0x0e199750c30c6928 Time: 0.306944
[08/10/2023-11:18:05] [V] [TRT] Conv_64 Set Tactic Name: ampere_h1688cudnn_128x128_ldg8_relu_exp_medium_nhwc_tn_v1 Tactic: 0x100c1ad308e08d35
[08/10/2023-11:18:05] [V] [TRT] Tactic: 0x100c1ad308e08d35 Time: 0.353591
[08/10/2023-11:18:05] [V] [TRT] Conv_64 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x128x32_stage5_warpsize2x2x1_g1_tensor16x8x16_t1r3s3 Tactic: 0x17ebf0c9f418f10a
[08/10/2023-11:18:05] [V] [TRT] Tactic: 0x17ebf0c9f418f10a Time: 0.29467
[08/10/2023-11:18:05] [V] [TRT] Conv_64 Set Tactic Name: ampere_h16816cudnn_128x64_ldg8_relu_exp_stages_64x4_medium_nhwc_tn_v1 Tactic: 0x196cbc423a9bb69e
[08/10/2023-11:18:05] [V] [TRT] Tactic: 0x196cbc423a9bb69e Time: 0.249614
[08/10/2023-11:18:05] [V] [TRT] Conv_64 Set Tactic Name: ampere_h16816cudnn_128x64_sliced1x2_ldg8_relu_exp_stages_64x3_small_nhwc_tn_v1 Tactic: 0x19822de884f42a6a
[08/10/2023-11:18:05] [V] [TRT] Tactic: 0x19822de884f42a6a Time: 0.163063
[08/10/2023-11:18:05] [V] [TRT] Conv_64 Set Tactic Name: ampere_h16816cudnn_64x64_sliced1x2_ldg8_relu_exp_stages_64x5_small_nhwc_tn_v1 Tactic: 0x1a5ba808ad4cc5a8
[08/10/2023-11:18:05] [V] [TRT] Tactic: 0x1a5ba808ad4cc5a8 Time: 0.24459
[08/10/2023-11:18:05] [V] [TRT] Conv_64 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x64x64_stage3_warpsize2x2x1_g1_tensor16x8x16 Tactic: 0x21b295c0c8f6c95a
[08/10/2023-11:18:05] [V] [TRT] Tactic: 0x21b295c0c8f6c95a Time: 0.171543
[08/10/2023-11:18:05] [V] [TRT] Conv_64 Set Tactic Name: ampere_h16816cudnn_128x128_ldg8_relu_exp_medium_nhwc_tn_v1 Tactic: 0x234580e8a194335c
[08/10/2023-11:18:05] [V] [TRT] Tactic: 0x234580e8a194335c Time: 0.251899
[08/10/2023-11:18:05] [V] [TRT] Conv_64 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x128x32_stage4_warpsize2x2x1_g1_tensor16x8x16_t1r3s3_aACCESS Tactic: 0x245530c34bd6090f
[08/10/2023-11:18:06] [V] [TRT] Tactic: 0x245530c34bd6090f Time: 0.256288
[08/10/2023-11:18:06] [V] [TRT] Conv_64 Set Tactic Name: ampere_h16816cudnn_64x64_sliced1x2_ldg8_relu_exp_stages_64x6_small_nhwc_tn_v1 Tactic: 0x24e01e7405cfdfe9
[08/10/2023-11:18:06] [V] [TRT] Tactic: 0x24e01e7405cfdfe9 Time: 0.359054
[08/10/2023-11:18:06] [V] [TRT] Conv_64 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x64x64_stage3_warpsize2x2x1_g1_tensor16x8x16_t1r3s3_aACCESS Tactic: 0x263a38afd75e3a43
[08/10/2023-11:18:06] [V] [TRT] Tactic: 0x263a38afd75e3a43 Time: 0.162725
[08/10/2023-11:18:06] [V] [TRT] Conv_64 Set Tactic Name: ampere_h16816cudnn_64x128_sliced1x2_ldg8_relu_exp_stages_64x4_large_nhwc_tn_v1 Tactic: 0x2721a7f18c2700db
[08/10/2023-11:18:06] [V] [TRT] Tactic: 0x2721a7f18c2700db Time: 0.487694
[08/10/2023-11:18:06] [V] [TRT] Conv_64 Set Tactic Name: ampere_h16816cudnn_128x64_sliced1x2_ldg8_relu_exp_stages_64x3_medium_nhwc_tn_v1 Tactic: 0x29329b5741ea05f2
[08/10/2023-11:18:06] [V] [TRT] Tactic: 0x29329b5741ea05f2 Time: 0.165056
[08/10/2023-11:18:06] [V] [TRT] Conv_64 Set Tactic Name: ampere_h16816cudnn_128x128_ldg8_relu_exp_large_nhwc_tn_v1 Tactic: 0x2970ae65966d569d
[08/10/2023-11:18:06] [V] [TRT] Tactic: 0x2970ae65966d569d Time: 0.250391
[08/10/2023-11:18:06] [V] [TRT] Conv_64 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x256x64_stage3_warpsize1x4x1_g1_tensor16x8x16 Tactic: 0x29dc14520e0e4eb6
[08/10/2023-11:18:06] [V] [TRT] Tactic: 0x29dc14520e0e4eb6 Time: 0.625856
[08/10/2023-11:18:06] [V] [TRT] Conv_64 Set Tactic Name: ampere_h16816cudnn_256x64_ldg8_relu_exp_stages_64x3_large_nhwc_tn_v1 Tactic: 0x2cb4a662694e89d3
[08/10/2023-11:18:06] [V] [TRT] Tactic: 0x2cb4a662694e89d3 Time: 0.196448
[08/10/2023-11:18:06] [V] [TRT] Conv_64 Set Tactic Name: ampere_h16816cudnn_128x128_ldg8_relu_exp_stages_64x3_medium_nhwc_tn_v1 Tactic: 0x2eaa2202de9404d6
[08/10/2023-11:18:06] [V] [TRT] Tactic: 0x2eaa2202de9404d6 Time: 0.348791
[08/10/2023-11:18:06] [V] [TRT] Conv_64 Set Tactic Name: ampere_h1688cudnn_256x128_ldg8_relu_exp_medium_nhwc_tn_v1 Tactic: 0x30c0f36d0aeeac6a
[08/10/2023-11:18:06] [V] [TRT] Tactic: 0x30c0f36d0aeeac6a Time: 0.372704
[08/10/2023-11:18:06] [V] [TRT] Conv_64 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x64x64_stage4_warpsize2x2x1_g1_tensor16x8x16_t1r3s3_aACCESS Tactic: 0x30e8a8d7a953e5e9
[08/10/2023-11:18:06] [V] [TRT] Tactic: 0x30e8a8d7a953e5e9 Time: 0.205262
[08/10/2023-11:18:06] [V] [TRT] Conv_64 Set Tactic Name: ampere_h16816cudnn_128x64_ldg8_relu_exp_stages_64x4_large_nhwc_tn_v1 Tactic: 0x3197d5044d71e98e
[08/10/2023-11:18:06] [V] [TRT] Tactic: 0x3197d5044d71e98e Time: 0.502459
[08/10/2023-11:18:06] [V] [TRT] Conv_64 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x64x64_stage4_warpsize2x2x1_g1_tensor16x8x16_t1r3s3 Tactic: 0x31d93dc22d2af081
[08/10/2023-11:18:06] [V] [TRT] Tactic: 0x31d93dc22d2af081 Time: 0.201207
[08/10/2023-11:18:06] [V] [TRT] Conv_64 Set Tactic Name: ampere_h1688cudnn_128x128_ldg8_relu_exp_small_nhwc_tn_v1 Tactic: 0x3369260c04f9ad73
[08/10/2023-11:18:06] [V] [TRT] Tactic: 0x3369260c04f9ad73 Time: 0.332846
[08/10/2023-11:18:06] [V] [TRT] Conv_64 Set Tactic Name: ampere_h16816cudnn_64x128_ldg8_relu_exp_stages_64x4_small_nhwc_tn_v1 Tactic: 0x3b5fa0f2a8fc2410
[08/10/2023-11:18:06] [V] [TRT] Tactic: 0x3b5fa0f2a8fc2410 Time: 0.429088
[08/10/2023-11:18:06] [V] [TRT] Conv_64 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x32x32_stage4_warpsize4x1x1_g1_tensor16x8x16_t1r3s3 Tactic: 0x3e7eb35b91b9fa63
[08/10/2023-11:18:06] [V] [TRT] Tactic: 0x3e7eb35b91b9fa63 Time: 0.113248
[08/10/2023-11:18:06] [V] [TRT] Conv_64 Set Tactic Name: ampere_h16816cudnn_256x64_ldg8_relu_exp_stages_64x3_medium_nhwc_tn_v1 Tactic: 0x3f031df0e579d52c
[08/10/2023-11:18:06] [V] [TRT] Tactic: 0x3f031df0e579d52c Time: 0.197257
[08/10/2023-11:18:06] [V] [TRT] Conv_64 Set Tactic Name: ampere_h16816cudnn_256x64_sliced1x2_ldg8_relu_exp_small_nhwc_tn_v1 Tactic: 0x42a5b8709d0039be
[08/10/2023-11:18:06] [V] [TRT] Tactic: 0x42a5b8709d0039be Time: 0.210469
[08/10/2023-11:18:06] [V] [TRT] Conv_64 Set Tactic Name: ampere_h16816cudnn_256x128_ldg8_relu_exp_small_nhwc_tn_v1 Tactic: 0x463794ee4acffd1f
[08/10/2023-11:18:06] [V] [TRT] Tactic: 0x463794ee4acffd1f Time: 0.300037
[08/10/2023-11:18:06] [V] [TRT] Conv_64 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x64x64_stage4_warpsize2x2x1_g1_tensor16x8x16 Tactic: 0x4a81ea1e51436a30
[08/10/2023-11:18:06] [V] [TRT] Tactic: 0x4a81ea1e51436a30 Time: 0.220069
[08/10/2023-11:18:06] [V] [TRT] Conv_64 Set Tactic Name: ampere_h1688cudnn_256x64_sliced1x2_ldg8_relu_exp_small_nhwc_tn_v1 Tactic: 0x4aed1956bd10a795
[08/10/2023-11:18:06] [V] [TRT] Tactic: 0x4aed1956bd10a795 Time: 0.235511
[08/10/2023-11:18:06] [V] [TRT] Conv_64 Set Tactic Name: ampere_h16816cudnn_64x128_sliced1x2_ldg8_relu_exp_stages_64x3_large_nhwc_tn_v1 Tactic: 0x4fc05923455fc266
[08/10/2023-11:18:06] [V] [TRT] Tactic: 0x4fc05923455fc266 Time: 0.295959
[08/10/2023-11:18:06] [V] [TRT] Conv_64 Set Tactic Name: ampere_h16816cudnn_256x64_ldg8_relu_exp_large_nhwc_tn_v1 Tactic: 0x5013c38f55afa9ba
[08/10/2023-11:18:06] [V] [TRT] Tactic: 0x5013c38f55afa9ba Time: 0.173961
[08/10/2023-11:18:06] [V] [TRT] Conv_64 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x64x32_stage3_warpsize4x1x1_g1_tensor16x8x16_t1r3s3 Tactic: 0x529f4431bdae94f5
[08/10/2023-11:18:06] [V] [TRT] Tactic: 0x529f4431bdae94f5 Time: 0.142921
[08/10/2023-11:18:06] [V] [TRT] Conv_64 Set Tactic Name: ampere_h16816cudnn_64x128_ldg8_relu_exp_stages_64x3_small_nhwc_tn_v1 Tactic: 0x53be5e206184e6ad
[08/10/2023-11:18:06] [V] [TRT] Tactic: 0x53be5e206184e6ad Time: 0.290903
[08/10/2023-11:18:06] [V] [TRT] Conv_64 Set Tactic Name: ampere_h16816cudnn_64x64_ldg8_relu_exp_stages_64x5_large_nhwc_tn_v1 Tactic: 0x544bff7ff9c5d908
[08/10/2023-11:18:06] [V] [TRT] Tactic: 0x544bff7ff9c5d908 Time: 0.256357
[08/10/2023-11:18:06] [V] [TRT] Conv_64 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x64x64_stage3_warpsize2x2x1_g1_tensor16x8x16 Tactic: 0x5568fd8a32f4a40f
[08/10/2023-11:18:06] [V] [TRT] Tactic: 0x5568fd8a32f4a40f Time: 0.155419
[08/10/2023-11:18:06] [V] [TRT] Conv_64 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x128x64_stage3_warpsize2x2x1_g1_tensor16x8x16 Tactic: 0x57c9a5ff682354a6
[08/10/2023-11:18:06] [V] [TRT] Tactic: 0x57c9a5ff682354a6 Time: 0.324896
[08/10/2023-11:18:06] [V] [TRT] Conv_64 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x32x64_stage5_warpsize2x2x1_g1_tensor16x8x16 Tactic: 0x5820b3dda403c4d0
[08/10/2023-11:18:06] [V] [TRT] Tactic: 0x5820b3dda403c4d0 Time: 0.179163
[08/10/2023-11:18:06] [V] [TRT] Conv_64 Set Tactic Name: ampere_h16816cudnn_128x128_ldg8_relu_exp_stages_64x3_small_nhwc_tn_v1 Tactic: 0x58816bf2e9c36afb
[08/10/2023-11:18:06] [V] [TRT] Tactic: 0x58816bf2e9c36afb Time: 0.340713
[08/10/2023-11:18:06] [V] [TRT] Conv_64 Set Tactic Name: ampere_h16816cudnn_128x64_ldg8_relu_exp_stages_64x3_large_nhwc_tn_v1 Tactic: 0x59762bd684092b33
[08/10/2023-11:18:06] [V] [TRT] Tactic: 0x59762bd684092b33 Time: 0.176096
[08/10/2023-11:18:06] [V] [TRT] Conv_64 Set Tactic Name: ampere_h16816cudnn_128x64_sliced1x2_ldg8_relu_exp_stages_64x4_medium_nhwc_tn_v1 Tactic: 0x59c5c647b4c76593
[08/10/2023-11:18:06] [V] [TRT] Tactic: 0x59c5c647b4c76593 Time: 0.253577
[08/10/2023-11:18:06] [V] [TRT] Conv_64 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x128x64_stage3_warpsize4x2x1_g1_tensor16x8x16 Tactic: 0x5a55274960724ba8
[08/10/2023-11:18:06] [V] [TRT] Tactic: 0x5a55274960724ba8 Time: 0.302683
[08/10/2023-11:18:06] [V] [TRT] Conv_64 Set Tactic Name: ampere_h1688cudnn_256x128_ldg8_relu_exp_large_nhwc_tn_v1 Tactic: 0x5c2e1c87d85b06f1
[08/10/2023-11:18:06] [V] [TRT] Tactic: 0x5c2e1c87d85b06f1 Time: 0.374478
[08/10/2023-11:18:06] [V] [TRT] Conv_64 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x256x64_stage3_warpsize1x4x1_g1_tensor16x8x16 Tactic: 0x5d067c18f40c23e3
[08/10/2023-11:18:06] [V] [TRT] Tactic: 0x5d067c18f40c23e3 Time: 0.637664
[08/10/2023-11:18:06] [V] [TRT] Conv_64 Set Tactic Name: ampere_h16816cudnn_64x64_ldg8_relu_exp_stages_64x5_medium_nhwc_tn_v1 Tactic: 0x5f31c22ec167f384
[08/10/2023-11:18:06] [V] [TRT] Tactic: 0x5f31c22ec167f384 Time: 0.263735
[08/10/2023-11:18:06] [V] [TRT] Conv_64 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x128x32_stage4_warpsize2x2x1_g1_tensor16x8x16_t1r3s3 Tactic: 0x60c3421152ef8e10
[08/10/2023-11:18:06] [V] [TRT] Tactic: 0x60c3421152ef8e10 Time: 0.247474
[08/10/2023-11:18:06] [V] [TRT] Conv_64 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x128x32_stage3_warpsize4x2x1_g1_tensor16x8x16 Tactic: 0x60da8c7151d91e47
[08/10/2023-11:18:06] [V] [TRT] Tactic: 0x60da8c7151d91e47 Time: 0.284558
[08/10/2023-11:18:06] [V] [TRT] Conv_64 Set Tactic Name: ampere_h1688cudnn_256x64_ldg8_relu_exp_medium_nhwc_tn_v1 Tactic: 0x6426696f872a3b13
[08/10/2023-11:18:06] [V] [TRT] Tactic: 0x6426696f872a3b13 Time: 0.223959
[08/10/2023-11:18:06] [V] [TRT] Conv_64 Set Tactic Name: ampere_h16816cudnn_128x64_ldg8_relu_exp_stages_64x3_medium_nhwc_tn_v1 Tactic: 0x699be152cfb6d6ff
[08/10/2023-11:18:06] [V] [TRT] Tactic: 0x699be152cfb6d6ff Time: 0.179227
[08/10/2023-11:18:06] [V] [TRT] Conv_64 Set Tactic Name: ampere_h16816cudnn_64x64_ldg8_relu_exp_stages_64x6_large_nhwc_tn_v1 Tactic: 0x6af049035146c349
[08/10/2023-11:18:06] [V] [TRT] Tactic: 0x6af049035146c349 Time: 0.38064
[08/10/2023-11:18:06] [V] [TRT] Conv_64 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x256x32_stage3_warpsize2x4x1_g1_tensor16x8x16 Tactic: 0x7005d10718f6c22d
[08/10/2023-11:18:06] [V] [TRT] Tactic: 0x7005d10718f6c22d Time: 0.522976
[08/10/2023-11:18:06] [V] [TRT] Conv_64 Set Tactic Name: ampere_h16816cudnn_64x64_sliced1x2_ldg8_relu_exp_stages_64x5_medium_nhwc_tn_v1 Tactic: 0x706f08da35c795a5
[08/10/2023-11:18:06] [V] [TRT] Tactic: 0x706f08da35c795a5 Time: 0.249897
[08/10/2023-11:18:06] [V] [TRT] Conv_64 Set Tactic Name: ampere_h16816cudnn_128x64_sliced1x2_ldg8_relu_exp_stages_64x4_small_nhwc_tn_v1 Tactic: 0x7163d33a4d8ce8d7
[08/10/2023-11:18:06] [V] [TRT] Tactic: 0x7163d33a4d8ce8d7 Time: 0.243296
[08/10/2023-11:18:06] [V] [TRT] Conv_64 Set Tactic Name: ampere_h1688cudnn_256x64_ldg8_relu_exp_small_nhwc_tn_v1 Tactic: 0x7aad3976677d7155
[08/10/2023-11:18:06] [V] [TRT] Tactic: 0x7aad3976677d7155 Time: 0.207525
[08/10/2023-11:18:06] [V] [TRT] Conv_64 Set Tactic Name: ampere_h16816cudnn_64x64_sliced1x2_ldg8_relu_exp_stages_64x5_large_nhwc_tn_v1 Tactic: 0x8015519605ab9963
[08/10/2023-11:18:06] [V] [TRT] Tactic: 0x8015519605ab9963 Time: 0.251662
[08/10/2023-11:18:06] [V] [TRT] Conv_64 Set Tactic Name: ampere_h16816cudnn_256x64_sliced1x2_ldg8_relu_exp_medium_nhwc_tn_v1 Tactic: 0x82f8a2214b0b4178
[08/10/2023-11:18:06] [V] [TRT] Tactic: 0x82f8a2214b0b4178 Time: 0.231479
[08/10/2023-11:18:06] [V] [TRT] Conv_64 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x32x32_stage4_warpsize4x1x1_g1_tensor16x8x16 Tactic: 0x834e11ecd4ab9454
[08/10/2023-11:18:06] [V] [TRT] Tactic: 0x834e11ecd4ab9454 Time: 0.115392
[08/10/2023-11:18:06] [V] [TRT] Conv_64 Set Tactic Name: ampere_h16816cudnn_128x64_sliced1x2_ldg8_relu_exp_stages_64x3_large_nhwc_tn_v1 Tactic: 0x83ccd4762c1376a1
[08/10/2023-11:18:06] [V] [TRT] Tactic: 0x83ccd4762c1376a1 Time: 0.168453
[08/10/2023-11:18:06] [V] [TRT] Conv_64 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x64x64_stage3_warpsize4x1x1_g1_tensor16x8x16_t1r3s3 Tactic: 0x84ee897dd1f4d2aa
[08/10/2023-11:18:06] [V] [TRT] Tactic: 0x84ee897dd1f4d2aa Time: 0.185221
[08/10/2023-11:18:06] [V] [TRT] Conv_64 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x128x64_stage3_warpsize2x2x1_g1_tensor16x8x16 Tactic: 0x866e7a5f6401b67f
[08/10/2023-11:18:06] [V] [TRT] Tactic: 0x866e7a5f6401b67f Time: 0.333275
[08/10/2023-11:18:06] [V] [TRT] Conv_64 Set Tactic Name: ampere_h16816cudnn_64x64_ldg8_relu_exp_stages_64x6_medium_nhwc_tn_v1 Tactic: 0x88971eec55aba850
[08/10/2023-11:18:06] [V] [TRT] Tactic: 0x88971eec55aba850 Time: 0.379689
[08/10/2023-11:18:06] [V] [TRT] Conv_64 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x128x64_stage3_warpsize4x2x1_g1_tensor16x8x16 Tactic: 0x8bf2f8e96c50a971
[08/10/2023-11:18:06] [V] [TRT] Tactic: 0x8bf2f8e96c50a971 Time: 0.296823
[08/10/2023-11:18:06] [V] [TRT] Conv_64 Set Tactic Name: ampere_h16816cudnn_256x128_ldg8_relu_exp_stages_64x3_large_nhwc_tn_v1 Tactic: 0x94576ece6beb35e3
[08/10/2023-11:18:06] [V] [TRT] Tactic: 0x94576ece6beb35e3 Time: 0.309399
[08/10/2023-11:18:06] [V] [TRT] Conv_64 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x64x64_stage3_warpsize4x1x1_g1_tensor16x8x16 Tactic: 0x9fff6e65019cc310
[08/10/2023-11:18:06] [V] [TRT] Tactic: 0x9fff6e65019cc310 Time: 0.186359
[08/10/2023-11:18:06] [V] [TRT] Conv_64 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x32x64_stage5_warpsize2x2x1_g1_tensor16x8x16_t1r3s3 Tactic: 0xa033e20ae9f412b2
[08/10/2023-11:18:06] [V] [TRT] Tactic: 0xa033e20ae9f412b2 Time: 0.163995
[08/10/2023-11:18:06] [V] [TRT] Conv_64 Set Tactic Name: ampere_h16816cudnn_64x128_ldg8_relu_exp_stages_64x4_large_nhwc_tn_v1 Tactic: 0xa111596c001b78db
[08/10/2023-11:18:06] [V] [TRT] Tactic: 0xa111596c001b78db Time: 0.442491
[08/10/2023-11:18:06] [V] [TRT] Conv_64 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x256x32_stage3_warpsize2x4x1_g1_tensor16x8x16 Tactic: 0xa1a20ea714d420f4
[08/10/2023-11:18:06] [V] [TRT] Tactic: 0xa1a20ea714d420f4 Time: 0.518414
[08/10/2023-11:18:06] [V] [TRT] Conv_64 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x32x32_stage4_warpsize4x1x1_g1_tensor16x8x16_t1r3s3 Tactic: 0xa40cb43c296a36a8
[08/10/2023-11:18:06] [V] [TRT] Tactic: 0xa40cb43c296a36a8 Time: 0.118843
[08/10/2023-11:18:06] [V] [TRT] Conv_64 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x64x32_stage3_warpsize4x1x1_g1_tensor16x8x16 Tactic: 0xa570c55d303796ff
[08/10/2023-11:18:06] [V] [TRT] Tactic: 0xa570c55d303796ff Time: 0.154235
[08/10/2023-11:18:06] [V] [TRT] Conv_64 Set Tactic Name: ampere_h16816cudnn_64x64_sliced1x2_ldg8_relu_exp_stages_64x6_medium_nhwc_tn_v1 Tactic: 0xa7c9d418a10bce71
[08/10/2023-11:18:06] [V] [TRT] Tactic: 0xa7c9d418a10bce71 Time: 0.390917
[08/10/2023-11:18:06] [V] [TRT] Conv_64 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x32x32_stage4_warpsize4x1x1_g1_tensor16x8x16 Tactic: 0xa83b68f30462f971
[08/10/2023-11:18:06] [V] [TRT] Tactic: 0xa83b68f30462f971 Time: 0.124082
[08/10/2023-11:18:06] [V] [TRT] Conv_64 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x256x32_stage3_warpsize2x4x1_g1_tensor16x8x16_t1r3s3 Tactic: 0xa9177bbe4e767df8
[08/10/2023-11:18:06] [V] [TRT] Tactic: 0xa9177bbe4e767df8 Time: 0.518505
[08/10/2023-11:18:06] [V] [TRT] Conv_64 Set Tactic Name: ampere_h1688cudnn_128x128_ldg8_relu_exp_large_nhwc_tn_v1 Tactic: 0xa927df92ac1ef1b8
[08/10/2023-11:18:06] [V] [TRT] Tactic: 0xa927df92ac1ef1b8 Time: 0.347799
[08/10/2023-11:18:06] [V] [TRT] Conv_64 Set Tactic Name: ampere_h16816cudnn_128x64_ldg8_relu_exp_stages_64x4_small_nhwc_tn_v1 Tactic: 0xabd92c9ae596b545
[08/10/2023-11:18:06] [V] [TRT] Tactic: 0xabd92c9ae596b545 Time: 0.244809
[08/10/2023-11:18:06] [V] [TRT] Conv_64 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x128x32_stage5_warpsize2x2x1_g1_tensor16x8x16 Tactic: 0xafd1e8bf6bd3d638
[08/10/2023-11:18:06] [V] [TRT] Tactic: 0xafd1e8bf6bd3d638 Time: 0.305189
[08/10/2023-11:18:06] [V] [TRT] Conv_64 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x128x32_stage3_warpsize4x2x1_g1_tensor16x8x16 Tactic: 0xb17d53d15dfbfc9e
[08/10/2023-11:18:06] [V] [TRT] Tactic: 0xb17d53d15dfbfc9e Time: 0.388919
[08/10/2023-11:18:06] [V] [TRT] Conv_64 Set Tactic Name: ampere_h16816cudnn_128x128_ldg8_relu_exp_small_nhwc_tn_v1 Tactic: 0xb33e57fb3e8a0a56
[08/10/2023-11:18:06] [V] [TRT] Tactic: 0xb33e57fb3e8a0a56 Time: 0.441093
[08/10/2023-11:18:06] [V] [TRT] Conv_64 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x64x32_stage5_warpsize2x2x1_g1_tensor16x8x16_t1r3s3 Tactic: 0xb4bec086187edcfc
[08/10/2023-11:18:06] [V] [TRT] Tactic: 0xb4bec086187edcfc Time: 0.163534
[08/10/2023-11:18:06] [V] [TRT] Conv_64 Set Tactic Name: ampere_h16816cudnn_256x64_ldg8_relu_exp_stages_64x3_small_nhwc_tn_v1 Tactic: 0xb6fa5ffcc1a9d518
[08/10/2023-11:18:06] [V] [TRT] Tactic: 0xb6fa5ffcc1a9d518 Time: 0.191406
[08/10/2023-11:18:06] [V] [TRT] Conv_64 Set Tactic Name: ampere_h16816cudnn_64x128_sliced1x2_ldg8_relu_exp_stages_64x3_medium_nhwc_tn_v1 Tactic: 0xb85e52e87caf60a2
[08/10/2023-11:18:06] [V] [TRT] Tactic: 0xb85e52e87caf60a2 Time: 0.292521
[08/10/2023-11:18:06] [V] [TRT] Conv_64 Set Tactic Name: ampere_h16816cudnn_64x128_ldg8_relu_exp_stages_64x3_medium_nhwc_tn_v1 Tactic: 0xb8d86216e1235cda
[08/10/2023-11:18:06] [V] [TRT] Tactic: 0xb8d86216e1235cda Time: 0.29173
[08/10/2023-11:18:06] [V] [TRT] Conv_64 Set Tactic Name: ampere_h16816cudnn_256x128_ldg8_relu_exp_stages_64x3_medium_nhwc_tn_v1 Tactic: 0xb9171d70ba2eda4b
[08/10/2023-11:18:06] [V] [TRT] Tactic: 0xb9171d70ba2eda4b Time: 0.310121
[08/10/2023-11:18:06] [V] [TRT] Conv_64 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x64x32_stage5_warpsize2x2x1_g1_tensor16x8x16 Tactic: 0xbd08239a9317f2fd
[08/10/2023-11:18:06] [V] [TRT] Tactic: 0xbd08239a9317f2fd Time: 0.170409
[08/10/2023-11:18:06] [V] [TRT] Conv_64 Set Tactic Name: ampere_h16816cudnn_64x128_sliced1x2_ldg8_relu_exp_stages_64x4_small_nhwc_tn_v1 Tactic: 0xbd6f5e6f24c05c10
[08/10/2023-11:18:06] [V] [TRT] Tactic: 0xbd6f5e6f24c05c10 Time: 0.477285
[08/10/2023-11:18:06] [V] [TRT] Conv_64 Set Tactic Name: ampere_h16816cudnn_64x64_sliced1x2_ldg8_relu_exp_stages_64x6_large_nhwc_tn_v1 Tactic: 0xbeaee7eaad288322
[08/10/2023-11:18:06] [V] [TRT] Tactic: 0xbeaee7eaad288322 Time: 0.401632
[08/10/2023-11:18:06] [V] [TRT] Conv_64 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x128x64_stage3_warpsize2x2x1_g1_tensor16x8x16_t1r3s3 Tactic: 0xc0a02dc6095497cc
[08/10/2023-11:18:06] [V] [TRT] Tactic: 0xc0a02dc6095497cc Time: 0.32491
[08/10/2023-11:18:06] [V] [TRT] Conv_64 Set Tactic Name: ampere_h16816cudnn_128x128_ldg8_relu_exp_stages_64x3_large_nhwc_tn_v1 Tactic: 0xc2cf926c41243630
[08/10/2023-11:18:06] [V] [TRT] Tactic: 0xc2cf926c41243630 Time: 0.345134
[08/10/2023-11:18:06] [V] [TRT] Conv_64 Set Tactic Name: ampere_h16816cudnn_128x64_ldg8_relu_exp_stages_64x3_small_nhwc_tn_v1 Tactic: 0xc338d2482cee77f8
[08/10/2023-11:18:06] [V] [TRT] Tactic: 0xc338d2482cee77f8 Time: 0.175739
[08/10/2023-11:18:06] [V] [TRT] Conv_64 Set Tactic Name: ampere_h1688cudnn_256x128_ldg8_relu_exp_small_nhwc_tn_v1 Tactic: 0xc660e51970bc5a3a
[08/10/2023-11:18:06] [V] [TRT] Tactic: 0xc660e51970bc5a3a Time: 0.343182
[08/10/2023-11:18:06] [V] [TRT] Conv_64 Set Tactic Name: ampere_h16816cudnn_64x128_ldg8_relu_exp_stages_64x4_medium_nhwc_tn_v1 Tactic: 0xc82f3f06140e3cbb
[08/10/2023-11:18:06] [V] [TRT] Tactic: 0xc82f3f06140e3cbb Time: 0.43691
[08/10/2023-11:18:06] [V] [TRT] Conv_64 Set Tactic Name: ampere_h16816cudnn_64x128_sliced1x2_ldg8_relu_exp_stages_64x4_medium_nhwc_tn_v1 Tactic: 0xc8a90ff8898200c3
[08/10/2023-11:18:06] [V] [TRT] Tactic: 0xc8a90ff8898200c3 Time: 0.468869
[08/10/2023-11:18:06] [V] [TRT] Conv_64 Set Tactic Name: ampere_h1688cudnn_256x64_sliced1x2_ldg8_relu_exp_medium_nhwc_tn_v1 Tactic: 0xc9cc55109bb4de26
[08/10/2023-11:18:06] [V] [TRT] Tactic: 0xc9cc55109bb4de26 Time: 0.2528
[08/10/2023-11:18:06] [V] [TRT] Conv_64 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x64x32_stage5_warpsize2x2x1_g1_tensor16x8x16 Tactic: 0xc9d24bd069159fa8
[08/10/2023-11:18:06] [V] [TRT] Tactic: 0xc9d24bd069159fa8 Time: 0.181714
[08/10/2023-11:18:06] [V] [TRT] Conv_64 Set Tactic Name: ampere_h16816cudnn_64x128_ldg8_relu_exp_stages_64x3_large_nhwc_tn_v1 Tactic: 0xc9f0a7bec963ba66
[08/10/2023-11:18:06] [V] [TRT] Tactic: 0xc9f0a7bec963ba66 Time: 0.293746
[08/10/2023-11:18:06] [V] [TRT] Conv_64 Set Tactic Name: ampere_h16816cudnn_256x64_ldg8_relu_exp_small_nhwc_tn_v1 Tactic: 0xca5d3a11fd48f571
[08/10/2023-11:18:06] [V] [TRT] Tactic: 0xca5d3a11fd48f571 Time: 0.163689
[08/10/2023-11:18:06] [V] [TRT] Conv_64 Set Tactic Name: ampere_h16816cudnn_64x64_ldg8_relu_exp_stages_64x5_small_nhwc_tn_v1 Tactic: 0xce0506e1512285c3
[08/10/2023-11:18:06] [V] [TRT] Tactic: 0xce0506e1512285c3 Time: 0.259022
[08/10/2023-11:18:06] [V] [TRT] Conv_64 Set Tactic Name: ampere_h1688cudnn_256x64_sliced1x2_ldg8_relu_exp_large_nhwc_tn_v1 Tactic: 0xd0a3e0c815f7fb5e
[08/10/2023-11:18:06] [V] [TRT] Tactic: 0xd0a3e0c815f7fb5e Time: 0.252306
[08/10/2023-11:18:06] [V] [TRT] Conv_64 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x64x32_stage3_warpsize4x1x1_g1_tensor16x8x16 Tactic: 0xd1aaad17ca35fbaa
[08/10/2023-11:18:06] [V] [TRT] Tactic: 0xd1aaad17ca35fbaa Time: 0.144178
[08/10/2023-11:18:06] [V] [TRT] Conv_64 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x128x64_stage3_warpsize4x2x1_g1_tensor16x8x16_t1r3s3 Tactic: 0xd2fca0486ca97266
[08/10/2023-11:18:06] [V] [TRT] Tactic: 0xd2fca0486ca97266 Time: 0.295401
[08/10/2023-11:18:06] [V] [TRT] Conv_64 Set Tactic Name: ampere_h16816cudnn_64x128_sliced1x2_ldg8_relu_exp_stages_64x3_small_nhwc_tn_v1 Tactic: 0xd58ea0bdedb89ead
[08/10/2023-11:18:06] [V] [TRT] Tactic: 0xd58ea0bdedb89ead Time: 0.289243
[08/10/2023-11:18:07] [V] [TRT] Conv_64 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x256x64_stage3_warpsize1x4x1_g1_tensor16x8x16_t1r3s3 Tactic: 0xd7c261fa01db2af9
[08/10/2023-11:18:07] [V] [TRT] Tactic: 0xd7c261fa01db2af9 Time: 0.610313
[08/10/2023-11:18:07] [V] [TRT] Conv_64 Set Tactic Name: ampere_h16816cudnn_256x64_sliced1x2_ldg8_relu_exp_large_nhwc_tn_v1 Tactic: 0xd8eb41ee35e76575
[08/10/2023-11:18:07] [V] [TRT] Tactic: 0xd8eb41ee35e76575 Time: 0.234825
[08/10/2023-11:18:07] [V] [TRT] Conv_64 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x128x32_stage5_warpsize2x2x1_g1_tensor16x8x16 Tactic: 0xdb0b80f591d1bb6d
[08/10/2023-11:18:07] [V] [TRT] Tactic: 0xdb0b80f591d1bb6d Time: 0.294569
[08/10/2023-11:18:07] [V] [TRT] Conv_64 Set Tactic Name: ampere_h16816cudnn_256x128_ldg8_relu_exp_large_nhwc_tn_v1 Tactic: 0xdc796d70e228a1d4
[08/10/2023-11:18:07] [V] [TRT] Tactic: 0xdc796d70e228a1d4 Time: 0.312425
[08/10/2023-11:18:07] [V] [TRT] Conv_64 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x32x32_stage4_warpsize4x1x1_g1_tensor16x8x16 Tactic: 0xdce100b9fe609424
[08/10/2023-11:18:07] [V] [TRT] Tactic: 0xdce100b9fe609424 Time: 0.130798
[08/10/2023-11:18:07] [V] [TRT] Conv_64 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x128x32_stage4_warpsize2x2x1_g1_tensor16x8x16 Tactic: 0xdfa020ef435ef810
[08/10/2023-11:18:07] [V] [TRT] Tactic: 0xdfa020ef435ef810 Time: 0.253093
[08/10/2023-11:18:07] [V] [TRT] Conv_64 Set Tactic Name: ampere_h1688cudnn_256x64_ldg8_relu_exp_large_nhwc_tn_v1 Tactic: 0xe0e3c0e8cf9a2d9e
[08/10/2023-11:18:07] [V] [TRT] Tactic: 0xe0e3c0e8cf9a2d9e Time: 0.22581
[08/10/2023-11:18:07] [V] [TRT] Conv_64 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x32x64_stage5_warpsize2x2x1_g1_tensor16x8x16 Tactic: 0xe1ff5ad20f5c6bf6
[08/10/2023-11:18:07] [V] [TRT] Tactic: 0xe1ff5ad20f5c6bf6 Time: 0.187282
[08/10/2023-11:18:07] [V] [TRT] Conv_64 Set Tactic Name: ampere_h16816cudnn_256x64_ldg8_relu_exp_medium_nhwc_tn_v1 Tactic: 0xe4711898bd599c36
[08/10/2023-11:18:07] [V] [TRT] Tactic: 0xe4711898bd599c36 Time: 0.170423
[08/10/2023-11:18:07] [V] [TRT] Conv_64 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x64x64_stage3_warpsize4x1x1_g1_tensor16x8x16 Tactic: 0xeb25062ffb9eae45
[08/10/2023-11:18:07] [V] [TRT] Tactic: 0xeb25062ffb9eae45 Time: 0.18917
[08/10/2023-11:18:07] [V] [TRT] Conv_64 Set Tactic Name: ampere_h16816cudnn_128x64_sliced1x2_ldg8_relu_exp_stages_64x4_large_nhwc_tn_v1 Tactic: 0xeb2d2aa4e56bb41c
[08/10/2023-11:18:07] [V] [TRT] Tactic: 0xeb2d2aa4e56bb41c Time: 0.263008
[08/10/2023-11:18:07] [V] [TRT] Conv_64 Set Tactic Name: ampere_h16816cudnn_64x64_ldg8_relu_exp_stages_64x6_small_nhwc_tn_v1 Tactic: 0xf0beb09df9a19f82
[08/10/2023-11:18:07] [V] [TRT] Tactic: 0xf0beb09df9a19f82 Time: 0.351941
[08/10/2023-11:18:07] [V] [TRT] Conv_64 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x64x64_stage4_warpsize2x2x1_g1_tensor16x8x16 Tactic: 0xf35e0311fa1cc516
[08/10/2023-11:18:07] [V] [TRT] Tactic: 0xf35e0311fa1cc516 Time: 0.209806
[08/10/2023-11:18:07] [V] [TRT] Conv_64 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x32x32_stage4_warpsize4x1x1_g1_tensor16x8x16 Tactic: 0xf79479a62ea9f901
[08/10/2023-11:18:07] [V] [TRT] Tactic: 0xf79479a62ea9f901 Time: 0.126185
[08/10/2023-11:18:07] [V] [TRT] Fastest Tactic: 0x3e7eb35b91b9fa63 Time: 0.113248
[08/10/2023-11:18:07] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x3e7eb35b91b9fa63
[08/10/2023-11:18:07] [V] [TRT] *************** Autotuning format combination: Half(1036800,1:16,5760,32) -> Half(32400,1:16,180,1) ***************
[08/10/2023-11:18:07] [V] [TRT] --------------- Timing Runner: Conv_64 (CaskConvolution)
[08/10/2023-11:18:07] [V] [TRT] Conv_64 Set Tactic Name: ampere_h16816cudnn_256x128_ldg8_relu_exp_medium_nhwc_tn_v1 Tactic: 0x03896956a39a1203
[08/10/2023-11:18:07] [V] [TRT] Tactic: 0x03896956a39a1203 Time: 0.303598
[08/10/2023-11:18:07] [V] [TRT] Conv_64 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x128x32_stage3_warpsize4x2x1_g1_tensor16x8x16_t1r3s3 Tactic: 0x048d6d0400f33439
[08/10/2023-11:18:07] [V] [TRT] Tactic: 0x048d6d0400f33439 Time: 0.284261
[08/10/2023-11:18:07] [V] [TRT] Conv_64 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x64x64_stage3_warpsize2x2x1_g1_tensor16x8x16_t1r3s3 Tactic: 0x05b6220f243edacd
[08/10/2023-11:18:07] [V] [TRT] Tactic: 0x05b6220f243edacd Time: 0.151826
[08/10/2023-11:18:07] [V] [TRT] Conv_64 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x64x32_stage3_warpsize4x1x1_g1_tensor16x8x16_t1r3s3_aACCESS Tactic: 0x0866ddee325d07a6
[08/10/2023-11:18:07] [V] [TRT] Tactic: 0x0866ddee325d07a6 Time: 0.161998
[08/10/2023-11:18:07] [V] [TRT] Conv_64 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x128x32_stage4_warpsize2x2x1_g1_tensor16x8x16 Tactic: 0x0e07ff4f4f7c1ac9
[08/10/2023-11:18:07] [V] [TRT] Tactic: 0x0e07ff4f4f7c1ac9 Time: 0.270514
[08/10/2023-11:18:07] [V] [TRT] Conv_64 Set Tactic Name: ampere_h16816cudnn_256x128_ldg8_relu_exp_stages_64x3_small_nhwc_tn_v1 Tactic: 0x0e199750c30c6928
[08/10/2023-11:18:07] [V] [TRT] Tactic: 0x0e199750c30c6928 Time: 0.301303
[08/10/2023-11:18:07] [V] [TRT] Conv_64 Set Tactic Name: ampere_h1688cudnn_128x128_ldg8_relu_exp_medium_nhwc_tn_v1 Tactic: 0x100c1ad308e08d35
[08/10/2023-11:18:07] [V] [TRT] Tactic: 0x100c1ad308e08d35 Time: 0.354889
[08/10/2023-11:18:07] [V] [TRT] Conv_64 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x128x32_stage5_warpsize2x2x1_g1_tensor16x8x16_t1r3s3 Tactic: 0x17ebf0c9f418f10a
[08/10/2023-11:18:07] [V] [TRT] Tactic: 0x17ebf0c9f418f10a Time: 0.292357
[08/10/2023-11:18:07] [V] [TRT] Conv_64 Set Tactic Name: ampere_h16816cudnn_128x64_ldg8_relu_exp_stages_64x4_medium_nhwc_tn_v1 Tactic: 0x196cbc423a9bb69e
[08/10/2023-11:18:07] [V] [TRT] Tactic: 0x196cbc423a9bb69e Time: 0.254583
[08/10/2023-11:18:07] [V] [TRT] Conv_64 Set Tactic Name: ampere_h16816cudnn_128x64_sliced1x2_ldg8_relu_exp_stages_64x3_small_nhwc_tn_v1 Tactic: 0x19822de884f42a6a
[08/10/2023-11:18:07] [V] [TRT] Tactic: 0x19822de884f42a6a Time: 0.168379
[08/10/2023-11:18:07] [V] [TRT] Conv_64 Set Tactic Name: ampere_h16816cudnn_64x64_sliced1x2_ldg8_relu_exp_stages_64x5_small_nhwc_tn_v1 Tactic: 0x1a5ba808ad4cc5a8
[08/10/2023-11:18:07] [V] [TRT] Tactic: 0x1a5ba808ad4cc5a8 Time: 0.253797
[08/10/2023-11:18:07] [V] [TRT] Conv_64 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x64x64_stage3_warpsize2x2x1_g1_tensor16x8x16 Tactic: 0x21b295c0c8f6c95a
[08/10/2023-11:18:07] [V] [TRT] Tactic: 0x21b295c0c8f6c95a Time: 0.171474
[08/10/2023-11:18:07] [V] [TRT] Conv_64 Set Tactic Name: ampere_h16816cudnn_128x128_ldg8_relu_exp_medium_nhwc_tn_v1 Tactic: 0x234580e8a194335c
[08/10/2023-11:18:07] [V] [TRT] Tactic: 0x234580e8a194335c Time: 0.252873
[08/10/2023-11:18:07] [V] [TRT] Conv_64 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x128x32_stage4_warpsize2x2x1_g1_tensor16x8x16_t1r3s3_aACCESS Tactic: 0x245530c34bd6090f
[08/10/2023-11:18:07] [V] [TRT] Tactic: 0x245530c34bd6090f Time: 0.257797
[08/10/2023-11:18:07] [V] [TRT] Conv_64 Set Tactic Name: ampere_h16816cudnn_64x64_sliced1x2_ldg8_relu_exp_stages_64x6_small_nhwc_tn_v1 Tactic: 0x24e01e7405cfdfe9
[08/10/2023-11:18:07] [V] [TRT] Tactic: 0x24e01e7405cfdfe9 Time: 0.36107
[08/10/2023-11:18:07] [V] [TRT] Conv_64 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x64x64_stage3_warpsize2x2x1_g1_tensor16x8x16_t1r3s3_aACCESS Tactic: 0x263a38afd75e3a43
[08/10/2023-11:18:07] [V] [TRT] Tactic: 0x263a38afd75e3a43 Time: 0.164974
[08/10/2023-11:18:07] [V] [TRT] Conv_64 Set Tactic Name: ampere_h16816cudnn_64x128_sliced1x2_ldg8_relu_exp_stages_64x4_large_nhwc_tn_v1 Tactic: 0x2721a7f18c2700db
[08/10/2023-11:18:07] [V] [TRT] Tactic: 0x2721a7f18c2700db Time: 0.490903
[08/10/2023-11:18:07] [V] [TRT] Conv_64 Set Tactic Name: ampere_h16816cudnn_128x64_sliced1x2_ldg8_relu_exp_stages_64x3_medium_nhwc_tn_v1 Tactic: 0x29329b5741ea05f2
[08/10/2023-11:18:07] [V] [TRT] Tactic: 0x29329b5741ea05f2 Time: 0.167378
[08/10/2023-11:18:07] [V] [TRT] Conv_64 Set Tactic Name: ampere_h16816cudnn_128x128_ldg8_relu_exp_large_nhwc_tn_v1 Tactic: 0x2970ae65966d569d
[08/10/2023-11:18:07] [V] [TRT] Tactic: 0x2970ae65966d569d Time: 0.252325
[08/10/2023-11:18:07] [V] [TRT] Conv_64 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x256x64_stage3_warpsize1x4x1_g1_tensor16x8x16 Tactic: 0x29dc14520e0e4eb6
[08/10/2023-11:18:07] [V] [TRT] Tactic: 0x29dc14520e0e4eb6 Time: 0.624238
[08/10/2023-11:18:07] [V] [TRT] Conv_64 Set Tactic Name: ampere_h16816cudnn_256x64_ldg8_relu_exp_stages_64x3_large_nhwc_tn_v1 Tactic: 0x2cb4a662694e89d3
[08/10/2023-11:18:07] [V] [TRT] Tactic: 0x2cb4a662694e89d3 Time: 0.198263
[08/10/2023-11:18:07] [V] [TRT] Conv_64 Set Tactic Name: ampere_h16816cudnn_128x128_ldg8_relu_exp_stages_64x3_medium_nhwc_tn_v1 Tactic: 0x2eaa2202de9404d6
[08/10/2023-11:18:07] [V] [TRT] Tactic: 0x2eaa2202de9404d6 Time: 0.344517
[08/10/2023-11:18:07] [V] [TRT] Conv_64 Set Tactic Name: ampere_h1688cudnn_256x128_ldg8_relu_exp_medium_nhwc_tn_v1 Tactic: 0x30c0f36d0aeeac6a
[08/10/2023-11:18:07] [V] [TRT] Tactic: 0x30c0f36d0aeeac6a Time: 0.364722
[08/10/2023-11:18:07] [V] [TRT] Conv_64 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x64x64_stage4_warpsize2x2x1_g1_tensor16x8x16_t1r3s3_aACCESS Tactic: 0x30e8a8d7a953e5e9
[08/10/2023-11:18:07] [V] [TRT] Tactic: 0x30e8a8d7a953e5e9 Time: 0.200242
[08/10/2023-11:18:07] [V] [TRT] Conv_64 Set Tactic Name: ampere_h16816cudnn_128x64_ldg8_relu_exp_stages_64x4_large_nhwc_tn_v1 Tactic: 0x3197d5044d71e98e
[08/10/2023-11:18:07] [V] [TRT] Tactic: 0x3197d5044d71e98e Time: 0.255611
[08/10/2023-11:18:07] [V] [TRT] Conv_64 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x64x64_stage4_warpsize2x2x1_g1_tensor16x8x16_t1r3s3 Tactic: 0x31d93dc22d2af081
[08/10/2023-11:18:07] [V] [TRT] Tactic: 0x31d93dc22d2af081 Time: 0.195195
[08/10/2023-11:18:07] [V] [TRT] Conv_64 Set Tactic Name: ampere_h1688cudnn_128x128_ldg8_relu_exp_small_nhwc_tn_v1 Tactic: 0x3369260c04f9ad73
[08/10/2023-11:18:07] [V] [TRT] Tactic: 0x3369260c04f9ad73 Time: 0.334213
[08/10/2023-11:18:07] [V] [TRT] Conv_64 Set Tactic Name: ampere_h16816cudnn_64x128_ldg8_relu_exp_stages_64x4_small_nhwc_tn_v1 Tactic: 0x3b5fa0f2a8fc2410
[08/10/2023-11:18:07] [V] [TRT] Tactic: 0x3b5fa0f2a8fc2410 Time: 0.429138
[08/10/2023-11:18:07] [V] [TRT] Conv_64 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x32x32_stage4_warpsize4x1x1_g1_tensor16x8x16_t1r3s3 Tactic: 0x3e7eb35b91b9fa63
[08/10/2023-11:18:07] [V] [TRT] Tactic: 0x3e7eb35b91b9fa63 Time: 0.122859
[08/10/2023-11:18:07] [V] [TRT] Conv_64 Set Tactic Name: ampere_h16816cudnn_256x64_ldg8_relu_exp_stages_64x3_medium_nhwc_tn_v1 Tactic: 0x3f031df0e579d52c
[08/10/2023-11:18:07] [V] [TRT] Tactic: 0x3f031df0e579d52c Time: 0.201751
[08/10/2023-11:18:07] [V] [TRT] Conv_64 Set Tactic Name: ampere_h16816cudnn_256x64_sliced1x2_ldg8_relu_exp_small_nhwc_tn_v1 Tactic: 0x42a5b8709d0039be
[08/10/2023-11:18:07] [V] [TRT] Tactic: 0x42a5b8709d0039be Time: 0.218377
[08/10/2023-11:18:07] [V] [TRT] Conv_64 Set Tactic Name: ampere_h16816cudnn_256x128_ldg8_relu_exp_small_nhwc_tn_v1 Tactic: 0x463794ee4acffd1f
[08/10/2023-11:18:07] [V] [TRT] Tactic: 0x463794ee4acffd1f Time: 0.305193
[08/10/2023-11:18:07] [V] [TRT] Conv_64 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x64x64_stage4_warpsize2x2x1_g1_tensor16x8x16 Tactic: 0x4a81ea1e51436a30
[08/10/2023-11:18:07] [V] [TRT] Tactic: 0x4a81ea1e51436a30 Time: 0.22384
[08/10/2023-11:18:07] [V] [TRT] Conv_64 Set Tactic Name: ampere_h1688cudnn_256x64_sliced1x2_ldg8_relu_exp_small_nhwc_tn_v1 Tactic: 0x4aed1956bd10a795
[08/10/2023-11:18:07] [V] [TRT] Tactic: 0x4aed1956bd10a795 Time: 0.238103
[08/10/2023-11:18:07] [V] [TRT] Conv_64 Set Tactic Name: ampere_h16816cudnn_64x128_sliced1x2_ldg8_relu_exp_stages_64x3_large_nhwc_tn_v1 Tactic: 0x4fc05923455fc266
[08/10/2023-11:18:07] [V] [TRT] Tactic: 0x4fc05923455fc266 Time: 0.297943
[08/10/2023-11:18:07] [V] [TRT] Conv_64 Set Tactic Name: ampere_h16816cudnn_256x64_ldg8_relu_exp_large_nhwc_tn_v1 Tactic: 0x5013c38f55afa9ba
[08/10/2023-11:18:07] [V] [TRT] Tactic: 0x5013c38f55afa9ba Time: 0.176914
[08/10/2023-11:18:07] [V] [TRT] Conv_64 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x64x32_stage3_warpsize4x1x1_g1_tensor16x8x16_t1r3s3 Tactic: 0x529f4431bdae94f5
[08/10/2023-11:18:07] [V] [TRT] Tactic: 0x529f4431bdae94f5 Time: 0.145614
[08/10/2023-11:18:07] [V] [TRT] Conv_64 Set Tactic Name: ampere_h16816cudnn_64x128_ldg8_relu_exp_stages_64x3_small_nhwc_tn_v1 Tactic: 0x53be5e206184e6ad
[08/10/2023-11:18:07] [V] [TRT] Tactic: 0x53be5e206184e6ad Time: 0.292951
[08/10/2023-11:18:07] [V] [TRT] Conv_64 Set Tactic Name: ampere_h16816cudnn_64x64_ldg8_relu_exp_stages_64x5_large_nhwc_tn_v1 Tactic: 0x544bff7ff9c5d908
[08/10/2023-11:18:07] [V] [TRT] Tactic: 0x544bff7ff9c5d908 Time: 0.263063
[08/10/2023-11:18:07] [V] [TRT] Conv_64 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x64x64_stage3_warpsize2x2x1_g1_tensor16x8x16 Tactic: 0x5568fd8a32f4a40f
[08/10/2023-11:18:07] [V] [TRT] Tactic: 0x5568fd8a32f4a40f Time: 0.156402
[08/10/2023-11:18:07] [V] [TRT] Conv_64 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x128x64_stage3_warpsize2x2x1_g1_tensor16x8x16 Tactic: 0x57c9a5ff682354a6
[08/10/2023-11:18:07] [V] [TRT] Tactic: 0x57c9a5ff682354a6 Time: 0.325632
[08/10/2023-11:18:07] [V] [TRT] Conv_64 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x32x64_stage5_warpsize2x2x1_g1_tensor16x8x16 Tactic: 0x5820b3dda403c4d0
[08/10/2023-11:18:07] [V] [TRT] Tactic: 0x5820b3dda403c4d0 Time: 0.181742
[08/10/2023-11:18:07] [V] [TRT] Conv_64 Set Tactic Name: ampere_h16816cudnn_128x128_ldg8_relu_exp_stages_64x3_small_nhwc_tn_v1 Tactic: 0x58816bf2e9c36afb
[08/10/2023-11:18:07] [V] [TRT] Tactic: 0x58816bf2e9c36afb Time: 0.340983
[08/10/2023-11:18:07] [V] [TRT] Conv_64 Set Tactic Name: ampere_h16816cudnn_128x64_ldg8_relu_exp_stages_64x3_large_nhwc_tn_v1 Tactic: 0x59762bd684092b33
[08/10/2023-11:18:07] [V] [TRT] Tactic: 0x59762bd684092b33 Time: 0.180503
[08/10/2023-11:18:07] [V] [TRT] Conv_64 Set Tactic Name: ampere_h16816cudnn_128x64_sliced1x2_ldg8_relu_exp_stages_64x4_medium_nhwc_tn_v1 Tactic: 0x59c5c647b4c76593
[08/10/2023-11:18:07] [V] [TRT] Tactic: 0x59c5c647b4c76593 Time: 0.255648
[08/10/2023-11:18:07] [V] [TRT] Conv_64 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x128x64_stage3_warpsize4x2x1_g1_tensor16x8x16 Tactic: 0x5a55274960724ba8
[08/10/2023-11:18:07] [V] [TRT] Tactic: 0x5a55274960724ba8 Time: 0.297472
[08/10/2023-11:18:07] [V] [TRT] Conv_64 Set Tactic Name: ampere_h1688cudnn_256x128_ldg8_relu_exp_large_nhwc_tn_v1 Tactic: 0x5c2e1c87d85b06f1
[08/10/2023-11:18:07] [V] [TRT] Tactic: 0x5c2e1c87d85b06f1 Time: 0.368347
[08/10/2023-11:18:07] [V] [TRT] Conv_64 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x256x64_stage3_warpsize1x4x1_g1_tensor16x8x16 Tactic: 0x5d067c18f40c23e3
[08/10/2023-11:18:07] [V] [TRT] Tactic: 0x5d067c18f40c23e3 Time: 0.624293
[08/10/2023-11:18:07] [V] [TRT] Conv_64 Set Tactic Name: ampere_h16816cudnn_64x64_ldg8_relu_exp_stages_64x5_medium_nhwc_tn_v1 Tactic: 0x5f31c22ec167f384
[08/10/2023-11:18:07] [V] [TRT] Tactic: 0x5f31c22ec167f384 Time: 0.261691
[08/10/2023-11:18:07] [V] [TRT] Conv_64 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x128x32_stage4_warpsize2x2x1_g1_tensor16x8x16_t1r3s3 Tactic: 0x60c3421152ef8e10
[08/10/2023-11:18:07] [V] [TRT] Tactic: 0x60c3421152ef8e10 Time: 0.248293
[08/10/2023-11:18:07] [V] [TRT] Conv_64 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x128x32_stage3_warpsize4x2x1_g1_tensor16x8x16 Tactic: 0x60da8c7151d91e47
[08/10/2023-11:18:07] [V] [TRT] Tactic: 0x60da8c7151d91e47 Time: 0.283666
[08/10/2023-11:18:07] [V] [TRT] Conv_64 Set Tactic Name: ampere_h1688cudnn_256x64_ldg8_relu_exp_medium_nhwc_tn_v1 Tactic: 0x6426696f872a3b13
[08/10/2023-11:18:07] [V] [TRT] Tactic: 0x6426696f872a3b13 Time: 0.226112
[08/10/2023-11:18:07] [V] [TRT] Conv_64 Set Tactic Name: ampere_h16816cudnn_128x64_ldg8_relu_exp_stages_64x3_medium_nhwc_tn_v1 Tactic: 0x699be152cfb6d6ff
[08/10/2023-11:18:07] [V] [TRT] Tactic: 0x699be152cfb6d6ff Time: 0.183095
[08/10/2023-11:18:07] [V] [TRT] Conv_64 Set Tactic Name: ampere_h16816cudnn_64x64_ldg8_relu_exp_stages_64x6_large_nhwc_tn_v1 Tactic: 0x6af049035146c349
[08/10/2023-11:18:07] [V] [TRT] Tactic: 0x6af049035146c349 Time: 0.380302
[08/10/2023-11:18:07] [V] [TRT] Conv_64 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x256x32_stage3_warpsize2x4x1_g1_tensor16x8x16 Tactic: 0x7005d10718f6c22d
[08/10/2023-11:18:07] [V] [TRT] Tactic: 0x7005d10718f6c22d Time: 0.522935
[08/10/2023-11:18:07] [V] [TRT] Conv_64 Set Tactic Name: ampere_h16816cudnn_64x64_sliced1x2_ldg8_relu_exp_stages_64x5_medium_nhwc_tn_v1 Tactic: 0x706f08da35c795a5
[08/10/2023-11:18:07] [V] [TRT] Tactic: 0x706f08da35c795a5 Time: 0.260526
[08/10/2023-11:18:07] [V] [TRT] Conv_64 Set Tactic Name: ampere_h16816cudnn_128x64_sliced1x2_ldg8_relu_exp_stages_64x4_small_nhwc_tn_v1 Tactic: 0x7163d33a4d8ce8d7
[08/10/2023-11:18:07] [V] [TRT] Tactic: 0x7163d33a4d8ce8d7 Time: 0.249746
[08/10/2023-11:18:07] [V] [TRT] Conv_64 Set Tactic Name: ampere_h1688cudnn_256x64_ldg8_relu_exp_small_nhwc_tn_v1 Tactic: 0x7aad3976677d7155
[08/10/2023-11:18:07] [V] [TRT] Tactic: 0x7aad3976677d7155 Time: 0.217303
[08/10/2023-11:18:07] [V] [TRT] Conv_64 Set Tactic Name: ampere_h16816cudnn_64x64_sliced1x2_ldg8_relu_exp_stages_64x5_large_nhwc_tn_v1 Tactic: 0x8015519605ab9963
[08/10/2023-11:18:07] [V] [TRT] Tactic: 0x8015519605ab9963 Time: 0.260494
[08/10/2023-11:18:07] [V] [TRT] Conv_64 Set Tactic Name: ampere_h16816cudnn_256x64_sliced1x2_ldg8_relu_exp_medium_nhwc_tn_v1 Tactic: 0x82f8a2214b0b4178
[08/10/2023-11:18:07] [V] [TRT] Tactic: 0x82f8a2214b0b4178 Time: 0.236325
[08/10/2023-11:18:07] [V] [TRT] Conv_64 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x32x32_stage4_warpsize4x1x1_g1_tensor16x8x16 Tactic: 0x834e11ecd4ab9454
[08/10/2023-11:18:07] [V] [TRT] Tactic: 0x834e11ecd4ab9454 Time: 0.122974
[08/10/2023-11:18:07] [V] [TRT] Conv_64 Set Tactic Name: ampere_h16816cudnn_128x64_sliced1x2_ldg8_relu_exp_stages_64x3_large_nhwc_tn_v1 Tactic: 0x83ccd4762c1376a1
[08/10/2023-11:18:07] [V] [TRT] Tactic: 0x83ccd4762c1376a1 Time: 0.170149
[08/10/2023-11:18:07] [V] [TRT] Conv_64 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x64x64_stage3_warpsize4x1x1_g1_tensor16x8x16_t1r3s3 Tactic: 0x84ee897dd1f4d2aa
[08/10/2023-11:18:07] [V] [TRT] Tactic: 0x84ee897dd1f4d2aa Time: 0.185522
[08/10/2023-11:18:07] [V] [TRT] Conv_64 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x128x64_stage3_warpsize2x2x1_g1_tensor16x8x16 Tactic: 0x866e7a5f6401b67f
[08/10/2023-11:18:07] [V] [TRT] Tactic: 0x866e7a5f6401b67f Time: 0.325906
[08/10/2023-11:18:07] [V] [TRT] Conv_64 Set Tactic Name: ampere_h16816cudnn_64x64_ldg8_relu_exp_stages_64x6_medium_nhwc_tn_v1 Tactic: 0x88971eec55aba850
[08/10/2023-11:18:07] [V] [TRT] Tactic: 0x88971eec55aba850 Time: 0.374226
[08/10/2023-11:18:07] [V] [TRT] Conv_64 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x128x64_stage3_warpsize4x2x1_g1_tensor16x8x16 Tactic: 0x8bf2f8e96c50a971
[08/10/2023-11:18:07] [V] [TRT] Tactic: 0x8bf2f8e96c50a971 Time: 0.291877
[08/10/2023-11:18:07] [V] [TRT] Conv_64 Set Tactic Name: ampere_h16816cudnn_256x128_ldg8_relu_exp_stages_64x3_large_nhwc_tn_v1 Tactic: 0x94576ece6beb35e3
[08/10/2023-11:18:07] [V] [TRT] Tactic: 0x94576ece6beb35e3 Time: 0.401463
[08/10/2023-11:18:07] [V] [TRT] Conv_64 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x64x64_stage3_warpsize4x1x1_g1_tensor16x8x16 Tactic: 0x9fff6e65019cc310
[08/10/2023-11:18:07] [V] [TRT] Tactic: 0x9fff6e65019cc310 Time: 0.183698
[08/10/2023-11:18:07] [V] [TRT] Conv_64 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x32x64_stage5_warpsize2x2x1_g1_tensor16x8x16_t1r3s3 Tactic: 0xa033e20ae9f412b2
[08/10/2023-11:18:07] [V] [TRT] Tactic: 0xa033e20ae9f412b2 Time: 0.164485
[08/10/2023-11:18:07] [V] [TRT] Conv_64 Set Tactic Name: ampere_h16816cudnn_64x128_ldg8_relu_exp_stages_64x4_large_nhwc_tn_v1 Tactic: 0xa111596c001b78db
[08/10/2023-11:18:07] [V] [TRT] Tactic: 0xa111596c001b78db Time: 0.441019
[08/10/2023-11:18:07] [V] [TRT] Conv_64 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x256x32_stage3_warpsize2x4x1_g1_tensor16x8x16 Tactic: 0xa1a20ea714d420f4
[08/10/2023-11:18:07] [V] [TRT] Tactic: 0xa1a20ea714d420f4 Time: 0.51888
[08/10/2023-11:18:07] [V] [TRT] Conv_64 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x32x32_stage4_warpsize4x1x1_g1_tensor16x8x16_t1r3s3 Tactic: 0xa40cb43c296a36a8
[08/10/2023-11:18:07] [V] [TRT] Tactic: 0xa40cb43c296a36a8 Time: 0.123499
[08/10/2023-11:18:07] [V] [TRT] Conv_64 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x64x32_stage3_warpsize4x1x1_g1_tensor16x8x16 Tactic: 0xa570c55d303796ff
[08/10/2023-11:18:07] [V] [TRT] Tactic: 0xa570c55d303796ff Time: 0.155762
[08/10/2023-11:18:08] [V] [TRT] Conv_64 Set Tactic Name: ampere_h16816cudnn_64x64_sliced1x2_ldg8_relu_exp_stages_64x6_medium_nhwc_tn_v1 Tactic: 0xa7c9d418a10bce71
[08/10/2023-11:18:08] [V] [TRT] Tactic: 0xa7c9d418a10bce71 Time: 0.390217
[08/10/2023-11:18:08] [V] [TRT] Conv_64 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x32x32_stage4_warpsize4x1x1_g1_tensor16x8x16 Tactic: 0xa83b68f30462f971
[08/10/2023-11:18:08] [V] [TRT] Tactic: 0xa83b68f30462f971 Time: 0.130985
[08/10/2023-11:18:08] [V] [TRT] Conv_64 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x256x32_stage3_warpsize2x4x1_g1_tensor16x8x16_t1r3s3 Tactic: 0xa9177bbe4e767df8
[08/10/2023-11:18:08] [V] [TRT] Tactic: 0xa9177bbe4e767df8 Time: 0.529362
[08/10/2023-11:18:08] [V] [TRT] Conv_64 Set Tactic Name: ampere_h1688cudnn_128x128_ldg8_relu_exp_large_nhwc_tn_v1 Tactic: 0xa927df92ac1ef1b8
[08/10/2023-11:18:08] [V] [TRT] Tactic: 0xa927df92ac1ef1b8 Time: 0.356023
[08/10/2023-11:18:08] [V] [TRT] Conv_64 Set Tactic Name: ampere_h16816cudnn_128x64_ldg8_relu_exp_stages_64x4_small_nhwc_tn_v1 Tactic: 0xabd92c9ae596b545
[08/10/2023-11:18:08] [V] [TRT] Tactic: 0xabd92c9ae596b545 Time: 0.248768
[08/10/2023-11:18:08] [V] [TRT] Conv_64 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x128x32_stage5_warpsize2x2x1_g1_tensor16x8x16 Tactic: 0xafd1e8bf6bd3d638
[08/10/2023-11:18:08] [V] [TRT] Tactic: 0xafd1e8bf6bd3d638 Time: 0.311099
[08/10/2023-11:18:08] [V] [TRT] Conv_64 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x128x32_stage3_warpsize4x2x1_g1_tensor16x8x16 Tactic: 0xb17d53d15dfbfc9e
[08/10/2023-11:18:08] [V] [TRT] Tactic: 0xb17d53d15dfbfc9e Time: 0.295493
[08/10/2023-11:18:08] [V] [TRT] Conv_64 Set Tactic Name: ampere_h16816cudnn_128x128_ldg8_relu_exp_small_nhwc_tn_v1 Tactic: 0xb33e57fb3e8a0a56
[08/10/2023-11:18:08] [V] [TRT] Tactic: 0xb33e57fb3e8a0a56 Time: 0.249079
[08/10/2023-11:18:08] [V] [TRT] Conv_64 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x64x32_stage5_warpsize2x2x1_g1_tensor16x8x16_t1r3s3 Tactic: 0xb4bec086187edcfc
[08/10/2023-11:18:08] [V] [TRT] Tactic: 0xb4bec086187edcfc Time: 0.163611
[08/10/2023-11:18:08] [V] [TRT] Conv_64 Set Tactic Name: ampere_h16816cudnn_256x64_ldg8_relu_exp_stages_64x3_small_nhwc_tn_v1 Tactic: 0xb6fa5ffcc1a9d518
[08/10/2023-11:18:08] [V] [TRT] Tactic: 0xb6fa5ffcc1a9d518 Time: 0.19355
[08/10/2023-11:18:08] [V] [TRT] Conv_64 Set Tactic Name: ampere_h16816cudnn_64x128_sliced1x2_ldg8_relu_exp_stages_64x3_medium_nhwc_tn_v1 Tactic: 0xb85e52e87caf60a2
[08/10/2023-11:18:08] [V] [TRT] Tactic: 0xb85e52e87caf60a2 Time: 0.294103
[08/10/2023-11:18:08] [V] [TRT] Conv_64 Set Tactic Name: ampere_h16816cudnn_64x128_ldg8_relu_exp_stages_64x3_medium_nhwc_tn_v1 Tactic: 0xb8d86216e1235cda
[08/10/2023-11:18:08] [V] [TRT] Tactic: 0xb8d86216e1235cda Time: 0.293518
[08/10/2023-11:18:08] [V] [TRT] Conv_64 Set Tactic Name: ampere_h16816cudnn_256x128_ldg8_relu_exp_stages_64x3_medium_nhwc_tn_v1 Tactic: 0xb9171d70ba2eda4b
[08/10/2023-11:18:08] [V] [TRT] Tactic: 0xb9171d70ba2eda4b Time: 0.304539
[08/10/2023-11:18:08] [V] [TRT] Conv_64 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x64x32_stage5_warpsize2x2x1_g1_tensor16x8x16 Tactic: 0xbd08239a9317f2fd
[08/10/2023-11:18:08] [V] [TRT] Tactic: 0xbd08239a9317f2fd Time: 0.168576
[08/10/2023-11:18:08] [V] [TRT] Conv_64 Set Tactic Name: ampere_h16816cudnn_64x128_sliced1x2_ldg8_relu_exp_stages_64x4_small_nhwc_tn_v1 Tactic: 0xbd6f5e6f24c05c10
[08/10/2023-11:18:08] [V] [TRT] Tactic: 0xbd6f5e6f24c05c10 Time: 0.469061
[08/10/2023-11:18:08] [V] [TRT] Conv_64 Set Tactic Name: ampere_h16816cudnn_64x64_sliced1x2_ldg8_relu_exp_stages_64x6_large_nhwc_tn_v1 Tactic: 0xbeaee7eaad288322
[08/10/2023-11:18:08] [V] [TRT] Tactic: 0xbeaee7eaad288322 Time: 0.393385
[08/10/2023-11:18:08] [V] [TRT] Conv_64 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x128x64_stage3_warpsize2x2x1_g1_tensor16x8x16_t1r3s3 Tactic: 0xc0a02dc6095497cc
[08/10/2023-11:18:08] [V] [TRT] Tactic: 0xc0a02dc6095497cc Time: 0.32091
[08/10/2023-11:18:08] [V] [TRT] Conv_64 Set Tactic Name: ampere_h16816cudnn_128x128_ldg8_relu_exp_stages_64x3_large_nhwc_tn_v1 Tactic: 0xc2cf926c41243630
[08/10/2023-11:18:08] [V] [TRT] Tactic: 0xc2cf926c41243630 Time: 0.345577
[08/10/2023-11:18:08] [V] [TRT] Conv_64 Set Tactic Name: ampere_h16816cudnn_128x64_ldg8_relu_exp_stages_64x3_small_nhwc_tn_v1 Tactic: 0xc338d2482cee77f8
[08/10/2023-11:18:08] [V] [TRT] Tactic: 0xc338d2482cee77f8 Time: 0.180462
[08/10/2023-11:18:08] [V] [TRT] Conv_64 Set Tactic Name: ampere_h1688cudnn_256x128_ldg8_relu_exp_small_nhwc_tn_v1 Tactic: 0xc660e51970bc5a3a
[08/10/2023-11:18:08] [V] [TRT] Tactic: 0xc660e51970bc5a3a Time: 0.342309
[08/10/2023-11:18:08] [V] [TRT] Conv_64 Set Tactic Name: ampere_h16816cudnn_64x128_ldg8_relu_exp_stages_64x4_medium_nhwc_tn_v1 Tactic: 0xc82f3f06140e3cbb
[08/10/2023-11:18:08] [V] [TRT] Tactic: 0xc82f3f06140e3cbb Time: 0.445024
[08/10/2023-11:18:08] [V] [TRT] Conv_64 Set Tactic Name: ampere_h16816cudnn_64x128_sliced1x2_ldg8_relu_exp_stages_64x4_medium_nhwc_tn_v1 Tactic: 0xc8a90ff8898200c3
[08/10/2023-11:18:08] [V] [TRT] Tactic: 0xc8a90ff8898200c3 Time: 0.480608
[08/10/2023-11:18:08] [V] [TRT] Conv_64 Set Tactic Name: ampere_h1688cudnn_256x64_sliced1x2_ldg8_relu_exp_medium_nhwc_tn_v1 Tactic: 0xc9cc55109bb4de26
[08/10/2023-11:18:08] [V] [TRT] Tactic: 0xc9cc55109bb4de26 Time: 0.258309
[08/10/2023-11:18:08] [V] [TRT] Conv_64 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x64x32_stage5_warpsize2x2x1_g1_tensor16x8x16 Tactic: 0xc9d24bd069159fa8
[08/10/2023-11:18:08] [V] [TRT] Tactic: 0xc9d24bd069159fa8 Time: 0.186377
[08/10/2023-11:18:08] [V] [TRT] Conv_64 Set Tactic Name: ampere_h16816cudnn_64x128_ldg8_relu_exp_stages_64x3_large_nhwc_tn_v1 Tactic: 0xc9f0a7bec963ba66
[08/10/2023-11:18:08] [V] [TRT] Tactic: 0xc9f0a7bec963ba66 Time: 0.295429
[08/10/2023-11:18:08] [V] [TRT] Conv_64 Set Tactic Name: ampere_h16816cudnn_256x64_ldg8_relu_exp_small_nhwc_tn_v1 Tactic: 0xca5d3a11fd48f571
[08/10/2023-11:18:08] [V] [TRT] Tactic: 0xca5d3a11fd48f571 Time: 0.167013
[08/10/2023-11:18:08] [V] [TRT] Conv_64 Set Tactic Name: ampere_h16816cudnn_64x64_ldg8_relu_exp_stages_64x5_small_nhwc_tn_v1 Tactic: 0xce0506e1512285c3
[08/10/2023-11:18:08] [V] [TRT] Tactic: 0xce0506e1512285c3 Time: 0.263058
[08/10/2023-11:18:08] [V] [TRT] Conv_64 Set Tactic Name: ampere_h1688cudnn_256x64_sliced1x2_ldg8_relu_exp_large_nhwc_tn_v1 Tactic: 0xd0a3e0c815f7fb5e
[08/10/2023-11:18:08] [V] [TRT] Tactic: 0xd0a3e0c815f7fb5e Time: 0.254327
[08/10/2023-11:18:08] [V] [TRT] Conv_64 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x64x32_stage3_warpsize4x1x1_g1_tensor16x8x16 Tactic: 0xd1aaad17ca35fbaa
[08/10/2023-11:18:08] [V] [TRT] Tactic: 0xd1aaad17ca35fbaa Time: 0.147406
[08/10/2023-11:18:08] [V] [TRT] Conv_64 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x128x64_stage3_warpsize4x2x1_g1_tensor16x8x16_t1r3s3 Tactic: 0xd2fca0486ca97266
[08/10/2023-11:18:08] [V] [TRT] Tactic: 0xd2fca0486ca97266 Time: 0.295259
[08/10/2023-11:18:08] [V] [TRT] Conv_64 Set Tactic Name: ampere_h16816cudnn_64x128_sliced1x2_ldg8_relu_exp_stages_64x3_small_nhwc_tn_v1 Tactic: 0xd58ea0bdedb89ead
[08/10/2023-11:18:08] [V] [TRT] Tactic: 0xd58ea0bdedb89ead Time: 0.291749
[08/10/2023-11:18:08] [V] [TRT] Conv_64 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x256x64_stage3_warpsize1x4x1_g1_tensor16x8x16_t1r3s3 Tactic: 0xd7c261fa01db2af9
[08/10/2023-11:18:08] [V] [TRT] Tactic: 0xd7c261fa01db2af9 Time: 0.61152
[08/10/2023-11:18:08] [V] [TRT] Conv_64 Set Tactic Name: ampere_h16816cudnn_256x64_sliced1x2_ldg8_relu_exp_large_nhwc_tn_v1 Tactic: 0xd8eb41ee35e76575
[08/10/2023-11:18:08] [V] [TRT] Tactic: 0xd8eb41ee35e76575 Time: 0.238299
[08/10/2023-11:18:08] [V] [TRT] Conv_64 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x128x32_stage5_warpsize2x2x1_g1_tensor16x8x16 Tactic: 0xdb0b80f591d1bb6d
[08/10/2023-11:18:08] [V] [TRT] Tactic: 0xdb0b80f591d1bb6d Time: 0.294185
[08/10/2023-11:18:08] [V] [TRT] Conv_64 Set Tactic Name: ampere_h16816cudnn_256x128_ldg8_relu_exp_large_nhwc_tn_v1 Tactic: 0xdc796d70e228a1d4
[08/10/2023-11:18:08] [V] [TRT] Tactic: 0xdc796d70e228a1d4 Time: 0.304786
[08/10/2023-11:18:08] [V] [TRT] Conv_64 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x32x32_stage4_warpsize4x1x1_g1_tensor16x8x16 Tactic: 0xdce100b9fe609424
[08/10/2023-11:18:08] [V] [TRT] Tactic: 0xdce100b9fe609424 Time: 0.132021
[08/10/2023-11:18:08] [V] [TRT] Conv_64 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x128x32_stage4_warpsize2x2x1_g1_tensor16x8x16 Tactic: 0xdfa020ef435ef810
[08/10/2023-11:18:08] [V] [TRT] Tactic: 0xdfa020ef435ef810 Time: 0.24832
[08/10/2023-11:18:08] [V] [TRT] Conv_64 Set Tactic Name: ampere_h1688cudnn_256x64_ldg8_relu_exp_large_nhwc_tn_v1 Tactic: 0xe0e3c0e8cf9a2d9e
[08/10/2023-11:18:08] [V] [TRT] Tactic: 0xe0e3c0e8cf9a2d9e Time: 0.227099
[08/10/2023-11:18:08] [V] [TRT] Conv_64 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x32x64_stage5_warpsize2x2x1_g1_tensor16x8x16 Tactic: 0xe1ff5ad20f5c6bf6
[08/10/2023-11:18:08] [V] [TRT] Tactic: 0xe1ff5ad20f5c6bf6 Time: 0.183895
[08/10/2023-11:18:08] [V] [TRT] Conv_64 Set Tactic Name: ampere_h16816cudnn_256x64_ldg8_relu_exp_medium_nhwc_tn_v1 Tactic: 0xe4711898bd599c36
[08/10/2023-11:18:08] [V] [TRT] Tactic: 0xe4711898bd599c36 Time: 0.172919
[08/10/2023-11:18:08] [V] [TRT] Conv_64 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x64x64_stage3_warpsize4x1x1_g1_tensor16x8x16 Tactic: 0xeb25062ffb9eae45
[08/10/2023-11:18:08] [V] [TRT] Tactic: 0xeb25062ffb9eae45 Time: 0.190629
[08/10/2023-11:18:08] [V] [TRT] Conv_64 Set Tactic Name: ampere_h16816cudnn_128x64_sliced1x2_ldg8_relu_exp_stages_64x4_large_nhwc_tn_v1 Tactic: 0xeb2d2aa4e56bb41c
[08/10/2023-11:18:08] [V] [TRT] Tactic: 0xeb2d2aa4e56bb41c Time: 0.263095
[08/10/2023-11:18:08] [V] [TRT] Conv_64 Set Tactic Name: ampere_h16816cudnn_64x64_ldg8_relu_exp_stages_64x6_small_nhwc_tn_v1 Tactic: 0xf0beb09df9a19f82
[08/10/2023-11:18:08] [V] [TRT] Tactic: 0xf0beb09df9a19f82 Time: 0.351534
[08/10/2023-11:18:08] [V] [TRT] Conv_64 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x64x64_stage4_warpsize2x2x1_g1_tensor16x8x16 Tactic: 0xf35e0311fa1cc516
[08/10/2023-11:18:08] [V] [TRT] Tactic: 0xf35e0311fa1cc516 Time: 0.211314
[08/10/2023-11:18:08] [V] [TRT] Conv_64 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x32x32_stage4_warpsize4x1x1_g1_tensor16x8x16 Tactic: 0xf79479a62ea9f901
[08/10/2023-11:18:08] [V] [TRT] Tactic: 0xf79479a62ea9f901 Time: 0.131266
[08/10/2023-11:18:08] [V] [TRT] Fastest Tactic: 0x3e7eb35b91b9fa63 Time: 0.122859
[08/10/2023-11:18:08] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x3e7eb35b91b9fa63
[08/10/2023-11:18:08] [V] [TRT] =============== Computing costs for 
[08/10/2023-11:18:08] [V] [TRT] *************** Autotuning format combination: Float(16588800,32400,180,1) -> Float(32400,32400,180,1) ***************
[08/10/2023-11:18:08] [V] [TRT] --------------- Timing Runner: Conv_67 (CudaDepthwiseConvolution)
[08/10/2023-11:18:08] [V] [TRT] CudaDepthwiseConvolution has no valid tactics for this config, skipping
[08/10/2023-11:18:08] [V] [TRT] --------------- Timing Runner: Conv_67 (FusedConvActConvolution)
[08/10/2023-11:18:08] [V] [TRT] Tactic: 0x000000000007ffff Time: 0.786592
[08/10/2023-11:18:08] [V] [TRT] Tactic: 0x00000000000fffff Time: 0.652357
[08/10/2023-11:18:08] [V] [TRT] Tactic: 0x000000000027ffff Time: 0.697472
[08/10/2023-11:18:08] [V] [TRT] Tactic: 0x000000000062ffff Time: 0.627113
[08/10/2023-11:18:08] [V] [TRT] Tactic: 0x000000000086ffff Time: 0.981682
[08/10/2023-11:18:08] [V] [TRT] Tactic: 0x000000000089ffff Time: 0.639163
[08/10/2023-11:18:08] [V] [TRT] Tactic: 0x00000000009fffff Time: 0.64763
[08/10/2023-11:18:08] [V] [TRT] Tactic: 0x0000000000a2ffff Time: 0.702263
[08/10/2023-11:18:08] [V] [TRT] Fastest Tactic: 0x000000000062ffff Time: 0.627113
[08/10/2023-11:18:08] [V] [TRT] --------------- Timing Runner: Conv_67 (CudnnConvolution)
[08/10/2023-11:18:08] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.601065
[08/10/2023-11:18:08] [V] [TRT] Tactic: 0x0000000000000001 Time: 0.44619
[08/10/2023-11:18:08] [V] [TRT] Tactic: 0x0000000000000002 Time: 0.592585
[08/10/2023-11:18:08] [V] [TRT] Tactic: 0x0000000000000004 Time: 3.0086
[08/10/2023-11:18:08] [V] [TRT] Tactic: 0x0000000000000005 Time: 1.05347
[08/10/2023-11:18:08] [V] [TRT] Tactic: 0x0000000000000006 Time: 0.432786
[08/10/2023-11:18:08] [V] [TRT] Tactic: 0x0000000000000038 Time: 0.59008
[08/10/2023-11:18:08] [V] [TRT] Tactic: 0x0000000000000039 Time: 0.439867
[08/10/2023-11:18:08] [V] [TRT] Tactic: 0x000000000000003a Time: 0.603593
[08/10/2023-11:18:09] [V] [TRT] Tactic: 0x000000000000003c Time: 2.94437
[08/10/2023-11:18:09] [V] [TRT] Tactic: 0x000000000000003d Time: 1.04138
[08/10/2023-11:18:09] [V] [TRT] Tactic: 0x000000000000003e Time: 0.435186
[08/10/2023-11:18:09] [V] [TRT] Tactic: 0x0000000000000070 Time: 0.590226
[08/10/2023-11:18:09] [V] [TRT] Tactic: 0x0000000000000071 Time: 0.781778
[08/10/2023-11:18:09] [V] [TRT] Tactic: 0x0000000000000072 Time: 0.601079
[08/10/2023-11:18:09] [V] [TRT] Tactic: 0x0000000000000074 Time: 2.92367
[08/10/2023-11:18:09] [V] [TRT] Tactic: 0x0000000000000075 Time: 1.06166
[08/10/2023-11:18:09] [V] [TRT] Tactic: 0x0000000000000076 Time: 0.433943
[08/10/2023-11:18:09] [V] [TRT] Fastest Tactic: 0x0000000000000006 Time: 0.432786
[08/10/2023-11:18:09] [V] [TRT] --------------- Timing Runner: Conv_67 (CaskConvolution)
[08/10/2023-11:18:09] [V] [TRT] Conv_67 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x128x8_stage3_warpsize1x4x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0x01cf8ce2da913006
[08/10/2023-11:18:09] [V] [TRT] Tactic: 0x01cf8ce2da913006 Time: 2.7207
[08/10/2023-11:18:09] [V] [TRT] Conv_67 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x64x8_stage3_warpsize2x2x1_g1_ffma_aligna4_alignc4 Tactic: 0x12dbf7d94ee3696d
[08/10/2023-11:18:09] [V] [TRT] Tactic: 0x12dbf7d94ee3696d Time: 1.47268
[08/10/2023-11:18:09] [V] [TRT] Conv_67 Set Tactic Name: ampere_scudnn_128x64_relu_small_nn_v1 Tactic: 0x3f243c490d502deb
[08/10/2023-11:18:09] [V] [TRT] Tactic: 0x3f243c490d502deb Time: 1.3869
[08/10/2023-11:18:09] [V] [TRT] Conv_67 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0x4727434768e46395
[08/10/2023-11:18:09] [V] [TRT] Tactic: 0x4727434768e46395 Time: 1.59329
[08/10/2023-11:18:09] [V] [TRT] Conv_67 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize256x128x8_stage3_warpsize4x2x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0x4efce38acc876f5c
[08/10/2023-11:18:09] [V] [TRT] Tactic: 0x4efce38acc876f5c Time: 2.73627
[08/10/2023-11:18:09] [V] [TRT] Conv_67 Set Tactic Name: ampere_scudnn_128x128_relu_small_nn_v1 Tactic: 0x503619c69ae500ff
[08/10/2023-11:18:09] [V] [TRT] Tactic: 0x503619c69ae500ff Time: 2.42933
[08/10/2023-11:18:09] [V] [TRT] Conv_67 Set Tactic Name: ampere_scudnn_128x128_relu_xregs_large_nn_v1 Tactic: 0x5403ad713f811a18
[08/10/2023-11:18:09] [V] [TRT] Tactic: 0x5403ad713f811a18 Time: 2.62442
[08/10/2023-11:18:09] [V] [TRT] Conv_67 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x128x8_stage3_warpsize1x4x1_g1_ffma_aligna4_alignc4 Tactic: 0x5aa723e0481da855
[08/10/2023-11:18:09] [V] [TRT] Tactic: 0x5aa723e0481da855 Time: 2.73041
[08/10/2023-11:18:09] [V] [TRT] Conv_67 Set Tactic Name: ampere_scudnn_128x64_relu_xregs_large_nn_v1 Tactic: 0x5deb29b7a8e275f7
[08/10/2023-11:18:09] [V] [TRT] Tactic: 0x5deb29b7a8e275f7 Time: 1.36401
[08/10/2023-11:18:09] [V] [TRT] Conv_67 Set Tactic Name: ampere_scudnn_winograd_128x128_ldg1_ldg4_relu_tile148t_nt_v1 Tactic: 0x94119b4c514b211a
[08/10/2023-11:18:09] [V] [TRT] Tactic: 0x94119b4c514b211a Time: 0.421518
[08/10/2023-11:18:09] [V] [TRT] Conv_67 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_aligna4_alignc4 Tactic: 0xa31d27de74b895ff
[08/10/2023-11:18:09] [V] [TRT] Tactic: 0xa31d27de74b895ff Time: 1.55131
[08/10/2023-11:18:09] [V] [TRT] Conv_67 Set Tactic Name: ampere_scudnn_128x32_relu_small_nn_v1 Tactic: 0xa8609adc4e0ceb90
[08/10/2023-11:18:09] [V] [TRT] Tactic: 0xa8609adc4e0ceb90 Time: 0.816306
[08/10/2023-11:18:09] [V] [TRT] Conv_67 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize256x128x8_stage3_warpsize4x2x1_g1_ffma_aligna4_alignc4 Tactic: 0xbb8c3889c7eacd30
[08/10/2023-11:18:09] [V] [TRT] Tactic: 0xbb8c3889c7eacd30 Time: 3.02963
[08/10/2023-11:18:09] [V] [TRT] Conv_67 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_aligna4_alignc4 Tactic: 0xd828f024626fa982
[08/10/2023-11:18:09] [V] [TRT] Tactic: 0xd828f024626fa982 Time: 2.15567
[08/10/2023-11:18:09] [V] [TRT] Conv_67 Set Tactic Name: ampere_scudnn_128x128_relu_medium_nn_v1 Tactic: 0xf067e6205da31c2e
[08/10/2023-11:18:09] [V] [TRT] Tactic: 0xf067e6205da31c2e Time: 2.46236
[08/10/2023-11:18:09] [V] [TRT] Conv_67 Set Tactic Name: ampere_scudnn_128x64_relu_medium_nn_v1 Tactic: 0xf64396b97c889179
[08/10/2023-11:18:09] [V] [TRT] Tactic: 0xf64396b97c889179 Time: 1.4216
[08/10/2023-11:18:09] [V] [TRT] Fastest Tactic: 0x94119b4c514b211a Time: 0.421518
[08/10/2023-11:18:09] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x94119b4c514b211a
[08/10/2023-11:18:09] [V] [TRT] *************** Autotuning format combination: Float(16588800,1,92160,512) -> Float(32400,1,180,1) ***************
[08/10/2023-11:18:09] [V] [TRT] --------------- Timing Runner: Conv_67 (CaskConvolution)
[08/10/2023-11:18:09] [V] [TRT] Conv_67 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_aligna4_alignc4 Tactic: 0x19b688348f983aa0
[08/10/2023-11:18:09] [V] [TRT] Tactic: 0x19b688348f983aa0 Time: 1.45354
[08/10/2023-11:18:09] [V] [TRT] Conv_67 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x64x8_stage3_warpsize2x2x1_g1_ffma_aligna4_alignc4 Tactic: 0x1da91d865428f237
[08/10/2023-11:18:09] [V] [TRT] Tactic: 0x1da91d865428f237 Time: 1.16045
[08/10/2023-11:18:09] [V] [TRT] Conv_67 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x256x8_stage3_warpsize1x4x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0x3f0c846d6379bc98
[08/10/2023-11:18:09] [V] [TRT] Tactic: 0x3f0c846d6379bc98 Time: 4.28302
[08/10/2023-11:18:09] [V] [TRT] Conv_67 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_aligna4_alignc4 Tactic: 0x62835fce994f06dd
[08/10/2023-11:18:09] [V] [TRT] Tactic: 0x62835fce994f06dd Time: 1.23134
[08/10/2023-11:18:09] [V] [TRT] Conv_67 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize256x64x8_stage3_warpsize2x2x1_g1_ffma_aligna4_alignc4 Tactic: 0x8014228ec08b4d49
[08/10/2023-11:18:10] [V] [TRT] Tactic: 0x8014228ec08b4d49 Time: 1.19719
[08/10/2023-11:18:10] [V] [TRT] Conv_67 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0x94a7db94ba744c45
[08/10/2023-11:18:10] [V] [TRT] Tactic: 0x94a7db94ba744c45 Time: 1.24965
[08/10/2023-11:18:10] [V] [TRT] Conv_67 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x128x8_stage3_warpsize1x4x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0xd15dd11d64344e83
[08/10/2023-11:18:10] [V] [TRT] Tactic: 0xd15dd11d64344e83 Time: 2.25136
[08/10/2023-11:18:10] [V] [TRT] Conv_67 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x64x8_stage3_warpsize2x2x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0xf48db81f02eca9ee
[08/10/2023-11:18:10] [V] [TRT] Tactic: 0xf48db81f02eca9ee Time: 1.16046
[08/10/2023-11:18:10] [V] [TRT] Fastest Tactic: 0x1da91d865428f237 Time: 1.16045
[08/10/2023-11:18:10] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x1da91d865428f237
[08/10/2023-11:18:10] [V] [TRT] *************** Autotuning format combination: Float(4147200,1:4,23040,128) -> Float(32400,1:4,180,1) ***************
[08/10/2023-11:18:10] [V] [TRT] --------------- Timing Runner: Conv_67 (CaskConvolution)
[08/10/2023-11:18:10] [V] [TRT] Conv_67 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize64x128x32_stage5_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0x112d7e4d280f396d
[08/10/2023-11:18:10] [V] [TRT] Tactic: 0x112d7e4d280f396d Time: 0.850789
[08/10/2023-11:18:10] [V] [TRT] Conv_67 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x64x32_stage5_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0x13cc2ad8dcd32ba2
[08/10/2023-11:18:10] [V] [TRT] Tactic: 0x13cc2ad8dcd32ba2 Time: 0.468763
[08/10/2023-11:18:10] [V] [TRT] Conv_67 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize256x128x32_stage3_warpsize4x2x1_g1_tensor16x8x8_t1r3s3 Tactic: 0x21246c8544eff903
[08/10/2023-11:18:10] [V] [TRT] Tactic: 0x21246c8544eff903 Time: 0.524174
[08/10/2023-11:18:10] [V] [TRT] Conv_67 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x128x32_stage4_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0x25b2b9d5c9d5ca0d
[08/10/2023-11:18:10] [V] [TRT] Tactic: 0x25b2b9d5c9d5ca0d Time: 0.587035
[08/10/2023-11:18:10] [V] [TRT] Conv_67 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage4_warpsize2x2x1_g1_tensor16x8x8_t1r3s3 Tactic: 0x2d80d2b384ab13f1
[08/10/2023-11:18:10] [V] [TRT] Tactic: 0x2d80d2b384ab13f1 Time: 0.512338
[08/10/2023-11:18:10] [V] [TRT] Conv_67 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize256x128x32_stage3_warpsize4x2x1_g1_tensor16x8x8 Tactic: 0x36303f399d3ee4fd
[08/10/2023-11:18:10] [V] [TRT] Tactic: 0x36303f399d3ee4fd Time: 0.635831
[08/10/2023-11:18:10] [V] [TRT] Conv_67 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage4_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0x385bfab9332b1413
[08/10/2023-11:18:10] [V] [TRT] Tactic: 0x385bfab9332b1413 Time: 0.578263
[08/10/2023-11:18:10] [V] [TRT] Conv_67 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x256x32_stage3_warpsize2x4x1_g1_tensor16x8x8 Tactic: 0x482b48242255b8ce
[08/10/2023-11:18:10] [V] [TRT] Tactic: 0x482b48242255b8ce Time: 1.02569
[08/10/2023-11:18:10] [V] [TRT] Conv_67 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x256x32_stage3_warpsize2x4x1_g1_tensor16x8x8_t1r3s3 Tactic: 0x614e89f7852edbc3
[08/10/2023-11:18:10] [V] [TRT] Tactic: 0x614e89f7852edbc3 Time: 0.97259
[08/10/2023-11:18:10] [V] [TRT] Conv_67 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0x65e41d81f093b482
[08/10/2023-11:18:10] [V] [TRT] Tactic: 0x65e41d81f093b482 Time: 0.597344
[08/10/2023-11:18:10] [V] [TRT] Conv_67 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize64x128x32_stage5_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0x65f71607d20d5438
[08/10/2023-11:18:10] [V] [TRT] Tactic: 0x65f71607d20d5438 Time: 0.794011
[08/10/2023-11:18:10] [V] [TRT] Conv_67 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x64x32_stage5_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0x6716429226d146f7
[08/10/2023-11:18:10] [V] [TRT] Tactic: 0x6716429226d146f7 Time: 0.422231
[08/10/2023-11:18:10] [V] [TRT] Conv_67 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize64x32x64_stage5_warpsize2x2x1_g1_tensor16x8x8_t1r3s3 Tactic: 0x679a281f7660c436
[08/10/2023-11:18:10] [V] [TRT] Tactic: 0x679a281f7660c436 Time: 0.397989
[08/10/2023-11:18:10] [V] [TRT] Conv_67 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize64x32x64_stage5_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0x6e2a3c7a7fc5e4e1
[08/10/2023-11:18:10] [V] [TRT] Tactic: 0x6e2a3c7a7fc5e4e1 Time: 0.423003
[08/10/2023-11:18:10] [V] [TRT] Conv_67 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x64x32_stage5_warpsize2x2x1_g1_tensor16x8x8_t1r3s3 Tactic: 0x7aa21282feb15f1a
[08/10/2023-11:18:10] [V] [TRT] Tactic: 0x7aa21282feb15f1a Time: 0.419479
[08/10/2023-11:18:10] [V] [TRT] Conv_67 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage4_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0x818413b69874bb35
[08/10/2023-11:18:10] [V] [TRT] Tactic: 0x818413b69874bb35 Time: 0.57248
[08/10/2023-11:18:10] [V] [TRT] Conv_67 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x256x32_stage3_warpsize2x4x1_g1_tensor16x8x8 Tactic: 0x998c97842e775a17
[08/10/2023-11:18:10] [V] [TRT] Tactic: 0x998c97842e775a17 Time: 1.03525
[08/10/2023-11:18:10] [V] [TRT] Conv_67 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8_t1r3s3 Tactic: 0x999e005e3b016ea6
[08/10/2023-11:18:10] [V] [TRT] Tactic: 0x999e005e3b016ea6 Time: 0.537765
[08/10/2023-11:18:10] [V] [TRT] Conv_67 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize256x64x32_stage3_warpsize4x2x1_g1_tensor16x8x8_t1r3s3 Tactic: 0xa9a06d0633580c0c
[08/10/2023-11:18:10] [V] [TRT] Tactic: 0xa9a06d0633580c0c Time: 0.338002
[08/10/2023-11:18:10] [V] [TRT] Conv_67 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0xb443c221fcb1565b
[08/10/2023-11:18:10] [V] [TRT] Tactic: 0xb443c221fcb1565b Time: 0.555598
[08/10/2023-11:18:10] [V] [TRT] Conv_67 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage3_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0xb47d7d926de02dee
[08/10/2023-11:18:10] [V] [TRT] Tactic: 0xb47d7d926de02dee Time: 0.424649
[08/10/2023-11:18:10] [V] [TRT] Conv_67 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize256x64x32_stage3_warpsize4x2x1_g1_tensor16x8x8 Tactic: 0xba0185279ccb2b85
[08/10/2023-11:18:10] [V] [TRT] Tactic: 0xba0185279ccb2b85 Time: 0.376965
[08/10/2023-11:18:10] [V] [TRT] Conv_67 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage3_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0xc0a715d897e240bb
[08/10/2023-11:18:10] [V] [TRT] Tactic: 0xc0a715d897e240bb Time: 0.423054
[08/10/2023-11:18:10] [V] [TRT] Conv_67 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize256x64x32_stage3_warpsize4x2x1_g1_tensor16x8x8 Tactic: 0xcedbed6d66c946d0
[08/10/2023-11:18:10] [V] [TRT] Tactic: 0xcedbed6d66c946d0 Time: 0.352571
[08/10/2023-11:18:10] [V] [TRT] Conv_67 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize64x128x32_stage5_warpsize2x2x1_g1_tensor16x8x8_t1r3s3 Tactic: 0xd4428a7355769d7d
[08/10/2023-11:18:10] [V] [TRT] Tactic: 0xd4428a7355769d7d Time: 0.765015
[08/10/2023-11:18:10] [V] [TRT] Conv_67 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize64x32x64_stage5_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0xd7f5d575d49a4bc7
[08/10/2023-11:18:10] [V] [TRT] Tactic: 0xd7f5d575d49a4bc7 Time: 0.476814
[08/10/2023-11:18:10] [V] [TRT] Conv_67 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x128x32_stage4_warpsize2x2x1_g1_tensor16x8x8_t1r3s3 Tactic: 0xd920b33c9bd27143
[08/10/2023-11:18:10] [V] [TRT] Tactic: 0xd920b33c9bd27143 Time: 0.676347
[08/10/2023-11:18:10] [V] [TRT] Conv_67 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize256x128x32_stage3_warpsize4x2x1_g1_tensor16x8x8 Tactic: 0xe797e099911c0624
[08/10/2023-11:18:10] [V] [TRT] Tactic: 0xe797e099911c0624 Time: 0.55605
[08/10/2023-11:18:10] [V] [TRT] Conv_67 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage3_warpsize2x2x1_g1_tensor16x8x8_t1r3s3 Tactic: 0xeb65d4c1e8fa2294
[08/10/2023-11:18:10] [V] [TRT] Tactic: 0xeb65d4c1e8fa2294 Time: 0.403717
[08/10/2023-11:18:10] [V] [TRT] Conv_67 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x128x32_stage4_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0xf4156675c5f728d4
[08/10/2023-11:18:10] [V] [TRT] Tactic: 0xf4156675c5f728d4 Time: 0.830674
[08/10/2023-11:18:10] [V] [TRT] Fastest Tactic: 0xa9a06d0633580c0c Time: 0.338002
[08/10/2023-11:18:10] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0xa9a06d0633580c0c
[08/10/2023-11:18:10] [V] [TRT] *************** Autotuning format combination: Half(16588800,32400,180,1) -> Half(32400,32400,180,1) ***************
[08/10/2023-11:18:10] [V] [TRT] --------------- Timing Runner: Conv_67 (CudnnConvolution)
[08/10/2023-11:18:10] [V] [TRT] Tactic: 0x0000000000000000 Time: 2.93896
[08/10/2023-11:18:10] [V] [TRT] Tactic: 0x0000000000000001 Time: 0.84149
[08/10/2023-11:18:10] [V] [TRT] Tactic: 0x0000000000000002 Time: 3.9056
[08/10/2023-11:18:10] [V] [TRT] Tactic: 0x0000000000000004 Time: 3.02849
[08/10/2023-11:18:11] [V] [TRT] Tactic: 0x0000000000000005 Time: 1.05529
[08/10/2023-11:18:11] [V] [TRT] Tactic: 0x0000000000000006 Time: 4.12591
[08/10/2023-11:18:11] [V] [TRT] Tactic: 0x0000000000000038 Time: 2.91569
[08/10/2023-11:18:11] [V] [TRT] Tactic: 0x000000000000003a Time: 3.35317
[08/10/2023-11:18:11] [V] [TRT] Tactic: 0x000000000000003c Time: 3.01218
[08/10/2023-11:18:11] [V] [TRT] Tactic: 0x000000000000003d Time: 1.06915
[08/10/2023-11:18:11] [V] [TRT] Tactic: 0x000000000000003e Time: 4.11028
[08/10/2023-11:18:11] [V] [TRT] Fastest Tactic: 0x0000000000000001 Time: 0.84149
[08/10/2023-11:18:11] [V] [TRT] --------------- Timing Runner: Conv_67 (CaskConvolution)
[08/10/2023-11:18:11] [V] [TRT] CaskConvolution has no valid tactics for this config, skipping
[08/10/2023-11:18:11] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CudnnConvolution Tactic: 0x0000000000000001
[08/10/2023-11:18:11] [V] [TRT] *************** Autotuning format combination: Half(8294400,32400:2,180,1) -> Half(32400,32400:2,180,1) ***************
[08/10/2023-11:18:11] [V] [TRT] --------------- Timing Runner: Conv_67 (FusedConvActConvolution)
[08/10/2023-11:18:11] [V] [TRT] FusedConvActConvolution has no valid tactics for this config, skipping
[08/10/2023-11:18:11] [V] [TRT] --------------- Timing Runner: Conv_67 (CaskConvolution)
[08/10/2023-11:18:11] [V] [TRT] Conv_67 Set Tactic Name: ampere_fp16x2_hcudnn_fp16x2_128x32_relu_medium_nn_v1 Tactic: 0x1e7896ba71ef1635
[08/10/2023-11:18:11] [V] [TRT] Tactic: 0x1e7896ba71ef1635 Time: 0.462313
[08/10/2023-11:18:11] [V] [TRT] Conv_67 Set Tactic Name: ampere_fp16x2_hcudnn_fp16x2_128x64_relu_large_nn_v1 Tactic: 0x2f735ffbb05a30fd
[08/10/2023-11:18:11] [V] [TRT] Tactic: 0x2f735ffbb05a30fd Time: 0.83403
[08/10/2023-11:18:11] [V] [TRT] Conv_67 Set Tactic Name: ampere_fp16x2_hcudnn_fp16x2_128x128_relu_medium_nn_v1 Tactic: 0x360278e347d63410
[08/10/2023-11:18:11] [V] [TRT] Tactic: 0x360278e347d63410 Time: 1.68247
[08/10/2023-11:18:11] [V] [TRT] Conv_67 Set Tactic Name: ampere_fp16x2_hcudnn_fp16x2_128x64_relu_medium_nn_v1 Tactic: 0x4cfee77ea8c324db
[08/10/2023-11:18:11] [V] [TRT] Tactic: 0x4cfee77ea8c324db Time: 0.82789
[08/10/2023-11:18:11] [V] [TRT] Conv_67 Set Tactic Name: ampere_fp16x2_hcudnn_fp16x2_128x64_relu_small_nn_v1 Tactic: 0x540fde3a7bee53dc
[08/10/2023-11:18:11] [V] [TRT] Tactic: 0x540fde3a7bee53dc Time: 0.81819
[08/10/2023-11:18:11] [V] [TRT] Conv_67 Set Tactic Name: ampere_fp16x2_hcudnn_fp16x2_128x128_relu_large_nn_v1 Tactic: 0x91f7e9c0851ad67c
[08/10/2023-11:18:11] [V] [TRT] Tactic: 0x91f7e9c0851ad67c Time: 1.73747
[08/10/2023-11:18:11] [V] [TRT] Conv_67 Set Tactic Name: ampere_fp16x2_hcudnn_fp16x2_128x32_relu_large_nn_v1 Tactic: 0xb837f96ef306f686
[08/10/2023-11:18:11] [V] [TRT] Tactic: 0xb837f96ef306f686 Time: 0.464389
[08/10/2023-11:18:11] [V] [TRT] Conv_67 Set Tactic Name: ampere_fp16x2_hcudnn_fp16x2_128x32_relu_small_nn_v1 Tactic: 0xc34b78af38b295a7
[08/10/2023-11:18:11] [V] [TRT] Tactic: 0xc34b78af38b295a7 Time: 0.466258
[08/10/2023-11:18:11] [V] [TRT] Conv_67 Set Tactic Name: ampere_fp16x2_hcudnn_winograd_fp16x2_128x128_ldg1_ldg4_relu_tile148t_nt_v1 Tactic: 0xc754debea88ae0b7
[08/10/2023-11:18:11] [V] [TRT] Tactic: 0xc754debea88ae0b7 Time: 0.249737
[08/10/2023-11:18:11] [V] [TRT] Conv_67 Set Tactic Name: ampere_fp16x2_hcudnn_fp16x2_128x128_relu_small_nn_v1 Tactic: 0xea8b68014eaeb55d
[08/10/2023-11:18:11] [V] [TRT] Tactic: 0xea8b68014eaeb55d Time: 1.74501
[08/10/2023-11:18:11] [V] [TRT] Fastest Tactic: 0xc754debea88ae0b7 Time: 0.249737
[08/10/2023-11:18:11] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0xc754debea88ae0b7
[08/10/2023-11:18:11] [V] [TRT] *************** Autotuning format combination: Half(2073600,1:8,11520,64) -> Float(32400,32400,180,1) ***************
[08/10/2023-11:18:11] [V] [TRT] --------------- Timing Runner: Conv_67 (CaskConvolution)
[08/10/2023-11:18:11] [V] [TRT] CaskConvolution has no valid tactics for this config, skipping
[08/10/2023-11:18:11] [V] [TRT] *************** Autotuning format combination: Half(2073600,1:8,11520,64) -> Half(32400,1:8,180,1) ***************
[08/10/2023-11:18:11] [V] [TRT] --------------- Timing Runner: Conv_67 (CudaDepthwiseConvolution)
[08/10/2023-11:18:11] [V] [TRT] CudaDepthwiseConvolution has no valid tactics for this config, skipping
[08/10/2023-11:18:11] [V] [TRT] --------------- Timing Runner: Conv_67 (CaskConvolution)
[08/10/2023-11:18:11] [V] [TRT] Conv_67 Set Tactic Name: ampere_h16816cudnn_256x128_ldg8_relu_exp_medium_nhwc_tn_v1 Tactic: 0x03896956a39a1203
[08/10/2023-11:18:11] [V] [TRT] Tactic: 0x03896956a39a1203 Time: 0.302702
[08/10/2023-11:18:11] [V] [TRT] Conv_67 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x128x32_stage3_warpsize4x2x1_g1_tensor16x8x16_t1r3s3 Tactic: 0x048d6d0400f33439
[08/10/2023-11:18:11] [V] [TRT] Tactic: 0x048d6d0400f33439 Time: 0.284251
[08/10/2023-11:18:11] [V] [TRT] Conv_67 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x64x64_stage3_warpsize2x2x1_g1_tensor16x8x16_t1r3s3 Tactic: 0x05b6220f243edacd
[08/10/2023-11:18:11] [V] [TRT] Tactic: 0x05b6220f243edacd Time: 0.151173
[08/10/2023-11:18:11] [V] [TRT] Conv_67 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x64x32_stage3_warpsize4x1x1_g1_tensor16x8x16_t1r3s3_aACCESS Tactic: 0x0866ddee325d07a6
[08/10/2023-11:18:11] [V] [TRT] Tactic: 0x0866ddee325d07a6 Time: 0.159488
[08/10/2023-11:18:11] [V] [TRT] Conv_67 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x128x32_stage4_warpsize2x2x1_g1_tensor16x8x16 Tactic: 0x0e07ff4f4f7c1ac9
[08/10/2023-11:18:11] [V] [TRT] Tactic: 0x0e07ff4f4f7c1ac9 Time: 0.269765
[08/10/2023-11:18:11] [V] [TRT] Conv_67 Set Tactic Name: ampere_h16816cudnn_256x128_ldg8_relu_exp_stages_64x3_small_nhwc_tn_v1 Tactic: 0x0e199750c30c6928
[08/10/2023-11:18:11] [V] [TRT] Tactic: 0x0e199750c30c6928 Time: 0.303104
[08/10/2023-11:18:11] [V] [TRT] Conv_67 Set Tactic Name: ampere_h1688cudnn_128x128_ldg8_relu_exp_medium_nhwc_tn_v1 Tactic: 0x100c1ad308e08d35
[08/10/2023-11:18:11] [V] [TRT] Tactic: 0x100c1ad308e08d35 Time: 0.347671
[08/10/2023-11:18:11] [V] [TRT] Conv_67 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x128x32_stage5_warpsize2x2x1_g1_tensor16x8x16_t1r3s3 Tactic: 0x17ebf0c9f418f10a
[08/10/2023-11:18:11] [V] [TRT] Tactic: 0x17ebf0c9f418f10a Time: 0.380128
[08/10/2023-11:18:11] [V] [TRT] Conv_67 Set Tactic Name: ampere_h16816cudnn_128x64_ldg8_relu_exp_stages_64x4_medium_nhwc_tn_v1 Tactic: 0x196cbc423a9bb69e
[08/10/2023-11:18:11] [V] [TRT] Tactic: 0x196cbc423a9bb69e Time: 0.448114
[08/10/2023-11:18:11] [V] [TRT] Conv_67 Set Tactic Name: ampere_h16816cudnn_128x64_sliced1x2_ldg8_relu_exp_stages_64x3_small_nhwc_tn_v1 Tactic: 0x19822de884f42a6a
[08/10/2023-11:18:11] [V] [TRT] Tactic: 0x19822de884f42a6a Time: 0.162473
[08/10/2023-11:18:11] [V] [TRT] Conv_67 Set Tactic Name: ampere_h16816cudnn_64x64_sliced1x2_ldg8_relu_exp_stages_64x5_small_nhwc_tn_v1 Tactic: 0x1a5ba808ad4cc5a8
[08/10/2023-11:18:11] [V] [TRT] Tactic: 0x1a5ba808ad4cc5a8 Time: 0.243543
[08/10/2023-11:18:11] [V] [TRT] Conv_67 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x64x64_stage3_warpsize2x2x1_g1_tensor16x8x16 Tactic: 0x21b295c0c8f6c95a
[08/10/2023-11:18:11] [V] [TRT] Tactic: 0x21b295c0c8f6c95a Time: 0.171282
[08/10/2023-11:18:11] [V] [TRT] Conv_67 Set Tactic Name: ampere_h16816cudnn_128x128_ldg8_relu_exp_medium_nhwc_tn_v1 Tactic: 0x234580e8a194335c
[08/10/2023-11:18:11] [V] [TRT] Tactic: 0x234580e8a194335c Time: 0.25088
[08/10/2023-11:18:11] [V] [TRT] Conv_67 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x128x32_stage4_warpsize2x2x1_g1_tensor16x8x16_t1r3s3_aACCESS Tactic: 0x245530c34bd6090f
[08/10/2023-11:18:11] [V] [TRT] Tactic: 0x245530c34bd6090f Time: 0.25771
[08/10/2023-11:18:11] [V] [TRT] Conv_67 Set Tactic Name: ampere_h16816cudnn_64x64_sliced1x2_ldg8_relu_exp_stages_64x6_small_nhwc_tn_v1 Tactic: 0x24e01e7405cfdfe9
[08/10/2023-11:18:11] [V] [TRT] Tactic: 0x24e01e7405cfdfe9 Time: 0.357746
[08/10/2023-11:18:11] [V] [TRT] Conv_67 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x64x64_stage3_warpsize2x2x1_g1_tensor16x8x16_t1r3s3_aACCESS Tactic: 0x263a38afd75e3a43
[08/10/2023-11:18:11] [V] [TRT] Tactic: 0x263a38afd75e3a43 Time: 0.163259
[08/10/2023-11:18:11] [V] [TRT] Conv_67 Set Tactic Name: ampere_h16816cudnn_64x128_sliced1x2_ldg8_relu_exp_stages_64x4_large_nhwc_tn_v1 Tactic: 0x2721a7f18c2700db
[08/10/2023-11:18:11] [V] [TRT] Tactic: 0x2721a7f18c2700db Time: 0.497746
[08/10/2023-11:18:11] [V] [TRT] Conv_67 Set Tactic Name: ampere_h16816cudnn_128x64_sliced1x2_ldg8_relu_exp_stages_64x3_medium_nhwc_tn_v1 Tactic: 0x29329b5741ea05f2
[08/10/2023-11:18:11] [V] [TRT] Tactic: 0x29329b5741ea05f2 Time: 0.169161
[08/10/2023-11:18:11] [V] [TRT] Conv_67 Set Tactic Name: ampere_h16816cudnn_128x128_ldg8_relu_exp_large_nhwc_tn_v1 Tactic: 0x2970ae65966d569d
[08/10/2023-11:18:11] [V] [TRT] Tactic: 0x2970ae65966d569d Time: 0.256302
[08/10/2023-11:18:11] [V] [TRT] Conv_67 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x256x64_stage3_warpsize1x4x1_g1_tensor16x8x16 Tactic: 0x29dc14520e0e4eb6
[08/10/2023-11:18:11] [V] [TRT] Tactic: 0x29dc14520e0e4eb6 Time: 0.638926
[08/10/2023-11:18:11] [V] [TRT] Conv_67 Set Tactic Name: ampere_h16816cudnn_256x64_ldg8_relu_exp_stages_64x3_large_nhwc_tn_v1 Tactic: 0x2cb4a662694e89d3
[08/10/2023-11:18:11] [V] [TRT] Tactic: 0x2cb4a662694e89d3 Time: 0.199945
[08/10/2023-11:18:11] [V] [TRT] Conv_67 Set Tactic Name: ampere_h16816cudnn_128x128_ldg8_relu_exp_stages_64x3_medium_nhwc_tn_v1 Tactic: 0x2eaa2202de9404d6
[08/10/2023-11:18:11] [V] [TRT] Tactic: 0x2eaa2202de9404d6 Time: 0.34528
[08/10/2023-11:18:11] [V] [TRT] Conv_67 Set Tactic Name: ampere_h1688cudnn_256x128_ldg8_relu_exp_medium_nhwc_tn_v1 Tactic: 0x30c0f36d0aeeac6a
[08/10/2023-11:18:11] [V] [TRT] Tactic: 0x30c0f36d0aeeac6a Time: 0.364741
[08/10/2023-11:18:11] [V] [TRT] Conv_67 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x64x64_stage4_warpsize2x2x1_g1_tensor16x8x16_t1r3s3_aACCESS Tactic: 0x30e8a8d7a953e5e9
[08/10/2023-11:18:11] [V] [TRT] Tactic: 0x30e8a8d7a953e5e9 Time: 0.201289
[08/10/2023-11:18:11] [V] [TRT] Conv_67 Set Tactic Name: ampere_h16816cudnn_128x64_ldg8_relu_exp_stages_64x4_large_nhwc_tn_v1 Tactic: 0x3197d5044d71e98e
[08/10/2023-11:18:11] [V] [TRT] Tactic: 0x3197d5044d71e98e Time: 0.25744
[08/10/2023-11:18:11] [V] [TRT] Conv_67 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x64x64_stage4_warpsize2x2x1_g1_tensor16x8x16_t1r3s3 Tactic: 0x31d93dc22d2af081
[08/10/2023-11:18:11] [V] [TRT] Tactic: 0x31d93dc22d2af081 Time: 0.195525
[08/10/2023-11:18:11] [V] [TRT] Conv_67 Set Tactic Name: ampere_h1688cudnn_128x128_ldg8_relu_exp_small_nhwc_tn_v1 Tactic: 0x3369260c04f9ad73
[08/10/2023-11:18:11] [V] [TRT] Tactic: 0x3369260c04f9ad73 Time: 0.331781
[08/10/2023-11:18:11] [V] [TRT] Conv_67 Set Tactic Name: ampere_h16816cudnn_64x128_ldg8_relu_exp_stages_64x4_small_nhwc_tn_v1 Tactic: 0x3b5fa0f2a8fc2410
[08/10/2023-11:18:11] [V] [TRT] Tactic: 0x3b5fa0f2a8fc2410 Time: 0.428745
[08/10/2023-11:18:11] [V] [TRT] Conv_67 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x32x32_stage4_warpsize4x1x1_g1_tensor16x8x16_t1r3s3 Tactic: 0x3e7eb35b91b9fa63
[08/10/2023-11:18:11] [V] [TRT] Tactic: 0x3e7eb35b91b9fa63 Time: 0.111673
[08/10/2023-11:18:11] [V] [TRT] Conv_67 Set Tactic Name: ampere_h16816cudnn_256x64_ldg8_relu_exp_stages_64x3_medium_nhwc_tn_v1 Tactic: 0x3f031df0e579d52c
[08/10/2023-11:18:11] [V] [TRT] Tactic: 0x3f031df0e579d52c Time: 0.196027
[08/10/2023-11:18:11] [V] [TRT] Conv_67 Set Tactic Name: ampere_h16816cudnn_256x64_sliced1x2_ldg8_relu_exp_small_nhwc_tn_v1 Tactic: 0x42a5b8709d0039be
[08/10/2023-11:18:11] [V] [TRT] Tactic: 0x42a5b8709d0039be Time: 0.210519
[08/10/2023-11:18:11] [V] [TRT] Conv_67 Set Tactic Name: ampere_h16816cudnn_256x128_ldg8_relu_exp_small_nhwc_tn_v1 Tactic: 0x463794ee4acffd1f
[08/10/2023-11:18:11] [V] [TRT] Tactic: 0x463794ee4acffd1f Time: 0.298725
[08/10/2023-11:18:11] [V] [TRT] Conv_67 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x64x64_stage4_warpsize2x2x1_g1_tensor16x8x16 Tactic: 0x4a81ea1e51436a30
[08/10/2023-11:18:11] [V] [TRT] Tactic: 0x4a81ea1e51436a30 Time: 0.219127
[08/10/2023-11:18:11] [V] [TRT] Conv_67 Set Tactic Name: ampere_h1688cudnn_256x64_sliced1x2_ldg8_relu_exp_small_nhwc_tn_v1 Tactic: 0x4aed1956bd10a795
[08/10/2023-11:18:11] [V] [TRT] Tactic: 0x4aed1956bd10a795 Time: 0.239941
[08/10/2023-11:18:11] [V] [TRT] Conv_67 Set Tactic Name: ampere_h16816cudnn_64x128_sliced1x2_ldg8_relu_exp_stages_64x3_large_nhwc_tn_v1 Tactic: 0x4fc05923455fc266
[08/10/2023-11:18:11] [V] [TRT] Tactic: 0x4fc05923455fc266 Time: 0.301975
[08/10/2023-11:18:11] [V] [TRT] Conv_67 Set Tactic Name: ampere_h16816cudnn_256x64_ldg8_relu_exp_large_nhwc_tn_v1 Tactic: 0x5013c38f55afa9ba
[08/10/2023-11:18:11] [V] [TRT] Tactic: 0x5013c38f55afa9ba Time: 0.177655
[08/10/2023-11:18:11] [V] [TRT] Conv_67 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x64x32_stage3_warpsize4x1x1_g1_tensor16x8x16_t1r3s3 Tactic: 0x529f4431bdae94f5
[08/10/2023-11:18:11] [V] [TRT] Tactic: 0x529f4431bdae94f5 Time: 0.147666
[08/10/2023-11:18:11] [V] [TRT] Conv_67 Set Tactic Name: ampere_h16816cudnn_64x128_ldg8_relu_exp_stages_64x3_small_nhwc_tn_v1 Tactic: 0x53be5e206184e6ad
[08/10/2023-11:18:11] [V] [TRT] Tactic: 0x53be5e206184e6ad Time: 0.296832
[08/10/2023-11:18:11] [V] [TRT] Conv_67 Set Tactic Name: ampere_h16816cudnn_64x64_ldg8_relu_exp_stages_64x5_large_nhwc_tn_v1 Tactic: 0x544bff7ff9c5d908
[08/10/2023-11:18:11] [V] [TRT] Tactic: 0x544bff7ff9c5d908 Time: 0.258231
[08/10/2023-11:18:11] [V] [TRT] Conv_67 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x64x64_stage3_warpsize2x2x1_g1_tensor16x8x16 Tactic: 0x5568fd8a32f4a40f
[08/10/2023-11:18:11] [V] [TRT] Tactic: 0x5568fd8a32f4a40f Time: 0.156078
[08/10/2023-11:18:11] [V] [TRT] Conv_67 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x128x64_stage3_warpsize2x2x1_g1_tensor16x8x16 Tactic: 0x57c9a5ff682354a6
[08/10/2023-11:18:11] [V] [TRT] Tactic: 0x57c9a5ff682354a6 Time: 0.325627
[08/10/2023-11:18:11] [V] [TRT] Conv_67 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x32x64_stage5_warpsize2x2x1_g1_tensor16x8x16 Tactic: 0x5820b3dda403c4d0
[08/10/2023-11:18:11] [V] [TRT] Tactic: 0x5820b3dda403c4d0 Time: 0.179762
[08/10/2023-11:18:11] [V] [TRT] Conv_67 Set Tactic Name: ampere_h16816cudnn_128x128_ldg8_relu_exp_stages_64x3_small_nhwc_tn_v1 Tactic: 0x58816bf2e9c36afb
[08/10/2023-11:18:11] [V] [TRT] Tactic: 0x58816bf2e9c36afb Time: 0.341737
[08/10/2023-11:18:11] [V] [TRT] Conv_67 Set Tactic Name: ampere_h16816cudnn_128x64_ldg8_relu_exp_stages_64x3_large_nhwc_tn_v1 Tactic: 0x59762bd684092b33
[08/10/2023-11:18:11] [V] [TRT] Tactic: 0x59762bd684092b33 Time: 0.176978
[08/10/2023-11:18:11] [V] [TRT] Conv_67 Set Tactic Name: ampere_h16816cudnn_128x64_sliced1x2_ldg8_relu_exp_stages_64x4_medium_nhwc_tn_v1 Tactic: 0x59c5c647b4c76593
[08/10/2023-11:18:11] [V] [TRT] Tactic: 0x59c5c647b4c76593 Time: 0.253486
[08/10/2023-11:18:11] [V] [TRT] Conv_67 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x128x64_stage3_warpsize4x2x1_g1_tensor16x8x16 Tactic: 0x5a55274960724ba8
[08/10/2023-11:18:11] [V] [TRT] Tactic: 0x5a55274960724ba8 Time: 0.296265
[08/10/2023-11:18:11] [V] [TRT] Conv_67 Set Tactic Name: ampere_h1688cudnn_256x128_ldg8_relu_exp_large_nhwc_tn_v1 Tactic: 0x5c2e1c87d85b06f1
[08/10/2023-11:18:11] [V] [TRT] Tactic: 0x5c2e1c87d85b06f1 Time: 0.36693
[08/10/2023-11:18:11] [V] [TRT] Conv_67 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x256x64_stage3_warpsize1x4x1_g1_tensor16x8x16 Tactic: 0x5d067c18f40c23e3
[08/10/2023-11:18:12] [V] [TRT] Tactic: 0x5d067c18f40c23e3 Time: 0.623639
[08/10/2023-11:18:12] [V] [TRT] Conv_67 Set Tactic Name: ampere_h16816cudnn_64x64_ldg8_relu_exp_stages_64x5_medium_nhwc_tn_v1 Tactic: 0x5f31c22ec167f384
[08/10/2023-11:18:12] [V] [TRT] Tactic: 0x5f31c22ec167f384 Time: 0.257856
[08/10/2023-11:18:12] [V] [TRT] Conv_67 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x128x32_stage4_warpsize2x2x1_g1_tensor16x8x16_t1r3s3 Tactic: 0x60c3421152ef8e10
[08/10/2023-11:18:12] [V] [TRT] Tactic: 0x60c3421152ef8e10 Time: 0.246999
[08/10/2023-11:18:12] [V] [TRT] Conv_67 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x128x32_stage3_warpsize4x2x1_g1_tensor16x8x16 Tactic: 0x60da8c7151d91e47
[08/10/2023-11:18:12] [V] [TRT] Tactic: 0x60da8c7151d91e47 Time: 0.283045
[08/10/2023-11:18:12] [V] [TRT] Conv_67 Set Tactic Name: ampere_h1688cudnn_256x64_ldg8_relu_exp_medium_nhwc_tn_v1 Tactic: 0x6426696f872a3b13
[08/10/2023-11:18:12] [V] [TRT] Tactic: 0x6426696f872a3b13 Time: 0.223813
[08/10/2023-11:18:12] [V] [TRT] Conv_67 Set Tactic Name: ampere_h16816cudnn_128x64_ldg8_relu_exp_stages_64x3_medium_nhwc_tn_v1 Tactic: 0x699be152cfb6d6ff
[08/10/2023-11:18:12] [V] [TRT] Tactic: 0x699be152cfb6d6ff Time: 0.178112
[08/10/2023-11:18:12] [V] [TRT] Conv_67 Set Tactic Name: ampere_h16816cudnn_64x64_ldg8_relu_exp_stages_64x6_large_nhwc_tn_v1 Tactic: 0x6af049035146c349
[08/10/2023-11:18:12] [V] [TRT] Tactic: 0x6af049035146c349 Time: 0.380197
[08/10/2023-11:18:12] [V] [TRT] Conv_67 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x256x32_stage3_warpsize2x4x1_g1_tensor16x8x16 Tactic: 0x7005d10718f6c22d
[08/10/2023-11:18:12] [V] [TRT] Tactic: 0x7005d10718f6c22d Time: 0.52325
[08/10/2023-11:18:12] [V] [TRT] Conv_67 Set Tactic Name: ampere_h16816cudnn_64x64_sliced1x2_ldg8_relu_exp_stages_64x5_medium_nhwc_tn_v1 Tactic: 0x706f08da35c795a5
[08/10/2023-11:18:12] [V] [TRT] Tactic: 0x706f08da35c795a5 Time: 0.255954
[08/10/2023-11:18:12] [V] [TRT] Conv_67 Set Tactic Name: ampere_h16816cudnn_128x64_sliced1x2_ldg8_relu_exp_stages_64x4_small_nhwc_tn_v1 Tactic: 0x7163d33a4d8ce8d7
[08/10/2023-11:18:12] [V] [TRT] Tactic: 0x7163d33a4d8ce8d7 Time: 0.249285
[08/10/2023-11:18:12] [V] [TRT] Conv_67 Set Tactic Name: ampere_h1688cudnn_256x64_ldg8_relu_exp_small_nhwc_tn_v1 Tactic: 0x7aad3976677d7155
[08/10/2023-11:18:12] [V] [TRT] Tactic: 0x7aad3976677d7155 Time: 0.213248
[08/10/2023-11:18:12] [V] [TRT] Conv_67 Set Tactic Name: ampere_h16816cudnn_64x64_sliced1x2_ldg8_relu_exp_stages_64x5_large_nhwc_tn_v1 Tactic: 0x8015519605ab9963
[08/10/2023-11:18:12] [V] [TRT] Tactic: 0x8015519605ab9963 Time: 0.257454
[08/10/2023-11:18:12] [V] [TRT] Conv_67 Set Tactic Name: ampere_h16816cudnn_256x64_sliced1x2_ldg8_relu_exp_medium_nhwc_tn_v1 Tactic: 0x82f8a2214b0b4178
[08/10/2023-11:18:12] [V] [TRT] Tactic: 0x82f8a2214b0b4178 Time: 0.326345
[08/10/2023-11:18:12] [V] [TRT] Conv_67 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x32x32_stage4_warpsize4x1x1_g1_tensor16x8x16 Tactic: 0x834e11ecd4ab9454
[08/10/2023-11:18:12] [V] [TRT] Tactic: 0x834e11ecd4ab9454 Time: 0.114752
[08/10/2023-11:18:12] [V] [TRT] Conv_67 Set Tactic Name: ampere_h16816cudnn_128x64_sliced1x2_ldg8_relu_exp_stages_64x3_large_nhwc_tn_v1 Tactic: 0x83ccd4762c1376a1
[08/10/2023-11:18:12] [V] [TRT] Tactic: 0x83ccd4762c1376a1 Time: 0.169157
[08/10/2023-11:18:12] [V] [TRT] Conv_67 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x64x64_stage3_warpsize4x1x1_g1_tensor16x8x16_t1r3s3 Tactic: 0x84ee897dd1f4d2aa
[08/10/2023-11:18:12] [V] [TRT] Tactic: 0x84ee897dd1f4d2aa Time: 0.183547
[08/10/2023-11:18:12] [V] [TRT] Conv_67 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x128x64_stage3_warpsize2x2x1_g1_tensor16x8x16 Tactic: 0x866e7a5f6401b67f
[08/10/2023-11:18:12] [V] [TRT] Tactic: 0x866e7a5f6401b67f Time: 0.327333
[08/10/2023-11:18:12] [V] [TRT] Conv_67 Set Tactic Name: ampere_h16816cudnn_64x64_ldg8_relu_exp_stages_64x6_medium_nhwc_tn_v1 Tactic: 0x88971eec55aba850
[08/10/2023-11:18:12] [V] [TRT] Tactic: 0x88971eec55aba850 Time: 0.370821
[08/10/2023-11:18:12] [V] [TRT] Conv_67 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x128x64_stage3_warpsize4x2x1_g1_tensor16x8x16 Tactic: 0x8bf2f8e96c50a971
[08/10/2023-11:18:12] [V] [TRT] Tactic: 0x8bf2f8e96c50a971 Time: 0.289495
[08/10/2023-11:18:12] [V] [TRT] Conv_67 Set Tactic Name: ampere_h16816cudnn_256x128_ldg8_relu_exp_stages_64x3_large_nhwc_tn_v1 Tactic: 0x94576ece6beb35e3
[08/10/2023-11:18:12] [V] [TRT] Tactic: 0x94576ece6beb35e3 Time: 0.304361
[08/10/2023-11:18:12] [V] [TRT] Conv_67 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x64x64_stage3_warpsize4x1x1_g1_tensor16x8x16 Tactic: 0x9fff6e65019cc310
[08/10/2023-11:18:12] [V] [TRT] Tactic: 0x9fff6e65019cc310 Time: 0.184105
[08/10/2023-11:18:12] [V] [TRT] Conv_67 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x32x64_stage5_warpsize2x2x1_g1_tensor16x8x16_t1r3s3 Tactic: 0xa033e20ae9f412b2
[08/10/2023-11:18:12] [V] [TRT] Tactic: 0xa033e20ae9f412b2 Time: 0.164297
[08/10/2023-11:18:12] [V] [TRT] Conv_67 Set Tactic Name: ampere_h16816cudnn_64x128_ldg8_relu_exp_stages_64x4_large_nhwc_tn_v1 Tactic: 0xa111596c001b78db
[08/10/2023-11:18:12] [V] [TRT] Tactic: 0xa111596c001b78db Time: 0.440855
[08/10/2023-11:18:12] [V] [TRT] Conv_67 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x256x32_stage3_warpsize2x4x1_g1_tensor16x8x16 Tactic: 0xa1a20ea714d420f4
[08/10/2023-11:18:12] [V] [TRT] Tactic: 0xa1a20ea714d420f4 Time: 0.518491
[08/10/2023-11:18:12] [V] [TRT] Conv_67 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x32x32_stage4_warpsize4x1x1_g1_tensor16x8x16_t1r3s3 Tactic: 0xa40cb43c296a36a8
[08/10/2023-11:18:12] [V] [TRT] Tactic: 0xa40cb43c296a36a8 Time: 0.117605
[08/10/2023-11:18:12] [V] [TRT] Conv_67 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x64x32_stage3_warpsize4x1x1_g1_tensor16x8x16 Tactic: 0xa570c55d303796ff
[08/10/2023-11:18:12] [V] [TRT] Tactic: 0xa570c55d303796ff Time: 0.154103
[08/10/2023-11:18:12] [V] [TRT] Conv_67 Set Tactic Name: ampere_h16816cudnn_64x64_sliced1x2_ldg8_relu_exp_stages_64x6_medium_nhwc_tn_v1 Tactic: 0xa7c9d418a10bce71
[08/10/2023-11:18:12] [V] [TRT] Tactic: 0xa7c9d418a10bce71 Time: 0.398053
[08/10/2023-11:18:12] [V] [TRT] Conv_67 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x32x32_stage4_warpsize4x1x1_g1_tensor16x8x16 Tactic: 0xa83b68f30462f971
[08/10/2023-11:18:12] [V] [TRT] Tactic: 0xa83b68f30462f971 Time: 0.127589
[08/10/2023-11:18:12] [V] [TRT] Conv_67 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x256x32_stage3_warpsize2x4x1_g1_tensor16x8x16_t1r3s3 Tactic: 0xa9177bbe4e767df8
[08/10/2023-11:18:12] [V] [TRT] Tactic: 0xa9177bbe4e767df8 Time: 0.531954
[08/10/2023-11:18:12] [V] [TRT] Conv_67 Set Tactic Name: ampere_h1688cudnn_128x128_ldg8_relu_exp_large_nhwc_tn_v1 Tactic: 0xa927df92ac1ef1b8
[08/10/2023-11:18:12] [V] [TRT] Tactic: 0xa927df92ac1ef1b8 Time: 0.355355
[08/10/2023-11:18:12] [V] [TRT] Conv_67 Set Tactic Name: ampere_h16816cudnn_128x64_ldg8_relu_exp_stages_64x4_small_nhwc_tn_v1 Tactic: 0xabd92c9ae596b545
[08/10/2023-11:18:12] [V] [TRT] Tactic: 0xabd92c9ae596b545 Time: 0.244613
[08/10/2023-11:18:12] [V] [TRT] Conv_67 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x128x32_stage5_warpsize2x2x1_g1_tensor16x8x16 Tactic: 0xafd1e8bf6bd3d638
[08/10/2023-11:18:12] [V] [TRT] Tactic: 0xafd1e8bf6bd3d638 Time: 0.306409
[08/10/2023-11:18:12] [V] [TRT] Conv_67 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x128x32_stage3_warpsize4x2x1_g1_tensor16x8x16 Tactic: 0xb17d53d15dfbfc9e
[08/10/2023-11:18:12] [V] [TRT] Tactic: 0xb17d53d15dfbfc9e Time: 0.289536
[08/10/2023-11:18:12] [V] [TRT] Conv_67 Set Tactic Name: ampere_h16816cudnn_128x128_ldg8_relu_exp_small_nhwc_tn_v1 Tactic: 0xb33e57fb3e8a0a56
[08/10/2023-11:18:12] [V] [TRT] Tactic: 0xb33e57fb3e8a0a56 Time: 0.248882
[08/10/2023-11:18:12] [V] [TRT] Conv_67 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x64x32_stage5_warpsize2x2x1_g1_tensor16x8x16_t1r3s3 Tactic: 0xb4bec086187edcfc
[08/10/2023-11:18:12] [V] [TRT] Tactic: 0xb4bec086187edcfc Time: 0.164128
[08/10/2023-11:18:12] [V] [TRT] Conv_67 Set Tactic Name: ampere_h16816cudnn_256x64_ldg8_relu_exp_stages_64x3_small_nhwc_tn_v1 Tactic: 0xb6fa5ffcc1a9d518
[08/10/2023-11:18:12] [V] [TRT] Tactic: 0xb6fa5ffcc1a9d518 Time: 0.191739
[08/10/2023-11:18:12] [V] [TRT] Conv_67 Set Tactic Name: ampere_h16816cudnn_64x128_sliced1x2_ldg8_relu_exp_stages_64x3_medium_nhwc_tn_v1 Tactic: 0xb85e52e87caf60a2
[08/10/2023-11:18:12] [V] [TRT] Tactic: 0xb85e52e87caf60a2 Time: 0.291429
[08/10/2023-11:18:12] [V] [TRT] Conv_67 Set Tactic Name: ampere_h16816cudnn_64x128_ldg8_relu_exp_stages_64x3_medium_nhwc_tn_v1 Tactic: 0xb8d86216e1235cda
[08/10/2023-11:18:12] [V] [TRT] Tactic: 0xb8d86216e1235cda Time: 0.291776
[08/10/2023-11:18:12] [V] [TRT] Conv_67 Set Tactic Name: ampere_h16816cudnn_256x128_ldg8_relu_exp_stages_64x3_medium_nhwc_tn_v1 Tactic: 0xb9171d70ba2eda4b
[08/10/2023-11:18:12] [V] [TRT] Tactic: 0xb9171d70ba2eda4b Time: 0.303374
[08/10/2023-11:18:12] [V] [TRT] Conv_67 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x64x32_stage5_warpsize2x2x1_g1_tensor16x8x16 Tactic: 0xbd08239a9317f2fd
[08/10/2023-11:18:12] [V] [TRT] Tactic: 0xbd08239a9317f2fd Time: 0.168078
[08/10/2023-11:18:12] [V] [TRT] Conv_67 Set Tactic Name: ampere_h16816cudnn_64x128_sliced1x2_ldg8_relu_exp_stages_64x4_small_nhwc_tn_v1 Tactic: 0xbd6f5e6f24c05c10
[08/10/2023-11:18:12] [V] [TRT] Tactic: 0xbd6f5e6f24c05c10 Time: 0.46923
[08/10/2023-11:18:12] [V] [TRT] Conv_67 Set Tactic Name: ampere_h16816cudnn_64x64_sliced1x2_ldg8_relu_exp_stages_64x6_large_nhwc_tn_v1 Tactic: 0xbeaee7eaad288322
[08/10/2023-11:18:12] [V] [TRT] Tactic: 0xbeaee7eaad288322 Time: 0.39355
[08/10/2023-11:18:12] [V] [TRT] Conv_67 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x128x64_stage3_warpsize2x2x1_g1_tensor16x8x16_t1r3s3 Tactic: 0xc0a02dc6095497cc
[08/10/2023-11:18:12] [V] [TRT] Tactic: 0xc0a02dc6095497cc Time: 0.318496
[08/10/2023-11:18:12] [V] [TRT] Conv_67 Set Tactic Name: ampere_h16816cudnn_128x128_ldg8_relu_exp_stages_64x3_large_nhwc_tn_v1 Tactic: 0xc2cf926c41243630
[08/10/2023-11:18:12] [V] [TRT] Tactic: 0xc2cf926c41243630 Time: 0.344398
[08/10/2023-11:18:12] [V] [TRT] Conv_67 Set Tactic Name: ampere_h16816cudnn_128x64_ldg8_relu_exp_stages_64x3_small_nhwc_tn_v1 Tactic: 0xc338d2482cee77f8
[08/10/2023-11:18:12] [V] [TRT] Tactic: 0xc338d2482cee77f8 Time: 0.175685
[08/10/2023-11:18:12] [V] [TRT] Conv_67 Set Tactic Name: ampere_h1688cudnn_256x128_ldg8_relu_exp_small_nhwc_tn_v1 Tactic: 0xc660e51970bc5a3a
[08/10/2023-11:18:12] [V] [TRT] Tactic: 0xc660e51970bc5a3a Time: 0.342514
[08/10/2023-11:18:12] [V] [TRT] Conv_67 Set Tactic Name: ampere_h16816cudnn_64x128_ldg8_relu_exp_stages_64x4_medium_nhwc_tn_v1 Tactic: 0xc82f3f06140e3cbb
[08/10/2023-11:18:12] [V] [TRT] Tactic: 0xc82f3f06140e3cbb Time: 0.437006
[08/10/2023-11:18:12] [V] [TRT] Conv_67 Set Tactic Name: ampere_h16816cudnn_64x128_sliced1x2_ldg8_relu_exp_stages_64x4_medium_nhwc_tn_v1 Tactic: 0xc8a90ff8898200c3
[08/10/2023-11:18:12] [V] [TRT] Tactic: 0xc8a90ff8898200c3 Time: 0.480562
[08/10/2023-11:18:12] [V] [TRT] Conv_67 Set Tactic Name: ampere_h1688cudnn_256x64_sliced1x2_ldg8_relu_exp_medium_nhwc_tn_v1 Tactic: 0xc9cc55109bb4de26
[08/10/2023-11:18:12] [V] [TRT] Tactic: 0xc9cc55109bb4de26 Time: 0.256242
[08/10/2023-11:18:12] [V] [TRT] Conv_67 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x64x32_stage5_warpsize2x2x1_g1_tensor16x8x16 Tactic: 0xc9d24bd069159fa8
[08/10/2023-11:18:12] [V] [TRT] Tactic: 0xc9d24bd069159fa8 Time: 0.185623
[08/10/2023-11:18:12] [V] [TRT] Conv_67 Set Tactic Name: ampere_h16816cudnn_64x128_ldg8_relu_exp_stages_64x3_large_nhwc_tn_v1 Tactic: 0xc9f0a7bec963ba66
[08/10/2023-11:18:12] [V] [TRT] Tactic: 0xc9f0a7bec963ba66 Time: 0.300041
[08/10/2023-11:18:12] [V] [TRT] Conv_67 Set Tactic Name: ampere_h16816cudnn_256x64_ldg8_relu_exp_small_nhwc_tn_v1 Tactic: 0xca5d3a11fd48f571
[08/10/2023-11:18:12] [V] [TRT] Tactic: 0xca5d3a11fd48f571 Time: 0.164119
[08/10/2023-11:18:12] [V] [TRT] Conv_67 Set Tactic Name: ampere_h16816cudnn_64x64_ldg8_relu_exp_stages_64x5_small_nhwc_tn_v1 Tactic: 0xce0506e1512285c3
[08/10/2023-11:18:12] [V] [TRT] Tactic: 0xce0506e1512285c3 Time: 0.259255
[08/10/2023-11:18:12] [V] [TRT] Conv_67 Set Tactic Name: ampere_h1688cudnn_256x64_sliced1x2_ldg8_relu_exp_large_nhwc_tn_v1 Tactic: 0xd0a3e0c815f7fb5e
[08/10/2023-11:18:12] [V] [TRT] Tactic: 0xd0a3e0c815f7fb5e Time: 0.253865
[08/10/2023-11:18:12] [V] [TRT] Conv_67 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x64x32_stage3_warpsize4x1x1_g1_tensor16x8x16 Tactic: 0xd1aaad17ca35fbaa
[08/10/2023-11:18:12] [V] [TRT] Tactic: 0xd1aaad17ca35fbaa Time: 0.143753
[08/10/2023-11:18:12] [V] [TRT] Conv_67 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x128x64_stage3_warpsize4x2x1_g1_tensor16x8x16_t1r3s3 Tactic: 0xd2fca0486ca97266
[08/10/2023-11:18:12] [V] [TRT] Tactic: 0xd2fca0486ca97266 Time: 0.295566
[08/10/2023-11:18:12] [V] [TRT] Conv_67 Set Tactic Name: ampere_h16816cudnn_64x128_sliced1x2_ldg8_relu_exp_stages_64x3_small_nhwc_tn_v1 Tactic: 0xd58ea0bdedb89ead
[08/10/2023-11:18:12] [V] [TRT] Tactic: 0xd58ea0bdedb89ead Time: 0.289307
[08/10/2023-11:18:12] [V] [TRT] Conv_67 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x256x64_stage3_warpsize1x4x1_g1_tensor16x8x16_t1r3s3 Tactic: 0xd7c261fa01db2af9
[08/10/2023-11:18:12] [V] [TRT] Tactic: 0xd7c261fa01db2af9 Time: 0.610057
[08/10/2023-11:18:12] [V] [TRT] Conv_67 Set Tactic Name: ampere_h16816cudnn_256x64_sliced1x2_ldg8_relu_exp_large_nhwc_tn_v1 Tactic: 0xd8eb41ee35e76575
[08/10/2023-11:18:12] [V] [TRT] Tactic: 0xd8eb41ee35e76575 Time: 0.234601
[08/10/2023-11:18:12] [V] [TRT] Conv_67 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x128x32_stage5_warpsize2x2x1_g1_tensor16x8x16 Tactic: 0xdb0b80f591d1bb6d
[08/10/2023-11:18:12] [V] [TRT] Tactic: 0xdb0b80f591d1bb6d Time: 0.295305
[08/10/2023-11:18:12] [V] [TRT] Conv_67 Set Tactic Name: ampere_h16816cudnn_256x128_ldg8_relu_exp_large_nhwc_tn_v1 Tactic: 0xdc796d70e228a1d4
[08/10/2023-11:18:12] [V] [TRT] Tactic: 0xdc796d70e228a1d4 Time: 0.304192
[08/10/2023-11:18:12] [V] [TRT] Conv_67 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x32x32_stage4_warpsize4x1x1_g1_tensor16x8x16 Tactic: 0xdce100b9fe609424
[08/10/2023-11:18:12] [V] [TRT] Tactic: 0xdce100b9fe609424 Time: 0.126926
[08/10/2023-11:18:12] [V] [TRT] Conv_67 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x128x32_stage4_warpsize2x2x1_g1_tensor16x8x16 Tactic: 0xdfa020ef435ef810
[08/10/2023-11:18:12] [V] [TRT] Tactic: 0xdfa020ef435ef810 Time: 0.247959
[08/10/2023-11:18:12] [V] [TRT] Conv_67 Set Tactic Name: ampere_h1688cudnn_256x64_ldg8_relu_exp_large_nhwc_tn_v1 Tactic: 0xe0e3c0e8cf9a2d9e
[08/10/2023-11:18:12] [V] [TRT] Tactic: 0xe0e3c0e8cf9a2d9e Time: 0.301719
[08/10/2023-11:18:12] [V] [TRT] Conv_67 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x32x64_stage5_warpsize2x2x1_g1_tensor16x8x16 Tactic: 0xe1ff5ad20f5c6bf6
[08/10/2023-11:18:12] [V] [TRT] Tactic: 0xe1ff5ad20f5c6bf6 Time: 0.187374
[08/10/2023-11:18:12] [V] [TRT] Conv_67 Set Tactic Name: ampere_h16816cudnn_256x64_ldg8_relu_exp_medium_nhwc_tn_v1 Tactic: 0xe4711898bd599c36
[08/10/2023-11:18:12] [V] [TRT] Tactic: 0xe4711898bd599c36 Time: 0.174405
[08/10/2023-11:18:12] [V] [TRT] Conv_67 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x64x64_stage3_warpsize4x1x1_g1_tensor16x8x16 Tactic: 0xeb25062ffb9eae45
[08/10/2023-11:18:12] [V] [TRT] Tactic: 0xeb25062ffb9eae45 Time: 0.193778
[08/10/2023-11:18:12] [V] [TRT] Conv_67 Set Tactic Name: ampere_h16816cudnn_128x64_sliced1x2_ldg8_relu_exp_stages_64x4_large_nhwc_tn_v1 Tactic: 0xeb2d2aa4e56bb41c
[08/10/2023-11:18:12] [V] [TRT] Tactic: 0xeb2d2aa4e56bb41c Time: 0.266903
[08/10/2023-11:18:12] [V] [TRT] Conv_67 Set Tactic Name: ampere_h16816cudnn_64x64_ldg8_relu_exp_stages_64x6_small_nhwc_tn_v1 Tactic: 0xf0beb09df9a19f82
[08/10/2023-11:18:12] [V] [TRT] Tactic: 0xf0beb09df9a19f82 Time: 0.357701
[08/10/2023-11:18:12] [V] [TRT] Conv_67 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x64x64_stage4_warpsize2x2x1_g1_tensor16x8x16 Tactic: 0xf35e0311fa1cc516
[08/10/2023-11:18:12] [V] [TRT] Tactic: 0xf35e0311fa1cc516 Time: 0.208613
[08/10/2023-11:18:12] [V] [TRT] Conv_67 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x32x32_stage4_warpsize4x1x1_g1_tensor16x8x16 Tactic: 0xf79479a62ea9f901
[08/10/2023-11:18:12] [V] [TRT] Tactic: 0xf79479a62ea9f901 Time: 0.126647
[08/10/2023-11:18:12] [V] [TRT] Fastest Tactic: 0x3e7eb35b91b9fa63 Time: 0.111673
[08/10/2023-11:18:12] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x3e7eb35b91b9fa63
[08/10/2023-11:18:12] [V] [TRT] *************** Autotuning format combination: Half(1036800,1:16,5760,32) -> Half(32400,1:16,180,1) ***************
[08/10/2023-11:18:12] [V] [TRT] --------------- Timing Runner: Conv_67 (CaskConvolution)
[08/10/2023-11:18:12] [V] [TRT] Conv_67 Set Tactic Name: ampere_h16816cudnn_256x128_ldg8_relu_exp_medium_nhwc_tn_v1 Tactic: 0x03896956a39a1203
[08/10/2023-11:18:12] [V] [TRT] Tactic: 0x03896956a39a1203 Time: 0.304955
[08/10/2023-11:18:12] [V] [TRT] Conv_67 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x128x32_stage3_warpsize4x2x1_g1_tensor16x8x16_t1r3s3 Tactic: 0x048d6d0400f33439
[08/10/2023-11:18:12] [V] [TRT] Tactic: 0x048d6d0400f33439 Time: 0.284375
[08/10/2023-11:18:12] [V] [TRT] Conv_67 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x64x64_stage3_warpsize2x2x1_g1_tensor16x8x16_t1r3s3 Tactic: 0x05b6220f243edacd
[08/10/2023-11:18:12] [V] [TRT] Tactic: 0x05b6220f243edacd Time: 0.153152
[08/10/2023-11:18:12] [V] [TRT] Conv_67 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x64x32_stage3_warpsize4x1x1_g1_tensor16x8x16_t1r3s3_aACCESS Tactic: 0x0866ddee325d07a6
[08/10/2023-11:18:12] [V] [TRT] Tactic: 0x0866ddee325d07a6 Time: 0.161655
[08/10/2023-11:18:12] [V] [TRT] Conv_67 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x128x32_stage4_warpsize2x2x1_g1_tensor16x8x16 Tactic: 0x0e07ff4f4f7c1ac9
[08/10/2023-11:18:12] [V] [TRT] Tactic: 0x0e07ff4f4f7c1ac9 Time: 0.271881
[08/10/2023-11:18:12] [V] [TRT] Conv_67 Set Tactic Name: ampere_h16816cudnn_256x128_ldg8_relu_exp_stages_64x3_small_nhwc_tn_v1 Tactic: 0x0e199750c30c6928
[08/10/2023-11:18:12] [V] [TRT] Tactic: 0x0e199750c30c6928 Time: 0.301605
[08/10/2023-11:18:12] [V] [TRT] Conv_67 Set Tactic Name: ampere_h1688cudnn_128x128_ldg8_relu_exp_medium_nhwc_tn_v1 Tactic: 0x100c1ad308e08d35
[08/10/2023-11:18:12] [V] [TRT] Tactic: 0x100c1ad308e08d35 Time: 0.347232
[08/10/2023-11:18:12] [V] [TRT] Conv_67 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x128x32_stage5_warpsize2x2x1_g1_tensor16x8x16_t1r3s3 Tactic: 0x17ebf0c9f418f10a
[08/10/2023-11:18:12] [V] [TRT] Tactic: 0x17ebf0c9f418f10a Time: 0.287451
[08/10/2023-11:18:12] [V] [TRT] Conv_67 Set Tactic Name: ampere_h16816cudnn_128x64_ldg8_relu_exp_stages_64x4_medium_nhwc_tn_v1 Tactic: 0x196cbc423a9bb69e
[08/10/2023-11:18:12] [V] [TRT] Tactic: 0x196cbc423a9bb69e Time: 0.249303
[08/10/2023-11:18:12] [V] [TRT] Conv_67 Set Tactic Name: ampere_h16816cudnn_128x64_sliced1x2_ldg8_relu_exp_stages_64x3_small_nhwc_tn_v1 Tactic: 0x19822de884f42a6a
[08/10/2023-11:18:12] [V] [TRT] Tactic: 0x19822de884f42a6a Time: 0.164151
[08/10/2023-11:18:12] [V] [TRT] Conv_67 Set Tactic Name: ampere_h16816cudnn_64x64_sliced1x2_ldg8_relu_exp_stages_64x5_small_nhwc_tn_v1 Tactic: 0x1a5ba808ad4cc5a8
[08/10/2023-11:18:12] [V] [TRT] Tactic: 0x1a5ba808ad4cc5a8 Time: 0.248626
[08/10/2023-11:18:12] [V] [TRT] Conv_67 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x64x64_stage3_warpsize2x2x1_g1_tensor16x8x16 Tactic: 0x21b295c0c8f6c95a
[08/10/2023-11:18:12] [V] [TRT] Tactic: 0x21b295c0c8f6c95a Time: 0.171337
[08/10/2023-11:18:12] [V] [TRT] Conv_67 Set Tactic Name: ampere_h16816cudnn_128x128_ldg8_relu_exp_medium_nhwc_tn_v1 Tactic: 0x234580e8a194335c
[08/10/2023-11:18:12] [V] [TRT] Tactic: 0x234580e8a194335c Time: 0.252439
[08/10/2023-11:18:12] [V] [TRT] Conv_67 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x128x32_stage4_warpsize2x2x1_g1_tensor16x8x16_t1r3s3_aACCESS Tactic: 0x245530c34bd6090f
[08/10/2023-11:18:12] [V] [TRT] Tactic: 0x245530c34bd6090f Time: 0.257943
[08/10/2023-11:18:12] [V] [TRT] Conv_67 Set Tactic Name: ampere_h16816cudnn_64x64_sliced1x2_ldg8_relu_exp_stages_64x6_small_nhwc_tn_v1 Tactic: 0x24e01e7405cfdfe9
[08/10/2023-11:18:12] [V] [TRT] Tactic: 0x24e01e7405cfdfe9 Time: 0.365042
[08/10/2023-11:18:12] [V] [TRT] Conv_67 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x64x64_stage3_warpsize2x2x1_g1_tensor16x8x16_t1r3s3_aACCESS Tactic: 0x263a38afd75e3a43
[08/10/2023-11:18:13] [V] [TRT] Tactic: 0x263a38afd75e3a43 Time: 0.169061
[08/10/2023-11:18:13] [V] [TRT] Conv_67 Set Tactic Name: ampere_h16816cudnn_64x128_sliced1x2_ldg8_relu_exp_stages_64x4_large_nhwc_tn_v1 Tactic: 0x2721a7f18c2700db
[08/10/2023-11:18:13] [V] [TRT] Tactic: 0x2721a7f18c2700db Time: 0.499095
[08/10/2023-11:18:13] [V] [TRT] Conv_67 Set Tactic Name: ampere_h16816cudnn_128x64_sliced1x2_ldg8_relu_exp_stages_64x3_medium_nhwc_tn_v1 Tactic: 0x29329b5741ea05f2
[08/10/2023-11:18:13] [V] [TRT] Tactic: 0x29329b5741ea05f2 Time: 0.170807
[08/10/2023-11:18:13] [V] [TRT] Conv_67 Set Tactic Name: ampere_h16816cudnn_128x128_ldg8_relu_exp_large_nhwc_tn_v1 Tactic: 0x2970ae65966d569d
[08/10/2023-11:18:13] [V] [TRT] Tactic: 0x2970ae65966d569d Time: 0.258368
[08/10/2023-11:18:13] [V] [TRT] Conv_67 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x256x64_stage3_warpsize1x4x1_g1_tensor16x8x16 Tactic: 0x29dc14520e0e4eb6
[08/10/2023-11:18:13] [V] [TRT] Tactic: 0x29dc14520e0e4eb6 Time: 0.624626
[08/10/2023-11:18:13] [V] [TRT] Conv_67 Set Tactic Name: ampere_h16816cudnn_256x64_ldg8_relu_exp_stages_64x3_large_nhwc_tn_v1 Tactic: 0x2cb4a662694e89d3
[08/10/2023-11:18:13] [V] [TRT] Tactic: 0x2cb4a662694e89d3 Time: 0.198331
[08/10/2023-11:18:13] [V] [TRT] Conv_67 Set Tactic Name: ampere_h16816cudnn_128x128_ldg8_relu_exp_stages_64x3_medium_nhwc_tn_v1 Tactic: 0x2eaa2202de9404d6
[08/10/2023-11:18:13] [V] [TRT] Tactic: 0x2eaa2202de9404d6 Time: 0.345673
[08/10/2023-11:18:13] [V] [TRT] Conv_67 Set Tactic Name: ampere_h1688cudnn_256x128_ldg8_relu_exp_medium_nhwc_tn_v1 Tactic: 0x30c0f36d0aeeac6a
[08/10/2023-11:18:13] [V] [TRT] Tactic: 0x30c0f36d0aeeac6a Time: 0.363977
[08/10/2023-11:18:13] [V] [TRT] Conv_67 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x64x64_stage4_warpsize2x2x1_g1_tensor16x8x16_t1r3s3_aACCESS Tactic: 0x30e8a8d7a953e5e9
[08/10/2023-11:18:13] [V] [TRT] Tactic: 0x30e8a8d7a953e5e9 Time: 0.199954
[08/10/2023-11:18:13] [V] [TRT] Conv_67 Set Tactic Name: ampere_h16816cudnn_128x64_ldg8_relu_exp_stages_64x4_large_nhwc_tn_v1 Tactic: 0x3197d5044d71e98e
[08/10/2023-11:18:13] [V] [TRT] Tactic: 0x3197d5044d71e98e Time: 0.256398
[08/10/2023-11:18:13] [V] [TRT] Conv_67 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x64x64_stage4_warpsize2x2x1_g1_tensor16x8x16_t1r3s3 Tactic: 0x31d93dc22d2af081
[08/10/2023-11:18:13] [V] [TRT] Tactic: 0x31d93dc22d2af081 Time: 0.193774
[08/10/2023-11:18:13] [V] [TRT] Conv_67 Set Tactic Name: ampere_h1688cudnn_128x128_ldg8_relu_exp_small_nhwc_tn_v1 Tactic: 0x3369260c04f9ad73
[08/10/2023-11:18:13] [V] [TRT] Tactic: 0x3369260c04f9ad73 Time: 0.332974
[08/10/2023-11:18:13] [V] [TRT] Conv_67 Set Tactic Name: ampere_h16816cudnn_64x128_ldg8_relu_exp_stages_64x4_small_nhwc_tn_v1 Tactic: 0x3b5fa0f2a8fc2410
[08/10/2023-11:18:13] [V] [TRT] Tactic: 0x3b5fa0f2a8fc2410 Time: 0.428562
[08/10/2023-11:18:13] [V] [TRT] Conv_67 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x32x32_stage4_warpsize4x1x1_g1_tensor16x8x16_t1r3s3 Tactic: 0x3e7eb35b91b9fa63
[08/10/2023-11:18:13] [V] [TRT] Tactic: 0x3e7eb35b91b9fa63 Time: 0.119769
[08/10/2023-11:18:13] [V] [TRT] Conv_67 Set Tactic Name: ampere_h16816cudnn_256x64_ldg8_relu_exp_stages_64x3_medium_nhwc_tn_v1 Tactic: 0x3f031df0e579d52c
[08/10/2023-11:18:13] [V] [TRT] Tactic: 0x3f031df0e579d52c Time: 0.196727
[08/10/2023-11:18:13] [V] [TRT] Conv_67 Set Tactic Name: ampere_h16816cudnn_256x64_sliced1x2_ldg8_relu_exp_small_nhwc_tn_v1 Tactic: 0x42a5b8709d0039be
[08/10/2023-11:18:13] [V] [TRT] Tactic: 0x42a5b8709d0039be Time: 0.212672
[08/10/2023-11:18:13] [V] [TRT] Conv_67 Set Tactic Name: ampere_h16816cudnn_256x128_ldg8_relu_exp_small_nhwc_tn_v1 Tactic: 0x463794ee4acffd1f
[08/10/2023-11:18:13] [V] [TRT] Tactic: 0x463794ee4acffd1f Time: 0.299291
[08/10/2023-11:18:13] [V] [TRT] Conv_67 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x64x64_stage4_warpsize2x2x1_g1_tensor16x8x16 Tactic: 0x4a81ea1e51436a30
[08/10/2023-11:18:13] [V] [TRT] Tactic: 0x4a81ea1e51436a30 Time: 0.222272
[08/10/2023-11:18:13] [V] [TRT] Conv_67 Set Tactic Name: ampere_h1688cudnn_256x64_sliced1x2_ldg8_relu_exp_small_nhwc_tn_v1 Tactic: 0x4aed1956bd10a795
[08/10/2023-11:18:13] [V] [TRT] Tactic: 0x4aed1956bd10a795 Time: 0.242318
[08/10/2023-11:18:13] [V] [TRT] Conv_67 Set Tactic Name: ampere_h16816cudnn_64x128_sliced1x2_ldg8_relu_exp_stages_64x3_large_nhwc_tn_v1 Tactic: 0x4fc05923455fc266
[08/10/2023-11:18:13] [V] [TRT] Tactic: 0x4fc05923455fc266 Time: 0.304407
[08/10/2023-11:18:13] [V] [TRT] Conv_67 Set Tactic Name: ampere_h16816cudnn_256x64_ldg8_relu_exp_large_nhwc_tn_v1 Tactic: 0x5013c38f55afa9ba
[08/10/2023-11:18:13] [V] [TRT] Tactic: 0x5013c38f55afa9ba Time: 0.182414
[08/10/2023-11:18:13] [V] [TRT] Conv_67 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x64x32_stage3_warpsize4x1x1_g1_tensor16x8x16_t1r3s3 Tactic: 0x529f4431bdae94f5
[08/10/2023-11:18:13] [V] [TRT] Tactic: 0x529f4431bdae94f5 Time: 0.148146
[08/10/2023-11:18:13] [V] [TRT] Conv_67 Set Tactic Name: ampere_h16816cudnn_64x128_ldg8_relu_exp_stages_64x3_small_nhwc_tn_v1 Tactic: 0x53be5e206184e6ad
[08/10/2023-11:18:13] [V] [TRT] Tactic: 0x53be5e206184e6ad Time: 0.293975
[08/10/2023-11:18:13] [V] [TRT] Conv_67 Set Tactic Name: ampere_h16816cudnn_64x64_ldg8_relu_exp_stages_64x5_large_nhwc_tn_v1 Tactic: 0x544bff7ff9c5d908
[08/10/2023-11:18:13] [V] [TRT] Tactic: 0x544bff7ff9c5d908 Time: 0.263515
[08/10/2023-11:18:13] [V] [TRT] Conv_67 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x64x64_stage3_warpsize2x2x1_g1_tensor16x8x16 Tactic: 0x5568fd8a32f4a40f
[08/10/2023-11:18:13] [V] [TRT] Tactic: 0x5568fd8a32f4a40f Time: 0.155506
[08/10/2023-11:18:13] [V] [TRT] Conv_67 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x128x64_stage3_warpsize2x2x1_g1_tensor16x8x16 Tactic: 0x57c9a5ff682354a6
[08/10/2023-11:18:13] [V] [TRT] Tactic: 0x57c9a5ff682354a6 Time: 0.326002
[08/10/2023-11:18:13] [V] [TRT] Conv_67 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x32x64_stage5_warpsize2x2x1_g1_tensor16x8x16 Tactic: 0x5820b3dda403c4d0
[08/10/2023-11:18:13] [V] [TRT] Tactic: 0x5820b3dda403c4d0 Time: 0.180786
[08/10/2023-11:18:13] [V] [TRT] Conv_67 Set Tactic Name: ampere_h16816cudnn_128x128_ldg8_relu_exp_stages_64x3_small_nhwc_tn_v1 Tactic: 0x58816bf2e9c36afb
[08/10/2023-11:18:13] [V] [TRT] Tactic: 0x58816bf2e9c36afb Time: 0.341385
[08/10/2023-11:18:13] [V] [TRT] Conv_67 Set Tactic Name: ampere_h16816cudnn_128x64_ldg8_relu_exp_stages_64x3_large_nhwc_tn_v1 Tactic: 0x59762bd684092b33
[08/10/2023-11:18:13] [V] [TRT] Tactic: 0x59762bd684092b33 Time: 0.18048
[08/10/2023-11:18:13] [V] [TRT] Conv_67 Set Tactic Name: ampere_h16816cudnn_128x64_sliced1x2_ldg8_relu_exp_stages_64x4_medium_nhwc_tn_v1 Tactic: 0x59c5c647b4c76593
[08/10/2023-11:18:13] [V] [TRT] Tactic: 0x59c5c647b4c76593 Time: 0.255109
[08/10/2023-11:18:13] [V] [TRT] Conv_67 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x128x64_stage3_warpsize4x2x1_g1_tensor16x8x16 Tactic: 0x5a55274960724ba8
[08/10/2023-11:18:13] [V] [TRT] Tactic: 0x5a55274960724ba8 Time: 0.368439
[08/10/2023-11:18:13] [V] [TRT] Conv_67 Set Tactic Name: ampere_h1688cudnn_256x128_ldg8_relu_exp_large_nhwc_tn_v1 Tactic: 0x5c2e1c87d85b06f1
[08/10/2023-11:18:13] [V] [TRT] Tactic: 0x5c2e1c87d85b06f1 Time: 0.61291
[08/10/2023-11:18:13] [V] [TRT] Conv_67 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x256x64_stage3_warpsize1x4x1_g1_tensor16x8x16 Tactic: 0x5d067c18f40c23e3
[08/10/2023-11:18:13] [V] [TRT] Tactic: 0x5d067c18f40c23e3 Time: 0.624466
[08/10/2023-11:18:13] [V] [TRT] Conv_67 Set Tactic Name: ampere_h16816cudnn_64x64_ldg8_relu_exp_stages_64x5_medium_nhwc_tn_v1 Tactic: 0x5f31c22ec167f384
[08/10/2023-11:18:13] [V] [TRT] Tactic: 0x5f31c22ec167f384 Time: 0.260791
[08/10/2023-11:18:13] [V] [TRT] Conv_67 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x128x32_stage4_warpsize2x2x1_g1_tensor16x8x16_t1r3s3 Tactic: 0x60c3421152ef8e10
[08/10/2023-11:18:13] [V] [TRT] Tactic: 0x60c3421152ef8e10 Time: 0.248128
[08/10/2023-11:18:13] [V] [TRT] Conv_67 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x128x32_stage3_warpsize4x2x1_g1_tensor16x8x16 Tactic: 0x60da8c7151d91e47
[08/10/2023-11:18:13] [V] [TRT] Tactic: 0x60da8c7151d91e47 Time: 0.283438
[08/10/2023-11:18:13] [V] [TRT] Conv_67 Set Tactic Name: ampere_h1688cudnn_256x64_ldg8_relu_exp_medium_nhwc_tn_v1 Tactic: 0x6426696f872a3b13
[08/10/2023-11:18:13] [V] [TRT] Tactic: 0x6426696f872a3b13 Time: 0.227077
[08/10/2023-11:18:13] [V] [TRT] Conv_67 Set Tactic Name: ampere_h16816cudnn_128x64_ldg8_relu_exp_stages_64x3_medium_nhwc_tn_v1 Tactic: 0x699be152cfb6d6ff
[08/10/2023-11:18:13] [V] [TRT] Tactic: 0x699be152cfb6d6ff Time: 0.181691
[08/10/2023-11:18:13] [V] [TRT] Conv_67 Set Tactic Name: ampere_h16816cudnn_64x64_ldg8_relu_exp_stages_64x6_large_nhwc_tn_v1 Tactic: 0x6af049035146c349
[08/10/2023-11:18:13] [V] [TRT] Tactic: 0x6af049035146c349 Time: 0.380187
[08/10/2023-11:18:13] [V] [TRT] Conv_67 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x256x32_stage3_warpsize2x4x1_g1_tensor16x8x16 Tactic: 0x7005d10718f6c22d
[08/10/2023-11:18:13] [V] [TRT] Tactic: 0x7005d10718f6c22d Time: 0.523177
[08/10/2023-11:18:13] [V] [TRT] Conv_67 Set Tactic Name: ampere_h16816cudnn_64x64_sliced1x2_ldg8_relu_exp_stages_64x5_medium_nhwc_tn_v1 Tactic: 0x706f08da35c795a5
[08/10/2023-11:18:13] [V] [TRT] Tactic: 0x706f08da35c795a5 Time: 0.259744
[08/10/2023-11:18:13] [V] [TRT] Conv_67 Set Tactic Name: ampere_h16816cudnn_128x64_sliced1x2_ldg8_relu_exp_stages_64x4_small_nhwc_tn_v1 Tactic: 0x7163d33a4d8ce8d7
[08/10/2023-11:18:13] [V] [TRT] Tactic: 0x7163d33a4d8ce8d7 Time: 0.249687
[08/10/2023-11:18:13] [V] [TRT] Conv_67 Set Tactic Name: ampere_h1688cudnn_256x64_ldg8_relu_exp_small_nhwc_tn_v1 Tactic: 0x7aad3976677d7155
[08/10/2023-11:18:13] [V] [TRT] Tactic: 0x7aad3976677d7155 Time: 0.21525
[08/10/2023-11:18:13] [V] [TRT] Conv_67 Set Tactic Name: ampere_h16816cudnn_64x64_sliced1x2_ldg8_relu_exp_stages_64x5_large_nhwc_tn_v1 Tactic: 0x8015519605ab9963
[08/10/2023-11:18:13] [V] [TRT] Tactic: 0x8015519605ab9963 Time: 0.260407
[08/10/2023-11:18:13] [V] [TRT] Conv_67 Set Tactic Name: ampere_h16816cudnn_256x64_sliced1x2_ldg8_relu_exp_medium_nhwc_tn_v1 Tactic: 0x82f8a2214b0b4178
[08/10/2023-11:18:13] [V] [TRT] Tactic: 0x82f8a2214b0b4178 Time: 0.237198
[08/10/2023-11:18:13] [V] [TRT] Conv_67 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x32x32_stage4_warpsize4x1x1_g1_tensor16x8x16 Tactic: 0x834e11ecd4ab9454
[08/10/2023-11:18:13] [V] [TRT] Tactic: 0x834e11ecd4ab9454 Time: 0.121627
[08/10/2023-11:18:13] [V] [TRT] Conv_67 Set Tactic Name: ampere_h16816cudnn_128x64_sliced1x2_ldg8_relu_exp_stages_64x3_large_nhwc_tn_v1 Tactic: 0x83ccd4762c1376a1
[08/10/2023-11:18:13] [V] [TRT] Tactic: 0x83ccd4762c1376a1 Time: 0.170135
[08/10/2023-11:18:13] [V] [TRT] Conv_67 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x64x64_stage3_warpsize4x1x1_g1_tensor16x8x16_t1r3s3 Tactic: 0x84ee897dd1f4d2aa
[08/10/2023-11:18:13] [V] [TRT] Tactic: 0x84ee897dd1f4d2aa Time: 0.184667
[08/10/2023-11:18:13] [V] [TRT] Conv_67 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x128x64_stage3_warpsize2x2x1_g1_tensor16x8x16 Tactic: 0x866e7a5f6401b67f
[08/10/2023-11:18:13] [V] [TRT] Tactic: 0x866e7a5f6401b67f Time: 0.326825
[08/10/2023-11:18:13] [V] [TRT] Conv_67 Set Tactic Name: ampere_h16816cudnn_64x64_ldg8_relu_exp_stages_64x6_medium_nhwc_tn_v1 Tactic: 0x88971eec55aba850
[08/10/2023-11:18:13] [V] [TRT] Tactic: 0x88971eec55aba850 Time: 0.372119
[08/10/2023-11:18:13] [V] [TRT] Conv_67 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x128x64_stage3_warpsize4x2x1_g1_tensor16x8x16 Tactic: 0x8bf2f8e96c50a971
[08/10/2023-11:18:13] [V] [TRT] Tactic: 0x8bf2f8e96c50a971 Time: 0.291054
[08/10/2023-11:18:13] [V] [TRT] Conv_67 Set Tactic Name: ampere_h16816cudnn_256x128_ldg8_relu_exp_stages_64x3_large_nhwc_tn_v1 Tactic: 0x94576ece6beb35e3
[08/10/2023-11:18:13] [V] [TRT] Tactic: 0x94576ece6beb35e3 Time: 0.305275
[08/10/2023-11:18:13] [V] [TRT] Conv_67 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x64x64_stage3_warpsize4x1x1_g1_tensor16x8x16 Tactic: 0x9fff6e65019cc310
[08/10/2023-11:18:13] [V] [TRT] Tactic: 0x9fff6e65019cc310 Time: 0.184786
[08/10/2023-11:18:13] [V] [TRT] Conv_67 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x32x64_stage5_warpsize2x2x1_g1_tensor16x8x16_t1r3s3 Tactic: 0xa033e20ae9f412b2
[08/10/2023-11:18:13] [V] [TRT] Tactic: 0xa033e20ae9f412b2 Time: 0.164219
[08/10/2023-11:18:13] [V] [TRT] Conv_67 Set Tactic Name: ampere_h16816cudnn_64x128_ldg8_relu_exp_stages_64x4_large_nhwc_tn_v1 Tactic: 0xa111596c001b78db
[08/10/2023-11:18:13] [V] [TRT] Tactic: 0xa111596c001b78db Time: 0.441623
[08/10/2023-11:18:13] [V] [TRT] Conv_67 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x256x32_stage3_warpsize2x4x1_g1_tensor16x8x16 Tactic: 0xa1a20ea714d420f4
[08/10/2023-11:18:13] [V] [TRT] Tactic: 0xa1a20ea714d420f4 Time: 0.519753
[08/10/2023-11:18:13] [V] [TRT] Conv_67 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x32x32_stage4_warpsize4x1x1_g1_tensor16x8x16_t1r3s3 Tactic: 0xa40cb43c296a36a8
[08/10/2023-11:18:13] [V] [TRT] Tactic: 0xa40cb43c296a36a8 Time: 0.123774
[08/10/2023-11:18:13] [V] [TRT] Conv_67 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x64x32_stage3_warpsize4x1x1_g1_tensor16x8x16 Tactic: 0xa570c55d303796ff
[08/10/2023-11:18:13] [V] [TRT] Tactic: 0xa570c55d303796ff Time: 0.156078
[08/10/2023-11:18:13] [V] [TRT] Conv_67 Set Tactic Name: ampere_h16816cudnn_64x64_sliced1x2_ldg8_relu_exp_stages_64x6_medium_nhwc_tn_v1 Tactic: 0xa7c9d418a10bce71
[08/10/2023-11:18:13] [V] [TRT] Tactic: 0xa7c9d418a10bce71 Time: 0.398295
[08/10/2023-11:18:13] [V] [TRT] Conv_67 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x32x32_stage4_warpsize4x1x1_g1_tensor16x8x16 Tactic: 0xa83b68f30462f971
[08/10/2023-11:18:13] [V] [TRT] Tactic: 0xa83b68f30462f971 Time: 0.132249
[08/10/2023-11:18:13] [V] [TRT] Conv_67 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x256x32_stage3_warpsize2x4x1_g1_tensor16x8x16_t1r3s3 Tactic: 0xa9177bbe4e767df8
[08/10/2023-11:18:13] [V] [TRT] Tactic: 0xa9177bbe4e767df8 Time: 0.530866
[08/10/2023-11:18:13] [V] [TRT] Conv_67 Set Tactic Name: ampere_h1688cudnn_128x128_ldg8_relu_exp_large_nhwc_tn_v1 Tactic: 0xa927df92ac1ef1b8
[08/10/2023-11:18:13] [V] [TRT] Tactic: 0xa927df92ac1ef1b8 Time: 0.35787
[08/10/2023-11:18:13] [V] [TRT] Conv_67 Set Tactic Name: ampere_h16816cudnn_128x64_ldg8_relu_exp_stages_64x4_small_nhwc_tn_v1 Tactic: 0xabd92c9ae596b545
[08/10/2023-11:18:13] [V] [TRT] Tactic: 0xabd92c9ae596b545 Time: 0.245038
[08/10/2023-11:18:13] [V] [TRT] Conv_67 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x128x32_stage5_warpsize2x2x1_g1_tensor16x8x16 Tactic: 0xafd1e8bf6bd3d638
[08/10/2023-11:18:13] [V] [TRT] Tactic: 0xafd1e8bf6bd3d638 Time: 0.304233
[08/10/2023-11:18:13] [V] [TRT] Conv_67 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x128x32_stage3_warpsize4x2x1_g1_tensor16x8x16 Tactic: 0xb17d53d15dfbfc9e
[08/10/2023-11:18:13] [V] [TRT] Tactic: 0xb17d53d15dfbfc9e Time: 0.289271
[08/10/2023-11:18:13] [V] [TRT] Conv_67 Set Tactic Name: ampere_h16816cudnn_128x128_ldg8_relu_exp_small_nhwc_tn_v1 Tactic: 0xb33e57fb3e8a0a56
[08/10/2023-11:18:13] [V] [TRT] Tactic: 0xb33e57fb3e8a0a56 Time: 0.250181
[08/10/2023-11:18:13] [V] [TRT] Conv_67 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x64x32_stage5_warpsize2x2x1_g1_tensor16x8x16_t1r3s3 Tactic: 0xb4bec086187edcfc
[08/10/2023-11:18:13] [V] [TRT] Tactic: 0xb4bec086187edcfc Time: 0.163497
[08/10/2023-11:18:13] [V] [TRT] Conv_67 Set Tactic Name: ampere_h16816cudnn_256x64_ldg8_relu_exp_stages_64x3_small_nhwc_tn_v1 Tactic: 0xb6fa5ffcc1a9d518
[08/10/2023-11:18:13] [V] [TRT] Tactic: 0xb6fa5ffcc1a9d518 Time: 0.192617
[08/10/2023-11:18:13] [V] [TRT] Conv_67 Set Tactic Name: ampere_h16816cudnn_64x128_sliced1x2_ldg8_relu_exp_stages_64x3_medium_nhwc_tn_v1 Tactic: 0xb85e52e87caf60a2
[08/10/2023-11:18:13] [V] [TRT] Tactic: 0xb85e52e87caf60a2 Time: 0.294537
[08/10/2023-11:18:13] [V] [TRT] Conv_67 Set Tactic Name: ampere_h16816cudnn_64x128_ldg8_relu_exp_stages_64x3_medium_nhwc_tn_v1 Tactic: 0xb8d86216e1235cda
[08/10/2023-11:18:13] [V] [TRT] Tactic: 0xb8d86216e1235cda Time: 0.294235
[08/10/2023-11:18:13] [V] [TRT] Conv_67 Set Tactic Name: ampere_h16816cudnn_256x128_ldg8_relu_exp_stages_64x3_medium_nhwc_tn_v1 Tactic: 0xb9171d70ba2eda4b
[08/10/2023-11:18:13] [V] [TRT] Tactic: 0xb9171d70ba2eda4b Time: 0.305381
[08/10/2023-11:18:13] [V] [TRT] Conv_67 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x64x32_stage5_warpsize2x2x1_g1_tensor16x8x16 Tactic: 0xbd08239a9317f2fd
[08/10/2023-11:18:13] [V] [TRT] Tactic: 0xbd08239a9317f2fd Time: 0.168242
[08/10/2023-11:18:13] [V] [TRT] Conv_67 Set Tactic Name: ampere_h16816cudnn_64x128_sliced1x2_ldg8_relu_exp_stages_64x4_small_nhwc_tn_v1 Tactic: 0xbd6f5e6f24c05c10
[08/10/2023-11:18:13] [V] [TRT] Tactic: 0xbd6f5e6f24c05c10 Time: 0.470441
[08/10/2023-11:18:13] [V] [TRT] Conv_67 Set Tactic Name: ampere_h16816cudnn_64x64_sliced1x2_ldg8_relu_exp_stages_64x6_large_nhwc_tn_v1 Tactic: 0xbeaee7eaad288322
[08/10/2023-11:18:13] [V] [TRT] Tactic: 0xbeaee7eaad288322 Time: 0.393902
[08/10/2023-11:18:13] [V] [TRT] Conv_67 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x128x64_stage3_warpsize2x2x1_g1_tensor16x8x16_t1r3s3 Tactic: 0xc0a02dc6095497cc
[08/10/2023-11:18:13] [V] [TRT] Tactic: 0xc0a02dc6095497cc Time: 0.320622
[08/10/2023-11:18:13] [V] [TRT] Conv_67 Set Tactic Name: ampere_h16816cudnn_128x128_ldg8_relu_exp_stages_64x3_large_nhwc_tn_v1 Tactic: 0xc2cf926c41243630
[08/10/2023-11:18:13] [V] [TRT] Tactic: 0xc2cf926c41243630 Time: 0.344187
[08/10/2023-11:18:13] [V] [TRT] Conv_67 Set Tactic Name: ampere_h16816cudnn_128x64_ldg8_relu_exp_stages_64x3_small_nhwc_tn_v1 Tactic: 0xc338d2482cee77f8
[08/10/2023-11:18:13] [V] [TRT] Tactic: 0xc338d2482cee77f8 Time: 0.180279
[08/10/2023-11:18:13] [V] [TRT] Conv_67 Set Tactic Name: ampere_h1688cudnn_256x128_ldg8_relu_exp_small_nhwc_tn_v1 Tactic: 0xc660e51970bc5a3a
[08/10/2023-11:18:13] [V] [TRT] Tactic: 0xc660e51970bc5a3a Time: 0.342025
[08/10/2023-11:18:13] [V] [TRT] Conv_67 Set Tactic Name: ampere_h16816cudnn_64x128_ldg8_relu_exp_stages_64x4_medium_nhwc_tn_v1 Tactic: 0xc82f3f06140e3cbb
[08/10/2023-11:18:13] [V] [TRT] Tactic: 0xc82f3f06140e3cbb Time: 0.436215
[08/10/2023-11:18:13] [V] [TRT] Conv_67 Set Tactic Name: ampere_h16816cudnn_64x128_sliced1x2_ldg8_relu_exp_stages_64x4_medium_nhwc_tn_v1 Tactic: 0xc8a90ff8898200c3
[08/10/2023-11:18:13] [V] [TRT] Tactic: 0xc8a90ff8898200c3 Time: 0.471241
[08/10/2023-11:18:13] [V] [TRT] Conv_67 Set Tactic Name: ampere_h1688cudnn_256x64_sliced1x2_ldg8_relu_exp_medium_nhwc_tn_v1 Tactic: 0xc9cc55109bb4de26
[08/10/2023-11:18:13] [V] [TRT] Tactic: 0xc9cc55109bb4de26 Time: 0.259497
[08/10/2023-11:18:13] [V] [TRT] Conv_67 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x64x32_stage5_warpsize2x2x1_g1_tensor16x8x16 Tactic: 0xc9d24bd069159fa8
[08/10/2023-11:18:13] [V] [TRT] Tactic: 0xc9d24bd069159fa8 Time: 0.185202
[08/10/2023-11:18:13] [V] [TRT] Conv_67 Set Tactic Name: ampere_h16816cudnn_64x128_ldg8_relu_exp_stages_64x3_large_nhwc_tn_v1 Tactic: 0xc9f0a7bec963ba66
[08/10/2023-11:18:13] [V] [TRT] Tactic: 0xc9f0a7bec963ba66 Time: 0.301408
[08/10/2023-11:18:13] [V] [TRT] Conv_67 Set Tactic Name: ampere_h16816cudnn_256x64_ldg8_relu_exp_small_nhwc_tn_v1 Tactic: 0xca5d3a11fd48f571
[08/10/2023-11:18:13] [V] [TRT] Tactic: 0xca5d3a11fd48f571 Time: 0.168791
[08/10/2023-11:18:13] [V] [TRT] Conv_67 Set Tactic Name: ampere_h16816cudnn_64x64_ldg8_relu_exp_stages_64x5_small_nhwc_tn_v1 Tactic: 0xce0506e1512285c3
[08/10/2023-11:18:13] [V] [TRT] Tactic: 0xce0506e1512285c3 Time: 0.506853
[08/10/2023-11:18:13] [V] [TRT] Conv_67 Set Tactic Name: ampere_h1688cudnn_256x64_sliced1x2_ldg8_relu_exp_large_nhwc_tn_v1 Tactic: 0xd0a3e0c815f7fb5e
[08/10/2023-11:18:13] [V] [TRT] Tactic: 0xd0a3e0c815f7fb5e Time: 0.255717
[08/10/2023-11:18:13] [V] [TRT] Conv_67 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x64x32_stage3_warpsize4x1x1_g1_tensor16x8x16 Tactic: 0xd1aaad17ca35fbaa
[08/10/2023-11:18:13] [V] [TRT] Tactic: 0xd1aaad17ca35fbaa Time: 0.147707
[08/10/2023-11:18:13] [V] [TRT] Conv_67 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x128x64_stage3_warpsize4x2x1_g1_tensor16x8x16_t1r3s3 Tactic: 0xd2fca0486ca97266
[08/10/2023-11:18:13] [V] [TRT] Tactic: 0xd2fca0486ca97266 Time: 0.295227
[08/10/2023-11:18:13] [V] [TRT] Conv_67 Set Tactic Name: ampere_h16816cudnn_64x128_sliced1x2_ldg8_relu_exp_stages_64x3_small_nhwc_tn_v1 Tactic: 0xd58ea0bdedb89ead
[08/10/2023-11:18:13] [V] [TRT] Tactic: 0xd58ea0bdedb89ead Time: 0.291762
[08/10/2023-11:18:13] [V] [TRT] Conv_67 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x256x64_stage3_warpsize1x4x1_g1_tensor16x8x16_t1r3s3 Tactic: 0xd7c261fa01db2af9
[08/10/2023-11:18:13] [V] [TRT] Tactic: 0xd7c261fa01db2af9 Time: 0.610757
[08/10/2023-11:18:14] [V] [TRT] Conv_67 Set Tactic Name: ampere_h16816cudnn_256x64_sliced1x2_ldg8_relu_exp_large_nhwc_tn_v1 Tactic: 0xd8eb41ee35e76575
[08/10/2023-11:18:14] [V] [TRT] Tactic: 0xd8eb41ee35e76575 Time: 0.238962
[08/10/2023-11:18:14] [V] [TRT] Conv_67 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x128x32_stage5_warpsize2x2x1_g1_tensor16x8x16 Tactic: 0xdb0b80f591d1bb6d
[08/10/2023-11:18:14] [V] [TRT] Tactic: 0xdb0b80f591d1bb6d Time: 0.292503
[08/10/2023-11:18:14] [V] [TRT] Conv_67 Set Tactic Name: ampere_h16816cudnn_256x128_ldg8_relu_exp_large_nhwc_tn_v1 Tactic: 0xdc796d70e228a1d4
[08/10/2023-11:18:14] [V] [TRT] Tactic: 0xdc796d70e228a1d4 Time: 0.304256
[08/10/2023-11:18:14] [V] [TRT] Conv_67 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x32x32_stage4_warpsize4x1x1_g1_tensor16x8x16 Tactic: 0xdce100b9fe609424
[08/10/2023-11:18:14] [V] [TRT] Tactic: 0xdce100b9fe609424 Time: 0.13123
[08/10/2023-11:18:14] [V] [TRT] Conv_67 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x128x32_stage4_warpsize2x2x1_g1_tensor16x8x16 Tactic: 0xdfa020ef435ef810
[08/10/2023-11:18:14] [V] [TRT] Tactic: 0xdfa020ef435ef810 Time: 0.24811
[08/10/2023-11:18:14] [V] [TRT] Conv_67 Set Tactic Name: ampere_h1688cudnn_256x64_ldg8_relu_exp_large_nhwc_tn_v1 Tactic: 0xe0e3c0e8cf9a2d9e
[08/10/2023-11:18:14] [V] [TRT] Tactic: 0xe0e3c0e8cf9a2d9e Time: 0.224297
[08/10/2023-11:18:14] [V] [TRT] Conv_67 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x32x64_stage5_warpsize2x2x1_g1_tensor16x8x16 Tactic: 0xe1ff5ad20f5c6bf6
[08/10/2023-11:18:14] [V] [TRT] Tactic: 0xe1ff5ad20f5c6bf6 Time: 0.183579
[08/10/2023-11:18:14] [V] [TRT] Conv_67 Set Tactic Name: ampere_h16816cudnn_256x64_ldg8_relu_exp_medium_nhwc_tn_v1 Tactic: 0xe4711898bd599c36
[08/10/2023-11:18:14] [V] [TRT] Tactic: 0xe4711898bd599c36 Time: 0.171579
[08/10/2023-11:18:14] [V] [TRT] Conv_67 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x64x64_stage3_warpsize4x1x1_g1_tensor16x8x16 Tactic: 0xeb25062ffb9eae45
[08/10/2023-11:18:14] [V] [TRT] Tactic: 0xeb25062ffb9eae45 Time: 0.193947
[08/10/2023-11:18:14] [V] [TRT] Conv_67 Set Tactic Name: ampere_h16816cudnn_128x64_sliced1x2_ldg8_relu_exp_stages_64x4_large_nhwc_tn_v1 Tactic: 0xeb2d2aa4e56bb41c
[08/10/2023-11:18:14] [V] [TRT] Tactic: 0xeb2d2aa4e56bb41c Time: 0.268608
[08/10/2023-11:18:14] [V] [TRT] Conv_67 Set Tactic Name: ampere_h16816cudnn_64x64_ldg8_relu_exp_stages_64x6_small_nhwc_tn_v1 Tactic: 0xf0beb09df9a19f82
[08/10/2023-11:18:14] [V] [TRT] Tactic: 0xf0beb09df9a19f82 Time: 0.357883
[08/10/2023-11:18:14] [V] [TRT] Conv_67 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x64x64_stage4_warpsize2x2x1_g1_tensor16x8x16 Tactic: 0xf35e0311fa1cc516
[08/10/2023-11:18:14] [V] [TRT] Tactic: 0xf35e0311fa1cc516 Time: 0.208841
[08/10/2023-11:18:14] [V] [TRT] Conv_67 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x32x32_stage4_warpsize4x1x1_g1_tensor16x8x16 Tactic: 0xf79479a62ea9f901
[08/10/2023-11:18:14] [V] [TRT] Tactic: 0xf79479a62ea9f901 Time: 0.128834
[08/10/2023-11:18:14] [V] [TRT] Fastest Tactic: 0x3e7eb35b91b9fa63 Time: 0.119769
[08/10/2023-11:18:14] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x3e7eb35b91b9fa63
[08/10/2023-11:18:14] [V] [TRT] =============== Computing costs for 
[08/10/2023-11:18:14] [V] [TRT] *************** Autotuning format combination: Float(16588800,32400,180,1) -> Float(97200,32400,180,1) ***************
[08/10/2023-11:18:14] [V] [TRT] --------------- Timing Runner: Conv_70 (CudaDepthwiseConvolution)
[08/10/2023-11:18:14] [V] [TRT] CudaDepthwiseConvolution has no valid tactics for this config, skipping
[08/10/2023-11:18:14] [V] [TRT] --------------- Timing Runner: Conv_70 (FusedConvActConvolution)
[08/10/2023-11:18:14] [V] [TRT] Tactic: 0x000000000007ffff Time: 0.68544
[08/10/2023-11:18:14] [V] [TRT] Tactic: 0x00000000000fffff Time: 0.652512
[08/10/2023-11:18:14] [V] [TRT] Tactic: 0x000000000027ffff Time: 0.698971
[08/10/2023-11:18:14] [V] [TRT] Tactic: 0x000000000062ffff Time: 0.628754
[08/10/2023-11:18:14] [V] [TRT] Tactic: 0x000000000086ffff Time: 0.989257
[08/10/2023-11:18:14] [V] [TRT] Tactic: 0x000000000089ffff Time: 0.640114
[08/10/2023-11:18:14] [V] [TRT] Tactic: 0x00000000009fffff Time: 0.647735
[08/10/2023-11:18:14] [V] [TRT] Tactic: 0x0000000000a2ffff Time: 0.687666
[08/10/2023-11:18:14] [V] [TRT] Fastest Tactic: 0x000000000062ffff Time: 0.628754
[08/10/2023-11:18:14] [V] [TRT] --------------- Timing Runner: Conv_70 (CudnnConvolution)
[08/10/2023-11:18:14] [V] [TRT] Tactic: 0x0000000000000000 Time: 1.08458
[08/10/2023-11:18:14] [V] [TRT] Tactic: 0x0000000000000001 Time: 0.483813
[08/10/2023-11:18:14] [V] [TRT] Tactic: 0x0000000000000002 Time: 1.85474
[08/10/2023-11:18:14] [V] [TRT] Tactic: 0x0000000000000004 Time: 5.10026
[08/10/2023-11:18:14] [V] [TRT] Tactic: 0x0000000000000005 Time: 1.09206
[08/10/2023-11:18:14] [V] [TRT] Tactic: 0x0000000000000006 Time: 0.439177
[08/10/2023-11:18:14] [V] [TRT] Tactic: 0x0000000000000038 Time: 1.06321
[08/10/2023-11:18:14] [V] [TRT] Tactic: 0x0000000000000039 Time: 0.474162
[08/10/2023-11:18:14] [V] [TRT] Tactic: 0x000000000000003a Time: 1.83893
[08/10/2023-11:18:14] [V] [TRT] Tactic: 0x000000000000003c Time: 4.98794
[08/10/2023-11:18:14] [V] [TRT] Tactic: 0x000000000000003d Time: 1.09032
[08/10/2023-11:18:14] [V] [TRT] Tactic: 0x000000000000003e Time: 0.439031
[08/10/2023-11:18:14] [V] [TRT] Tactic: 0x0000000000000070 Time: 1.06216
[08/10/2023-11:18:14] [V] [TRT] Tactic: 0x0000000000000071 Time: 0.835909
[08/10/2023-11:18:14] [V] [TRT] Tactic: 0x0000000000000072 Time: 1.82486
[08/10/2023-11:18:15] [V] [TRT] Tactic: 0x0000000000000074 Time: 4.94746
[08/10/2023-11:18:15] [V] [TRT] Tactic: 0x0000000000000075 Time: 1.06901
[08/10/2023-11:18:15] [V] [TRT] Tactic: 0x0000000000000076 Time: 0.439872
[08/10/2023-11:18:15] [V] [TRT] Fastest Tactic: 0x000000000000003e Time: 0.439031
[08/10/2023-11:18:15] [V] [TRT] --------------- Timing Runner: Conv_70 (CaskConvolution)
[08/10/2023-11:18:15] [V] [TRT] Conv_70 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x128x8_stage3_warpsize1x4x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0x01cf8ce2da913006
[08/10/2023-11:18:15] [V] [TRT] Tactic: 0x01cf8ce2da913006 Time: 2.72028
[08/10/2023-11:18:15] [V] [TRT] Conv_70 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x64x8_stage3_warpsize2x2x1_g1_ffma_aligna4_alignc4 Tactic: 0x12dbf7d94ee3696d
[08/10/2023-11:18:15] [V] [TRT] Tactic: 0x12dbf7d94ee3696d Time: 1.47393
[08/10/2023-11:18:15] [V] [TRT] Conv_70 Set Tactic Name: ampere_scudnn_128x64_relu_small_nn_v1 Tactic: 0x3f243c490d502deb
[08/10/2023-11:18:15] [V] [TRT] Tactic: 0x3f243c490d502deb Time: 1.38725
[08/10/2023-11:18:15] [V] [TRT] Conv_70 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0x4727434768e46395
[08/10/2023-11:18:15] [V] [TRT] Tactic: 0x4727434768e46395 Time: 1.59123
[08/10/2023-11:18:15] [V] [TRT] Conv_70 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize256x128x8_stage3_warpsize4x2x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0x4efce38acc876f5c
[08/10/2023-11:18:15] [V] [TRT] Tactic: 0x4efce38acc876f5c Time: 2.73321
[08/10/2023-11:18:15] [V] [TRT] Conv_70 Set Tactic Name: ampere_scudnn_128x128_relu_small_nn_v1 Tactic: 0x503619c69ae500ff
[08/10/2023-11:18:15] [V] [TRT] Tactic: 0x503619c69ae500ff Time: 2.42993
[08/10/2023-11:18:15] [V] [TRT] Conv_70 Set Tactic Name: ampere_scudnn_128x128_relu_xregs_large_nn_v1 Tactic: 0x5403ad713f811a18
[08/10/2023-11:18:15] [V] [TRT] Tactic: 0x5403ad713f811a18 Time: 2.62436
[08/10/2023-11:18:15] [V] [TRT] Conv_70 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x128x8_stage3_warpsize1x4x1_g1_ffma_aligna4_alignc4 Tactic: 0x5aa723e0481da855
[08/10/2023-11:18:15] [V] [TRT] Tactic: 0x5aa723e0481da855 Time: 2.72091
[08/10/2023-11:18:15] [V] [TRT] Conv_70 Set Tactic Name: ampere_scudnn_128x64_relu_xregs_large_nn_v1 Tactic: 0x5deb29b7a8e275f7
[08/10/2023-11:18:15] [V] [TRT] Tactic: 0x5deb29b7a8e275f7 Time: 1.36641
[08/10/2023-11:18:15] [V] [TRT] Conv_70 Set Tactic Name: ampere_scudnn_winograd_128x128_ldg1_ldg4_relu_tile148t_nt_v1 Tactic: 0x94119b4c514b211a
[08/10/2023-11:18:15] [V] [TRT] Tactic: 0x94119b4c514b211a Time: 0.421449
[08/10/2023-11:18:15] [V] [TRT] Conv_70 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_aligna4_alignc4 Tactic: 0xa31d27de74b895ff
[08/10/2023-11:18:15] [V] [TRT] Tactic: 0xa31d27de74b895ff Time: 1.55197
[08/10/2023-11:18:15] [V] [TRT] Conv_70 Set Tactic Name: ampere_scudnn_128x32_relu_small_nn_v1 Tactic: 0xa8609adc4e0ceb90
[08/10/2023-11:18:15] [V] [TRT] Tactic: 0xa8609adc4e0ceb90 Time: 0.818633
[08/10/2023-11:18:15] [V] [TRT] Conv_70 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize256x128x8_stage3_warpsize4x2x1_g1_ffma_aligna4_alignc4 Tactic: 0xbb8c3889c7eacd30
[08/10/2023-11:18:15] [V] [TRT] Tactic: 0xbb8c3889c7eacd30 Time: 2.75368
[08/10/2023-11:18:15] [V] [TRT] Conv_70 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_aligna4_alignc4 Tactic: 0xd828f024626fa982
[08/10/2023-11:18:15] [V] [TRT] Tactic: 0xd828f024626fa982 Time: 2.15564
[08/10/2023-11:18:15] [V] [TRT] Conv_70 Set Tactic Name: ampere_scudnn_128x128_relu_medium_nn_v1 Tactic: 0xf067e6205da31c2e
[08/10/2023-11:18:15] [V] [TRT] Tactic: 0xf067e6205da31c2e Time: 2.46349
[08/10/2023-11:18:15] [V] [TRT] Conv_70 Set Tactic Name: ampere_scudnn_128x64_relu_medium_nn_v1 Tactic: 0xf64396b97c889179
[08/10/2023-11:18:15] [V] [TRT] Tactic: 0xf64396b97c889179 Time: 1.41627
[08/10/2023-11:18:15] [V] [TRT] Fastest Tactic: 0x94119b4c514b211a Time: 0.421449
[08/10/2023-11:18:15] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x94119b4c514b211a
[08/10/2023-11:18:15] [V] [TRT] *************** Autotuning format combination: Float(16588800,1,92160,512) -> Float(97200,1,540,3) ***************
[08/10/2023-11:18:15] [V] [TRT] --------------- Timing Runner: Conv_70 (CaskConvolution)
[08/10/2023-11:18:15] [V] [TRT] Conv_70 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_aligna4_alignc4 Tactic: 0x19b688348f983aa0
[08/10/2023-11:18:15] [V] [TRT] Tactic: 0x19b688348f983aa0 Time: 1.45397
[08/10/2023-11:18:15] [V] [TRT] Conv_70 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x64x8_stage3_warpsize2x2x1_g1_ffma_aligna4_alignc4 Tactic: 0x1da91d865428f237
[08/10/2023-11:18:15] [V] [TRT] Tactic: 0x1da91d865428f237 Time: 1.15977
[08/10/2023-11:18:15] [V] [TRT] Conv_70 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x256x8_stage3_warpsize1x4x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0x3f0c846d6379bc98
[08/10/2023-11:18:15] [V] [TRT] Tactic: 0x3f0c846d6379bc98 Time: 4.28451
[08/10/2023-11:18:15] [V] [TRT] Conv_70 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_aligna4_alignc4 Tactic: 0x62835fce994f06dd
[08/10/2023-11:18:15] [V] [TRT] Tactic: 0x62835fce994f06dd Time: 1.23192
[08/10/2023-11:18:15] [V] [TRT] Conv_70 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize256x64x8_stage3_warpsize2x2x1_g1_ffma_aligna4_alignc4 Tactic: 0x8014228ec08b4d49
[08/10/2023-11:18:15] [V] [TRT] Tactic: 0x8014228ec08b4d49 Time: 1.20387
[08/10/2023-11:18:15] [V] [TRT] Conv_70 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0x94a7db94ba744c45
[08/10/2023-11:18:15] [V] [TRT] Tactic: 0x94a7db94ba744c45 Time: 1.25167
[08/10/2023-11:18:15] [V] [TRT] Conv_70 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x128x8_stage3_warpsize1x4x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0xd15dd11d64344e83
[08/10/2023-11:18:15] [V] [TRT] Tactic: 0xd15dd11d64344e83 Time: 2.25284
[08/10/2023-11:18:15] [V] [TRT] Conv_70 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x64x8_stage3_warpsize2x2x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0xf48db81f02eca9ee
[08/10/2023-11:18:15] [V] [TRT] Tactic: 0xf48db81f02eca9ee Time: 1.1601
[08/10/2023-11:18:15] [V] [TRT] Fastest Tactic: 0x1da91d865428f237 Time: 1.15977
[08/10/2023-11:18:15] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x1da91d865428f237
[08/10/2023-11:18:15] [V] [TRT] *************** Autotuning format combination: Float(4147200,1:4,23040,128) -> Float(32400,1:4,180,1) ***************
[08/10/2023-11:18:15] [V] [TRT] --------------- Timing Runner: Conv_70 (CaskConvolution)
[08/10/2023-11:18:15] [V] [TRT] Conv_70 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize64x128x32_stage5_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0x112d7e4d280f396d
[08/10/2023-11:18:16] [V] [TRT] Tactic: 0x112d7e4d280f396d Time: 0.849801
[08/10/2023-11:18:16] [V] [TRT] Conv_70 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x64x32_stage5_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0x13cc2ad8dcd32ba2
[08/10/2023-11:18:16] [V] [TRT] Tactic: 0x13cc2ad8dcd32ba2 Time: 0.468229
[08/10/2023-11:18:16] [V] [TRT] Conv_70 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize256x128x32_stage3_warpsize4x2x1_g1_tensor16x8x8_t1r3s3 Tactic: 0x21246c8544eff903
[08/10/2023-11:18:16] [V] [TRT] Tactic: 0x21246c8544eff903 Time: 0.525426
[08/10/2023-11:18:16] [V] [TRT] Conv_70 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x128x32_stage4_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0x25b2b9d5c9d5ca0d
[08/10/2023-11:18:16] [V] [TRT] Tactic: 0x25b2b9d5c9d5ca0d Time: 0.586761
[08/10/2023-11:18:16] [V] [TRT] Conv_70 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage4_warpsize2x2x1_g1_tensor16x8x8_t1r3s3 Tactic: 0x2d80d2b384ab13f1
[08/10/2023-11:18:16] [V] [TRT] Tactic: 0x2d80d2b384ab13f1 Time: 0.512809
[08/10/2023-11:18:16] [V] [TRT] Conv_70 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize256x128x32_stage3_warpsize4x2x1_g1_tensor16x8x8 Tactic: 0x36303f399d3ee4fd
[08/10/2023-11:18:16] [V] [TRT] Tactic: 0x36303f399d3ee4fd Time: 0.635762
[08/10/2023-11:18:16] [V] [TRT] Conv_70 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage4_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0x385bfab9332b1413
[08/10/2023-11:18:16] [V] [TRT] Tactic: 0x385bfab9332b1413 Time: 0.582286
[08/10/2023-11:18:16] [V] [TRT] Conv_70 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x256x32_stage3_warpsize2x4x1_g1_tensor16x8x8 Tactic: 0x482b48242255b8ce
[08/10/2023-11:18:16] [V] [TRT] Tactic: 0x482b48242255b8ce Time: 1.02491
[08/10/2023-11:18:16] [V] [TRT] Conv_70 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x256x32_stage3_warpsize2x4x1_g1_tensor16x8x8_t1r3s3 Tactic: 0x614e89f7852edbc3
[08/10/2023-11:18:16] [V] [TRT] Tactic: 0x614e89f7852edbc3 Time: 0.973591
[08/10/2023-11:18:16] [V] [TRT] Conv_70 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0x65e41d81f093b482
[08/10/2023-11:18:16] [V] [TRT] Tactic: 0x65e41d81f093b482 Time: 0.596521
[08/10/2023-11:18:16] [V] [TRT] Conv_70 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize64x128x32_stage5_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0x65f71607d20d5438
[08/10/2023-11:18:16] [V] [TRT] Tactic: 0x65f71607d20d5438 Time: 0.794057
[08/10/2023-11:18:16] [V] [TRT] Conv_70 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x64x32_stage5_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0x6716429226d146f7
[08/10/2023-11:18:16] [V] [TRT] Tactic: 0x6716429226d146f7 Time: 0.421239
[08/10/2023-11:18:16] [V] [TRT] Conv_70 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize64x32x64_stage5_warpsize2x2x1_g1_tensor16x8x8_t1r3s3 Tactic: 0x679a281f7660c436
[08/10/2023-11:18:16] [V] [TRT] Tactic: 0x679a281f7660c436 Time: 0.396974
[08/10/2023-11:18:16] [V] [TRT] Conv_70 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize64x32x64_stage5_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0x6e2a3c7a7fc5e4e1
[08/10/2023-11:18:16] [V] [TRT] Tactic: 0x6e2a3c7a7fc5e4e1 Time: 0.423593
[08/10/2023-11:18:16] [V] [TRT] Conv_70 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x64x32_stage5_warpsize2x2x1_g1_tensor16x8x8_t1r3s3 Tactic: 0x7aa21282feb15f1a
[08/10/2023-11:18:16] [V] [TRT] Tactic: 0x7aa21282feb15f1a Time: 0.420183
[08/10/2023-11:18:16] [V] [TRT] Conv_70 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage4_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0x818413b69874bb35
[08/10/2023-11:18:16] [V] [TRT] Tactic: 0x818413b69874bb35 Time: 0.569655
[08/10/2023-11:18:16] [V] [TRT] Conv_70 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x256x32_stage3_warpsize2x4x1_g1_tensor16x8x8 Tactic: 0x998c97842e775a17
[08/10/2023-11:18:16] [V] [TRT] Tactic: 0x998c97842e775a17 Time: 1.03444
[08/10/2023-11:18:16] [V] [TRT] Conv_70 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8_t1r3s3 Tactic: 0x999e005e3b016ea6
[08/10/2023-11:18:16] [V] [TRT] Tactic: 0x999e005e3b016ea6 Time: 0.537243
[08/10/2023-11:18:16] [V] [TRT] Conv_70 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize256x64x32_stage3_warpsize4x2x1_g1_tensor16x8x8_t1r3s3 Tactic: 0xa9a06d0633580c0c
[08/10/2023-11:18:16] [V] [TRT] Tactic: 0xa9a06d0633580c0c Time: 0.337559
[08/10/2023-11:18:16] [V] [TRT] Conv_70 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0xb443c221fcb1565b
[08/10/2023-11:18:16] [V] [TRT] Tactic: 0xb443c221fcb1565b Time: 0.555817
[08/10/2023-11:18:16] [V] [TRT] Conv_70 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage3_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0xb47d7d926de02dee
[08/10/2023-11:18:16] [V] [TRT] Tactic: 0xb47d7d926de02dee Time: 0.425294
[08/10/2023-11:18:16] [V] [TRT] Conv_70 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize256x64x32_stage3_warpsize4x2x1_g1_tensor16x8x8 Tactic: 0xba0185279ccb2b85
[08/10/2023-11:18:16] [V] [TRT] Tactic: 0xba0185279ccb2b85 Time: 0.444617
[08/10/2023-11:18:16] [V] [TRT] Conv_70 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage3_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0xc0a715d897e240bb
[08/10/2023-11:18:16] [V] [TRT] Tactic: 0xc0a715d897e240bb Time: 0.423159
[08/10/2023-11:18:16] [V] [TRT] Conv_70 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize256x64x32_stage3_warpsize4x2x1_g1_tensor16x8x8 Tactic: 0xcedbed6d66c946d0
[08/10/2023-11:18:16] [V] [TRT] Tactic: 0xcedbed6d66c946d0 Time: 0.353202
[08/10/2023-11:18:16] [V] [TRT] Conv_70 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize64x128x32_stage5_warpsize2x2x1_g1_tensor16x8x8_t1r3s3 Tactic: 0xd4428a7355769d7d
[08/10/2023-11:18:16] [V] [TRT] Tactic: 0xd4428a7355769d7d Time: 0.765056
[08/10/2023-11:18:16] [V] [TRT] Conv_70 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize64x32x64_stage5_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0xd7f5d575d49a4bc7
[08/10/2023-11:18:16] [V] [TRT] Tactic: 0xd7f5d575d49a4bc7 Time: 0.477545
[08/10/2023-11:18:16] [V] [TRT] Conv_70 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x128x32_stage4_warpsize2x2x1_g1_tensor16x8x8_t1r3s3 Tactic: 0xd920b33c9bd27143
[08/10/2023-11:18:16] [V] [TRT] Tactic: 0xd920b33c9bd27143 Time: 0.579547
[08/10/2023-11:18:16] [V] [TRT] Conv_70 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize256x128x32_stage3_warpsize4x2x1_g1_tensor16x8x8 Tactic: 0xe797e099911c0624
[08/10/2023-11:18:16] [V] [TRT] Tactic: 0xe797e099911c0624 Time: 0.565774
[08/10/2023-11:18:16] [V] [TRT] Conv_70 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage3_warpsize2x2x1_g1_tensor16x8x8_t1r3s3 Tactic: 0xeb65d4c1e8fa2294
[08/10/2023-11:18:16] [V] [TRT] Tactic: 0xeb65d4c1e8fa2294 Time: 0.404174
[08/10/2023-11:18:16] [V] [TRT] Conv_70 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x128x32_stage4_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0xf4156675c5f728d4
[08/10/2023-11:18:16] [V] [TRT] Tactic: 0xf4156675c5f728d4 Time: 0.832329
[08/10/2023-11:18:16] [V] [TRT] Fastest Tactic: 0xa9a06d0633580c0c Time: 0.337559
[08/10/2023-11:18:16] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0xa9a06d0633580c0c
[08/10/2023-11:18:16] [V] [TRT] *************** Autotuning format combination: Half(16588800,32400,180,1) -> Half(97200,32400,180,1) ***************
[08/10/2023-11:18:16] [V] [TRT] --------------- Timing Runner: Conv_70 (CudnnConvolution)
[08/10/2023-11:18:16] [V] [TRT] Tactic: 0x0000000000000000 Time: 2.92483
[08/10/2023-11:18:16] [V] [TRT] Tactic: 0x0000000000000001 Time: 0.846615
[08/10/2023-11:18:16] [V] [TRT] Tactic: 0x0000000000000002 Time: 3.35652
[08/10/2023-11:18:16] [V] [TRT] Tactic: 0x0000000000000004 Time: 5.01824
[08/10/2023-11:18:16] [V] [TRT] Tactic: 0x0000000000000005 Time: 1.07446
[08/10/2023-11:18:16] [V] [TRT] Tactic: 0x0000000000000006 Time: 4.13883
[08/10/2023-11:18:16] [V] [TRT] Tactic: 0x0000000000000038 Time: 2.91205
[08/10/2023-11:18:17] [V] [TRT] Tactic: 0x000000000000003a Time: 3.37679
[08/10/2023-11:18:17] [V] [TRT] Tactic: 0x000000000000003c Time: 4.96741
[08/10/2023-11:18:17] [V] [TRT] Tactic: 0x000000000000003d Time: 1.08194
[08/10/2023-11:18:17] [V] [TRT] Tactic: 0x000000000000003e Time: 4.17961
[08/10/2023-11:18:17] [V] [TRT] Fastest Tactic: 0x0000000000000001 Time: 0.846615
[08/10/2023-11:18:17] [V] [TRT] --------------- Timing Runner: Conv_70 (CaskConvolution)
[08/10/2023-11:18:17] [V] [TRT] CaskConvolution has no valid tactics for this config, skipping
[08/10/2023-11:18:17] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CudnnConvolution Tactic: 0x0000000000000001
[08/10/2023-11:18:17] [V] [TRT] *************** Autotuning format combination: Half(8294400,32400:2,180,1) -> Half(64800,32400:2,180,1) ***************
[08/10/2023-11:18:17] [V] [TRT] --------------- Timing Runner: Conv_70 (FusedConvActConvolution)
[08/10/2023-11:18:17] [V] [TRT] FusedConvActConvolution has no valid tactics for this config, skipping
[08/10/2023-11:18:17] [V] [TRT] --------------- Timing Runner: Conv_70 (CaskConvolution)
[08/10/2023-11:18:17] [V] [TRT] Conv_70 Set Tactic Name: ampere_fp16x2_hcudnn_fp16x2_128x32_relu_medium_nn_v1 Tactic: 0x1e7896ba71ef1635
[08/10/2023-11:18:17] [V] [TRT] Tactic: 0x1e7896ba71ef1635 Time: 0.462309
[08/10/2023-11:18:17] [V] [TRT] Conv_70 Set Tactic Name: ampere_fp16x2_hcudnn_fp16x2_128x64_relu_large_nn_v1 Tactic: 0x2f735ffbb05a30fd
[08/10/2023-11:18:17] [V] [TRT] Tactic: 0x2f735ffbb05a30fd Time: 0.833454
[08/10/2023-11:18:17] [V] [TRT] Conv_70 Set Tactic Name: ampere_fp16x2_hcudnn_fp16x2_128x128_relu_medium_nn_v1 Tactic: 0x360278e347d63410
[08/10/2023-11:18:17] [V] [TRT] Tactic: 0x360278e347d63410 Time: 1.67786
[08/10/2023-11:18:17] [V] [TRT] Conv_70 Set Tactic Name: ampere_fp16x2_hcudnn_fp16x2_128x64_relu_medium_nn_v1 Tactic: 0x4cfee77ea8c324db
[08/10/2023-11:18:17] [V] [TRT] Tactic: 0x4cfee77ea8c324db Time: 0.84832
[08/10/2023-11:18:17] [V] [TRT] Conv_70 Set Tactic Name: ampere_fp16x2_hcudnn_fp16x2_128x64_relu_small_nn_v1 Tactic: 0x540fde3a7bee53dc
[08/10/2023-11:18:17] [V] [TRT] Tactic: 0x540fde3a7bee53dc Time: 0.838043
[08/10/2023-11:18:17] [V] [TRT] Conv_70 Set Tactic Name: ampere_fp16x2_hcudnn_fp16x2_128x128_relu_large_nn_v1 Tactic: 0x91f7e9c0851ad67c
[08/10/2023-11:18:17] [V] [TRT] Tactic: 0x91f7e9c0851ad67c Time: 1.78659
[08/10/2023-11:18:17] [V] [TRT] Conv_70 Set Tactic Name: ampere_fp16x2_hcudnn_fp16x2_128x32_relu_large_nn_v1 Tactic: 0xb837f96ef306f686
[08/10/2023-11:18:17] [V] [TRT] Tactic: 0xb837f96ef306f686 Time: 0.461957
[08/10/2023-11:18:17] [V] [TRT] Conv_70 Set Tactic Name: ampere_fp16x2_hcudnn_fp16x2_128x32_relu_small_nn_v1 Tactic: 0xc34b78af38b295a7
[08/10/2023-11:18:17] [V] [TRT] Tactic: 0xc34b78af38b295a7 Time: 0.458615
[08/10/2023-11:18:17] [V] [TRT] Conv_70 Set Tactic Name: ampere_fp16x2_hcudnn_winograd_fp16x2_128x128_ldg1_ldg4_relu_tile148t_nt_v1 Tactic: 0xc754debea88ae0b7
[08/10/2023-11:18:17] [V] [TRT] Tactic: 0xc754debea88ae0b7 Time: 0.24592
[08/10/2023-11:18:17] [V] [TRT] Conv_70 Set Tactic Name: ampere_fp16x2_hcudnn_fp16x2_128x128_relu_small_nn_v1 Tactic: 0xea8b68014eaeb55d
[08/10/2023-11:18:17] [V] [TRT] Tactic: 0xea8b68014eaeb55d Time: 1.71494
[08/10/2023-11:18:17] [V] [TRT] Fastest Tactic: 0xc754debea88ae0b7 Time: 0.24592
[08/10/2023-11:18:17] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0xc754debea88ae0b7
[08/10/2023-11:18:17] [V] [TRT] *************** Autotuning format combination: Half(2073600,1:8,11520,64) -> Float(97200,32400,180,1) ***************
[08/10/2023-11:18:17] [V] [TRT] --------------- Timing Runner: Conv_70 (CaskConvolution)
[08/10/2023-11:18:17] [V] [TRT] CaskConvolution has no valid tactics for this config, skipping
[08/10/2023-11:18:17] [V] [TRT] *************** Autotuning format combination: Half(2073600,1:8,11520,64) -> Half(32400,1:8,180,1) ***************
[08/10/2023-11:18:17] [V] [TRT] --------------- Timing Runner: Conv_70 (CudaDepthwiseConvolution)
[08/10/2023-11:18:17] [V] [TRT] CudaDepthwiseConvolution has no valid tactics for this config, skipping
[08/10/2023-11:18:17] [V] [TRT] --------------- Timing Runner: Conv_70 (CaskConvolution)
[08/10/2023-11:18:17] [V] [TRT] Conv_70 Set Tactic Name: ampere_h16816cudnn_256x128_ldg8_relu_exp_medium_nhwc_tn_v1 Tactic: 0x03896956a39a1203
[08/10/2023-11:18:17] [V] [TRT] Tactic: 0x03896956a39a1203 Time: 0.302688
[08/10/2023-11:18:17] [V] [TRT] Conv_70 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x128x32_stage3_warpsize4x2x1_g1_tensor16x8x16_t1r3s3 Tactic: 0x048d6d0400f33439
[08/10/2023-11:18:17] [V] [TRT] Tactic: 0x048d6d0400f33439 Time: 0.282816
[08/10/2023-11:18:17] [V] [TRT] Conv_70 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x64x64_stage3_warpsize2x2x1_g1_tensor16x8x16_t1r3s3 Tactic: 0x05b6220f243edacd
[08/10/2023-11:18:17] [V] [TRT] Tactic: 0x05b6220f243edacd Time: 0.149815
[08/10/2023-11:18:17] [V] [TRT] Conv_70 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x64x32_stage3_warpsize4x1x1_g1_tensor16x8x16_t1r3s3_aACCESS Tactic: 0x0866ddee325d07a6
[08/10/2023-11:18:17] [V] [TRT] Tactic: 0x0866ddee325d07a6 Time: 0.160599
[08/10/2023-11:18:17] [V] [TRT] Conv_70 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x128x32_stage4_warpsize2x2x1_g1_tensor16x8x16 Tactic: 0x0e07ff4f4f7c1ac9
[08/10/2023-11:18:17] [V] [TRT] Tactic: 0x0e07ff4f4f7c1ac9 Time: 0.269591
[08/10/2023-11:18:17] [V] [TRT] Conv_70 Set Tactic Name: ampere_h16816cudnn_256x128_ldg8_relu_exp_stages_64x3_small_nhwc_tn_v1 Tactic: 0x0e199750c30c6928
[08/10/2023-11:18:17] [V] [TRT] Tactic: 0x0e199750c30c6928 Time: 0.300558
[08/10/2023-11:18:17] [V] [TRT] Conv_70 Set Tactic Name: ampere_h1688cudnn_128x128_ldg8_relu_exp_medium_nhwc_tn_v1 Tactic: 0x100c1ad308e08d35
[08/10/2023-11:18:17] [V] [TRT] Tactic: 0x100c1ad308e08d35 Time: 0.346624
[08/10/2023-11:18:17] [V] [TRT] Conv_70 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x128x32_stage5_warpsize2x2x1_g1_tensor16x8x16_t1r3s3 Tactic: 0x17ebf0c9f418f10a
[08/10/2023-11:18:17] [V] [TRT] Tactic: 0x17ebf0c9f418f10a Time: 0.28821
[08/10/2023-11:18:17] [V] [TRT] Conv_70 Set Tactic Name: ampere_h16816cudnn_128x64_ldg8_relu_exp_stages_64x4_medium_nhwc_tn_v1 Tactic: 0x196cbc423a9bb69e
[08/10/2023-11:18:17] [V] [TRT] Tactic: 0x196cbc423a9bb69e Time: 0.248315
[08/10/2023-11:18:17] [V] [TRT] Conv_70 Set Tactic Name: ampere_h16816cudnn_128x64_sliced1x2_ldg8_relu_exp_stages_64x3_small_nhwc_tn_v1 Tactic: 0x19822de884f42a6a
[08/10/2023-11:18:17] [V] [TRT] Tactic: 0x19822de884f42a6a Time: 0.162382
[08/10/2023-11:18:17] [V] [TRT] Conv_70 Set Tactic Name: ampere_h16816cudnn_64x64_sliced1x2_ldg8_relu_exp_stages_64x5_small_nhwc_tn_v1 Tactic: 0x1a5ba808ad4cc5a8
[08/10/2023-11:18:17] [V] [TRT] Tactic: 0x1a5ba808ad4cc5a8 Time: 0.243835
[08/10/2023-11:18:17] [V] [TRT] Conv_70 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x64x64_stage3_warpsize2x2x1_g1_tensor16x8x16 Tactic: 0x21b295c0c8f6c95a
[08/10/2023-11:18:17] [V] [TRT] Tactic: 0x21b295c0c8f6c95a Time: 0.171269
[08/10/2023-11:18:17] [V] [TRT] Conv_70 Set Tactic Name: ampere_h16816cudnn_128x128_ldg8_relu_exp_medium_nhwc_tn_v1 Tactic: 0x234580e8a194335c
[08/10/2023-11:18:17] [V] [TRT] Tactic: 0x234580e8a194335c Time: 0.256494
[08/10/2023-11:18:17] [V] [TRT] Conv_70 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x128x32_stage4_warpsize2x2x1_g1_tensor16x8x16_t1r3s3_aACCESS Tactic: 0x245530c34bd6090f
[08/10/2023-11:18:17] [V] [TRT] Tactic: 0x245530c34bd6090f Time: 0.263232
[08/10/2023-11:18:17] [V] [TRT] Conv_70 Set Tactic Name: ampere_h16816cudnn_64x64_sliced1x2_ldg8_relu_exp_stages_64x6_small_nhwc_tn_v1 Tactic: 0x24e01e7405cfdfe9
[08/10/2023-11:18:17] [V] [TRT] Tactic: 0x24e01e7405cfdfe9 Time: 0.367122
[08/10/2023-11:18:17] [V] [TRT] Conv_70 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x64x64_stage3_warpsize2x2x1_g1_tensor16x8x16_t1r3s3_aACCESS Tactic: 0x263a38afd75e3a43
[08/10/2023-11:18:17] [V] [TRT] Tactic: 0x263a38afd75e3a43 Time: 0.167246
[08/10/2023-11:18:17] [V] [TRT] Conv_70 Set Tactic Name: ampere_h16816cudnn_64x128_sliced1x2_ldg8_relu_exp_stages_64x4_large_nhwc_tn_v1 Tactic: 0x2721a7f18c2700db
[08/10/2023-11:18:17] [V] [TRT] Tactic: 0x2721a7f18c2700db Time: 0.491854
[08/10/2023-11:18:17] [V] [TRT] Conv_70 Set Tactic Name: ampere_h16816cudnn_128x64_sliced1x2_ldg8_relu_exp_stages_64x3_medium_nhwc_tn_v1 Tactic: 0x29329b5741ea05f2
[08/10/2023-11:18:17] [V] [TRT] Tactic: 0x29329b5741ea05f2 Time: 0.165307
[08/10/2023-11:18:17] [V] [TRT] Conv_70 Set Tactic Name: ampere_h16816cudnn_128x128_ldg8_relu_exp_large_nhwc_tn_v1 Tactic: 0x2970ae65966d569d
[08/10/2023-11:18:17] [V] [TRT] Tactic: 0x2970ae65966d569d Time: 0.250853
[08/10/2023-11:18:17] [V] [TRT] Conv_70 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x256x64_stage3_warpsize1x4x1_g1_tensor16x8x16 Tactic: 0x29dc14520e0e4eb6
[08/10/2023-11:18:17] [V] [TRT] Tactic: 0x29dc14520e0e4eb6 Time: 0.627442
[08/10/2023-11:18:17] [V] [TRT] Conv_70 Set Tactic Name: ampere_h16816cudnn_256x64_ldg8_relu_exp_stages_64x3_large_nhwc_tn_v1 Tactic: 0x2cb4a662694e89d3
[08/10/2023-11:18:17] [V] [TRT] Tactic: 0x2cb4a662694e89d3 Time: 0.196571
[08/10/2023-11:18:17] [V] [TRT] Conv_70 Set Tactic Name: ampere_h16816cudnn_128x128_ldg8_relu_exp_stages_64x3_medium_nhwc_tn_v1 Tactic: 0x2eaa2202de9404d6
[08/10/2023-11:18:17] [V] [TRT] Tactic: 0x2eaa2202de9404d6 Time: 0.344617
[08/10/2023-11:18:17] [V] [TRT] Conv_70 Set Tactic Name: ampere_h1688cudnn_256x128_ldg8_relu_exp_medium_nhwc_tn_v1 Tactic: 0x30c0f36d0aeeac6a
[08/10/2023-11:18:17] [V] [TRT] Tactic: 0x30c0f36d0aeeac6a Time: 0.36405
[08/10/2023-11:18:17] [V] [TRT] Conv_70 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x64x64_stage4_warpsize2x2x1_g1_tensor16x8x16_t1r3s3_aACCESS Tactic: 0x30e8a8d7a953e5e9
[08/10/2023-11:18:17] [V] [TRT] Tactic: 0x30e8a8d7a953e5e9 Time: 0.200069
[08/10/2023-11:18:17] [V] [TRT] Conv_70 Set Tactic Name: ampere_h16816cudnn_128x64_ldg8_relu_exp_stages_64x4_large_nhwc_tn_v1 Tactic: 0x3197d5044d71e98e
[08/10/2023-11:18:17] [V] [TRT] Tactic: 0x3197d5044d71e98e Time: 0.254469
[08/10/2023-11:18:17] [V] [TRT] Conv_70 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x64x64_stage4_warpsize2x2x1_g1_tensor16x8x16_t1r3s3 Tactic: 0x31d93dc22d2af081
[08/10/2023-11:18:17] [V] [TRT] Tactic: 0x31d93dc22d2af081 Time: 0.196576
[08/10/2023-11:18:17] [V] [TRT] Conv_70 Set Tactic Name: ampere_h1688cudnn_128x128_ldg8_relu_exp_small_nhwc_tn_v1 Tactic: 0x3369260c04f9ad73
[08/10/2023-11:18:17] [V] [TRT] Tactic: 0x3369260c04f9ad73 Time: 0.331566
[08/10/2023-11:18:17] [V] [TRT] Conv_70 Set Tactic Name: ampere_h16816cudnn_64x128_ldg8_relu_exp_stages_64x4_small_nhwc_tn_v1 Tactic: 0x3b5fa0f2a8fc2410
[08/10/2023-11:18:17] [V] [TRT] Tactic: 0x3b5fa0f2a8fc2410 Time: 0.429097
[08/10/2023-11:18:17] [V] [TRT] Conv_70 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x32x32_stage4_warpsize4x1x1_g1_tensor16x8x16_t1r3s3 Tactic: 0x3e7eb35b91b9fa63
[08/10/2023-11:18:17] [V] [TRT] Tactic: 0x3e7eb35b91b9fa63 Time: 0.112363
[08/10/2023-11:18:17] [V] [TRT] Conv_70 Set Tactic Name: ampere_h16816cudnn_256x64_ldg8_relu_exp_stages_64x3_medium_nhwc_tn_v1 Tactic: 0x3f031df0e579d52c
[08/10/2023-11:18:17] [V] [TRT] Tactic: 0x3f031df0e579d52c Time: 0.197257
[08/10/2023-11:18:17] [V] [TRT] Conv_70 Set Tactic Name: ampere_h16816cudnn_256x64_sliced1x2_ldg8_relu_exp_small_nhwc_tn_v1 Tactic: 0x42a5b8709d0039be
[08/10/2023-11:18:17] [V] [TRT] Tactic: 0x42a5b8709d0039be Time: 0.215177
[08/10/2023-11:18:17] [V] [TRT] Conv_70 Set Tactic Name: ampere_h16816cudnn_256x128_ldg8_relu_exp_small_nhwc_tn_v1 Tactic: 0x463794ee4acffd1f
[08/10/2023-11:18:17] [V] [TRT] Tactic: 0x463794ee4acffd1f Time: 0.306249
[08/10/2023-11:18:17] [V] [TRT] Conv_70 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x64x64_stage4_warpsize2x2x1_g1_tensor16x8x16 Tactic: 0x4a81ea1e51436a30
[08/10/2023-11:18:17] [V] [TRT] Tactic: 0x4a81ea1e51436a30 Time: 0.223863
[08/10/2023-11:18:17] [V] [TRT] Conv_70 Set Tactic Name: ampere_h1688cudnn_256x64_sliced1x2_ldg8_relu_exp_small_nhwc_tn_v1 Tactic: 0x4aed1956bd10a795
[08/10/2023-11:18:17] [V] [TRT] Tactic: 0x4aed1956bd10a795 Time: 0.239168
[08/10/2023-11:18:17] [V] [TRT] Conv_70 Set Tactic Name: ampere_h16816cudnn_64x128_sliced1x2_ldg8_relu_exp_stages_64x3_large_nhwc_tn_v1 Tactic: 0x4fc05923455fc266
[08/10/2023-11:18:17] [V] [TRT] Tactic: 0x4fc05923455fc266 Time: 0.301998
[08/10/2023-11:18:17] [V] [TRT] Conv_70 Set Tactic Name: ampere_h16816cudnn_256x64_ldg8_relu_exp_large_nhwc_tn_v1 Tactic: 0x5013c38f55afa9ba
[08/10/2023-11:18:17] [V] [TRT] Tactic: 0x5013c38f55afa9ba Time: 0.174821
[08/10/2023-11:18:17] [V] [TRT] Conv_70 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x64x32_stage3_warpsize4x1x1_g1_tensor16x8x16_t1r3s3 Tactic: 0x529f4431bdae94f5
[08/10/2023-11:18:17] [V] [TRT] Tactic: 0x529f4431bdae94f5 Time: 0.141931
[08/10/2023-11:18:17] [V] [TRT] Conv_70 Set Tactic Name: ampere_h16816cudnn_64x128_ldg8_relu_exp_stages_64x3_small_nhwc_tn_v1 Tactic: 0x53be5e206184e6ad
[08/10/2023-11:18:17] [V] [TRT] Tactic: 0x53be5e206184e6ad Time: 0.291118
[08/10/2023-11:18:17] [V] [TRT] Conv_70 Set Tactic Name: ampere_h16816cudnn_64x64_ldg8_relu_exp_stages_64x5_large_nhwc_tn_v1 Tactic: 0x544bff7ff9c5d908
[08/10/2023-11:18:17] [V] [TRT] Tactic: 0x544bff7ff9c5d908 Time: 0.257207
[08/10/2023-11:18:17] [V] [TRT] Conv_70 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x64x64_stage3_warpsize2x2x1_g1_tensor16x8x16 Tactic: 0x5568fd8a32f4a40f
[08/10/2023-11:18:17] [V] [TRT] Tactic: 0x5568fd8a32f4a40f Time: 0.155424
[08/10/2023-11:18:17] [V] [TRT] Conv_70 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x128x64_stage3_warpsize2x2x1_g1_tensor16x8x16 Tactic: 0x57c9a5ff682354a6
[08/10/2023-11:18:17] [V] [TRT] Tactic: 0x57c9a5ff682354a6 Time: 0.324517
[08/10/2023-11:18:17] [V] [TRT] Conv_70 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x32x64_stage5_warpsize2x2x1_g1_tensor16x8x16 Tactic: 0x5820b3dda403c4d0
[08/10/2023-11:18:17] [V] [TRT] Tactic: 0x5820b3dda403c4d0 Time: 0.180535
[08/10/2023-11:18:17] [V] [TRT] Conv_70 Set Tactic Name: ampere_h16816cudnn_128x128_ldg8_relu_exp_stages_64x3_small_nhwc_tn_v1 Tactic: 0x58816bf2e9c36afb
[08/10/2023-11:18:17] [V] [TRT] Tactic: 0x58816bf2e9c36afb Time: 0.341115
[08/10/2023-11:18:17] [V] [TRT] Conv_70 Set Tactic Name: ampere_h16816cudnn_128x64_ldg8_relu_exp_stages_64x3_large_nhwc_tn_v1 Tactic: 0x59762bd684092b33
[08/10/2023-11:18:17] [V] [TRT] Tactic: 0x59762bd684092b33 Time: 0.176343
[08/10/2023-11:18:17] [V] [TRT] Conv_70 Set Tactic Name: ampere_h16816cudnn_128x64_sliced1x2_ldg8_relu_exp_stages_64x4_medium_nhwc_tn_v1 Tactic: 0x59c5c647b4c76593
[08/10/2023-11:18:17] [V] [TRT] Tactic: 0x59c5c647b4c76593 Time: 0.252293
[08/10/2023-11:18:17] [V] [TRT] Conv_70 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x128x64_stage3_warpsize4x2x1_g1_tensor16x8x16 Tactic: 0x5a55274960724ba8
[08/10/2023-11:18:17] [V] [TRT] Tactic: 0x5a55274960724ba8 Time: 0.295689
[08/10/2023-11:18:17] [V] [TRT] Conv_70 Set Tactic Name: ampere_h1688cudnn_256x128_ldg8_relu_exp_large_nhwc_tn_v1 Tactic: 0x5c2e1c87d85b06f1
[08/10/2023-11:18:17] [V] [TRT] Tactic: 0x5c2e1c87d85b06f1 Time: 0.366866
[08/10/2023-11:18:17] [V] [TRT] Conv_70 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x256x64_stage3_warpsize1x4x1_g1_tensor16x8x16 Tactic: 0x5d067c18f40c23e3
[08/10/2023-11:18:17] [V] [TRT] Tactic: 0x5d067c18f40c23e3 Time: 0.623154
[08/10/2023-11:18:17] [V] [TRT] Conv_70 Set Tactic Name: ampere_h16816cudnn_64x64_ldg8_relu_exp_stages_64x5_medium_nhwc_tn_v1 Tactic: 0x5f31c22ec167f384
[08/10/2023-11:18:17] [V] [TRT] Tactic: 0x5f31c22ec167f384 Time: 0.257486
[08/10/2023-11:18:17] [V] [TRT] Conv_70 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x128x32_stage4_warpsize2x2x1_g1_tensor16x8x16_t1r3s3 Tactic: 0x60c3421152ef8e10
[08/10/2023-11:18:17] [V] [TRT] Tactic: 0x60c3421152ef8e10 Time: 0.247314
[08/10/2023-11:18:17] [V] [TRT] Conv_70 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x128x32_stage3_warpsize4x2x1_g1_tensor16x8x16 Tactic: 0x60da8c7151d91e47
[08/10/2023-11:18:17] [V] [TRT] Tactic: 0x60da8c7151d91e47 Time: 0.282482
[08/10/2023-11:18:17] [V] [TRT] Conv_70 Set Tactic Name: ampere_h1688cudnn_256x64_ldg8_relu_exp_medium_nhwc_tn_v1 Tactic: 0x6426696f872a3b13
[08/10/2023-11:18:17] [V] [TRT] Tactic: 0x6426696f872a3b13 Time: 0.224855
[08/10/2023-11:18:17] [V] [TRT] Conv_70 Set Tactic Name: ampere_h16816cudnn_128x64_ldg8_relu_exp_stages_64x3_medium_nhwc_tn_v1 Tactic: 0x699be152cfb6d6ff
[08/10/2023-11:18:17] [V] [TRT] Tactic: 0x699be152cfb6d6ff Time: 0.182363
[08/10/2023-11:18:17] [V] [TRT] Conv_70 Set Tactic Name: ampere_h16816cudnn_64x64_ldg8_relu_exp_stages_64x6_large_nhwc_tn_v1 Tactic: 0x6af049035146c349
[08/10/2023-11:18:17] [V] [TRT] Tactic: 0x6af049035146c349 Time: 0.387666
[08/10/2023-11:18:17] [V] [TRT] Conv_70 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x256x32_stage3_warpsize2x4x1_g1_tensor16x8x16 Tactic: 0x7005d10718f6c22d
[08/10/2023-11:18:17] [V] [TRT] Tactic: 0x7005d10718f6c22d Time: 0.535543
[08/10/2023-11:18:17] [V] [TRT] Conv_70 Set Tactic Name: ampere_h16816cudnn_64x64_sliced1x2_ldg8_relu_exp_stages_64x5_medium_nhwc_tn_v1 Tactic: 0x706f08da35c795a5
[08/10/2023-11:18:17] [V] [TRT] Tactic: 0x706f08da35c795a5 Time: 0.256192
[08/10/2023-11:18:17] [V] [TRT] Conv_70 Set Tactic Name: ampere_h16816cudnn_128x64_sliced1x2_ldg8_relu_exp_stages_64x4_small_nhwc_tn_v1 Tactic: 0x7163d33a4d8ce8d7
[08/10/2023-11:18:17] [V] [TRT] Tactic: 0x7163d33a4d8ce8d7 Time: 0.247424
[08/10/2023-11:18:17] [V] [TRT] Conv_70 Set Tactic Name: ampere_h1688cudnn_256x64_ldg8_relu_exp_small_nhwc_tn_v1 Tactic: 0x7aad3976677d7155
[08/10/2023-11:18:18] [V] [TRT] Tactic: 0x7aad3976677d7155 Time: 0.209705
[08/10/2023-11:18:18] [V] [TRT] Conv_70 Set Tactic Name: ampere_h16816cudnn_64x64_sliced1x2_ldg8_relu_exp_stages_64x5_large_nhwc_tn_v1 Tactic: 0x8015519605ab9963
[08/10/2023-11:18:18] [V] [TRT] Tactic: 0x8015519605ab9963 Time: 0.250953
[08/10/2023-11:18:18] [V] [TRT] Conv_70 Set Tactic Name: ampere_h16816cudnn_256x64_sliced1x2_ldg8_relu_exp_medium_nhwc_tn_v1 Tactic: 0x82f8a2214b0b4178
[08/10/2023-11:18:18] [V] [TRT] Tactic: 0x82f8a2214b0b4178 Time: 0.232594
[08/10/2023-11:18:18] [V] [TRT] Conv_70 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x32x32_stage4_warpsize4x1x1_g1_tensor16x8x16 Tactic: 0x834e11ecd4ab9454
[08/10/2023-11:18:18] [V] [TRT] Tactic: 0x834e11ecd4ab9454 Time: 0.114441
[08/10/2023-11:18:18] [V] [TRT] Conv_70 Set Tactic Name: ampere_h16816cudnn_128x64_sliced1x2_ldg8_relu_exp_stages_64x3_large_nhwc_tn_v1 Tactic: 0x83ccd4762c1376a1
[08/10/2023-11:18:18] [V] [TRT] Tactic: 0x83ccd4762c1376a1 Time: 0.168101
[08/10/2023-11:18:18] [V] [TRT] Conv_70 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x64x64_stage3_warpsize4x1x1_g1_tensor16x8x16_t1r3s3 Tactic: 0x84ee897dd1f4d2aa
[08/10/2023-11:18:18] [V] [TRT] Tactic: 0x84ee897dd1f4d2aa Time: 0.184425
[08/10/2023-11:18:18] [V] [TRT] Conv_70 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x128x64_stage3_warpsize2x2x1_g1_tensor16x8x16 Tactic: 0x866e7a5f6401b67f
[08/10/2023-11:18:18] [V] [TRT] Tactic: 0x866e7a5f6401b67f Time: 0.326135
[08/10/2023-11:18:18] [V] [TRT] Conv_70 Set Tactic Name: ampere_h16816cudnn_64x64_ldg8_relu_exp_stages_64x6_medium_nhwc_tn_v1 Tactic: 0x88971eec55aba850
[08/10/2023-11:18:18] [V] [TRT] Tactic: 0x88971eec55aba850 Time: 0.371003
[08/10/2023-11:18:18] [V] [TRT] Conv_70 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x128x64_stage3_warpsize4x2x1_g1_tensor16x8x16 Tactic: 0x8bf2f8e96c50a971
[08/10/2023-11:18:18] [V] [TRT] Tactic: 0x8bf2f8e96c50a971 Time: 0.289221
[08/10/2023-11:18:18] [V] [TRT] Conv_70 Set Tactic Name: ampere_h16816cudnn_256x128_ldg8_relu_exp_stages_64x3_large_nhwc_tn_v1 Tactic: 0x94576ece6beb35e3
[08/10/2023-11:18:18] [V] [TRT] Tactic: 0x94576ece6beb35e3 Time: 0.303342
[08/10/2023-11:18:18] [V] [TRT] Conv_70 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x64x64_stage3_warpsize4x1x1_g1_tensor16x8x16 Tactic: 0x9fff6e65019cc310
[08/10/2023-11:18:18] [V] [TRT] Tactic: 0x9fff6e65019cc310 Time: 0.184073
[08/10/2023-11:18:18] [V] [TRT] Conv_70 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x32x64_stage5_warpsize2x2x1_g1_tensor16x8x16_t1r3s3 Tactic: 0xa033e20ae9f412b2
[08/10/2023-11:18:18] [V] [TRT] Tactic: 0xa033e20ae9f412b2 Time: 0.16336
[08/10/2023-11:18:18] [V] [TRT] Conv_70 Set Tactic Name: ampere_h16816cudnn_64x128_ldg8_relu_exp_stages_64x4_large_nhwc_tn_v1 Tactic: 0xa111596c001b78db
[08/10/2023-11:18:18] [V] [TRT] Tactic: 0xa111596c001b78db Time: 0.440763
[08/10/2023-11:18:18] [V] [TRT] Conv_70 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x256x32_stage3_warpsize2x4x1_g1_tensor16x8x16 Tactic: 0xa1a20ea714d420f4
[08/10/2023-11:18:18] [V] [TRT] Tactic: 0xa1a20ea714d420f4 Time: 0.529408
[08/10/2023-11:18:18] [V] [TRT] Conv_70 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x32x32_stage4_warpsize4x1x1_g1_tensor16x8x16_t1r3s3 Tactic: 0xa40cb43c296a36a8
[08/10/2023-11:18:18] [V] [TRT] Tactic: 0xa40cb43c296a36a8 Time: 0.120297
[08/10/2023-11:18:18] [V] [TRT] Conv_70 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x64x32_stage3_warpsize4x1x1_g1_tensor16x8x16 Tactic: 0xa570c55d303796ff
[08/10/2023-11:18:18] [V] [TRT] Tactic: 0xa570c55d303796ff Time: 0.156635
[08/10/2023-11:18:18] [V] [TRT] Conv_70 Set Tactic Name: ampere_h16816cudnn_64x64_sliced1x2_ldg8_relu_exp_stages_64x6_medium_nhwc_tn_v1 Tactic: 0xa7c9d418a10bce71
[08/10/2023-11:18:18] [V] [TRT] Tactic: 0xa7c9d418a10bce71 Time: 0.398679
[08/10/2023-11:18:18] [V] [TRT] Conv_70 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x32x32_stage4_warpsize4x1x1_g1_tensor16x8x16 Tactic: 0xa83b68f30462f971
[08/10/2023-11:18:18] [V] [TRT] Tactic: 0xa83b68f30462f971 Time: 0.126187
[08/10/2023-11:18:18] [V] [TRT] Conv_70 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x256x32_stage3_warpsize2x4x1_g1_tensor16x8x16_t1r3s3 Tactic: 0xa9177bbe4e767df8
[08/10/2023-11:18:18] [V] [TRT] Tactic: 0xa9177bbe4e767df8 Time: 0.520229
[08/10/2023-11:18:18] [V] [TRT] Conv_70 Set Tactic Name: ampere_h1688cudnn_128x128_ldg8_relu_exp_large_nhwc_tn_v1 Tactic: 0xa927df92ac1ef1b8
[08/10/2023-11:18:18] [V] [TRT] Tactic: 0xa927df92ac1ef1b8 Time: 0.34912
[08/10/2023-11:18:18] [V] [TRT] Conv_70 Set Tactic Name: ampere_h16816cudnn_128x64_ldg8_relu_exp_stages_64x4_small_nhwc_tn_v1 Tactic: 0xabd92c9ae596b545
[08/10/2023-11:18:18] [V] [TRT] Tactic: 0xabd92c9ae596b545 Time: 0.244306
[08/10/2023-11:18:18] [V] [TRT] Conv_70 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x128x32_stage5_warpsize2x2x1_g1_tensor16x8x16 Tactic: 0xafd1e8bf6bd3d638
[08/10/2023-11:18:18] [V] [TRT] Tactic: 0xafd1e8bf6bd3d638 Time: 0.307022
[08/10/2023-11:18:18] [V] [TRT] Conv_70 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x128x32_stage3_warpsize4x2x1_g1_tensor16x8x16 Tactic: 0xb17d53d15dfbfc9e
[08/10/2023-11:18:18] [V] [TRT] Tactic: 0xb17d53d15dfbfc9e Time: 0.290811
[08/10/2023-11:18:18] [V] [TRT] Conv_70 Set Tactic Name: ampere_h16816cudnn_128x128_ldg8_relu_exp_small_nhwc_tn_v1 Tactic: 0xb33e57fb3e8a0a56
[08/10/2023-11:18:18] [V] [TRT] Tactic: 0xb33e57fb3e8a0a56 Time: 0.248265
[08/10/2023-11:18:18] [V] [TRT] Conv_70 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x64x32_stage5_warpsize2x2x1_g1_tensor16x8x16_t1r3s3 Tactic: 0xb4bec086187edcfc
[08/10/2023-11:18:18] [V] [TRT] Tactic: 0xb4bec086187edcfc Time: 0.162953
[08/10/2023-11:18:18] [V] [TRT] Conv_70 Set Tactic Name: ampere_h16816cudnn_256x64_ldg8_relu_exp_stages_64x3_small_nhwc_tn_v1 Tactic: 0xb6fa5ffcc1a9d518
[08/10/2023-11:18:18] [V] [TRT] Tactic: 0xb6fa5ffcc1a9d518 Time: 0.191927
[08/10/2023-11:18:18] [V] [TRT] Conv_70 Set Tactic Name: ampere_h16816cudnn_64x128_sliced1x2_ldg8_relu_exp_stages_64x3_medium_nhwc_tn_v1 Tactic: 0xb85e52e87caf60a2
[08/10/2023-11:18:18] [V] [TRT] Tactic: 0xb85e52e87caf60a2 Time: 0.291694
[08/10/2023-11:18:18] [V] [TRT] Conv_70 Set Tactic Name: ampere_h16816cudnn_64x128_ldg8_relu_exp_stages_64x3_medium_nhwc_tn_v1 Tactic: 0xb8d86216e1235cda
[08/10/2023-11:18:18] [V] [TRT] Tactic: 0xb8d86216e1235cda Time: 0.291584
[08/10/2023-11:18:18] [V] [TRT] Conv_70 Set Tactic Name: ampere_h16816cudnn_256x128_ldg8_relu_exp_stages_64x3_medium_nhwc_tn_v1 Tactic: 0xb9171d70ba2eda4b
[08/10/2023-11:18:18] [V] [TRT] Tactic: 0xb9171d70ba2eda4b Time: 0.303365
[08/10/2023-11:18:18] [V] [TRT] Conv_70 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x64x32_stage5_warpsize2x2x1_g1_tensor16x8x16 Tactic: 0xbd08239a9317f2fd
[08/10/2023-11:18:18] [V] [TRT] Tactic: 0xbd08239a9317f2fd Time: 0.168032
[08/10/2023-11:18:18] [V] [TRT] Conv_70 Set Tactic Name: ampere_h16816cudnn_64x128_sliced1x2_ldg8_relu_exp_stages_64x4_small_nhwc_tn_v1 Tactic: 0xbd6f5e6f24c05c10
[08/10/2023-11:18:18] [V] [TRT] Tactic: 0xbd6f5e6f24c05c10 Time: 0.46789
[08/10/2023-11:18:18] [V] [TRT] Conv_70 Set Tactic Name: ampere_h16816cudnn_64x64_sliced1x2_ldg8_relu_exp_stages_64x6_large_nhwc_tn_v1 Tactic: 0xbeaee7eaad288322
[08/10/2023-11:18:18] [V] [TRT] Tactic: 0xbeaee7eaad288322 Time: 0.391653
[08/10/2023-11:18:18] [V] [TRT] Conv_70 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x128x64_stage3_warpsize2x2x1_g1_tensor16x8x16_t1r3s3 Tactic: 0xc0a02dc6095497cc
[08/10/2023-11:18:18] [V] [TRT] Tactic: 0xc0a02dc6095497cc Time: 0.31963
[08/10/2023-11:18:18] [V] [TRT] Conv_70 Set Tactic Name: ampere_h16816cudnn_128x128_ldg8_relu_exp_stages_64x3_large_nhwc_tn_v1 Tactic: 0xc2cf926c41243630
[08/10/2023-11:18:18] [V] [TRT] Tactic: 0xc2cf926c41243630 Time: 0.343287
[08/10/2023-11:18:18] [V] [TRT] Conv_70 Set Tactic Name: ampere_h16816cudnn_128x64_ldg8_relu_exp_stages_64x3_small_nhwc_tn_v1 Tactic: 0xc338d2482cee77f8
[08/10/2023-11:18:18] [V] [TRT] Tactic: 0xc338d2482cee77f8 Time: 0.176521
[08/10/2023-11:18:18] [V] [TRT] Conv_70 Set Tactic Name: ampere_h1688cudnn_256x128_ldg8_relu_exp_small_nhwc_tn_v1 Tactic: 0xc660e51970bc5a3a
[08/10/2023-11:18:18] [V] [TRT] Tactic: 0xc660e51970bc5a3a Time: 0.342624
[08/10/2023-11:18:18] [V] [TRT] Conv_70 Set Tactic Name: ampere_h16816cudnn_64x128_ldg8_relu_exp_stages_64x4_medium_nhwc_tn_v1 Tactic: 0xc82f3f06140e3cbb
[08/10/2023-11:18:18] [V] [TRT] Tactic: 0xc82f3f06140e3cbb Time: 0.444544
[08/10/2023-11:18:18] [V] [TRT] Conv_70 Set Tactic Name: ampere_h16816cudnn_64x128_sliced1x2_ldg8_relu_exp_stages_64x4_medium_nhwc_tn_v1 Tactic: 0xc8a90ff8898200c3
[08/10/2023-11:18:18] [V] [TRT] Tactic: 0xc8a90ff8898200c3 Time: 0.479849
[08/10/2023-11:18:18] [V] [TRT] Conv_70 Set Tactic Name: ampere_h1688cudnn_256x64_sliced1x2_ldg8_relu_exp_medium_nhwc_tn_v1 Tactic: 0xc9cc55109bb4de26
[08/10/2023-11:18:18] [V] [TRT] Tactic: 0xc9cc55109bb4de26 Time: 0.256421
[08/10/2023-11:18:18] [V] [TRT] Conv_70 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x64x32_stage5_warpsize2x2x1_g1_tensor16x8x16 Tactic: 0xc9d24bd069159fa8
[08/10/2023-11:18:18] [V] [TRT] Tactic: 0xc9d24bd069159fa8 Time: 0.185701
[08/10/2023-11:18:18] [V] [TRT] Conv_70 Set Tactic Name: ampere_h16816cudnn_64x128_ldg8_relu_exp_stages_64x3_large_nhwc_tn_v1 Tactic: 0xc9f0a7bec963ba66
[08/10/2023-11:18:18] [V] [TRT] Tactic: 0xc9f0a7bec963ba66 Time: 0.292864
[08/10/2023-11:18:18] [V] [TRT] Conv_70 Set Tactic Name: ampere_h16816cudnn_256x64_ldg8_relu_exp_small_nhwc_tn_v1 Tactic: 0xca5d3a11fd48f571
[08/10/2023-11:18:18] [V] [TRT] Tactic: 0xca5d3a11fd48f571 Time: 0.163781
[08/10/2023-11:18:18] [V] [TRT] Conv_70 Set Tactic Name: ampere_h16816cudnn_64x64_ldg8_relu_exp_stages_64x5_small_nhwc_tn_v1 Tactic: 0xce0506e1512285c3
[08/10/2023-11:18:18] [V] [TRT] Tactic: 0xce0506e1512285c3 Time: 0.259607
[08/10/2023-11:18:18] [V] [TRT] Conv_70 Set Tactic Name: ampere_h1688cudnn_256x64_sliced1x2_ldg8_relu_exp_large_nhwc_tn_v1 Tactic: 0xd0a3e0c815f7fb5e
[08/10/2023-11:18:18] [V] [TRT] Tactic: 0xd0a3e0c815f7fb5e Time: 0.252681
[08/10/2023-11:18:18] [V] [TRT] Conv_70 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x64x32_stage3_warpsize4x1x1_g1_tensor16x8x16 Tactic: 0xd1aaad17ca35fbaa
[08/10/2023-11:18:18] [V] [TRT] Tactic: 0xd1aaad17ca35fbaa Time: 0.145618
[08/10/2023-11:18:18] [V] [TRT] Conv_70 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x128x64_stage3_warpsize4x2x1_g1_tensor16x8x16_t1r3s3 Tactic: 0xd2fca0486ca97266
[08/10/2023-11:18:18] [V] [TRT] Tactic: 0xd2fca0486ca97266 Time: 0.294213
[08/10/2023-11:18:18] [V] [TRT] Conv_70 Set Tactic Name: ampere_h16816cudnn_64x128_sliced1x2_ldg8_relu_exp_stages_64x3_small_nhwc_tn_v1 Tactic: 0xd58ea0bdedb89ead
[08/10/2023-11:18:18] [V] [TRT] Tactic: 0xd58ea0bdedb89ead Time: 0.289358
[08/10/2023-11:18:18] [V] [TRT] Conv_70 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x256x64_stage3_warpsize1x4x1_g1_tensor16x8x16_t1r3s3 Tactic: 0xd7c261fa01db2af9
[08/10/2023-11:18:18] [V] [TRT] Tactic: 0xd7c261fa01db2af9 Time: 0.610958
[08/10/2023-11:18:18] [V] [TRT] Conv_70 Set Tactic Name: ampere_h16816cudnn_256x64_sliced1x2_ldg8_relu_exp_large_nhwc_tn_v1 Tactic: 0xd8eb41ee35e76575
[08/10/2023-11:18:18] [V] [TRT] Tactic: 0xd8eb41ee35e76575 Time: 0.233614
[08/10/2023-11:18:18] [V] [TRT] Conv_70 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x128x32_stage5_warpsize2x2x1_g1_tensor16x8x16 Tactic: 0xdb0b80f591d1bb6d
[08/10/2023-11:18:18] [V] [TRT] Tactic: 0xdb0b80f591d1bb6d Time: 0.29323
[08/10/2023-11:18:18] [V] [TRT] Conv_70 Set Tactic Name: ampere_h16816cudnn_256x128_ldg8_relu_exp_large_nhwc_tn_v1 Tactic: 0xdc796d70e228a1d4
[08/10/2023-11:18:18] [V] [TRT] Tactic: 0xdc796d70e228a1d4 Time: 0.305358
[08/10/2023-11:18:18] [V] [TRT] Conv_70 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x32x32_stage4_warpsize4x1x1_g1_tensor16x8x16 Tactic: 0xdce100b9fe609424
[08/10/2023-11:18:18] [V] [TRT] Tactic: 0xdce100b9fe609424 Time: 0.128743
[08/10/2023-11:18:18] [V] [TRT] Conv_70 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x128x32_stage4_warpsize2x2x1_g1_tensor16x8x16 Tactic: 0xdfa020ef435ef810
[08/10/2023-11:18:18] [V] [TRT] Tactic: 0xdfa020ef435ef810 Time: 0.247451
[08/10/2023-11:18:18] [V] [TRT] Conv_70 Set Tactic Name: ampere_h1688cudnn_256x64_ldg8_relu_exp_large_nhwc_tn_v1 Tactic: 0xe0e3c0e8cf9a2d9e
[08/10/2023-11:18:18] [V] [TRT] Tactic: 0xe0e3c0e8cf9a2d9e Time: 0.221691
[08/10/2023-11:18:18] [V] [TRT] Conv_70 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x32x64_stage5_warpsize2x2x1_g1_tensor16x8x16 Tactic: 0xe1ff5ad20f5c6bf6
[08/10/2023-11:18:18] [V] [TRT] Tactic: 0xe1ff5ad20f5c6bf6 Time: 0.187611
[08/10/2023-11:18:18] [V] [TRT] Conv_70 Set Tactic Name: ampere_h16816cudnn_256x64_ldg8_relu_exp_medium_nhwc_tn_v1 Tactic: 0xe4711898bd599c36
[08/10/2023-11:18:18] [V] [TRT] Tactic: 0xe4711898bd599c36 Time: 0.174158
[08/10/2023-11:18:18] [V] [TRT] Conv_70 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x64x64_stage3_warpsize4x1x1_g1_tensor16x8x16 Tactic: 0xeb25062ffb9eae45
[08/10/2023-11:18:18] [V] [TRT] Tactic: 0xeb25062ffb9eae45 Time: 0.192613
[08/10/2023-11:18:18] [V] [TRT] Conv_70 Set Tactic Name: ampere_h16816cudnn_128x64_sliced1x2_ldg8_relu_exp_stages_64x4_large_nhwc_tn_v1 Tactic: 0xeb2d2aa4e56bb41c
[08/10/2023-11:18:18] [V] [TRT] Tactic: 0xeb2d2aa4e56bb41c Time: 0.268009
[08/10/2023-11:18:18] [V] [TRT] Conv_70 Set Tactic Name: ampere_h16816cudnn_64x64_ldg8_relu_exp_stages_64x6_small_nhwc_tn_v1 Tactic: 0xf0beb09df9a19f82
[08/10/2023-11:18:18] [V] [TRT] Tactic: 0xf0beb09df9a19f82 Time: 0.358784
[08/10/2023-11:18:18] [V] [TRT] Conv_70 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x64x64_stage4_warpsize2x2x1_g1_tensor16x8x16 Tactic: 0xf35e0311fa1cc516
[08/10/2023-11:18:18] [V] [TRT] Tactic: 0xf35e0311fa1cc516 Time: 0.210903
[08/10/2023-11:18:18] [V] [TRT] Conv_70 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x32x32_stage4_warpsize4x1x1_g1_tensor16x8x16 Tactic: 0xf79479a62ea9f901
[08/10/2023-11:18:18] [V] [TRT] Tactic: 0xf79479a62ea9f901 Time: 0.125943
[08/10/2023-11:18:18] [V] [TRT] Fastest Tactic: 0x3e7eb35b91b9fa63 Time: 0.112363
[08/10/2023-11:18:18] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x3e7eb35b91b9fa63
[08/10/2023-11:18:18] [V] [TRT] *************** Autotuning format combination: Half(1036800,1:16,5760,32) -> Half(32400,1:16,180,1) ***************
[08/10/2023-11:18:18] [V] [TRT] --------------- Timing Runner: Conv_70 (CaskConvolution)
[08/10/2023-11:18:18] [V] [TRT] Conv_70 Set Tactic Name: ampere_h16816cudnn_256x128_ldg8_relu_exp_medium_nhwc_tn_v1 Tactic: 0x03896956a39a1203
[08/10/2023-11:18:18] [V] [TRT] Tactic: 0x03896956a39a1203 Time: 0.30512
[08/10/2023-11:18:18] [V] [TRT] Conv_70 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x128x32_stage3_warpsize4x2x1_g1_tensor16x8x16_t1r3s3 Tactic: 0x048d6d0400f33439
[08/10/2023-11:18:18] [V] [TRT] Tactic: 0x048d6d0400f33439 Time: 0.284087
[08/10/2023-11:18:18] [V] [TRT] Conv_70 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x64x64_stage3_warpsize2x2x1_g1_tensor16x8x16_t1r3s3 Tactic: 0x05b6220f243edacd
[08/10/2023-11:18:18] [V] [TRT] Tactic: 0x05b6220f243edacd Time: 0.152741
[08/10/2023-11:18:18] [V] [TRT] Conv_70 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x64x32_stage3_warpsize4x1x1_g1_tensor16x8x16_t1r3s3_aACCESS Tactic: 0x0866ddee325d07a6
[08/10/2023-11:18:18] [V] [TRT] Tactic: 0x0866ddee325d07a6 Time: 0.160777
[08/10/2023-11:18:18] [V] [TRT] Conv_70 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x128x32_stage4_warpsize2x2x1_g1_tensor16x8x16 Tactic: 0x0e07ff4f4f7c1ac9
[08/10/2023-11:18:18] [V] [TRT] Tactic: 0x0e07ff4f4f7c1ac9 Time: 0.270199
[08/10/2023-11:18:18] [V] [TRT] Conv_70 Set Tactic Name: ampere_h16816cudnn_256x128_ldg8_relu_exp_stages_64x3_small_nhwc_tn_v1 Tactic: 0x0e199750c30c6928
[08/10/2023-11:18:18] [V] [TRT] Tactic: 0x0e199750c30c6928 Time: 0.302939
[08/10/2023-11:18:18] [V] [TRT] Conv_70 Set Tactic Name: ampere_h1688cudnn_128x128_ldg8_relu_exp_medium_nhwc_tn_v1 Tactic: 0x100c1ad308e08d35
[08/10/2023-11:18:18] [V] [TRT] Tactic: 0x100c1ad308e08d35 Time: 0.347319
[08/10/2023-11:18:18] [V] [TRT] Conv_70 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x128x32_stage5_warpsize2x2x1_g1_tensor16x8x16_t1r3s3 Tactic: 0x17ebf0c9f418f10a
[08/10/2023-11:18:18] [V] [TRT] Tactic: 0x17ebf0c9f418f10a Time: 0.286839
[08/10/2023-11:18:18] [V] [TRT] Conv_70 Set Tactic Name: ampere_h16816cudnn_128x64_ldg8_relu_exp_stages_64x4_medium_nhwc_tn_v1 Tactic: 0x196cbc423a9bb69e
[08/10/2023-11:18:18] [V] [TRT] Tactic: 0x196cbc423a9bb69e Time: 0.248544
[08/10/2023-11:18:18] [V] [TRT] Conv_70 Set Tactic Name: ampere_h16816cudnn_128x64_sliced1x2_ldg8_relu_exp_stages_64x3_small_nhwc_tn_v1 Tactic: 0x19822de884f42a6a
[08/10/2023-11:18:18] [V] [TRT] Tactic: 0x19822de884f42a6a Time: 0.164425
[08/10/2023-11:18:18] [V] [TRT] Conv_70 Set Tactic Name: ampere_h16816cudnn_64x64_sliced1x2_ldg8_relu_exp_stages_64x5_small_nhwc_tn_v1 Tactic: 0x1a5ba808ad4cc5a8
[08/10/2023-11:18:18] [V] [TRT] Tactic: 0x1a5ba808ad4cc5a8 Time: 0.252256
[08/10/2023-11:18:18] [V] [TRT] Conv_70 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x64x64_stage3_warpsize2x2x1_g1_tensor16x8x16 Tactic: 0x21b295c0c8f6c95a
[08/10/2023-11:18:18] [V] [TRT] Tactic: 0x21b295c0c8f6c95a Time: 0.172183
[08/10/2023-11:18:18] [V] [TRT] Conv_70 Set Tactic Name: ampere_h16816cudnn_128x128_ldg8_relu_exp_medium_nhwc_tn_v1 Tactic: 0x234580e8a194335c
[08/10/2023-11:18:18] [V] [TRT] Tactic: 0x234580e8a194335c Time: 0.252503
[08/10/2023-11:18:18] [V] [TRT] Conv_70 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x128x32_stage4_warpsize2x2x1_g1_tensor16x8x16_t1r3s3_aACCESS Tactic: 0x245530c34bd6090f
[08/10/2023-11:18:18] [V] [TRT] Tactic: 0x245530c34bd6090f Time: 0.258318
[08/10/2023-11:18:18] [V] [TRT] Conv_70 Set Tactic Name: ampere_h16816cudnn_64x64_sliced1x2_ldg8_relu_exp_stages_64x6_small_nhwc_tn_v1 Tactic: 0x24e01e7405cfdfe9
[08/10/2023-11:18:18] [V] [TRT] Tactic: 0x24e01e7405cfdfe9 Time: 0.35957
[08/10/2023-11:18:18] [V] [TRT] Conv_70 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x64x64_stage3_warpsize2x2x1_g1_tensor16x8x16_t1r3s3_aACCESS Tactic: 0x263a38afd75e3a43
[08/10/2023-11:18:18] [V] [TRT] Tactic: 0x263a38afd75e3a43 Time: 0.168613
[08/10/2023-11:18:18] [V] [TRT] Conv_70 Set Tactic Name: ampere_h16816cudnn_64x128_sliced1x2_ldg8_relu_exp_stages_64x4_large_nhwc_tn_v1 Tactic: 0x2721a7f18c2700db
[08/10/2023-11:18:18] [V] [TRT] Tactic: 0x2721a7f18c2700db Time: 0.50053
[08/10/2023-11:18:18] [V] [TRT] Conv_70 Set Tactic Name: ampere_h16816cudnn_128x64_sliced1x2_ldg8_relu_exp_stages_64x3_medium_nhwc_tn_v1 Tactic: 0x29329b5741ea05f2
[08/10/2023-11:18:18] [V] [TRT] Tactic: 0x29329b5741ea05f2 Time: 0.170149
[08/10/2023-11:18:18] [V] [TRT] Conv_70 Set Tactic Name: ampere_h16816cudnn_128x128_ldg8_relu_exp_large_nhwc_tn_v1 Tactic: 0x2970ae65966d569d
[08/10/2023-11:18:18] [V] [TRT] Tactic: 0x2970ae65966d569d Time: 0.258039
[08/10/2023-11:18:18] [V] [TRT] Conv_70 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x256x64_stage3_warpsize1x4x1_g1_tensor16x8x16 Tactic: 0x29dc14520e0e4eb6
[08/10/2023-11:18:18] [V] [TRT] Tactic: 0x29dc14520e0e4eb6 Time: 0.638062
[08/10/2023-11:18:18] [V] [TRT] Conv_70 Set Tactic Name: ampere_h16816cudnn_256x64_ldg8_relu_exp_stages_64x3_large_nhwc_tn_v1 Tactic: 0x2cb4a662694e89d3
[08/10/2023-11:18:18] [V] [TRT] Tactic: 0x2cb4a662694e89d3 Time: 0.197838
[08/10/2023-11:18:18] [V] [TRT] Conv_70 Set Tactic Name: ampere_h16816cudnn_128x128_ldg8_relu_exp_stages_64x3_medium_nhwc_tn_v1 Tactic: 0x2eaa2202de9404d6
[08/10/2023-11:18:18] [V] [TRT] Tactic: 0x2eaa2202de9404d6 Time: 0.345641
[08/10/2023-11:18:18] [V] [TRT] Conv_70 Set Tactic Name: ampere_h1688cudnn_256x128_ldg8_relu_exp_medium_nhwc_tn_v1 Tactic: 0x30c0f36d0aeeac6a
[08/10/2023-11:18:18] [V] [TRT] Tactic: 0x30c0f36d0aeeac6a Time: 0.365984
[08/10/2023-11:18:18] [V] [TRT] Conv_70 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x64x64_stage4_warpsize2x2x1_g1_tensor16x8x16_t1r3s3_aACCESS Tactic: 0x30e8a8d7a953e5e9
[08/10/2023-11:18:18] [V] [TRT] Tactic: 0x30e8a8d7a953e5e9 Time: 0.19893
[08/10/2023-11:18:18] [V] [TRT] Conv_70 Set Tactic Name: ampere_h16816cudnn_128x64_ldg8_relu_exp_stages_64x4_large_nhwc_tn_v1 Tactic: 0x3197d5044d71e98e
[08/10/2023-11:18:18] [V] [TRT] Tactic: 0x3197d5044d71e98e Time: 0.256087
[08/10/2023-11:18:18] [V] [TRT] Conv_70 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x64x64_stage4_warpsize2x2x1_g1_tensor16x8x16_t1r3s3 Tactic: 0x31d93dc22d2af081
[08/10/2023-11:18:18] [V] [TRT] Tactic: 0x31d93dc22d2af081 Time: 0.195186
[08/10/2023-11:18:18] [V] [TRT] Conv_70 Set Tactic Name: ampere_h1688cudnn_128x128_ldg8_relu_exp_small_nhwc_tn_v1 Tactic: 0x3369260c04f9ad73
[08/10/2023-11:18:18] [V] [TRT] Tactic: 0x3369260c04f9ad73 Time: 0.332425
[08/10/2023-11:18:18] [V] [TRT] Conv_70 Set Tactic Name: ampere_h16816cudnn_64x128_ldg8_relu_exp_stages_64x4_small_nhwc_tn_v1 Tactic: 0x3b5fa0f2a8fc2410
[08/10/2023-11:18:18] [V] [TRT] Tactic: 0x3b5fa0f2a8fc2410 Time: 0.429303
[08/10/2023-11:18:18] [V] [TRT] Conv_70 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x32x32_stage4_warpsize4x1x1_g1_tensor16x8x16_t1r3s3 Tactic: 0x3e7eb35b91b9fa63
[08/10/2023-11:18:18] [V] [TRT] Tactic: 0x3e7eb35b91b9fa63 Time: 0.119451
[08/10/2023-11:18:18] [V] [TRT] Conv_70 Set Tactic Name: ampere_h16816cudnn_256x64_ldg8_relu_exp_stages_64x3_medium_nhwc_tn_v1 Tactic: 0x3f031df0e579d52c
[08/10/2023-11:18:18] [V] [TRT] Tactic: 0x3f031df0e579d52c Time: 0.19712
[08/10/2023-11:18:19] [V] [TRT] Conv_70 Set Tactic Name: ampere_h16816cudnn_256x64_sliced1x2_ldg8_relu_exp_small_nhwc_tn_v1 Tactic: 0x42a5b8709d0039be
[08/10/2023-11:18:19] [V] [TRT] Tactic: 0x42a5b8709d0039be Time: 0.214642
[08/10/2023-11:18:19] [V] [TRT] Conv_70 Set Tactic Name: ampere_h16816cudnn_256x128_ldg8_relu_exp_small_nhwc_tn_v1 Tactic: 0x463794ee4acffd1f
[08/10/2023-11:18:19] [V] [TRT] Tactic: 0x463794ee4acffd1f Time: 0.299113
[08/10/2023-11:18:19] [V] [TRT] Conv_70 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x64x64_stage4_warpsize2x2x1_g1_tensor16x8x16 Tactic: 0x4a81ea1e51436a30
[08/10/2023-11:18:19] [V] [TRT] Tactic: 0x4a81ea1e51436a30 Time: 0.217403
[08/10/2023-11:18:19] [V] [TRT] Conv_70 Set Tactic Name: ampere_h1688cudnn_256x64_sliced1x2_ldg8_relu_exp_small_nhwc_tn_v1 Tactic: 0x4aed1956bd10a795
[08/10/2023-11:18:19] [V] [TRT] Tactic: 0x4aed1956bd10a795 Time: 0.24336
[08/10/2023-11:18:19] [V] [TRT] Conv_70 Set Tactic Name: ampere_h16816cudnn_64x128_sliced1x2_ldg8_relu_exp_stages_64x3_large_nhwc_tn_v1 Tactic: 0x4fc05923455fc266
[08/10/2023-11:18:19] [V] [TRT] Tactic: 0x4fc05923455fc266 Time: 0.305134
[08/10/2023-11:18:19] [V] [TRT] Conv_70 Set Tactic Name: ampere_h16816cudnn_256x64_ldg8_relu_exp_large_nhwc_tn_v1 Tactic: 0x5013c38f55afa9ba
[08/10/2023-11:18:19] [V] [TRT] Tactic: 0x5013c38f55afa9ba Time: 0.180773
[08/10/2023-11:18:19] [V] [TRT] Conv_70 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x64x32_stage3_warpsize4x1x1_g1_tensor16x8x16_t1r3s3 Tactic: 0x529f4431bdae94f5
[08/10/2023-11:18:19] [V] [TRT] Tactic: 0x529f4431bdae94f5 Time: 0.147639
[08/10/2023-11:18:19] [V] [TRT] Conv_70 Set Tactic Name: ampere_h16816cudnn_64x128_ldg8_relu_exp_stages_64x3_small_nhwc_tn_v1 Tactic: 0x53be5e206184e6ad
[08/10/2023-11:18:19] [V] [TRT] Tactic: 0x53be5e206184e6ad Time: 0.299689
[08/10/2023-11:18:19] [V] [TRT] Conv_70 Set Tactic Name: ampere_h16816cudnn_64x64_ldg8_relu_exp_stages_64x5_large_nhwc_tn_v1 Tactic: 0x544bff7ff9c5d908
[08/10/2023-11:18:19] [V] [TRT] Tactic: 0x544bff7ff9c5d908 Time: 0.262903
[08/10/2023-11:18:19] [V] [TRT] Conv_70 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x64x64_stage3_warpsize2x2x1_g1_tensor16x8x16 Tactic: 0x5568fd8a32f4a40f
[08/10/2023-11:18:19] [V] [TRT] Tactic: 0x5568fd8a32f4a40f Time: 0.155506
[08/10/2023-11:18:19] [V] [TRT] Conv_70 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x128x64_stage3_warpsize2x2x1_g1_tensor16x8x16 Tactic: 0x57c9a5ff682354a6
[08/10/2023-11:18:19] [V] [TRT] Tactic: 0x57c9a5ff682354a6 Time: 0.326665
[08/10/2023-11:18:19] [V] [TRT] Conv_70 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x32x64_stage5_warpsize2x2x1_g1_tensor16x8x16 Tactic: 0x5820b3dda403c4d0
[08/10/2023-11:18:19] [V] [TRT] Tactic: 0x5820b3dda403c4d0 Time: 0.181038
[08/10/2023-11:18:19] [V] [TRT] Conv_70 Set Tactic Name: ampere_h16816cudnn_128x128_ldg8_relu_exp_stages_64x3_small_nhwc_tn_v1 Tactic: 0x58816bf2e9c36afb
[08/10/2023-11:18:19] [V] [TRT] Tactic: 0x58816bf2e9c36afb Time: 0.342354
[08/10/2023-11:18:19] [V] [TRT] Conv_70 Set Tactic Name: ampere_h16816cudnn_128x64_ldg8_relu_exp_stages_64x3_large_nhwc_tn_v1 Tactic: 0x59762bd684092b33
[08/10/2023-11:18:19] [V] [TRT] Tactic: 0x59762bd684092b33 Time: 0.180978
[08/10/2023-11:18:19] [V] [TRT] Conv_70 Set Tactic Name: ampere_h16816cudnn_128x64_sliced1x2_ldg8_relu_exp_stages_64x4_medium_nhwc_tn_v1 Tactic: 0x59c5c647b4c76593
[08/10/2023-11:18:19] [V] [TRT] Tactic: 0x59c5c647b4c76593 Time: 0.255593
[08/10/2023-11:18:19] [V] [TRT] Conv_70 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x128x64_stage3_warpsize4x2x1_g1_tensor16x8x16 Tactic: 0x5a55274960724ba8
[08/10/2023-11:18:19] [V] [TRT] Tactic: 0x5a55274960724ba8 Time: 0.297499
[08/10/2023-11:18:19] [V] [TRT] Conv_70 Set Tactic Name: ampere_h1688cudnn_256x128_ldg8_relu_exp_large_nhwc_tn_v1 Tactic: 0x5c2e1c87d85b06f1
[08/10/2023-11:18:19] [V] [TRT] Tactic: 0x5c2e1c87d85b06f1 Time: 0.368864
[08/10/2023-11:18:19] [V] [TRT] Conv_70 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x256x64_stage3_warpsize1x4x1_g1_tensor16x8x16 Tactic: 0x5d067c18f40c23e3
[08/10/2023-11:18:19] [V] [TRT] Tactic: 0x5d067c18f40c23e3 Time: 0.623758
[08/10/2023-11:18:19] [V] [TRT] Conv_70 Set Tactic Name: ampere_h16816cudnn_64x64_ldg8_relu_exp_stages_64x5_medium_nhwc_tn_v1 Tactic: 0x5f31c22ec167f384
[08/10/2023-11:18:19] [V] [TRT] Tactic: 0x5f31c22ec167f384 Time: 0.261911
[08/10/2023-11:18:19] [V] [TRT] Conv_70 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x128x32_stage4_warpsize2x2x1_g1_tensor16x8x16_t1r3s3 Tactic: 0x60c3421152ef8e10
[08/10/2023-11:18:19] [V] [TRT] Tactic: 0x60c3421152ef8e10 Time: 0.248238
[08/10/2023-11:18:19] [V] [TRT] Conv_70 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x128x32_stage3_warpsize4x2x1_g1_tensor16x8x16 Tactic: 0x60da8c7151d91e47
[08/10/2023-11:18:19] [V] [TRT] Tactic: 0x60da8c7151d91e47 Time: 0.283945
[08/10/2023-11:18:19] [V] [TRT] Conv_70 Set Tactic Name: ampere_h1688cudnn_256x64_ldg8_relu_exp_medium_nhwc_tn_v1 Tactic: 0x6426696f872a3b13
[08/10/2023-11:18:19] [V] [TRT] Tactic: 0x6426696f872a3b13 Time: 0.226693
[08/10/2023-11:18:19] [V] [TRT] Conv_70 Set Tactic Name: ampere_h16816cudnn_128x64_ldg8_relu_exp_stages_64x3_medium_nhwc_tn_v1 Tactic: 0x699be152cfb6d6ff
[08/10/2023-11:18:19] [V] [TRT] Tactic: 0x699be152cfb6d6ff Time: 0.182866
[08/10/2023-11:18:19] [V] [TRT] Conv_70 Set Tactic Name: ampere_h16816cudnn_64x64_ldg8_relu_exp_stages_64x6_large_nhwc_tn_v1 Tactic: 0x6af049035146c349
[08/10/2023-11:18:19] [V] [TRT] Tactic: 0x6af049035146c349 Time: 0.380489
[08/10/2023-11:18:19] [V] [TRT] Conv_70 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x256x32_stage3_warpsize2x4x1_g1_tensor16x8x16 Tactic: 0x7005d10718f6c22d
[08/10/2023-11:18:19] [V] [TRT] Tactic: 0x7005d10718f6c22d Time: 0.523794
[08/10/2023-11:18:19] [V] [TRT] Conv_70 Set Tactic Name: ampere_h16816cudnn_64x64_sliced1x2_ldg8_relu_exp_stages_64x5_medium_nhwc_tn_v1 Tactic: 0x706f08da35c795a5
[08/10/2023-11:18:19] [V] [TRT] Tactic: 0x706f08da35c795a5 Time: 0.255803
[08/10/2023-11:18:19] [V] [TRT] Conv_70 Set Tactic Name: ampere_h16816cudnn_128x64_sliced1x2_ldg8_relu_exp_stages_64x4_small_nhwc_tn_v1 Tactic: 0x7163d33a4d8ce8d7
[08/10/2023-11:18:19] [V] [TRT] Tactic: 0x7163d33a4d8ce8d7 Time: 0.248626
[08/10/2023-11:18:19] [V] [TRT] Conv_70 Set Tactic Name: ampere_h1688cudnn_256x64_ldg8_relu_exp_small_nhwc_tn_v1 Tactic: 0x7aad3976677d7155
[08/10/2023-11:18:19] [V] [TRT] Tactic: 0x7aad3976677d7155 Time: 0.216791
[08/10/2023-11:18:19] [V] [TRT] Conv_70 Set Tactic Name: ampere_h16816cudnn_64x64_sliced1x2_ldg8_relu_exp_stages_64x5_large_nhwc_tn_v1 Tactic: 0x8015519605ab9963
[08/10/2023-11:18:19] [V] [TRT] Tactic: 0x8015519605ab9963 Time: 0.260242
[08/10/2023-11:18:19] [V] [TRT] Conv_70 Set Tactic Name: ampere_h16816cudnn_256x64_sliced1x2_ldg8_relu_exp_medium_nhwc_tn_v1 Tactic: 0x82f8a2214b0b4178
[08/10/2023-11:18:19] [V] [TRT] Tactic: 0x82f8a2214b0b4178 Time: 0.240571
[08/10/2023-11:18:19] [V] [TRT] Conv_70 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x32x32_stage4_warpsize4x1x1_g1_tensor16x8x16 Tactic: 0x834e11ecd4ab9454
[08/10/2023-11:18:19] [V] [TRT] Tactic: 0x834e11ecd4ab9454 Time: 0.122226
[08/10/2023-11:18:19] [V] [TRT] Conv_70 Set Tactic Name: ampere_h16816cudnn_128x64_sliced1x2_ldg8_relu_exp_stages_64x3_large_nhwc_tn_v1 Tactic: 0x83ccd4762c1376a1
[08/10/2023-11:18:19] [V] [TRT] Tactic: 0x83ccd4762c1376a1 Time: 0.169792
[08/10/2023-11:18:19] [V] [TRT] Conv_70 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x64x64_stage3_warpsize4x1x1_g1_tensor16x8x16_t1r3s3 Tactic: 0x84ee897dd1f4d2aa
[08/10/2023-11:18:19] [V] [TRT] Tactic: 0x84ee897dd1f4d2aa Time: 0.186158
[08/10/2023-11:18:19] [V] [TRT] Conv_70 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x128x64_stage3_warpsize2x2x1_g1_tensor16x8x16 Tactic: 0x866e7a5f6401b67f
[08/10/2023-11:18:19] [V] [TRT] Tactic: 0x866e7a5f6401b67f Time: 0.326473
[08/10/2023-11:18:19] [V] [TRT] Conv_70 Set Tactic Name: ampere_h16816cudnn_64x64_ldg8_relu_exp_stages_64x6_medium_nhwc_tn_v1 Tactic: 0x88971eec55aba850
[08/10/2023-11:18:19] [V] [TRT] Tactic: 0x88971eec55aba850 Time: 0.373079
[08/10/2023-11:18:19] [V] [TRT] Conv_70 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x128x64_stage3_warpsize4x2x1_g1_tensor16x8x16 Tactic: 0x8bf2f8e96c50a971
[08/10/2023-11:18:19] [V] [TRT] Tactic: 0x8bf2f8e96c50a971 Time: 0.290487
[08/10/2023-11:18:19] [V] [TRT] Conv_70 Set Tactic Name: ampere_h16816cudnn_256x128_ldg8_relu_exp_stages_64x3_large_nhwc_tn_v1 Tactic: 0x94576ece6beb35e3
[08/10/2023-11:18:19] [V] [TRT] Tactic: 0x94576ece6beb35e3 Time: 0.306121
[08/10/2023-11:18:19] [V] [TRT] Conv_70 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x64x64_stage3_warpsize4x1x1_g1_tensor16x8x16 Tactic: 0x9fff6e65019cc310
[08/10/2023-11:18:19] [V] [TRT] Tactic: 0x9fff6e65019cc310 Time: 0.184256
[08/10/2023-11:18:19] [V] [TRT] Conv_70 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x32x64_stage5_warpsize2x2x1_g1_tensor16x8x16_t1r3s3 Tactic: 0xa033e20ae9f412b2
[08/10/2023-11:18:19] [V] [TRT] Tactic: 0xa033e20ae9f412b2 Time: 0.164471
[08/10/2023-11:18:19] [V] [TRT] Conv_70 Set Tactic Name: ampere_h16816cudnn_64x128_ldg8_relu_exp_stages_64x4_large_nhwc_tn_v1 Tactic: 0xa111596c001b78db
[08/10/2023-11:18:19] [V] [TRT] Tactic: 0xa111596c001b78db Time: 0.441385
[08/10/2023-11:18:19] [V] [TRT] Conv_70 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x256x32_stage3_warpsize2x4x1_g1_tensor16x8x16 Tactic: 0xa1a20ea714d420f4
[08/10/2023-11:18:19] [V] [TRT] Tactic: 0xa1a20ea714d420f4 Time: 0.519209
[08/10/2023-11:18:19] [V] [TRT] Conv_70 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x32x32_stage4_warpsize4x1x1_g1_tensor16x8x16_t1r3s3 Tactic: 0xa40cb43c296a36a8
[08/10/2023-11:18:19] [V] [TRT] Tactic: 0xa40cb43c296a36a8 Time: 0.123387
[08/10/2023-11:18:19] [V] [TRT] Conv_70 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x64x32_stage3_warpsize4x1x1_g1_tensor16x8x16 Tactic: 0xa570c55d303796ff
[08/10/2023-11:18:19] [V] [TRT] Tactic: 0xa570c55d303796ff Time: 0.155671
[08/10/2023-11:18:19] [V] [TRT] Conv_70 Set Tactic Name: ampere_h16816cudnn_64x64_sliced1x2_ldg8_relu_exp_stages_64x6_medium_nhwc_tn_v1 Tactic: 0xa7c9d418a10bce71
[08/10/2023-11:18:19] [V] [TRT] Tactic: 0xa7c9d418a10bce71 Time: 0.390629
[08/10/2023-11:18:19] [V] [TRT] Conv_70 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x32x32_stage4_warpsize4x1x1_g1_tensor16x8x16 Tactic: 0xa83b68f30462f971
[08/10/2023-11:18:19] [V] [TRT] Tactic: 0xa83b68f30462f971 Time: 0.13229
[08/10/2023-11:18:19] [V] [TRT] Conv_70 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x256x32_stage3_warpsize2x4x1_g1_tensor16x8x16_t1r3s3 Tactic: 0xa9177bbe4e767df8
[08/10/2023-11:18:19] [V] [TRT] Tactic: 0xa9177bbe4e767df8 Time: 0.530674
[08/10/2023-11:18:19] [V] [TRT] Conv_70 Set Tactic Name: ampere_h1688cudnn_128x128_ldg8_relu_exp_large_nhwc_tn_v1 Tactic: 0xa927df92ac1ef1b8
[08/10/2023-11:18:19] [V] [TRT] Tactic: 0xa927df92ac1ef1b8 Time: 0.357211
[08/10/2023-11:18:19] [V] [TRT] Conv_70 Set Tactic Name: ampere_h16816cudnn_128x64_ldg8_relu_exp_stages_64x4_small_nhwc_tn_v1 Tactic: 0xabd92c9ae596b545
[08/10/2023-11:18:19] [V] [TRT] Tactic: 0xabd92c9ae596b545 Time: 0.250432
[08/10/2023-11:18:19] [V] [TRT] Conv_70 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x128x32_stage5_warpsize2x2x1_g1_tensor16x8x16 Tactic: 0xafd1e8bf6bd3d638
[08/10/2023-11:18:19] [V] [TRT] Tactic: 0xafd1e8bf6bd3d638 Time: 0.305125
[08/10/2023-11:18:19] [V] [TRT] Conv_70 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x128x32_stage3_warpsize4x2x1_g1_tensor16x8x16 Tactic: 0xb17d53d15dfbfc9e
[08/10/2023-11:18:19] [V] [TRT] Tactic: 0xb17d53d15dfbfc9e Time: 0.290848
[08/10/2023-11:18:19] [V] [TRT] Conv_70 Set Tactic Name: ampere_h16816cudnn_128x128_ldg8_relu_exp_small_nhwc_tn_v1 Tactic: 0xb33e57fb3e8a0a56
[08/10/2023-11:18:19] [V] [TRT] Tactic: 0xb33e57fb3e8a0a56 Time: 0.249733
[08/10/2023-11:18:19] [V] [TRT] Conv_70 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x64x32_stage5_warpsize2x2x1_g1_tensor16x8x16_t1r3s3 Tactic: 0xb4bec086187edcfc
[08/10/2023-11:18:19] [V] [TRT] Tactic: 0xb4bec086187edcfc Time: 0.163941
[08/10/2023-11:18:19] [V] [TRT] Conv_70 Set Tactic Name: ampere_h16816cudnn_256x64_ldg8_relu_exp_stages_64x3_small_nhwc_tn_v1 Tactic: 0xb6fa5ffcc1a9d518
[08/10/2023-11:18:19] [V] [TRT] Tactic: 0xb6fa5ffcc1a9d518 Time: 0.192302
[08/10/2023-11:18:19] [V] [TRT] Conv_70 Set Tactic Name: ampere_h16816cudnn_64x128_sliced1x2_ldg8_relu_exp_stages_64x3_medium_nhwc_tn_v1 Tactic: 0xb85e52e87caf60a2
[08/10/2023-11:18:19] [V] [TRT] Tactic: 0xb85e52e87caf60a2 Time: 0.294889
[08/10/2023-11:18:19] [V] [TRT] Conv_70 Set Tactic Name: ampere_h16816cudnn_64x128_ldg8_relu_exp_stages_64x3_medium_nhwc_tn_v1 Tactic: 0xb8d86216e1235cda
[08/10/2023-11:18:19] [V] [TRT] Tactic: 0xb8d86216e1235cda Time: 0.293472
[08/10/2023-11:18:19] [V] [TRT] Conv_70 Set Tactic Name: ampere_h16816cudnn_256x128_ldg8_relu_exp_stages_64x3_medium_nhwc_tn_v1 Tactic: 0xb9171d70ba2eda4b
[08/10/2023-11:18:19] [V] [TRT] Tactic: 0xb9171d70ba2eda4b Time: 0.305486
[08/10/2023-11:18:19] [V] [TRT] Conv_70 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x64x32_stage5_warpsize2x2x1_g1_tensor16x8x16 Tactic: 0xbd08239a9317f2fd
[08/10/2023-11:18:19] [V] [TRT] Tactic: 0xbd08239a9317f2fd Time: 0.16971
[08/10/2023-11:18:19] [V] [TRT] Conv_70 Set Tactic Name: ampere_h16816cudnn_64x128_sliced1x2_ldg8_relu_exp_stages_64x4_small_nhwc_tn_v1 Tactic: 0xbd6f5e6f24c05c10
[08/10/2023-11:18:19] [V] [TRT] Tactic: 0xbd6f5e6f24c05c10 Time: 0.470258
[08/10/2023-11:18:19] [V] [TRT] Conv_70 Set Tactic Name: ampere_h16816cudnn_64x64_sliced1x2_ldg8_relu_exp_stages_64x6_large_nhwc_tn_v1 Tactic: 0xbeaee7eaad288322
[08/10/2023-11:18:19] [V] [TRT] Tactic: 0xbeaee7eaad288322 Time: 0.393582
[08/10/2023-11:18:19] [V] [TRT] Conv_70 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x128x64_stage3_warpsize2x2x1_g1_tensor16x8x16_t1r3s3 Tactic: 0xc0a02dc6095497cc
[08/10/2023-11:18:19] [V] [TRT] Tactic: 0xc0a02dc6095497cc Time: 0.319776
[08/10/2023-11:18:19] [V] [TRT] Conv_70 Set Tactic Name: ampere_h16816cudnn_128x128_ldg8_relu_exp_stages_64x3_large_nhwc_tn_v1 Tactic: 0xc2cf926c41243630
[08/10/2023-11:18:19] [V] [TRT] Tactic: 0xc2cf926c41243630 Time: 0.344645
[08/10/2023-11:18:19] [V] [TRT] Conv_70 Set Tactic Name: ampere_h16816cudnn_128x64_ldg8_relu_exp_stages_64x3_small_nhwc_tn_v1 Tactic: 0xc338d2482cee77f8
[08/10/2023-11:18:19] [V] [TRT] Tactic: 0xc338d2482cee77f8 Time: 0.17947
[08/10/2023-11:18:19] [V] [TRT] Conv_70 Set Tactic Name: ampere_h1688cudnn_256x128_ldg8_relu_exp_small_nhwc_tn_v1 Tactic: 0xc660e51970bc5a3a
[08/10/2023-11:18:19] [V] [TRT] Tactic: 0xc660e51970bc5a3a Time: 0.342725
[08/10/2023-11:18:19] [V] [TRT] Conv_70 Set Tactic Name: ampere_h16816cudnn_64x128_ldg8_relu_exp_stages_64x4_medium_nhwc_tn_v1 Tactic: 0xc82f3f06140e3cbb
[08/10/2023-11:18:19] [V] [TRT] Tactic: 0xc82f3f06140e3cbb Time: 0.435685
[08/10/2023-11:18:19] [V] [TRT] Conv_70 Set Tactic Name: ampere_h16816cudnn_64x128_sliced1x2_ldg8_relu_exp_stages_64x4_medium_nhwc_tn_v1 Tactic: 0xc8a90ff8898200c3
[08/10/2023-11:18:19] [V] [TRT] Tactic: 0xc8a90ff8898200c3 Time: 0.471058
[08/10/2023-11:18:19] [V] [TRT] Conv_70 Set Tactic Name: ampere_h1688cudnn_256x64_sliced1x2_ldg8_relu_exp_medium_nhwc_tn_v1 Tactic: 0xc9cc55109bb4de26
[08/10/2023-11:18:19] [V] [TRT] Tactic: 0xc9cc55109bb4de26 Time: 0.254043
[08/10/2023-11:18:19] [V] [TRT] Conv_70 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x64x32_stage5_warpsize2x2x1_g1_tensor16x8x16 Tactic: 0xc9d24bd069159fa8
[08/10/2023-11:18:19] [V] [TRT] Tactic: 0xc9d24bd069159fa8 Time: 0.184183
[08/10/2023-11:18:19] [V] [TRT] Conv_70 Set Tactic Name: ampere_h16816cudnn_64x128_ldg8_relu_exp_stages_64x3_large_nhwc_tn_v1 Tactic: 0xc9f0a7bec963ba66
[08/10/2023-11:18:19] [V] [TRT] Tactic: 0xc9f0a7bec963ba66 Time: 0.302002
[08/10/2023-11:18:19] [V] [TRT] Conv_70 Set Tactic Name: ampere_h16816cudnn_256x64_ldg8_relu_exp_small_nhwc_tn_v1 Tactic: 0xca5d3a11fd48f571
[08/10/2023-11:18:19] [V] [TRT] Tactic: 0xca5d3a11fd48f571 Time: 0.170112
[08/10/2023-11:18:19] [V] [TRT] Conv_70 Set Tactic Name: ampere_h16816cudnn_64x64_ldg8_relu_exp_stages_64x5_small_nhwc_tn_v1 Tactic: 0xce0506e1512285c3
[08/10/2023-11:18:19] [V] [TRT] Tactic: 0xce0506e1512285c3 Time: 0.268448
[08/10/2023-11:18:19] [V] [TRT] Conv_70 Set Tactic Name: ampere_h1688cudnn_256x64_sliced1x2_ldg8_relu_exp_large_nhwc_tn_v1 Tactic: 0xd0a3e0c815f7fb5e
[08/10/2023-11:18:19] [V] [TRT] Tactic: 0xd0a3e0c815f7fb5e Time: 0.259237
[08/10/2023-11:18:19] [V] [TRT] Conv_70 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x64x32_stage3_warpsize4x1x1_g1_tensor16x8x16 Tactic: 0xd1aaad17ca35fbaa
[08/10/2023-11:18:19] [V] [TRT] Tactic: 0xd1aaad17ca35fbaa Time: 0.148037
[08/10/2023-11:18:19] [V] [TRT] Conv_70 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x128x64_stage3_warpsize4x2x1_g1_tensor16x8x16_t1r3s3 Tactic: 0xd2fca0486ca97266
[08/10/2023-11:18:19] [V] [TRT] Tactic: 0xd2fca0486ca97266 Time: 0.29595
[08/10/2023-11:18:19] [V] [TRT] Conv_70 Set Tactic Name: ampere_h16816cudnn_64x128_sliced1x2_ldg8_relu_exp_stages_64x3_small_nhwc_tn_v1 Tactic: 0xd58ea0bdedb89ead
[08/10/2023-11:18:19] [V] [TRT] Tactic: 0xd58ea0bdedb89ead Time: 0.292379
[08/10/2023-11:18:19] [V] [TRT] Conv_70 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x256x64_stage3_warpsize1x4x1_g1_tensor16x8x16_t1r3s3 Tactic: 0xd7c261fa01db2af9
[08/10/2023-11:18:19] [V] [TRT] Tactic: 0xd7c261fa01db2af9 Time: 0.611547
[08/10/2023-11:18:19] [V] [TRT] Conv_70 Set Tactic Name: ampere_h16816cudnn_256x64_sliced1x2_ldg8_relu_exp_large_nhwc_tn_v1 Tactic: 0xd8eb41ee35e76575
[08/10/2023-11:18:19] [V] [TRT] Tactic: 0xd8eb41ee35e76575 Time: 0.238633
[08/10/2023-11:18:19] [V] [TRT] Conv_70 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x128x32_stage5_warpsize2x2x1_g1_tensor16x8x16 Tactic: 0xdb0b80f591d1bb6d
[08/10/2023-11:18:19] [V] [TRT] Tactic: 0xdb0b80f591d1bb6d Time: 0.29253
[08/10/2023-11:18:19] [V] [TRT] Conv_70 Set Tactic Name: ampere_h16816cudnn_256x128_ldg8_relu_exp_large_nhwc_tn_v1 Tactic: 0xdc796d70e228a1d4
[08/10/2023-11:18:19] [V] [TRT] Tactic: 0xdc796d70e228a1d4 Time: 0.304905
[08/10/2023-11:18:19] [V] [TRT] Conv_70 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x32x32_stage4_warpsize4x1x1_g1_tensor16x8x16 Tactic: 0xdce100b9fe609424
[08/10/2023-11:18:19] [V] [TRT] Tactic: 0xdce100b9fe609424 Time: 0.13149
[08/10/2023-11:18:19] [V] [TRT] Conv_70 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x128x32_stage4_warpsize2x2x1_g1_tensor16x8x16 Tactic: 0xdfa020ef435ef810
[08/10/2023-11:18:19] [V] [TRT] Tactic: 0xdfa020ef435ef810 Time: 0.24859
[08/10/2023-11:18:19] [V] [TRT] Conv_70 Set Tactic Name: ampere_h1688cudnn_256x64_ldg8_relu_exp_large_nhwc_tn_v1 Tactic: 0xe0e3c0e8cf9a2d9e
[08/10/2023-11:18:19] [V] [TRT] Tactic: 0xe0e3c0e8cf9a2d9e Time: 0.225125
[08/10/2023-11:18:19] [V] [TRT] Conv_70 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x32x64_stage5_warpsize2x2x1_g1_tensor16x8x16 Tactic: 0xe1ff5ad20f5c6bf6
[08/10/2023-11:18:19] [V] [TRT] Tactic: 0xe1ff5ad20f5c6bf6 Time: 0.183616
[08/10/2023-11:18:19] [V] [TRT] Conv_70 Set Tactic Name: ampere_h16816cudnn_256x64_ldg8_relu_exp_medium_nhwc_tn_v1 Tactic: 0xe4711898bd599c36
[08/10/2023-11:18:19] [V] [TRT] Tactic: 0xe4711898bd599c36 Time: 0.17195
[08/10/2023-11:18:19] [V] [TRT] Conv_70 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x64x64_stage3_warpsize4x1x1_g1_tensor16x8x16 Tactic: 0xeb25062ffb9eae45
[08/10/2023-11:18:19] [V] [TRT] Tactic: 0xeb25062ffb9eae45 Time: 0.194482
[08/10/2023-11:18:19] [V] [TRT] Conv_70 Set Tactic Name: ampere_h16816cudnn_128x64_sliced1x2_ldg8_relu_exp_stages_64x4_large_nhwc_tn_v1 Tactic: 0xeb2d2aa4e56bb41c
[08/10/2023-11:18:19] [V] [TRT] Tactic: 0xeb2d2aa4e56bb41c Time: 0.268809
[08/10/2023-11:18:19] [V] [TRT] Conv_70 Set Tactic Name: ampere_h16816cudnn_64x64_ldg8_relu_exp_stages_64x6_small_nhwc_tn_v1 Tactic: 0xf0beb09df9a19f82
[08/10/2023-11:18:19] [V] [TRT] Tactic: 0xf0beb09df9a19f82 Time: 0.358551
[08/10/2023-11:18:19] [V] [TRT] Conv_70 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x64x64_stage4_warpsize2x2x1_g1_tensor16x8x16 Tactic: 0xf35e0311fa1cc516
[08/10/2023-11:18:19] [V] [TRT] Tactic: 0xf35e0311fa1cc516 Time: 0.20837
[08/10/2023-11:18:19] [V] [TRT] Conv_70 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x32x32_stage4_warpsize4x1x1_g1_tensor16x8x16 Tactic: 0xf79479a62ea9f901
[08/10/2023-11:18:19] [V] [TRT] Tactic: 0xf79479a62ea9f901 Time: 0.130898
[08/10/2023-11:18:19] [V] [TRT] Fastest Tactic: 0x3e7eb35b91b9fa63 Time: 0.119451
[08/10/2023-11:18:19] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x3e7eb35b91b9fa63
[08/10/2023-11:18:19] [V] [TRT] =============== Computing costs for 
[08/10/2023-11:18:19] [V] [TRT] *************** Autotuning format combination: Float(16588800,32400,180,1) -> Float(64800,32400,180,1) ***************
[08/10/2023-11:18:19] [V] [TRT] *************** Autotuning format combination: Float(16588800,1,92160,512) -> Float(64800,1,360,2) ***************
[08/10/2023-11:18:20] [V] [TRT] *************** Autotuning format combination: Float(4147200,1:4,23040,128) -> Float(32400,1:4,180,1) ***************
[08/10/2023-11:18:20] [V] [TRT] *************** Autotuning format combination: Half(16588800,32400,180,1) -> Half(64800,32400,180,1) ***************
[08/10/2023-11:18:20] [W] [TRT] Weights [name=Conv_73.weight] had the following issues when converted to FP16:
[08/10/2023-11:18:20] [W] [TRT]  - Subnormal FP16 values detected. 
[08/10/2023-11:18:20] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.
[08/10/2023-11:18:20] [V] [TRT] *************** Autotuning format combination: Half(8294400,32400:2,180,1) -> Half(32400,32400:2,180,1) ***************
[08/10/2023-11:18:20] [W] [TRT] Weights [name=Conv_73.weight] had the following issues when converted to FP16:
[08/10/2023-11:18:20] [W] [TRT]  - Subnormal FP16 values detected. 
[08/10/2023-11:18:20] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.
[08/10/2023-11:18:20] [V] [TRT] *************** Autotuning format combination: Half(2073600,1:8,11520,64) -> Float(64800,32400,180,1) ***************
[08/10/2023-11:18:20] [W] [TRT] Weights [name=Conv_73.weight] had the following issues when converted to FP16:
[08/10/2023-11:18:20] [W] [TRT]  - Subnormal FP16 values detected. 
[08/10/2023-11:18:20] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.
[08/10/2023-11:18:20] [V] [TRT] --------------- Timing Runner: Conv_73 (CaskConvolution)
[08/10/2023-11:18:20] [V] [TRT] CaskConvolution has no valid tactics for this config, skipping
[08/10/2023-11:18:20] [V] [TRT] *************** Autotuning format combination: Half(2073600,1:8,11520,64) -> Half(32400,1:8,180,1) ***************
[08/10/2023-11:18:20] [W] [TRT] Weights [name=Conv_73.weight] had the following issues when converted to FP16:
[08/10/2023-11:18:20] [W] [TRT]  - Subnormal FP16 values detected. 
[08/10/2023-11:18:20] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.
[08/10/2023-11:18:20] [V] [TRT] *************** Autotuning format combination: Half(1036800,1:16,5760,32) -> Half(32400,1:16,180,1) ***************
[08/10/2023-11:18:20] [W] [TRT] Weights [name=Conv_73.weight] had the following issues when converted to FP16:
[08/10/2023-11:18:20] [W] [TRT]  - Subnormal FP16 values detected. 
[08/10/2023-11:18:20] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.
[08/10/2023-11:18:20] [V] [TRT] =============== Computing costs for 
[08/10/2023-11:18:20] [V] [TRT] *************** Autotuning format combination: Float(16588800,32400,180,1) -> Float(64800,32400,180,1) ***************
[08/10/2023-11:18:20] [V] [TRT] *************** Autotuning format combination: Float(16588800,1,92160,512) -> Float(64800,1,360,2) ***************
[08/10/2023-11:18:20] [V] [TRT] *************** Autotuning format combination: Float(4147200,1:4,23040,128) -> Float(32400,1:4,180,1) ***************
[08/10/2023-11:18:20] [V] [TRT] *************** Autotuning format combination: Half(16588800,32400,180,1) -> Half(64800,32400,180,1) ***************
[08/10/2023-11:18:20] [V] [TRT] *************** Autotuning format combination: Half(8294400,32400:2,180,1) -> Half(32400,32400:2,180,1) ***************
[08/10/2023-11:18:20] [V] [TRT] *************** Autotuning format combination: Half(2073600,1:8,11520,64) -> Float(64800,32400,180,1) ***************
[08/10/2023-11:18:20] [V] [TRT] --------------- Timing Runner: Conv_76 (CaskConvolution)
[08/10/2023-11:18:20] [V] [TRT] CaskConvolution has no valid tactics for this config, skipping
[08/10/2023-11:18:20] [V] [TRT] *************** Autotuning format combination: Half(2073600,1:8,11520,64) -> Half(32400,1:8,180,1) ***************
[08/10/2023-11:18:20] [V] [TRT] *************** Autotuning format combination: Half(1036800,1:16,5760,32) -> Half(32400,1:16,180,1) ***************
[08/10/2023-11:18:20] [V] [TRT] =============== Computing costs for 
[08/10/2023-11:18:20] [V] [TRT] *************** Autotuning format combination: Float(16588800,32400,180,1) -> Float(32400,32400,180,1) ***************
[08/10/2023-11:18:20] [V] [TRT] *************** Autotuning format combination: Float(16588800,1,92160,512) -> Float(32400,1,180,1) ***************
[08/10/2023-11:18:20] [V] [TRT] *************** Autotuning format combination: Float(4147200,1:4,23040,128) -> Float(32400,1:4,180,1) ***************
[08/10/2023-11:18:20] [V] [TRT] *************** Autotuning format combination: Half(16588800,32400,180,1) -> Half(32400,32400,180,1) ***************
[08/10/2023-11:18:20] [V] [TRT] *************** Autotuning format combination: Half(8294400,32400:2,180,1) -> Half(32400,32400:2,180,1) ***************
[08/10/2023-11:18:20] [V] [TRT] *************** Autotuning format combination: Half(2073600,1:8,11520,64) -> Float(32400,32400,180,1) ***************
[08/10/2023-11:18:20] [V] [TRT] --------------- Timing Runner: Conv_79 (CaskConvolution)
[08/10/2023-11:18:20] [V] [TRT] CaskConvolution has no valid tactics for this config, skipping
[08/10/2023-11:18:20] [V] [TRT] *************** Autotuning format combination: Half(2073600,1:8,11520,64) -> Half(32400,1:8,180,1) ***************
[08/10/2023-11:18:20] [V] [TRT] *************** Autotuning format combination: Half(1036800,1:16,5760,32) -> Half(32400,1:16,180,1) ***************
[08/10/2023-11:18:20] [V] [TRT] =============== Computing costs for 
[08/10/2023-11:18:20] [V] [TRT] *************** Autotuning format combination: Float(16588800,32400,180,1) -> Float(64800,32400,180,1) ***************
[08/10/2023-11:18:20] [V] [TRT] *************** Autotuning format combination: Float(16588800,1,92160,512) -> Float(64800,1,360,2) ***************
[08/10/2023-11:18:20] [V] [TRT] *************** Autotuning format combination: Float(4147200,1:4,23040,128) -> Float(32400,1:4,180,1) ***************
[08/10/2023-11:18:20] [V] [TRT] *************** Autotuning format combination: Half(16588800,32400,180,1) -> Half(64800,32400,180,1) ***************
[08/10/2023-11:18:20] [V] [TRT] *************** Autotuning format combination: Half(8294400,32400:2,180,1) -> Half(32400,32400:2,180,1) ***************
[08/10/2023-11:18:20] [V] [TRT] *************** Autotuning format combination: Half(2073600,1:8,11520,64) -> Float(64800,32400,180,1) ***************
[08/10/2023-11:18:20] [V] [TRT] --------------- Timing Runner: Conv_82 (CaskConvolution)
[08/10/2023-11:18:20] [V] [TRT] CaskConvolution has no valid tactics for this config, skipping
[08/10/2023-11:18:20] [V] [TRT] *************** Autotuning format combination: Half(2073600,1:8,11520,64) -> Half(32400,1:8,180,1) ***************
[08/10/2023-11:18:20] [V] [TRT] *************** Autotuning format combination: Half(1036800,1:16,5760,32) -> Half(32400,1:16,180,1) ***************
[08/10/2023-11:18:20] [V] [TRT] =============== Computing costs for 
[08/10/2023-11:18:20] [V] [TRT] *************** Autotuning format combination: Float(16588800,32400,180,1) -> Float(32400,32400,180,1) ***************
[08/10/2023-11:18:20] [V] [TRT] *************** Autotuning format combination: Float(16588800,1,92160,512) -> Float(32400,1,180,1) ***************
[08/10/2023-11:18:20] [V] [TRT] *************** Autotuning format combination: Float(4147200,1:4,23040,128) -> Float(32400,1:4,180,1) ***************
[08/10/2023-11:18:20] [V] [TRT] *************** Autotuning format combination: Half(16588800,32400,180,1) -> Half(32400,32400,180,1) ***************
[08/10/2023-11:18:20] [V] [TRT] *************** Autotuning format combination: Half(8294400,32400:2,180,1) -> Half(32400,32400:2,180,1) ***************
[08/10/2023-11:18:20] [V] [TRT] *************** Autotuning format combination: Half(2073600,1:8,11520,64) -> Float(32400,32400,180,1) ***************
[08/10/2023-11:18:20] [V] [TRT] --------------- Timing Runner: Conv_85 (CaskConvolution)
[08/10/2023-11:18:20] [V] [TRT] CaskConvolution has no valid tactics for this config, skipping
[08/10/2023-11:18:20] [V] [TRT] *************** Autotuning format combination: Half(2073600,1:8,11520,64) -> Half(32400,1:8,180,1) ***************
[08/10/2023-11:18:20] [V] [TRT] *************** Autotuning format combination: Half(1036800,1:16,5760,32) -> Half(32400,1:16,180,1) ***************
[08/10/2023-11:18:20] [V] [TRT] =============== Computing costs for 
[08/10/2023-11:18:20] [V] [TRT] *************** Autotuning format combination: Float(2073600,32400,180,1) -> Float(16588800,32400,180,1) ***************
[08/10/2023-11:18:20] [V] [TRT] *************** Autotuning format combination: Float(2073600,1,11520,64) -> Float(16588800,1,92160,512) ***************
[08/10/2023-11:18:20] [V] [TRT] *************** Autotuning format combination: Float(518400,1:4,2880,16) -> Float(4147200,1:4,23040,128) ***************
[08/10/2023-11:18:20] [V] [TRT] *************** Autotuning format combination: Half(2073600,32400,180,1) -> Half(16588800,32400,180,1) ***************
[08/10/2023-11:18:20] [W] [TRT] Weights [name=Conv_86 + Relu_87 || Conv_89 + Relu_90 || Conv_92 + Relu_93 || Conv_95 + Relu_96 || Conv_98 + Relu_99 || Conv_101 + Relu_102 || Conv_104 + Relu_105 || Conv_107 + Relu_108.weight] had the following issues when converted to FP16:
[08/10/2023-11:18:20] [W] [TRT]  - Subnormal FP16 values detected. 
[08/10/2023-11:18:20] [W] [TRT]  - Values less than smallest positive FP16 Subnormal value detected. Converting to FP16 minimum subnormalized value. 
[08/10/2023-11:18:20] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.
[08/10/2023-11:18:20] [V] [TRT] *************** Autotuning format combination: Half(1036800,32400:2,180,1) -> Half(8294400,32400:2,180,1) ***************
[08/10/2023-11:18:20] [W] [TRT] Weights [name=Conv_86 + Relu_87 || Conv_89 + Relu_90 || Conv_92 + Relu_93 || Conv_95 + Relu_96 || Conv_98 + Relu_99 || Conv_101 + Relu_102 || Conv_104 + Relu_105 || Conv_107 + Relu_108.weight] had the following issues when converted to FP16:
[08/10/2023-11:18:20] [W] [TRT]  - Subnormal FP16 values detected. 
[08/10/2023-11:18:20] [W] [TRT]  - Values less than smallest positive FP16 Subnormal value detected. Converting to FP16 minimum subnormalized value. 
[08/10/2023-11:18:20] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.
[08/10/2023-11:18:20] [V] [TRT] *************** Autotuning format combination: Half(259200,1:8,1440,8) -> Float(16588800,32400,180,1) ***************
[08/10/2023-11:18:20] [W] [TRT] Weights [name=Conv_86 + Relu_87 || Conv_89 + Relu_90 || Conv_92 + Relu_93 || Conv_95 + Relu_96 || Conv_98 + Relu_99 || Conv_101 + Relu_102 || Conv_104 + Relu_105 || Conv_107 + Relu_108.weight] had the following issues when converted to FP16:
[08/10/2023-11:18:20] [W] [TRT]  - Subnormal FP16 values detected. 
[08/10/2023-11:18:20] [W] [TRT]  - Values less than smallest positive FP16 Subnormal value detected. Converting to FP16 minimum subnormalized value. 
[08/10/2023-11:18:20] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.
[08/10/2023-11:18:20] [V] [TRT] --------------- Timing Runner: Conv_86 + Relu_87 || Conv_89 + Relu_90 || Conv_92 + Relu_93 || Conv_95 + Relu_96 || Conv_98 + Relu_99 || Conv_101 + Relu_102 || Conv_104 + Relu_105 || Conv_107 + Relu_108 (CaskConvolution)
[08/10/2023-11:18:20] [V] [TRT] CaskConvolution has no valid tactics for this config, skipping
[08/10/2023-11:18:20] [V] [TRT] *************** Autotuning format combination: Half(259200,1:8,1440,8) -> Half(2073600,1:8,11520,64) ***************
[08/10/2023-11:18:20] [W] [TRT] Weights [name=Conv_86 + Relu_87 || Conv_89 + Relu_90 || Conv_92 + Relu_93 || Conv_95 + Relu_96 || Conv_98 + Relu_99 || Conv_101 + Relu_102 || Conv_104 + Relu_105 || Conv_107 + Relu_108.weight] had the following issues when converted to FP16:
[08/10/2023-11:18:20] [W] [TRT]  - Subnormal FP16 values detected. 
[08/10/2023-11:18:20] [W] [TRT]  - Values less than smallest positive FP16 Subnormal value detected. Converting to FP16 minimum subnormalized value. 
[08/10/2023-11:18:20] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.
[08/10/2023-11:18:20] [V] [TRT] *************** Autotuning format combination: Half(129600,1:16,720,4) -> Half(1036800,1:16,5760,32) ***************
[08/10/2023-11:18:20] [W] [TRT] Weights [name=Conv_86 + Relu_87 || Conv_89 + Relu_90 || Conv_92 + Relu_93 || Conv_95 + Relu_96 || Conv_98 + Relu_99 || Conv_101 + Relu_102 || Conv_104 + Relu_105 || Conv_107 + Relu_108.weight] had the following issues when converted to FP16:
[08/10/2023-11:18:20] [W] [TRT]  - Subnormal FP16 values detected. 
[08/10/2023-11:18:20] [W] [TRT]  - Values less than smallest positive FP16 Subnormal value detected. Converting to FP16 minimum subnormalized value. 
[08/10/2023-11:18:20] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.
[08/10/2023-11:18:20] [V] [TRT] =============== Computing costs for 
[08/10/2023-11:18:20] [V] [TRT] *************** Autotuning format combination: Float(16588800,32400,180,1) -> Float(97200,32400,180,1) ***************
[08/10/2023-11:18:20] [V] [TRT] *************** Autotuning format combination: Float(16588800,1,92160,512) -> Float(97200,1,540,3) ***************
[08/10/2023-11:18:20] [V] [TRT] *************** Autotuning format combination: Float(4147200,1:4,23040,128) -> Float(32400,1:4,180,1) ***************
[08/10/2023-11:18:20] [V] [TRT] *************** Autotuning format combination: Half(16588800,32400,180,1) -> Half(97200,32400,180,1) ***************
[08/10/2023-11:18:20] [W] [TRT] Weights [name=Conv_88.weight] had the following issues when converted to FP16:
[08/10/2023-11:18:20] [W] [TRT]  - Subnormal FP16 values detected. 
[08/10/2023-11:18:20] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.
[08/10/2023-11:18:20] [V] [TRT] *************** Autotuning format combination: Half(8294400,32400:2,180,1) -> Half(64800,32400:2,180,1) ***************
[08/10/2023-11:18:20] [W] [TRT] Weights [name=Conv_88.weight] had the following issues when converted to FP16:
[08/10/2023-11:18:20] [W] [TRT]  - Subnormal FP16 values detected. 
[08/10/2023-11:18:20] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.
[08/10/2023-11:18:20] [V] [TRT] *************** Autotuning format combination: Half(2073600,1:8,11520,64) -> Float(97200,32400,180,1) ***************
[08/10/2023-11:18:20] [W] [TRT] Weights [name=Conv_88.weight] had the following issues when converted to FP16:
[08/10/2023-11:18:20] [W] [TRT]  - Subnormal FP16 values detected. 
[08/10/2023-11:18:20] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.
[08/10/2023-11:18:20] [V] [TRT] --------------- Timing Runner: Conv_88 (CaskConvolution)
[08/10/2023-11:18:20] [V] [TRT] CaskConvolution has no valid tactics for this config, skipping
[08/10/2023-11:18:20] [V] [TRT] *************** Autotuning format combination: Half(2073600,1:8,11520,64) -> Half(32400,1:8,180,1) ***************
[08/10/2023-11:18:20] [W] [TRT] Weights [name=Conv_88.weight] had the following issues when converted to FP16:
[08/10/2023-11:18:20] [W] [TRT]  - Subnormal FP16 values detected. 
[08/10/2023-11:18:20] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.
[08/10/2023-11:18:20] [V] [TRT] *************** Autotuning format combination: Half(1036800,1:16,5760,32) -> Half(32400,1:16,180,1) ***************
[08/10/2023-11:18:20] [W] [TRT] Weights [name=Conv_88.weight] had the following issues when converted to FP16:
[08/10/2023-11:18:20] [W] [TRT]  - Subnormal FP16 values detected. 
[08/10/2023-11:18:20] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.
[08/10/2023-11:18:20] [V] [TRT] =============== Computing costs for 
[08/10/2023-11:18:20] [V] [TRT] *************** Autotuning format combination: Float(16588800,32400,180,1) -> Float(64800,32400,180,1) ***************
[08/10/2023-11:18:20] [V] [TRT] *************** Autotuning format combination: Float(16588800,1,92160,512) -> Float(64800,1,360,2) ***************
[08/10/2023-11:18:20] [V] [TRT] *************** Autotuning format combination: Float(4147200,1:4,23040,128) -> Float(32400,1:4,180,1) ***************
[08/10/2023-11:18:20] [V] [TRT] *************** Autotuning format combination: Half(16588800,32400,180,1) -> Half(64800,32400,180,1) ***************
[08/10/2023-11:18:20] [V] [TRT] *************** Autotuning format combination: Half(8294400,32400:2,180,1) -> Half(32400,32400:2,180,1) ***************
[08/10/2023-11:18:20] [V] [TRT] *************** Autotuning format combination: Half(2073600,1:8,11520,64) -> Float(64800,32400,180,1) ***************
[08/10/2023-11:18:20] [V] [TRT] --------------- Timing Runner: Conv_91 (CaskConvolution)
[08/10/2023-11:18:20] [V] [TRT] CaskConvolution has no valid tactics for this config, skipping
[08/10/2023-11:18:20] [V] [TRT] *************** Autotuning format combination: Half(2073600,1:8,11520,64) -> Half(32400,1:8,180,1) ***************
[08/10/2023-11:18:20] [V] [TRT] *************** Autotuning format combination: Half(1036800,1:16,5760,32) -> Half(32400,1:16,180,1) ***************
[08/10/2023-11:18:20] [V] [TRT] =============== Computing costs for 
[08/10/2023-11:18:20] [V] [TRT] *************** Autotuning format combination: Float(16588800,32400,180,1) -> Float(64800,32400,180,1) ***************
[08/10/2023-11:18:20] [V] [TRT] *************** Autotuning format combination: Float(16588800,1,92160,512) -> Float(64800,1,360,2) ***************
[08/10/2023-11:18:20] [V] [TRT] *************** Autotuning format combination: Float(4147200,1:4,23040,128) -> Float(32400,1:4,180,1) ***************
[08/10/2023-11:18:20] [V] [TRT] *************** Autotuning format combination: Half(16588800,32400,180,1) -> Half(64800,32400,180,1) ***************
[08/10/2023-11:18:20] [V] [TRT] *************** Autotuning format combination: Half(8294400,32400:2,180,1) -> Half(32400,32400:2,180,1) ***************
[08/10/2023-11:18:20] [V] [TRT] *************** Autotuning format combination: Half(2073600,1:8,11520,64) -> Float(64800,32400,180,1) ***************
[08/10/2023-11:18:20] [V] [TRT] --------------- Timing Runner: Conv_94 (CaskConvolution)
[08/10/2023-11:18:20] [V] [TRT] CaskConvolution has no valid tactics for this config, skipping
[08/10/2023-11:18:20] [V] [TRT] *************** Autotuning format combination: Half(2073600,1:8,11520,64) -> Half(32400,1:8,180,1) ***************
[08/10/2023-11:18:20] [V] [TRT] *************** Autotuning format combination: Half(1036800,1:16,5760,32) -> Half(32400,1:16,180,1) ***************
[08/10/2023-11:18:20] [V] [TRT] =============== Computing costs for 
[08/10/2023-11:18:20] [V] [TRT] *************** Autotuning format combination: Float(16588800,32400,180,1) -> Float(64800,32400,180,1) ***************
[08/10/2023-11:18:20] [V] [TRT] *************** Autotuning format combination: Float(16588800,1,92160,512) -> Float(64800,1,360,2) ***************
[08/10/2023-11:18:20] [V] [TRT] *************** Autotuning format combination: Float(4147200,1:4,23040,128) -> Float(32400,1:4,180,1) ***************
[08/10/2023-11:18:20] [V] [TRT] *************** Autotuning format combination: Half(16588800,32400,180,1) -> Half(64800,32400,180,1) ***************
[08/10/2023-11:18:20] [V] [TRT] *************** Autotuning format combination: Half(8294400,32400:2,180,1) -> Half(32400,32400:2,180,1) ***************
[08/10/2023-11:18:20] [V] [TRT] *************** Autotuning format combination: Half(2073600,1:8,11520,64) -> Float(64800,32400,180,1) ***************
[08/10/2023-11:18:20] [V] [TRT] --------------- Timing Runner: Conv_97 (CaskConvolution)
[08/10/2023-11:18:20] [V] [TRT] CaskConvolution has no valid tactics for this config, skipping
[08/10/2023-11:18:20] [V] [TRT] *************** Autotuning format combination: Half(2073600,1:8,11520,64) -> Half(32400,1:8,180,1) ***************
[08/10/2023-11:18:20] [V] [TRT] *************** Autotuning format combination: Half(1036800,1:16,5760,32) -> Half(32400,1:16,180,1) ***************
[08/10/2023-11:18:20] [V] [TRT] =============== Computing costs for 
[08/10/2023-11:18:20] [V] [TRT] *************** Autotuning format combination: Float(16588800,32400,180,1) -> Float(64800,32400,180,1) ***************
[08/10/2023-11:18:20] [V] [TRT] *************** Autotuning format combination: Float(16588800,1,92160,512) -> Float(64800,1,360,2) ***************
[08/10/2023-11:18:20] [V] [TRT] *************** Autotuning format combination: Float(4147200,1:4,23040,128) -> Float(32400,1:4,180,1) ***************
[08/10/2023-11:18:20] [V] [TRT] *************** Autotuning format combination: Half(16588800,32400,180,1) -> Half(64800,32400,180,1) ***************
[08/10/2023-11:18:20] [W] [TRT] Weights [name=Conv_100.weight] had the following issues when converted to FP16:
[08/10/2023-11:18:20] [W] [TRT]  - Subnormal FP16 values detected. 
[08/10/2023-11:18:20] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.
[08/10/2023-11:18:20] [V] [TRT] *************** Autotuning format combination: Half(8294400,32400:2,180,1) -> Half(32400,32400:2,180,1) ***************
[08/10/2023-11:18:20] [W] [TRT] Weights [name=Conv_100.weight] had the following issues when converted to FP16:
[08/10/2023-11:18:20] [W] [TRT]  - Subnormal FP16 values detected. 
[08/10/2023-11:18:20] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.
[08/10/2023-11:18:20] [V] [TRT] *************** Autotuning format combination: Half(2073600,1:8,11520,64) -> Float(64800,32400,180,1) ***************
[08/10/2023-11:18:20] [W] [TRT] Weights [name=Conv_100.weight] had the following issues when converted to FP16:
[08/10/2023-11:18:20] [W] [TRT]  - Subnormal FP16 values detected. 
[08/10/2023-11:18:20] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.
[08/10/2023-11:18:20] [V] [TRT] --------------- Timing Runner: Conv_100 (CaskConvolution)
[08/10/2023-11:18:20] [V] [TRT] CaskConvolution has no valid tactics for this config, skipping
[08/10/2023-11:18:20] [V] [TRT] *************** Autotuning format combination: Half(2073600,1:8,11520,64) -> Half(32400,1:8,180,1) ***************
[08/10/2023-11:18:20] [W] [TRT] Weights [name=Conv_100.weight] had the following issues when converted to FP16:
[08/10/2023-11:18:20] [W] [TRT]  - Subnormal FP16 values detected. 
[08/10/2023-11:18:20] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.
[08/10/2023-11:18:20] [V] [TRT] *************** Autotuning format combination: Half(1036800,1:16,5760,32) -> Half(32400,1:16,180,1) ***************
[08/10/2023-11:18:20] [W] [TRT] Weights [name=Conv_100.weight] had the following issues when converted to FP16:
[08/10/2023-11:18:20] [W] [TRT]  - Subnormal FP16 values detected. 
[08/10/2023-11:18:20] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.
[08/10/2023-11:18:20] [V] [TRT] =============== Computing costs for 
[08/10/2023-11:18:20] [V] [TRT] *************** Autotuning format combination: Float(16588800,32400,180,1) -> Float(32400,32400,180,1) ***************
[08/10/2023-11:18:20] [V] [TRT] *************** Autotuning format combination: Float(16588800,1,92160,512) -> Float(32400,1,180,1) ***************
[08/10/2023-11:18:20] [V] [TRT] *************** Autotuning format combination: Float(4147200,1:4,23040,128) -> Float(32400,1:4,180,1) ***************
[08/10/2023-11:18:20] [V] [TRT] *************** Autotuning format combination: Half(16588800,32400,180,1) -> Half(32400,32400,180,1) ***************
[08/10/2023-11:18:20] [V] [TRT] *************** Autotuning format combination: Half(8294400,32400:2,180,1) -> Half(32400,32400:2,180,1) ***************
[08/10/2023-11:18:20] [V] [TRT] *************** Autotuning format combination: Half(2073600,1:8,11520,64) -> Float(32400,32400,180,1) ***************
[08/10/2023-11:18:20] [V] [TRT] --------------- Timing Runner: Conv_103 (CaskConvolution)
[08/10/2023-11:18:20] [V] [TRT] CaskConvolution has no valid tactics for this config, skipping
[08/10/2023-11:18:20] [V] [TRT] *************** Autotuning format combination: Half(2073600,1:8,11520,64) -> Half(32400,1:8,180,1) ***************
[08/10/2023-11:18:20] [V] [TRT] *************** Autotuning format combination: Half(1036800,1:16,5760,32) -> Half(32400,1:16,180,1) ***************
[08/10/2023-11:18:20] [V] [TRT] =============== Computing costs for 
[08/10/2023-11:18:20] [V] [TRT] *************** Autotuning format combination: Float(16588800,32400,180,1) -> Float(97200,32400,180,1) ***************
[08/10/2023-11:18:20] [V] [TRT] *************** Autotuning format combination: Float(16588800,1,92160,512) -> Float(97200,1,540,3) ***************
[08/10/2023-11:18:20] [V] [TRT] *************** Autotuning format combination: Float(4147200,1:4,23040,128) -> Float(32400,1:4,180,1) ***************
[08/10/2023-11:18:20] [V] [TRT] *************** Autotuning format combination: Half(16588800,32400,180,1) -> Half(97200,32400,180,1) ***************
[08/10/2023-11:18:20] [V] [TRT] *************** Autotuning format combination: Half(8294400,32400:2,180,1) -> Half(64800,32400:2,180,1) ***************
[08/10/2023-11:18:20] [V] [TRT] *************** Autotuning format combination: Half(2073600,1:8,11520,64) -> Float(97200,32400,180,1) ***************
[08/10/2023-11:18:20] [V] [TRT] --------------- Timing Runner: Conv_106 (CaskConvolution)
[08/10/2023-11:18:20] [V] [TRT] CaskConvolution has no valid tactics for this config, skipping
[08/10/2023-11:18:20] [V] [TRT] *************** Autotuning format combination: Half(2073600,1:8,11520,64) -> Half(32400,1:8,180,1) ***************
[08/10/2023-11:18:20] [V] [TRT] *************** Autotuning format combination: Half(1036800,1:16,5760,32) -> Half(32400,1:16,180,1) ***************
[08/10/2023-11:18:20] [V] [TRT] =============== Computing costs for 
[08/10/2023-11:18:20] [V] [TRT] *************** Autotuning format combination: Float(16588800,32400,180,1) -> Float(64800,32400,180,1) ***************
[08/10/2023-11:18:20] [V] [TRT] *************** Autotuning format combination: Float(16588800,1,92160,512) -> Float(64800,1,360,2) ***************
[08/10/2023-11:18:20] [V] [TRT] *************** Autotuning format combination: Float(4147200,1:4,23040,128) -> Float(32400,1:4,180,1) ***************
[08/10/2023-11:18:20] [V] [TRT] *************** Autotuning format combination: Half(16588800,32400,180,1) -> Half(64800,32400,180,1) ***************
[08/10/2023-11:18:20] [V] [TRT] *************** Autotuning format combination: Half(8294400,32400:2,180,1) -> Half(32400,32400:2,180,1) ***************
[08/10/2023-11:18:20] [V] [TRT] *************** Autotuning format combination: Half(2073600,1:8,11520,64) -> Float(64800,32400,180,1) ***************
[08/10/2023-11:18:20] [V] [TRT] --------------- Timing Runner: Conv_109 (CaskConvolution)
[08/10/2023-11:18:20] [V] [TRT] CaskConvolution has no valid tactics for this config, skipping
[08/10/2023-11:18:20] [V] [TRT] *************** Autotuning format combination: Half(2073600,1:8,11520,64) -> Half(32400,1:8,180,1) ***************
[08/10/2023-11:18:20] [V] [TRT] *************** Autotuning format combination: Half(1036800,1:16,5760,32) -> Half(32400,1:16,180,1) ***************
[08/10/2023-11:18:20] [V] [TRT] =============== Computing costs for 
[08/10/2023-11:18:20] [V] [TRT] *************** Autotuning format combination: Float(2073600,32400,180,1) -> Float(16588800,32400,180,1) ***************
[08/10/2023-11:18:20] [V] [TRT] *************** Autotuning format combination: Float(2073600,1,11520,64) -> Float(16588800,1,92160,512) ***************
[08/10/2023-11:18:20] [V] [TRT] *************** Autotuning format combination: Float(518400,1:4,2880,16) -> Float(4147200,1:4,23040,128) ***************
[08/10/2023-11:18:20] [V] [TRT] *************** Autotuning format combination: Half(2073600,32400,180,1) -> Half(16588800,32400,180,1) ***************
[08/10/2023-11:18:20] [W] [TRT] Weights [name=Conv_110 + Relu_111 || Conv_113 + Relu_114 || Conv_116 + Relu_117 || Conv_119 + Relu_120 || Conv_122 + Relu_123 || Conv_125 + Relu_126 || Conv_128 + Relu_129 || Conv_131 + Relu_132.weight] had the following issues when converted to FP16:
[08/10/2023-11:18:20] [W] [TRT]  - Subnormal FP16 values detected. 
[08/10/2023-11:18:20] [W] [TRT]  - Values less than smallest positive FP16 Subnormal value detected. Converting to FP16 minimum subnormalized value. 
[08/10/2023-11:18:20] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.
[08/10/2023-11:18:20] [V] [TRT] *************** Autotuning format combination: Half(1036800,32400:2,180,1) -> Half(8294400,32400:2,180,1) ***************
[08/10/2023-11:18:20] [W] [TRT] Weights [name=Conv_110 + Relu_111 || Conv_113 + Relu_114 || Conv_116 + Relu_117 || Conv_119 + Relu_120 || Conv_122 + Relu_123 || Conv_125 + Relu_126 || Conv_128 + Relu_129 || Conv_131 + Relu_132.weight] had the following issues when converted to FP16:
[08/10/2023-11:18:20] [W] [TRT]  - Subnormal FP16 values detected. 
[08/10/2023-11:18:20] [W] [TRT]  - Values less than smallest positive FP16 Subnormal value detected. Converting to FP16 minimum subnormalized value. 
[08/10/2023-11:18:20] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.
[08/10/2023-11:18:20] [V] [TRT] *************** Autotuning format combination: Half(259200,1:8,1440,8) -> Float(16588800,32400,180,1) ***************
[08/10/2023-11:18:20] [W] [TRT] Weights [name=Conv_110 + Relu_111 || Conv_113 + Relu_114 || Conv_116 + Relu_117 || Conv_119 + Relu_120 || Conv_122 + Relu_123 || Conv_125 + Relu_126 || Conv_128 + Relu_129 || Conv_131 + Relu_132.weight] had the following issues when converted to FP16:
[08/10/2023-11:18:20] [W] [TRT]  - Subnormal FP16 values detected. 
[08/10/2023-11:18:20] [W] [TRT]  - Values less than smallest positive FP16 Subnormal value detected. Converting to FP16 minimum subnormalized value. 
[08/10/2023-11:18:20] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.
[08/10/2023-11:18:20] [V] [TRT] --------------- Timing Runner: Conv_110 + Relu_111 || Conv_113 + Relu_114 || Conv_116 + Relu_117 || Conv_119 + Relu_120 || Conv_122 + Relu_123 || Conv_125 + Relu_126 || Conv_128 + Relu_129 || Conv_131 + Relu_132 (CaskConvolution)
[08/10/2023-11:18:20] [V] [TRT] CaskConvolution has no valid tactics for this config, skipping
[08/10/2023-11:18:20] [V] [TRT] *************** Autotuning format combination: Half(259200,1:8,1440,8) -> Half(2073600,1:8,11520,64) ***************
[08/10/2023-11:18:20] [W] [TRT] Weights [name=Conv_110 + Relu_111 || Conv_113 + Relu_114 || Conv_116 + Relu_117 || Conv_119 + Relu_120 || Conv_122 + Relu_123 || Conv_125 + Relu_126 || Conv_128 + Relu_129 || Conv_131 + Relu_132.weight] had the following issues when converted to FP16:
[08/10/2023-11:18:20] [W] [TRT]  - Subnormal FP16 values detected. 
[08/10/2023-11:18:20] [W] [TRT]  - Values less than smallest positive FP16 Subnormal value detected. Converting to FP16 minimum subnormalized value. 
[08/10/2023-11:18:20] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.
[08/10/2023-11:18:20] [V] [TRT] *************** Autotuning format combination: Half(129600,1:16,720,4) -> Half(1036800,1:16,5760,32) ***************
[08/10/2023-11:18:20] [W] [TRT] Weights [name=Conv_110 + Relu_111 || Conv_113 + Relu_114 || Conv_116 + Relu_117 || Conv_119 + Relu_120 || Conv_122 + Relu_123 || Conv_125 + Relu_126 || Conv_128 + Relu_129 || Conv_131 + Relu_132.weight] had the following issues when converted to FP16:
[08/10/2023-11:18:20] [W] [TRT]  - Subnormal FP16 values detected. 
[08/10/2023-11:18:20] [W] [TRT]  - Values less than smallest positive FP16 Subnormal value detected. Converting to FP16 minimum subnormalized value. 
[08/10/2023-11:18:20] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.
[08/10/2023-11:18:20] [V] [TRT] =============== Computing costs for 
[08/10/2023-11:18:20] [V] [TRT] *************** Autotuning format combination: Float(16588800,32400,180,1) -> Float(64800,32400,180,1) ***************
[08/10/2023-11:18:20] [V] [TRT] *************** Autotuning format combination: Float(16588800,1,92160,512) -> Float(64800,1,360,2) ***************
[08/10/2023-11:18:20] [V] [TRT] *************** Autotuning format combination: Float(4147200,1:4,23040,128) -> Float(32400,1:4,180,1) ***************
[08/10/2023-11:18:20] [V] [TRT] *************** Autotuning format combination: Half(16588800,32400,180,1) -> Half(64800,32400,180,1) ***************
[08/10/2023-11:18:20] [V] [TRT] *************** Autotuning format combination: Half(8294400,32400:2,180,1) -> Half(32400,32400:2,180,1) ***************
[08/10/2023-11:18:20] [V] [TRT] *************** Autotuning format combination: Half(2073600,1:8,11520,64) -> Float(64800,32400,180,1) ***************
[08/10/2023-11:18:20] [V] [TRT] --------------- Timing Runner: Conv_112 (CaskConvolution)
[08/10/2023-11:18:20] [V] [TRT] CaskConvolution has no valid tactics for this config, skipping
[08/10/2023-11:18:20] [V] [TRT] *************** Autotuning format combination: Half(2073600,1:8,11520,64) -> Half(32400,1:8,180,1) ***************
[08/10/2023-11:18:20] [V] [TRT] *************** Autotuning format combination: Half(1036800,1:16,5760,32) -> Half(32400,1:16,180,1) ***************
[08/10/2023-11:18:20] [V] [TRT] =============== Computing costs for 
[08/10/2023-11:18:20] [V] [TRT] *************** Autotuning format combination: Float(16588800,32400,180,1) -> Float(64800,32400,180,1) ***************
[08/10/2023-11:18:20] [V] [TRT] *************** Autotuning format combination: Float(16588800,1,92160,512) -> Float(64800,1,360,2) ***************
[08/10/2023-11:18:20] [V] [TRT] *************** Autotuning format combination: Float(4147200,1:4,23040,128) -> Float(32400,1:4,180,1) ***************
[08/10/2023-11:18:20] [V] [TRT] *************** Autotuning format combination: Half(16588800,32400,180,1) -> Half(64800,32400,180,1) ***************
[08/10/2023-11:18:20] [V] [TRT] *************** Autotuning format combination: Half(8294400,32400:2,180,1) -> Half(32400,32400:2,180,1) ***************
[08/10/2023-11:18:20] [V] [TRT] *************** Autotuning format combination: Half(2073600,1:8,11520,64) -> Float(64800,32400,180,1) ***************
[08/10/2023-11:18:20] [V] [TRT] --------------- Timing Runner: Conv_115 (CaskConvolution)
[08/10/2023-11:18:20] [V] [TRT] CaskConvolution has no valid tactics for this config, skipping
[08/10/2023-11:18:20] [V] [TRT] *************** Autotuning format combination: Half(2073600,1:8,11520,64) -> Half(32400,1:8,180,1) ***************
[08/10/2023-11:18:20] [V] [TRT] *************** Autotuning format combination: Half(1036800,1:16,5760,32) -> Half(32400,1:16,180,1) ***************
[08/10/2023-11:18:20] [V] [TRT] =============== Computing costs for 
[08/10/2023-11:18:20] [V] [TRT] *************** Autotuning format combination: Float(16588800,32400,180,1) -> Float(64800,32400,180,1) ***************
[08/10/2023-11:18:20] [V] [TRT] *************** Autotuning format combination: Float(16588800,1,92160,512) -> Float(64800,1,360,2) ***************
[08/10/2023-11:18:20] [V] [TRT] *************** Autotuning format combination: Float(4147200,1:4,23040,128) -> Float(32400,1:4,180,1) ***************
[08/10/2023-11:18:20] [V] [TRT] *************** Autotuning format combination: Half(16588800,32400,180,1) -> Half(64800,32400,180,1) ***************
[08/10/2023-11:18:20] [V] [TRT] *************** Autotuning format combination: Half(8294400,32400:2,180,1) -> Half(32400,32400:2,180,1) ***************
[08/10/2023-11:18:20] [V] [TRT] *************** Autotuning format combination: Half(2073600,1:8,11520,64) -> Float(64800,32400,180,1) ***************
[08/10/2023-11:18:20] [V] [TRT] --------------- Timing Runner: Conv_118 (CaskConvolution)
[08/10/2023-11:18:20] [V] [TRT] CaskConvolution has no valid tactics for this config, skipping
[08/10/2023-11:18:20] [V] [TRT] *************** Autotuning format combination: Half(2073600,1:8,11520,64) -> Half(32400,1:8,180,1) ***************
[08/10/2023-11:18:20] [V] [TRT] *************** Autotuning format combination: Half(1036800,1:16,5760,32) -> Half(32400,1:16,180,1) ***************
[08/10/2023-11:18:20] [V] [TRT] =============== Computing costs for 
[08/10/2023-11:18:20] [V] [TRT] *************** Autotuning format combination: Float(16588800,32400,180,1) -> Float(32400,32400,180,1) ***************
[08/10/2023-11:18:20] [V] [TRT] *************** Autotuning format combination: Float(16588800,1,92160,512) -> Float(32400,1,180,1) ***************
[08/10/2023-11:18:20] [V] [TRT] *************** Autotuning format combination: Float(4147200,1:4,23040,128) -> Float(32400,1:4,180,1) ***************
[08/10/2023-11:18:20] [V] [TRT] *************** Autotuning format combination: Half(16588800,32400,180,1) -> Half(32400,32400,180,1) ***************
[08/10/2023-11:18:20] [V] [TRT] *************** Autotuning format combination: Half(8294400,32400:2,180,1) -> Half(32400,32400:2,180,1) ***************
[08/10/2023-11:18:20] [V] [TRT] *************** Autotuning format combination: Half(2073600,1:8,11520,64) -> Float(32400,32400,180,1) ***************
[08/10/2023-11:18:20] [V] [TRT] --------------- Timing Runner: Conv_121 (CaskConvolution)
[08/10/2023-11:18:20] [V] [TRT] CaskConvolution has no valid tactics for this config, skipping
[08/10/2023-11:18:20] [V] [TRT] *************** Autotuning format combination: Half(2073600,1:8,11520,64) -> Half(32400,1:8,180,1) ***************
[08/10/2023-11:18:20] [V] [TRT] *************** Autotuning format combination: Half(1036800,1:16,5760,32) -> Half(32400,1:16,180,1) ***************
[08/10/2023-11:18:20] [V] [TRT] =============== Computing costs for 
[08/10/2023-11:18:20] [V] [TRT] *************** Autotuning format combination: Float(16588800,32400,180,1) -> Float(97200,32400,180,1) ***************
[08/10/2023-11:18:20] [V] [TRT] *************** Autotuning format combination: Float(16588800,1,92160,512) -> Float(97200,1,540,3) ***************
[08/10/2023-11:18:20] [V] [TRT] *************** Autotuning format combination: Float(4147200,1:4,23040,128) -> Float(32400,1:4,180,1) ***************
[08/10/2023-11:18:20] [V] [TRT] *************** Autotuning format combination: Half(16588800,32400,180,1) -> Half(97200,32400,180,1) ***************
[08/10/2023-11:18:20] [W] [TRT] Weights [name=Conv_124.weight] had the following issues when converted to FP16:
[08/10/2023-11:18:20] [W] [TRT]  - Subnormal FP16 values detected. 
[08/10/2023-11:18:20] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.
[08/10/2023-11:18:20] [V] [TRT] *************** Autotuning format combination: Half(8294400,32400:2,180,1) -> Half(64800,32400:2,180,1) ***************
[08/10/2023-11:18:20] [W] [TRT] Weights [name=Conv_124.weight] had the following issues when converted to FP16:
[08/10/2023-11:18:20] [W] [TRT]  - Subnormal FP16 values detected. 
[08/10/2023-11:18:20] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.
[08/10/2023-11:18:20] [V] [TRT] *************** Autotuning format combination: Half(2073600,1:8,11520,64) -> Float(97200,32400,180,1) ***************
[08/10/2023-11:18:20] [W] [TRT] Weights [name=Conv_124.weight] had the following issues when converted to FP16:
[08/10/2023-11:18:20] [W] [TRT]  - Subnormal FP16 values detected. 
[08/10/2023-11:18:20] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.
[08/10/2023-11:18:20] [V] [TRT] --------------- Timing Runner: Conv_124 (CaskConvolution)
[08/10/2023-11:18:20] [V] [TRT] CaskConvolution has no valid tactics for this config, skipping
[08/10/2023-11:18:20] [V] [TRT] *************** Autotuning format combination: Half(2073600,1:8,11520,64) -> Half(32400,1:8,180,1) ***************
[08/10/2023-11:18:20] [W] [TRT] Weights [name=Conv_124.weight] had the following issues when converted to FP16:
[08/10/2023-11:18:20] [W] [TRT]  - Subnormal FP16 values detected. 
[08/10/2023-11:18:20] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.
[08/10/2023-11:18:20] [V] [TRT] *************** Autotuning format combination: Half(1036800,1:16,5760,32) -> Half(32400,1:16,180,1) ***************
[08/10/2023-11:18:20] [W] [TRT] Weights [name=Conv_124.weight] had the following issues when converted to FP16:
[08/10/2023-11:18:20] [W] [TRT]  - Subnormal FP16 values detected. 
[08/10/2023-11:18:20] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.
[08/10/2023-11:18:20] [V] [TRT] =============== Computing costs for 
[08/10/2023-11:18:20] [V] [TRT] *************** Autotuning format combination: Float(16588800,32400,180,1) -> Float(64800,32400,180,1) ***************
[08/10/2023-11:18:20] [V] [TRT] *************** Autotuning format combination: Float(16588800,1,92160,512) -> Float(64800,1,360,2) ***************
[08/10/2023-11:18:20] [V] [TRT] *************** Autotuning format combination: Float(4147200,1:4,23040,128) -> Float(32400,1:4,180,1) ***************
[08/10/2023-11:18:20] [V] [TRT] *************** Autotuning format combination: Half(16588800,32400,180,1) -> Half(64800,32400,180,1) ***************
[08/10/2023-11:18:20] [W] [TRT] Weights [name=Conv_127.weight] had the following issues when converted to FP16:
[08/10/2023-11:18:20] [W] [TRT]  - Subnormal FP16 values detected. 
[08/10/2023-11:18:20] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.
[08/10/2023-11:18:20] [V] [TRT] *************** Autotuning format combination: Half(8294400,32400:2,180,1) -> Half(32400,32400:2,180,1) ***************
[08/10/2023-11:18:20] [W] [TRT] Weights [name=Conv_127.weight] had the following issues when converted to FP16:
[08/10/2023-11:18:20] [W] [TRT]  - Subnormal FP16 values detected. 
[08/10/2023-11:18:20] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.
[08/10/2023-11:18:20] [V] [TRT] *************** Autotuning format combination: Half(2073600,1:8,11520,64) -> Float(64800,32400,180,1) ***************
[08/10/2023-11:18:20] [W] [TRT] Weights [name=Conv_127.weight] had the following issues when converted to FP16:
[08/10/2023-11:18:20] [W] [TRT]  - Subnormal FP16 values detected. 
[08/10/2023-11:18:20] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.
[08/10/2023-11:18:20] [V] [TRT] --------------- Timing Runner: Conv_127 (CaskConvolution)
[08/10/2023-11:18:20] [V] [TRT] CaskConvolution has no valid tactics for this config, skipping
[08/10/2023-11:18:20] [V] [TRT] *************** Autotuning format combination: Half(2073600,1:8,11520,64) -> Half(32400,1:8,180,1) ***************
[08/10/2023-11:18:20] [W] [TRT] Weights [name=Conv_127.weight] had the following issues when converted to FP16:
[08/10/2023-11:18:20] [W] [TRT]  - Subnormal FP16 values detected. 
[08/10/2023-11:18:20] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.
[08/10/2023-11:18:20] [V] [TRT] *************** Autotuning format combination: Half(1036800,1:16,5760,32) -> Half(32400,1:16,180,1) ***************
[08/10/2023-11:18:20] [W] [TRT] Weights [name=Conv_127.weight] had the following issues when converted to FP16:
[08/10/2023-11:18:20] [W] [TRT]  - Subnormal FP16 values detected. 
[08/10/2023-11:18:20] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.
[08/10/2023-11:18:20] [V] [TRT] =============== Computing costs for 
[08/10/2023-11:18:20] [V] [TRT] *************** Autotuning format combination: Float(16588800,32400,180,1) -> Float(64800,32400,180,1) ***************
[08/10/2023-11:18:20] [V] [TRT] *************** Autotuning format combination: Float(16588800,1,92160,512) -> Float(64800,1,360,2) ***************
[08/10/2023-11:18:20] [V] [TRT] *************** Autotuning format combination: Float(4147200,1:4,23040,128) -> Float(32400,1:4,180,1) ***************
[08/10/2023-11:18:20] [V] [TRT] *************** Autotuning format combination: Half(16588800,32400,180,1) -> Half(64800,32400,180,1) ***************
[08/10/2023-11:18:20] [V] [TRT] *************** Autotuning format combination: Half(8294400,32400:2,180,1) -> Half(32400,32400:2,180,1) ***************
[08/10/2023-11:18:20] [V] [TRT] *************** Autotuning format combination: Half(2073600,1:8,11520,64) -> Float(64800,32400,180,1) ***************
[08/10/2023-11:18:20] [V] [TRT] --------------- Timing Runner: Conv_130 (CaskConvolution)
[08/10/2023-11:18:20] [V] [TRT] CaskConvolution has no valid tactics for this config, skipping
[08/10/2023-11:18:20] [V] [TRT] *************** Autotuning format combination: Half(2073600,1:8,11520,64) -> Half(32400,1:8,180,1) ***************
[08/10/2023-11:18:20] [V] [TRT] *************** Autotuning format combination: Half(1036800,1:16,5760,32) -> Half(32400,1:16,180,1) ***************
[08/10/2023-11:18:20] [V] [TRT] =============== Computing costs for 
[08/10/2023-11:18:20] [V] [TRT] *************** Autotuning format combination: Float(16588800,32400,180,1) -> Float(32400,32400,180,1) ***************
[08/10/2023-11:18:20] [V] [TRT] *************** Autotuning format combination: Float(16588800,1,92160,512) -> Float(32400,1,180,1) ***************
[08/10/2023-11:18:20] [V] [TRT] *************** Autotuning format combination: Float(4147200,1:4,23040,128) -> Float(32400,1:4,180,1) ***************
[08/10/2023-11:18:20] [V] [TRT] *************** Autotuning format combination: Half(16588800,32400,180,1) -> Half(32400,32400,180,1) ***************
[08/10/2023-11:18:20] [V] [TRT] *************** Autotuning format combination: Half(8294400,32400:2,180,1) -> Half(32400,32400:2,180,1) ***************
[08/10/2023-11:18:20] [V] [TRT] *************** Autotuning format combination: Half(2073600,1:8,11520,64) -> Float(32400,32400,180,1) ***************
[08/10/2023-11:18:20] [V] [TRT] --------------- Timing Runner: Conv_133 (CaskConvolution)
[08/10/2023-11:18:20] [V] [TRT] CaskConvolution has no valid tactics for this config, skipping
[08/10/2023-11:18:20] [V] [TRT] *************** Autotuning format combination: Half(2073600,1:8,11520,64) -> Half(32400,1:8,180,1) ***************
[08/10/2023-11:18:21] [V] [TRT] *************** Autotuning format combination: Half(1036800,1:16,5760,32) -> Half(32400,1:16,180,1) ***************
[08/10/2023-11:18:21] [V] [TRT] =============== Computing costs for 
[08/10/2023-11:18:21] [V] [TRT] *************** Autotuning format combination: Float(2073600,32400,180,1) -> Float(16588800,32400,180,1) ***************
[08/10/2023-11:18:21] [V] [TRT] *************** Autotuning format combination: Float(2073600,1,11520,64) -> Float(16588800,1,92160,512) ***************
[08/10/2023-11:18:21] [V] [TRT] *************** Autotuning format combination: Float(518400,1:4,2880,16) -> Float(4147200,1:4,23040,128) ***************
[08/10/2023-11:18:21] [V] [TRT] *************** Autotuning format combination: Half(2073600,32400,180,1) -> Half(16588800,32400,180,1) ***************
[08/10/2023-11:18:21] [W] [TRT] Weights [name=Conv_134 + Relu_135 || Conv_137 + Relu_138 || Conv_140 + Relu_141 || Conv_143 + Relu_144 || Conv_146 + Relu_147 || Conv_149 + Relu_150 || Conv_152 + Relu_153 || Conv_155 + Relu_156.weight] had the following issues when converted to FP16:
[08/10/2023-11:18:21] [W] [TRT]  - Subnormal FP16 values detected. 
[08/10/2023-11:18:21] [W] [TRT]  - Values less than smallest positive FP16 Subnormal value detected. Converting to FP16 minimum subnormalized value. 
[08/10/2023-11:18:21] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.
[08/10/2023-11:18:21] [V] [TRT] *************** Autotuning format combination: Half(1036800,32400:2,180,1) -> Half(8294400,32400:2,180,1) ***************
[08/10/2023-11:18:21] [W] [TRT] Weights [name=Conv_134 + Relu_135 || Conv_137 + Relu_138 || Conv_140 + Relu_141 || Conv_143 + Relu_144 || Conv_146 + Relu_147 || Conv_149 + Relu_150 || Conv_152 + Relu_153 || Conv_155 + Relu_156.weight] had the following issues when converted to FP16:
[08/10/2023-11:18:21] [W] [TRT]  - Subnormal FP16 values detected. 
[08/10/2023-11:18:21] [W] [TRT]  - Values less than smallest positive FP16 Subnormal value detected. Converting to FP16 minimum subnormalized value. 
[08/10/2023-11:18:21] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.
[08/10/2023-11:18:21] [V] [TRT] *************** Autotuning format combination: Half(259200,1:8,1440,8) -> Float(16588800,32400,180,1) ***************
[08/10/2023-11:18:21] [W] [TRT] Weights [name=Conv_134 + Relu_135 || Conv_137 + Relu_138 || Conv_140 + Relu_141 || Conv_143 + Relu_144 || Conv_146 + Relu_147 || Conv_149 + Relu_150 || Conv_152 + Relu_153 || Conv_155 + Relu_156.weight] had the following issues when converted to FP16:
[08/10/2023-11:18:21] [W] [TRT]  - Subnormal FP16 values detected. 
[08/10/2023-11:18:21] [W] [TRT]  - Values less than smallest positive FP16 Subnormal value detected. Converting to FP16 minimum subnormalized value. 
[08/10/2023-11:18:21] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.
[08/10/2023-11:18:21] [V] [TRT] --------------- Timing Runner: Conv_134 + Relu_135 || Conv_137 + Relu_138 || Conv_140 + Relu_141 || Conv_143 + Relu_144 || Conv_146 + Relu_147 || Conv_149 + Relu_150 || Conv_152 + Relu_153 || Conv_155 + Relu_156 (CaskConvolution)
[08/10/2023-11:18:21] [V] [TRT] CaskConvolution has no valid tactics for this config, skipping
[08/10/2023-11:18:21] [V] [TRT] *************** Autotuning format combination: Half(259200,1:8,1440,8) -> Half(2073600,1:8,11520,64) ***************
[08/10/2023-11:18:21] [W] [TRT] Weights [name=Conv_134 + Relu_135 || Conv_137 + Relu_138 || Conv_140 + Relu_141 || Conv_143 + Relu_144 || Conv_146 + Relu_147 || Conv_149 + Relu_150 || Conv_152 + Relu_153 || Conv_155 + Relu_156.weight] had the following issues when converted to FP16:
[08/10/2023-11:18:21] [W] [TRT]  - Subnormal FP16 values detected. 
[08/10/2023-11:18:21] [W] [TRT]  - Values less than smallest positive FP16 Subnormal value detected. Converting to FP16 minimum subnormalized value. 
[08/10/2023-11:18:21] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.
[08/10/2023-11:18:21] [V] [TRT] *************** Autotuning format combination: Half(129600,1:16,720,4) -> Half(1036800,1:16,5760,32) ***************
[08/10/2023-11:18:21] [W] [TRT] Weights [name=Conv_134 + Relu_135 || Conv_137 + Relu_138 || Conv_140 + Relu_141 || Conv_143 + Relu_144 || Conv_146 + Relu_147 || Conv_149 + Relu_150 || Conv_152 + Relu_153 || Conv_155 + Relu_156.weight] had the following issues when converted to FP16:
[08/10/2023-11:18:21] [W] [TRT]  - Subnormal FP16 values detected. 
[08/10/2023-11:18:21] [W] [TRT]  - Values less than smallest positive FP16 Subnormal value detected. Converting to FP16 minimum subnormalized value. 
[08/10/2023-11:18:21] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.
[08/10/2023-11:18:21] [V] [TRT] =============== Computing costs for 
[08/10/2023-11:18:21] [V] [TRT] *************** Autotuning format combination: Float(16588800,32400,180,1) -> Float(64800,32400,180,1) ***************
[08/10/2023-11:18:21] [V] [TRT] *************** Autotuning format combination: Float(16588800,1,92160,512) -> Float(64800,1,360,2) ***************
[08/10/2023-11:18:21] [V] [TRT] *************** Autotuning format combination: Float(4147200,1:4,23040,128) -> Float(32400,1:4,180,1) ***************
[08/10/2023-11:18:21] [V] [TRT] *************** Autotuning format combination: Half(16588800,32400,180,1) -> Half(64800,32400,180,1) ***************
[08/10/2023-11:18:21] [V] [TRT] *************** Autotuning format combination: Half(8294400,32400:2,180,1) -> Half(32400,32400:2,180,1) ***************
[08/10/2023-11:18:21] [V] [TRT] *************** Autotuning format combination: Half(2073600,1:8,11520,64) -> Float(64800,32400,180,1) ***************
[08/10/2023-11:18:21] [V] [TRT] --------------- Timing Runner: Conv_136 (CaskConvolution)
[08/10/2023-11:18:21] [V] [TRT] CaskConvolution has no valid tactics for this config, skipping
[08/10/2023-11:18:21] [V] [TRT] *************** Autotuning format combination: Half(2073600,1:8,11520,64) -> Half(32400,1:8,180,1) ***************
[08/10/2023-11:18:21] [V] [TRT] *************** Autotuning format combination: Half(1036800,1:16,5760,32) -> Half(32400,1:16,180,1) ***************
[08/10/2023-11:18:21] [V] [TRT] =============== Computing costs for 
[08/10/2023-11:18:21] [V] [TRT] *************** Autotuning format combination: Float(16588800,32400,180,1) -> Float(32400,32400,180,1) ***************
[08/10/2023-11:18:21] [V] [TRT] *************** Autotuning format combination: Float(16588800,1,92160,512) -> Float(32400,1,180,1) ***************
[08/10/2023-11:18:21] [V] [TRT] *************** Autotuning format combination: Float(4147200,1:4,23040,128) -> Float(32400,1:4,180,1) ***************
[08/10/2023-11:18:21] [V] [TRT] *************** Autotuning format combination: Half(16588800,32400,180,1) -> Half(32400,32400,180,1) ***************
[08/10/2023-11:18:21] [V] [TRT] *************** Autotuning format combination: Half(8294400,32400:2,180,1) -> Half(32400,32400:2,180,1) ***************
[08/10/2023-11:18:21] [V] [TRT] *************** Autotuning format combination: Half(2073600,1:8,11520,64) -> Float(32400,32400,180,1) ***************
[08/10/2023-11:18:21] [V] [TRT] --------------- Timing Runner: Conv_139 (CaskConvolution)
[08/10/2023-11:18:21] [V] [TRT] CaskConvolution has no valid tactics for this config, skipping
[08/10/2023-11:18:21] [V] [TRT] *************** Autotuning format combination: Half(2073600,1:8,11520,64) -> Half(32400,1:8,180,1) ***************
[08/10/2023-11:18:21] [V] [TRT] *************** Autotuning format combination: Half(1036800,1:16,5760,32) -> Half(32400,1:16,180,1) ***************
[08/10/2023-11:18:21] [V] [TRT] =============== Computing costs for 
[08/10/2023-11:18:21] [V] [TRT] *************** Autotuning format combination: Float(16588800,32400,180,1) -> Float(97200,32400,180,1) ***************
[08/10/2023-11:18:21] [V] [TRT] *************** Autotuning format combination: Float(16588800,1,92160,512) -> Float(97200,1,540,3) ***************
[08/10/2023-11:18:21] [V] [TRT] *************** Autotuning format combination: Float(4147200,1:4,23040,128) -> Float(32400,1:4,180,1) ***************
[08/10/2023-11:18:21] [V] [TRT] *************** Autotuning format combination: Half(16588800,32400,180,1) -> Half(97200,32400,180,1) ***************
[08/10/2023-11:18:21] [V] [TRT] *************** Autotuning format combination: Half(8294400,32400:2,180,1) -> Half(64800,32400:2,180,1) ***************
[08/10/2023-11:18:21] [V] [TRT] *************** Autotuning format combination: Half(2073600,1:8,11520,64) -> Float(97200,32400,180,1) ***************
[08/10/2023-11:18:21] [V] [TRT] --------------- Timing Runner: Conv_142 (CaskConvolution)
[08/10/2023-11:18:21] [V] [TRT] CaskConvolution has no valid tactics for this config, skipping
[08/10/2023-11:18:21] [V] [TRT] *************** Autotuning format combination: Half(2073600,1:8,11520,64) -> Half(32400,1:8,180,1) ***************
[08/10/2023-11:18:21] [V] [TRT] *************** Autotuning format combination: Half(1036800,1:16,5760,32) -> Half(32400,1:16,180,1) ***************
[08/10/2023-11:18:21] [V] [TRT] =============== Computing costs for 
[08/10/2023-11:18:21] [V] [TRT] *************** Autotuning format combination: Float(16588800,32400,180,1) -> Float(64800,32400,180,1) ***************
[08/10/2023-11:18:21] [V] [TRT] *************** Autotuning format combination: Float(16588800,1,92160,512) -> Float(64800,1,360,2) ***************
[08/10/2023-11:18:21] [V] [TRT] *************** Autotuning format combination: Float(4147200,1:4,23040,128) -> Float(32400,1:4,180,1) ***************
[08/10/2023-11:18:21] [V] [TRT] *************** Autotuning format combination: Half(16588800,32400,180,1) -> Half(64800,32400,180,1) ***************
[08/10/2023-11:18:21] [W] [TRT] Weights [name=Conv_145.weight] had the following issues when converted to FP16:
[08/10/2023-11:18:21] [W] [TRT]  - Subnormal FP16 values detected. 
[08/10/2023-11:18:21] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.
[08/10/2023-11:18:21] [V] [TRT] *************** Autotuning format combination: Half(8294400,32400:2,180,1) -> Half(32400,32400:2,180,1) ***************
[08/10/2023-11:18:21] [W] [TRT] Weights [name=Conv_145.weight] had the following issues when converted to FP16:
[08/10/2023-11:18:21] [W] [TRT]  - Subnormal FP16 values detected. 
[08/10/2023-11:18:21] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.
[08/10/2023-11:18:21] [V] [TRT] *************** Autotuning format combination: Half(2073600,1:8,11520,64) -> Float(64800,32400,180,1) ***************
[08/10/2023-11:18:21] [W] [TRT] Weights [name=Conv_145.weight] had the following issues when converted to FP16:
[08/10/2023-11:18:21] [W] [TRT]  - Subnormal FP16 values detected. 
[08/10/2023-11:18:21] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.
[08/10/2023-11:18:21] [V] [TRT] --------------- Timing Runner: Conv_145 (CaskConvolution)
[08/10/2023-11:18:21] [V] [TRT] CaskConvolution has no valid tactics for this config, skipping
[08/10/2023-11:18:21] [V] [TRT] *************** Autotuning format combination: Half(2073600,1:8,11520,64) -> Half(32400,1:8,180,1) ***************
[08/10/2023-11:18:21] [W] [TRT] Weights [name=Conv_145.weight] had the following issues when converted to FP16:
[08/10/2023-11:18:21] [W] [TRT]  - Subnormal FP16 values detected. 
[08/10/2023-11:18:21] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.
[08/10/2023-11:18:21] [V] [TRT] *************** Autotuning format combination: Half(1036800,1:16,5760,32) -> Half(32400,1:16,180,1) ***************
[08/10/2023-11:18:21] [W] [TRT] Weights [name=Conv_145.weight] had the following issues when converted to FP16:
[08/10/2023-11:18:21] [W] [TRT]  - Subnormal FP16 values detected. 
[08/10/2023-11:18:21] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.
[08/10/2023-11:18:21] [V] [TRT] =============== Computing costs for 
[08/10/2023-11:18:21] [V] [TRT] *************** Autotuning format combination: Float(16588800,32400,180,1) -> Float(64800,32400,180,1) ***************
[08/10/2023-11:18:21] [V] [TRT] *************** Autotuning format combination: Float(16588800,1,92160,512) -> Float(64800,1,360,2) ***************
[08/10/2023-11:18:21] [V] [TRT] *************** Autotuning format combination: Float(4147200,1:4,23040,128) -> Float(32400,1:4,180,1) ***************
[08/10/2023-11:18:21] [V] [TRT] *************** Autotuning format combination: Half(16588800,32400,180,1) -> Half(64800,32400,180,1) ***************
[08/10/2023-11:18:21] [V] [TRT] *************** Autotuning format combination: Half(8294400,32400:2,180,1) -> Half(32400,32400:2,180,1) ***************
[08/10/2023-11:18:21] [V] [TRT] *************** Autotuning format combination: Half(2073600,1:8,11520,64) -> Float(64800,32400,180,1) ***************
[08/10/2023-11:18:21] [V] [TRT] --------------- Timing Runner: Conv_148 (CaskConvolution)
[08/10/2023-11:18:21] [V] [TRT] CaskConvolution has no valid tactics for this config, skipping
[08/10/2023-11:18:21] [V] [TRT] *************** Autotuning format combination: Half(2073600,1:8,11520,64) -> Half(32400,1:8,180,1) ***************
[08/10/2023-11:18:21] [V] [TRT] *************** Autotuning format combination: Half(1036800,1:16,5760,32) -> Half(32400,1:16,180,1) ***************
[08/10/2023-11:18:21] [V] [TRT] =============== Computing costs for 
[08/10/2023-11:18:21] [V] [TRT] *************** Autotuning format combination: Float(16588800,32400,180,1) -> Float(64800,32400,180,1) ***************
[08/10/2023-11:18:21] [V] [TRT] *************** Autotuning format combination: Float(16588800,1,92160,512) -> Float(64800,1,360,2) ***************
[08/10/2023-11:18:21] [V] [TRT] *************** Autotuning format combination: Float(4147200,1:4,23040,128) -> Float(32400,1:4,180,1) ***************
[08/10/2023-11:18:21] [V] [TRT] *************** Autotuning format combination: Half(16588800,32400,180,1) -> Half(64800,32400,180,1) ***************
[08/10/2023-11:18:21] [V] [TRT] *************** Autotuning format combination: Half(8294400,32400:2,180,1) -> Half(32400,32400:2,180,1) ***************
[08/10/2023-11:18:21] [V] [TRT] *************** Autotuning format combination: Half(2073600,1:8,11520,64) -> Float(64800,32400,180,1) ***************
[08/10/2023-11:18:21] [V] [TRT] --------------- Timing Runner: Conv_151 (CaskConvolution)
[08/10/2023-11:18:21] [V] [TRT] CaskConvolution has no valid tactics for this config, skipping
[08/10/2023-11:18:21] [V] [TRT] *************** Autotuning format combination: Half(2073600,1:8,11520,64) -> Half(32400,1:8,180,1) ***************
[08/10/2023-11:18:21] [V] [TRT] *************** Autotuning format combination: Half(1036800,1:16,5760,32) -> Half(32400,1:16,180,1) ***************
[08/10/2023-11:18:21] [V] [TRT] =============== Computing costs for 
[08/10/2023-11:18:21] [V] [TRT] *************** Autotuning format combination: Float(16588800,32400,180,1) -> Float(64800,32400,180,1) ***************
[08/10/2023-11:18:21] [V] [TRT] *************** Autotuning format combination: Float(16588800,1,92160,512) -> Float(64800,1,360,2) ***************
[08/10/2023-11:18:21] [V] [TRT] *************** Autotuning format combination: Float(4147200,1:4,23040,128) -> Float(32400,1:4,180,1) ***************
[08/10/2023-11:18:21] [V] [TRT] *************** Autotuning format combination: Half(16588800,32400,180,1) -> Half(64800,32400,180,1) ***************
[08/10/2023-11:18:21] [V] [TRT] *************** Autotuning format combination: Half(8294400,32400:2,180,1) -> Half(32400,32400:2,180,1) ***************
[08/10/2023-11:18:21] [V] [TRT] *************** Autotuning format combination: Half(2073600,1:8,11520,64) -> Float(64800,32400,180,1) ***************
[08/10/2023-11:18:21] [V] [TRT] --------------- Timing Runner: Conv_154 (CaskConvolution)
[08/10/2023-11:18:21] [V] [TRT] CaskConvolution has no valid tactics for this config, skipping
[08/10/2023-11:18:21] [V] [TRT] *************** Autotuning format combination: Half(2073600,1:8,11520,64) -> Half(32400,1:8,180,1) ***************
[08/10/2023-11:18:21] [V] [TRT] *************** Autotuning format combination: Half(1036800,1:16,5760,32) -> Half(32400,1:16,180,1) ***************
[08/10/2023-11:18:21] [V] [TRT] =============== Computing costs for 
[08/10/2023-11:18:21] [V] [TRT] *************** Autotuning format combination: Float(16588800,32400,180,1) -> Float(32400,32400,180,1) ***************
[08/10/2023-11:18:21] [V] [TRT] *************** Autotuning format combination: Float(16588800,1,92160,512) -> Float(32400,1,180,1) ***************
[08/10/2023-11:18:21] [V] [TRT] *************** Autotuning format combination: Float(4147200,1:4,23040,128) -> Float(32400,1:4,180,1) ***************
[08/10/2023-11:18:21] [V] [TRT] *************** Autotuning format combination: Half(16588800,32400,180,1) -> Half(32400,32400,180,1) ***************
[08/10/2023-11:18:21] [V] [TRT] *************** Autotuning format combination: Half(8294400,32400:2,180,1) -> Half(32400,32400:2,180,1) ***************
[08/10/2023-11:18:21] [V] [TRT] *************** Autotuning format combination: Half(2073600,1:8,11520,64) -> Float(32400,32400,180,1) ***************
[08/10/2023-11:18:21] [V] [TRT] --------------- Timing Runner: Conv_157 (CaskConvolution)
[08/10/2023-11:18:21] [V] [TRT] CaskConvolution has no valid tactics for this config, skipping
[08/10/2023-11:18:21] [V] [TRT] *************** Autotuning format combination: Half(2073600,1:8,11520,64) -> Half(32400,1:8,180,1) ***************
[08/10/2023-11:18:21] [V] [TRT] *************** Autotuning format combination: Half(1036800,1:16,5760,32) -> Half(32400,1:16,180,1) ***************
[08/10/2023-11:18:21] [V] [TRT] =============== Computing costs for 
[08/10/2023-11:18:21] [V] [TRT] *************** Autotuning format combination: Float(2073600,32400,180,1) -> Float(8294400,32400,180,1) ***************
[08/10/2023-11:18:21] [V] [TRT] --------------- Timing Runner: Conv_158 + Relu_159 || Conv_161 + Relu_162 || Conv_164 + Relu_165 || Conv_167 + Relu_168 (CudaDepthwiseConvolution)
[08/10/2023-11:18:21] [V] [TRT] CudaDepthwiseConvolution has no valid tactics for this config, skipping
[08/10/2023-11:18:21] [V] [TRT] --------------- Timing Runner: Conv_158 + Relu_159 || Conv_161 + Relu_162 || Conv_164 + Relu_165 || Conv_167 + Relu_168 (FusedConvActConvolution)
[08/10/2023-11:18:21] [V] [TRT] FusedConvActConvolution has no valid tactics for this config, skipping
[08/10/2023-11:18:21] [V] [TRT] --------------- Timing Runner: Conv_158 + Relu_159 || Conv_161 + Relu_162 || Conv_164 + Relu_165 || Conv_167 + Relu_168 (CudnnConvolution)
[08/10/2023-11:18:21] [V] [TRT] Tactic: 0x0000000000000000 Time: 6.40852
[08/10/2023-11:18:21] [V] [TRT] Tactic: 0x0000000000000001 Time: 2.56345
[08/10/2023-11:18:21] [V] [TRT] Tactic: 0x0000000000000002 Time: 9.9243
[08/10/2023-11:18:21] [V] [TRT] Tactic: 0x0000000000000004 skipped. Scratch requested: 8691449856, available: 4294967296
[08/10/2023-11:18:21] [V] [TRT] Tactic: 0x0000000000000005 Time: 25.2591
[08/10/2023-11:18:21] [V] [TRT] Tactic: 0x0000000000000006 Time: 4.49984
[08/10/2023-11:18:21] [V] [TRT] Tactic: 0x0000000000000038 Time: 6.56301
[08/10/2023-11:18:22] [V] [TRT] Tactic: 0x0000000000000039 Time: 2.48468
[08/10/2023-11:18:22] [V] [TRT] Tactic: 0x000000000000003a Time: 9.92811
[08/10/2023-11:18:22] [V] [TRT] Tactic: 0x000000000000003c skipped. Scratch requested: 8691449856, available: 4294967296
[08/10/2023-11:18:22] [V] [TRT] Tactic: 0x000000000000003d Time: 24.3622
[08/10/2023-11:18:22] [V] [TRT] Tactic: 0x000000000000003e Time: 4.20849
[08/10/2023-11:18:22] [V] [TRT] Tactic: 0x0000000000000070 Time: 6.24742
[08/10/2023-11:18:22] [V] [TRT] Tactic: 0x0000000000000071 Time: 6.1694
[08/10/2023-11:18:22] [V] [TRT] Tactic: 0x0000000000000072 Time: 9.2408
[08/10/2023-11:18:22] [V] [TRT] Tactic: 0x0000000000000074 skipped. Scratch requested: 8691449856, available: 4294967296
[08/10/2023-11:18:22] [V] [TRT] Tactic: 0x0000000000000075 Time: 17.737
[08/10/2023-11:18:22] [V] [TRT] Tactic: 0x0000000000000076 Time: 4.20448
[08/10/2023-11:18:22] [V] [TRT] Fastest Tactic: 0x0000000000000039 Time: 2.48468
[08/10/2023-11:18:22] [V] [TRT] Setting workspace to 8691449856enables more tactics for profiling
[08/10/2023-11:18:22] [V] [TRT] --------------- Timing Runner: Conv_158 + Relu_159 || Conv_161 + Relu_162 || Conv_164 + Relu_165 || Conv_167 + Relu_168 (CaskConvolution)
[08/10/2023-11:18:22] [V] [TRT] Conv_158 + Relu_159 || Conv_161 + Relu_162 || Conv_164 + Relu_165 || Conv_167 + Relu_168 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x128x8_stage3_warpsize1x4x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0x01cf8ce2da913006
[08/10/2023-11:18:22] [V] [TRT] Tactic: 0x01cf8ce2da913006 Time: 5.39782
[08/10/2023-11:18:22] [V] [TRT] Conv_158 + Relu_159 || Conv_161 + Relu_162 || Conv_164 + Relu_165 || Conv_167 + Relu_168 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x64x8_stage3_warpsize2x2x1_g1_ffma_aligna4_alignc4 Tactic: 0x12dbf7d94ee3696d
[08/10/2023-11:18:23] [V] [TRT] Tactic: 0x12dbf7d94ee3696d Time: 5.65459
[08/10/2023-11:18:23] [V] [TRT] Conv_158 + Relu_159 || Conv_161 + Relu_162 || Conv_164 + Relu_165 || Conv_167 + Relu_168 Set Tactic Name: ampere_scudnn_128x64_relu_small_nn_v1 Tactic: 0x3f243c490d502deb
[08/10/2023-11:18:23] [V] [TRT] Tactic: 0x3f243c490d502deb Time: 5.25792
[08/10/2023-11:18:23] [V] [TRT] Conv_158 + Relu_159 || Conv_161 + Relu_162 || Conv_164 + Relu_165 || Conv_167 + Relu_168 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0x4727434768e46395
[08/10/2023-11:18:23] [V] [TRT] Tactic: 0x4727434768e46395 Time: 6.21974
[08/10/2023-11:18:23] [V] [TRT] Conv_158 + Relu_159 || Conv_161 + Relu_162 || Conv_164 + Relu_165 || Conv_167 + Relu_168 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize256x128x8_stage3_warpsize4x2x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0x4efce38acc876f5c
[08/10/2023-11:18:23] [V] [TRT] Tactic: 0x4efce38acc876f5c Time: 5.24881
[08/10/2023-11:18:23] [V] [TRT] Conv_158 + Relu_159 || Conv_161 + Relu_162 || Conv_164 + Relu_165 || Conv_167 + Relu_168 Set Tactic Name: ampere_scudnn_128x128_relu_small_nn_v1 Tactic: 0x503619c69ae500ff
[08/10/2023-11:18:23] [V] [TRT] Tactic: 0x503619c69ae500ff Time: 4.73008
[08/10/2023-11:18:23] [V] [TRT] Conv_158 + Relu_159 || Conv_161 + Relu_162 || Conv_164 + Relu_165 || Conv_167 + Relu_168 Set Tactic Name: ampere_scudnn_128x128_relu_xregs_large_nn_v1 Tactic: 0x5403ad713f811a18
[08/10/2023-11:18:23] [V] [TRT] Tactic: 0x5403ad713f811a18 Time: 5.11953
[08/10/2023-11:18:23] [V] [TRT] Conv_158 + Relu_159 || Conv_161 + Relu_162 || Conv_164 + Relu_165 || Conv_167 + Relu_168 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x128x8_stage3_warpsize1x4x1_g1_ffma_aligna4_alignc4 Tactic: 0x5aa723e0481da855
[08/10/2023-11:18:23] [V] [TRT] Tactic: 0x5aa723e0481da855 Time: 5.34211
[08/10/2023-11:18:23] [V] [TRT] Conv_158 + Relu_159 || Conv_161 + Relu_162 || Conv_164 + Relu_165 || Conv_167 + Relu_168 Set Tactic Name: ampere_scudnn_128x64_relu_xregs_large_nn_v1 Tactic: 0x5deb29b7a8e275f7
[08/10/2023-11:18:23] [V] [TRT] Tactic: 0x5deb29b7a8e275f7 Time: 5.11523
[08/10/2023-11:18:23] [V] [TRT] Conv_158 + Relu_159 || Conv_161 + Relu_162 || Conv_164 + Relu_165 || Conv_167 + Relu_168 Set Tactic Name: ampere_scudnn_winograd_128x128_ldg1_ldg4_relu_tile148t_nt_v1 Tactic: 0x94119b4c514b211a
[08/10/2023-11:18:23] [V] [TRT] Tactic: 0x94119b4c514b211a Time: 3.19001
[08/10/2023-11:18:23] [V] [TRT] Conv_158 + Relu_159 || Conv_161 + Relu_162 || Conv_164 + Relu_165 || Conv_167 + Relu_168 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_aligna4_alignc4 Tactic: 0xa31d27de74b895ff
[08/10/2023-11:18:23] [V] [TRT] Tactic: 0xa31d27de74b895ff Time: 6.05112
[08/10/2023-11:18:23] [V] [TRT] Conv_158 + Relu_159 || Conv_161 + Relu_162 || Conv_164 + Relu_165 || Conv_167 + Relu_168 Set Tactic Name: ampere_scudnn_128x32_relu_small_nn_v1 Tactic: 0xa8609adc4e0ceb90
[08/10/2023-11:18:23] [V] [TRT] Tactic: 0xa8609adc4e0ceb90 Time: 5.92665
[08/10/2023-11:18:23] [V] [TRT] Conv_158 + Relu_159 || Conv_161 + Relu_162 || Conv_164 + Relu_165 || Conv_167 + Relu_168 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize256x128x8_stage3_warpsize4x2x1_g1_ffma_aligna4_alignc4 Tactic: 0xbb8c3889c7eacd30
[08/10/2023-11:18:23] [V] [TRT] Tactic: 0xbb8c3889c7eacd30 Time: 5.33379
[08/10/2023-11:18:23] [V] [TRT] Conv_158 + Relu_159 || Conv_161 + Relu_162 || Conv_164 + Relu_165 || Conv_167 + Relu_168 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_aligna4_alignc4 Tactic: 0xd828f024626fa982
[08/10/2023-11:18:23] [V] [TRT] Tactic: 0xd828f024626fa982 Time: 8.38834
[08/10/2023-11:18:23] [V] [TRT] Conv_158 + Relu_159 || Conv_161 + Relu_162 || Conv_164 + Relu_165 || Conv_167 + Relu_168 Set Tactic Name: ampere_scudnn_128x128_relu_medium_nn_v1 Tactic: 0xf067e6205da31c2e
[08/10/2023-11:18:23] [V] [TRT] Tactic: 0xf067e6205da31c2e Time: 4.79765
[08/10/2023-11:18:23] [V] [TRT] Conv_158 + Relu_159 || Conv_161 + Relu_162 || Conv_164 + Relu_165 || Conv_167 + Relu_168 Set Tactic Name: ampere_scudnn_128x64_relu_medium_nn_v1 Tactic: 0xf64396b97c889179
[08/10/2023-11:18:23] [V] [TRT] Tactic: 0xf64396b97c889179 Time: 5.46114
[08/10/2023-11:18:23] [V] [TRT] Fastest Tactic: 0x94119b4c514b211a Time: 3.19001
[08/10/2023-11:18:23] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CudnnConvolution Tactic: 0x0000000000000039
[08/10/2023-11:18:23] [V] [TRT] *************** Autotuning format combination: Float(2073600,1,11520,64) -> Float(8294400,1,46080,256) ***************
[08/10/2023-11:18:23] [V] [TRT] --------------- Timing Runner: Conv_158 + Relu_159 || Conv_161 + Relu_162 || Conv_164 + Relu_165 || Conv_167 + Relu_168 (CaskConvolution)
[08/10/2023-11:18:23] [V] [TRT] Conv_158 + Relu_159 || Conv_161 + Relu_162 || Conv_164 + Relu_165 || Conv_167 + Relu_168 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize64x128x32_stage5_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0x112d7e4d280f396d
[08/10/2023-11:18:23] [V] [TRT] Tactic: 0x112d7e4d280f396d Time: 1.62644
[08/10/2023-11:18:23] [V] [TRT] Conv_158 + Relu_159 || Conv_161 + Relu_162 || Conv_164 + Relu_165 || Conv_167 + Relu_168 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x64x32_stage5_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0x13cc2ad8dcd32ba2
[08/10/2023-11:18:23] [V] [TRT] Tactic: 0x13cc2ad8dcd32ba2 Time: 1.70153
[08/10/2023-11:18:23] [V] [TRT] Conv_158 + Relu_159 || Conv_161 + Relu_162 || Conv_164 + Relu_165 || Conv_167 + Relu_168 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_aligna4_alignc4 Tactic: 0x19b688348f983aa0
[08/10/2023-11:18:23] [V] [TRT] Tactic: 0x19b688348f983aa0 Time: 5.60178
[08/10/2023-11:18:23] [V] [TRT] Conv_158 + Relu_159 || Conv_161 + Relu_162 || Conv_164 + Relu_165 || Conv_167 + Relu_168 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x64x8_stage3_warpsize2x2x1_g1_ffma_aligna4_alignc4 Tactic: 0x1da91d865428f237
[08/10/2023-11:18:24] [V] [TRT] Tactic: 0x1da91d865428f237 Time: 4.53614
[08/10/2023-11:18:24] [V] [TRT] Conv_158 + Relu_159 || Conv_161 + Relu_162 || Conv_164 + Relu_165 || Conv_167 + Relu_168 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize256x128x32_stage3_warpsize4x2x1_g1_tensor16x8x8_t1r3s3 Tactic: 0x21246c8544eff903
[08/10/2023-11:18:24] [V] [TRT] Tactic: 0x21246c8544eff903 Time: 1.05091
[08/10/2023-11:18:24] [V] [TRT] Conv_158 + Relu_159 || Conv_161 + Relu_162 || Conv_164 + Relu_165 || Conv_167 + Relu_168 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x128x32_stage4_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0x25b2b9d5c9d5ca0d
[08/10/2023-11:18:24] [V] [TRT] Tactic: 0x25b2b9d5c9d5ca0d Time: 1.12009
[08/10/2023-11:18:24] [V] [TRT] Conv_158 + Relu_159 || Conv_161 + Relu_162 || Conv_164 + Relu_165 || Conv_167 + Relu_168 Set Tactic Name: ampere_scudnn_128x64_sliced1x2_ldg4_relu_exp_small_nhwc_tn_v1 Tactic: 0x27b316f52c109002
[08/10/2023-11:18:24] [V] [TRT] Tactic: 0x27b316f52c109002 Time: 5.23415
[08/10/2023-11:18:24] [V] [TRT] Conv_158 + Relu_159 || Conv_161 + Relu_162 || Conv_164 + Relu_165 || Conv_167 + Relu_168 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage4_warpsize2x2x1_g1_tensor16x8x8_t1r3s3 Tactic: 0x2d80d2b384ab13f1
[08/10/2023-11:18:24] [V] [TRT] Tactic: 0x2d80d2b384ab13f1 Time: 1.94928
[08/10/2023-11:18:24] [V] [TRT] Conv_158 + Relu_159 || Conv_161 + Relu_162 || Conv_164 + Relu_165 || Conv_167 + Relu_168 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize256x128x32_stage3_warpsize4x2x1_g1_tensor16x8x8 Tactic: 0x36303f399d3ee4fd
[08/10/2023-11:18:24] [V] [TRT] Tactic: 0x36303f399d3ee4fd Time: 1.2373
[08/10/2023-11:18:24] [V] [TRT] Conv_158 + Relu_159 || Conv_161 + Relu_162 || Conv_164 + Relu_165 || Conv_167 + Relu_168 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage4_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0x385bfab9332b1413
[08/10/2023-11:18:24] [V] [TRT] Tactic: 0x385bfab9332b1413 Time: 2.07579
[08/10/2023-11:18:24] [V] [TRT] Conv_158 + Relu_159 || Conv_161 + Relu_162 || Conv_164 + Relu_165 || Conv_167 + Relu_168 Set Tactic Name: ampere_scudnn_128x32_sliced1x4_ldg4_relu_exp_medium_nhwc_tn_v1 Tactic: 0x3e191488237fab8f
[08/10/2023-11:18:24] [V] [TRT] Tactic: 0x3e191488237fab8f Time: 5.62789
[08/10/2023-11:18:24] [V] [TRT] Conv_158 + Relu_159 || Conv_161 + Relu_162 || Conv_164 + Relu_165 || Conv_167 + Relu_168 Set Tactic Name: ampere_scudnn_128x32_sliced1x4_ldg4_relu_exp_small_nhwc_tn_v1 Tactic: 0x3e2b881168d9689d
[08/10/2023-11:18:24] [V] [TRT] Tactic: 0x3e2b881168d9689d Time: 5.52411
[08/10/2023-11:18:24] [V] [TRT] Conv_158 + Relu_159 || Conv_161 + Relu_162 || Conv_164 + Relu_165 || Conv_167 + Relu_168 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x256x8_stage3_warpsize1x4x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0x3f0c846d6379bc98
[08/10/2023-11:18:24] [V] [TRT] Tactic: 0x3f0c846d6379bc98 Time: 4.51973
[08/10/2023-11:18:24] [V] [TRT] Conv_158 + Relu_159 || Conv_161 + Relu_162 || Conv_164 + Relu_165 || Conv_167 + Relu_168 Set Tactic Name: ampere_scudnn_128x64_sliced1x2_ldg4_relu_exp_medium_nhwc_tn_v1 Tactic: 0x412c44dfeaf9161d
[08/10/2023-11:18:24] [V] [TRT] Tactic: 0x412c44dfeaf9161d Time: 5.25965
[08/10/2023-11:18:24] [V] [TRT] Conv_158 + Relu_159 || Conv_161 + Relu_162 || Conv_164 + Relu_165 || Conv_167 + Relu_168 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x256x32_stage3_warpsize2x4x1_g1_tensor16x8x8 Tactic: 0x482b48242255b8ce
[08/10/2023-11:18:24] [V] [TRT] Tactic: 0x482b48242255b8ce Time: 1.0607
[08/10/2023-11:18:24] [V] [TRT] Conv_158 + Relu_159 || Conv_161 + Relu_162 || Conv_164 + Relu_165 || Conv_167 + Relu_168 Set Tactic Name: ampere_scudnn_128x128_relu_exp_small_nhwc_tn_v1 Tactic: 0x5030121339a48bf3
[08/10/2023-11:18:24] [V] [TRT] Tactic: 0x5030121339a48bf3 Time: 4.74785
[08/10/2023-11:18:24] [V] [TRT] Conv_158 + Relu_159 || Conv_161 + Relu_162 || Conv_164 + Relu_165 || Conv_167 + Relu_168 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x256x32_stage3_warpsize2x4x1_g1_tensor16x8x8_t1r3s3 Tactic: 0x614e89f7852edbc3
[08/10/2023-11:18:24] [V] [TRT] Tactic: 0x614e89f7852edbc3 Time: 1.02573
[08/10/2023-11:18:24] [V] [TRT] Conv_158 + Relu_159 || Conv_161 + Relu_162 || Conv_164 + Relu_165 || Conv_167 + Relu_168 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_aligna4_alignc4 Tactic: 0x62835fce994f06dd
[08/10/2023-11:18:24] [V] [TRT] Tactic: 0x62835fce994f06dd Time: 4.82417
[08/10/2023-11:18:24] [V] [TRT] Conv_158 + Relu_159 || Conv_161 + Relu_162 || Conv_164 + Relu_165 || Conv_167 + Relu_168 Set Tactic Name: ampere_scudnn_128x128_ldg4_relu_exp_large_nhwc_tn_v1 Tactic: 0x634e99502974e4da
[08/10/2023-11:18:24] [V] [TRT] Tactic: 0x634e99502974e4da Time: 4.61489
[08/10/2023-11:18:24] [V] [TRT] Conv_158 + Relu_159 || Conv_161 + Relu_162 || Conv_164 + Relu_165 || Conv_167 + Relu_168 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0x65e41d81f093b482
[08/10/2023-11:18:24] [V] [TRT] Tactic: 0x65e41d81f093b482 Time: 1.16427
[08/10/2023-11:18:24] [V] [TRT] Conv_158 + Relu_159 || Conv_161 + Relu_162 || Conv_164 + Relu_165 || Conv_167 + Relu_168 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize64x128x32_stage5_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0x65f71607d20d5438
[08/10/2023-11:18:24] [V] [TRT] Tactic: 0x65f71607d20d5438 Time: 1.48831
[08/10/2023-11:18:24] [V] [TRT] Conv_158 + Relu_159 || Conv_161 + Relu_162 || Conv_164 + Relu_165 || Conv_167 + Relu_168 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x64x32_stage5_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0x6716429226d146f7
[08/10/2023-11:18:24] [V] [TRT] Tactic: 0x6716429226d146f7 Time: 1.59597
[08/10/2023-11:18:24] [V] [TRT] Conv_158 + Relu_159 || Conv_161 + Relu_162 || Conv_164 + Relu_165 || Conv_167 + Relu_168 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize64x32x64_stage5_warpsize2x2x1_g1_tensor16x8x8_t1r3s3 Tactic: 0x679a281f7660c436
[08/10/2023-11:18:24] [V] [TRT] Tactic: 0x679a281f7660c436 Time: 3.1305
[08/10/2023-11:18:24] [V] [TRT] Conv_158 + Relu_159 || Conv_161 + Relu_162 || Conv_164 + Relu_165 || Conv_167 + Relu_168 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize64x32x64_stage5_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0x6e2a3c7a7fc5e4e1
[08/10/2023-11:18:24] [V] [TRT] Tactic: 0x6e2a3c7a7fc5e4e1 Time: 3.28622
[08/10/2023-11:18:24] [V] [TRT] Conv_158 + Relu_159 || Conv_161 + Relu_162 || Conv_164 + Relu_165 || Conv_167 + Relu_168 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x64x32_stage5_warpsize2x2x1_g1_tensor16x8x8_t1r3s3 Tactic: 0x7aa21282feb15f1a
[08/10/2023-11:18:24] [V] [TRT] Tactic: 0x7aa21282feb15f1a Time: 1.54826
[08/10/2023-11:18:24] [V] [TRT] Conv_158 + Relu_159 || Conv_161 + Relu_162 || Conv_164 + Relu_165 || Conv_167 + Relu_168 Set Tactic Name: ampere_scudnn_128x128_ldg4_relu_exp_medium_nhwc_tn_v1 Tactic: 0x7bc32c782b800c48
[08/10/2023-11:18:24] [V] [TRT] Tactic: 0x7bc32c782b800c48 Time: 4.58347
[08/10/2023-11:18:24] [V] [TRT] Conv_158 + Relu_159 || Conv_161 + Relu_162 || Conv_164 + Relu_165 || Conv_167 + Relu_168 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize256x64x8_stage3_warpsize2x2x1_g1_ffma_aligna4_alignc4 Tactic: 0x8014228ec08b4d49
[08/10/2023-11:18:24] [V] [TRT] Tactic: 0x8014228ec08b4d49 Time: 4.51249
[08/10/2023-11:18:24] [V] [TRT] Conv_158 + Relu_159 || Conv_161 + Relu_162 || Conv_164 + Relu_165 || Conv_167 + Relu_168 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage4_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0x818413b69874bb35
[08/10/2023-11:18:24] [V] [TRT] Tactic: 0x818413b69874bb35 Time: 2.10116
[08/10/2023-11:18:24] [V] [TRT] Conv_158 + Relu_159 || Conv_161 + Relu_162 || Conv_164 + Relu_165 || Conv_167 + Relu_168 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0x94a7db94ba744c45
[08/10/2023-11:18:25] [V] [TRT] Tactic: 0x94a7db94ba744c45 Time: 4.88786
[08/10/2023-11:18:25] [V] [TRT] Conv_158 + Relu_159 || Conv_161 + Relu_162 || Conv_164 + Relu_165 || Conv_167 + Relu_168 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x256x32_stage3_warpsize2x4x1_g1_tensor16x8x8 Tactic: 0x998c97842e775a17
[08/10/2023-11:18:25] [V] [TRT] Tactic: 0x998c97842e775a17 Time: 1.0738
[08/10/2023-11:18:25] [V] [TRT] Conv_158 + Relu_159 || Conv_161 + Relu_162 || Conv_164 + Relu_165 || Conv_167 + Relu_168 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8_t1r3s3 Tactic: 0x999e005e3b016ea6
[08/10/2023-11:18:25] [V] [TRT] Tactic: 0x999e005e3b016ea6 Time: 1.07383
[08/10/2023-11:18:25] [V] [TRT] Conv_158 + Relu_159 || Conv_161 + Relu_162 || Conv_164 + Relu_165 || Conv_167 + Relu_168 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize256x64x32_stage3_warpsize4x2x1_g1_tensor16x8x8_t1r3s3 Tactic: 0xa9a06d0633580c0c
[08/10/2023-11:18:25] [V] [TRT] Tactic: 0xa9a06d0633580c0c Time: 1.2455
[08/10/2023-11:18:25] [V] [TRT] Conv_158 + Relu_159 || Conv_161 + Relu_162 || Conv_164 + Relu_165 || Conv_167 + Relu_168 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0xb443c221fcb1565b
[08/10/2023-11:18:25] [V] [TRT] Tactic: 0xb443c221fcb1565b Time: 1.10566
[08/10/2023-11:18:25] [V] [TRT] Conv_158 + Relu_159 || Conv_161 + Relu_162 || Conv_164 + Relu_165 || Conv_167 + Relu_168 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage3_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0xb47d7d926de02dee
[08/10/2023-11:18:25] [V] [TRT] Tactic: 0xb47d7d926de02dee Time: 1.60679
[08/10/2023-11:18:25] [V] [TRT] Conv_158 + Relu_159 || Conv_161 + Relu_162 || Conv_164 + Relu_165 || Conv_167 + Relu_168 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize256x64x32_stage3_warpsize4x2x1_g1_tensor16x8x8 Tactic: 0xba0185279ccb2b85
[08/10/2023-11:18:25] [V] [TRT] Tactic: 0xba0185279ccb2b85 Time: 1.40582
[08/10/2023-11:18:25] [V] [TRT] Conv_158 + Relu_159 || Conv_161 + Relu_162 || Conv_164 + Relu_165 || Conv_167 + Relu_168 Set Tactic Name: ampere_scudnn_128x64_sliced1x2_ldg4_relu_exp_large_nhwc_tn_v1 Tactic: 0xbdfdef6b84f7ccc9
[08/10/2023-11:18:25] [V] [TRT] Tactic: 0xbdfdef6b84f7ccc9 Time: 5.20218
[08/10/2023-11:18:25] [V] [TRT] Conv_158 + Relu_159 || Conv_161 + Relu_162 || Conv_164 + Relu_165 || Conv_167 + Relu_168 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage3_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0xc0a715d897e240bb
[08/10/2023-11:18:25] [V] [TRT] Tactic: 0xc0a715d897e240bb Time: 1.62172
[08/10/2023-11:18:25] [V] [TRT] Conv_158 + Relu_159 || Conv_161 + Relu_162 || Conv_164 + Relu_165 || Conv_167 + Relu_168 Set Tactic Name: ampere_scudnn_128x128_relu_exp_large_nhwc_tn_v1 Tactic: 0xca7eeb8d9143d738
[08/10/2023-11:18:25] [V] [TRT] Tactic: 0xca7eeb8d9143d738 Time: 4.77865
[08/10/2023-11:18:25] [V] [TRT] Conv_158 + Relu_159 || Conv_161 + Relu_162 || Conv_164 + Relu_165 || Conv_167 + Relu_168 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize256x64x32_stage3_warpsize4x2x1_g1_tensor16x8x8 Tactic: 0xcedbed6d66c946d0
[08/10/2023-11:18:25] [V] [TRT] Tactic: 0xcedbed6d66c946d0 Time: 1.30299
[08/10/2023-11:18:25] [V] [TRT] Conv_158 + Relu_159 || Conv_161 + Relu_162 || Conv_164 + Relu_165 || Conv_167 + Relu_168 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x128x8_stage3_warpsize1x4x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0xd15dd11d64344e83
[08/10/2023-11:18:25] [V] [TRT] Tactic: 0xd15dd11d64344e83 Time: 4.50011
[08/10/2023-11:18:25] [V] [TRT] Conv_158 + Relu_159 || Conv_161 + Relu_162 || Conv_164 + Relu_165 || Conv_167 + Relu_168 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize64x128x32_stage5_warpsize2x2x1_g1_tensor16x8x8_t1r3s3 Tactic: 0xd4428a7355769d7d
[08/10/2023-11:18:25] [V] [TRT] Tactic: 0xd4428a7355769d7d Time: 1.42524
[08/10/2023-11:18:25] [V] [TRT] Conv_158 + Relu_159 || Conv_161 + Relu_162 || Conv_164 + Relu_165 || Conv_167 + Relu_168 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize64x32x64_stage5_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0xd7f5d575d49a4bc7
[08/10/2023-11:18:25] [V] [TRT] Tactic: 0xd7f5d575d49a4bc7 Time: 3.53908
[08/10/2023-11:18:25] [V] [TRT] Conv_158 + Relu_159 || Conv_161 + Relu_162 || Conv_164 + Relu_165 || Conv_167 + Relu_168 Set Tactic Name: ampere_scudnn_128x128_relu_exp_medium_nhwc_tn_v1 Tactic: 0xd9031472c05adf51
[08/10/2023-11:18:25] [V] [TRT] Tactic: 0xd9031472c05adf51 Time: 4.77063
[08/10/2023-11:18:25] [V] [TRT] Conv_158 + Relu_159 || Conv_161 + Relu_162 || Conv_164 + Relu_165 || Conv_167 + Relu_168 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x128x32_stage4_warpsize2x2x1_g1_tensor16x8x8_t1r3s3 Tactic: 0xd920b33c9bd27143
[08/10/2023-11:18:25] [V] [TRT] Tactic: 0xd920b33c9bd27143 Time: 1.0819
[08/10/2023-11:18:25] [V] [TRT] Conv_158 + Relu_159 || Conv_161 + Relu_162 || Conv_164 + Relu_165 || Conv_167 + Relu_168 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize256x128x32_stage3_warpsize4x2x1_g1_tensor16x8x8 Tactic: 0xe797e099911c0624
[08/10/2023-11:18:25] [V] [TRT] Tactic: 0xe797e099911c0624 Time: 1.0964
[08/10/2023-11:18:25] [V] [TRT] Conv_158 + Relu_159 || Conv_161 + Relu_162 || Conv_164 + Relu_165 || Conv_167 + Relu_168 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage3_warpsize2x2x1_g1_tensor16x8x8_t1r3s3 Tactic: 0xeb65d4c1e8fa2294
[08/10/2023-11:18:25] [V] [TRT] Tactic: 0xeb65d4c1e8fa2294 Time: 1.55213
[08/10/2023-11:18:25] [V] [TRT] Conv_158 + Relu_159 || Conv_161 + Relu_162 || Conv_164 + Relu_165 || Conv_167 + Relu_168 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x128x32_stage4_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0xf4156675c5f728d4
[08/10/2023-11:18:25] [V] [TRT] Tactic: 0xf4156675c5f728d4 Time: 1.62999
[08/10/2023-11:18:25] [V] [TRT] Conv_158 + Relu_159 || Conv_161 + Relu_162 || Conv_164 + Relu_165 || Conv_167 + Relu_168 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x64x8_stage3_warpsize2x2x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0xf48db81f02eca9ee
[08/10/2023-11:18:25] [V] [TRT] Tactic: 0xf48db81f02eca9ee Time: 4.47443
[08/10/2023-11:18:25] [V] [TRT] Conv_158 + Relu_159 || Conv_161 + Relu_162 || Conv_164 + Relu_165 || Conv_167 + Relu_168 Set Tactic Name: ampere_scudnn_128x128_ldg4_relu_exp_small_nhwc_tn_v1 Tactic: 0xf90060ce8193b811
[08/10/2023-11:18:25] [V] [TRT] Tactic: 0xf90060ce8193b811 Time: 4.59002
[08/10/2023-11:18:25] [V] [TRT] Fastest Tactic: 0x614e89f7852edbc3 Time: 1.02573
[08/10/2023-11:18:25] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x614e89f7852edbc3
[08/10/2023-11:18:25] [V] [TRT] *************** Autotuning format combination: Float(518400,1:4,2880,16) -> Float(2073600,1:4,11520,64) ***************
[08/10/2023-11:18:25] [V] [TRT] --------------- Timing Runner: Conv_158 + Relu_159 || Conv_161 + Relu_162 || Conv_164 + Relu_165 || Conv_167 + Relu_168 (CaskConvolution)
[08/10/2023-11:18:25] [V] [TRT] Conv_158 + Relu_159 || Conv_161 + Relu_162 || Conv_164 + Relu_165 || Conv_167 + Relu_168 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize64x128x32_stage5_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0x112d7e4d280f396d
[08/10/2023-11:18:25] [V] [TRT] Tactic: 0x112d7e4d280f396d Time: 1.62073
[08/10/2023-11:18:25] [V] [TRT] Conv_158 + Relu_159 || Conv_161 + Relu_162 || Conv_164 + Relu_165 || Conv_167 + Relu_168 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x64x32_stage5_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0x13cc2ad8dcd32ba2
[08/10/2023-11:18:25] [V] [TRT] Tactic: 0x13cc2ad8dcd32ba2 Time: 1.70083
[08/10/2023-11:18:25] [V] [TRT] Conv_158 + Relu_159 || Conv_161 + Relu_162 || Conv_164 + Relu_165 || Conv_167 + Relu_168 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize256x128x32_stage3_warpsize4x2x1_g1_tensor16x8x8_t1r3s3 Tactic: 0x21246c8544eff903
[08/10/2023-11:18:25] [V] [TRT] Tactic: 0x21246c8544eff903 Time: 1.07249
[08/10/2023-11:18:25] [V] [TRT] Conv_158 + Relu_159 || Conv_161 + Relu_162 || Conv_164 + Relu_165 || Conv_167 + Relu_168 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x128x32_stage4_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0x25b2b9d5c9d5ca0d
[08/10/2023-11:18:25] [V] [TRT] Tactic: 0x25b2b9d5c9d5ca0d Time: 1.14789
[08/10/2023-11:18:25] [V] [TRT] Conv_158 + Relu_159 || Conv_161 + Relu_162 || Conv_164 + Relu_165 || Conv_167 + Relu_168 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage4_warpsize2x2x1_g1_tensor16x8x8_t1r3s3 Tactic: 0x2d80d2b384ab13f1
[08/10/2023-11:18:25] [V] [TRT] Tactic: 0x2d80d2b384ab13f1 Time: 1.99688
[08/10/2023-11:18:25] [V] [TRT] Conv_158 + Relu_159 || Conv_161 + Relu_162 || Conv_164 + Relu_165 || Conv_167 + Relu_168 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize256x128x32_stage3_warpsize4x2x1_g1_tensor16x8x8 Tactic: 0x36303f399d3ee4fd
[08/10/2023-11:18:25] [V] [TRT] Tactic: 0x36303f399d3ee4fd Time: 1.23824
[08/10/2023-11:18:25] [V] [TRT] Conv_158 + Relu_159 || Conv_161 + Relu_162 || Conv_164 + Relu_165 || Conv_167 + Relu_168 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage4_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0x385bfab9332b1413
[08/10/2023-11:18:25] [V] [TRT] Tactic: 0x385bfab9332b1413 Time: 2.08349
[08/10/2023-11:18:25] [V] [TRT] Conv_158 + Relu_159 || Conv_161 + Relu_162 || Conv_164 + Relu_165 || Conv_167 + Relu_168 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x256x32_stage3_warpsize2x4x1_g1_tensor16x8x8 Tactic: 0x482b48242255b8ce
[08/10/2023-11:18:25] [V] [TRT] Tactic: 0x482b48242255b8ce Time: 1.04085
[08/10/2023-11:18:25] [V] [TRT] Conv_158 + Relu_159 || Conv_161 + Relu_162 || Conv_164 + Relu_165 || Conv_167 + Relu_168 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x256x32_stage3_warpsize2x4x1_g1_tensor16x8x8_t1r3s3 Tactic: 0x614e89f7852edbc3
[08/10/2023-11:18:25] [V] [TRT] Tactic: 0x614e89f7852edbc3 Time: 1.02487
[08/10/2023-11:18:25] [V] [TRT] Conv_158 + Relu_159 || Conv_161 + Relu_162 || Conv_164 + Relu_165 || Conv_167 + Relu_168 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0x65e41d81f093b482
[08/10/2023-11:18:25] [V] [TRT] Tactic: 0x65e41d81f093b482 Time: 1.16672
[08/10/2023-11:18:25] [V] [TRT] Conv_158 + Relu_159 || Conv_161 + Relu_162 || Conv_164 + Relu_165 || Conv_167 + Relu_168 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize64x128x32_stage5_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0x65f71607d20d5438
[08/10/2023-11:18:25] [V] [TRT] Tactic: 0x65f71607d20d5438 Time: 1.47625
[08/10/2023-11:18:25] [V] [TRT] Conv_158 + Relu_159 || Conv_161 + Relu_162 || Conv_164 + Relu_165 || Conv_167 + Relu_168 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x64x32_stage5_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0x6716429226d146f7
[08/10/2023-11:18:26] [V] [TRT] Tactic: 0x6716429226d146f7 Time: 1.56
[08/10/2023-11:18:26] [V] [TRT] Conv_158 + Relu_159 || Conv_161 + Relu_162 || Conv_164 + Relu_165 || Conv_167 + Relu_168 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize64x32x64_stage5_warpsize2x2x1_g1_tensor16x8x8_t1r3s3 Tactic: 0x679a281f7660c436
[08/10/2023-11:18:26] [V] [TRT] Tactic: 0x679a281f7660c436 Time: 3.15904
[08/10/2023-11:18:26] [V] [TRT] Conv_158 + Relu_159 || Conv_161 + Relu_162 || Conv_164 + Relu_165 || Conv_167 + Relu_168 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize64x32x64_stage5_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0x6e2a3c7a7fc5e4e1
[08/10/2023-11:18:26] [V] [TRT] Tactic: 0x6e2a3c7a7fc5e4e1 Time: 3.37605
[08/10/2023-11:18:26] [V] [TRT] Conv_158 + Relu_159 || Conv_161 + Relu_162 || Conv_164 + Relu_165 || Conv_167 + Relu_168 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x64x32_stage5_warpsize2x2x1_g1_tensor16x8x8_t1r3s3 Tactic: 0x7aa21282feb15f1a
[08/10/2023-11:18:26] [V] [TRT] Tactic: 0x7aa21282feb15f1a Time: 1.54965
[08/10/2023-11:18:26] [V] [TRT] Conv_158 + Relu_159 || Conv_161 + Relu_162 || Conv_164 + Relu_165 || Conv_167 + Relu_168 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage4_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0x818413b69874bb35
[08/10/2023-11:18:26] [V] [TRT] Tactic: 0x818413b69874bb35 Time: 2.09444
[08/10/2023-11:18:26] [V] [TRT] Conv_158 + Relu_159 || Conv_161 + Relu_162 || Conv_164 + Relu_165 || Conv_167 + Relu_168 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x256x32_stage3_warpsize2x4x1_g1_tensor16x8x8 Tactic: 0x998c97842e775a17
[08/10/2023-11:18:26] [V] [TRT] Tactic: 0x998c97842e775a17 Time: 1.06722
[08/10/2023-11:18:26] [V] [TRT] Conv_158 + Relu_159 || Conv_161 + Relu_162 || Conv_164 + Relu_165 || Conv_167 + Relu_168 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8_t1r3s3 Tactic: 0x999e005e3b016ea6
[08/10/2023-11:18:26] [V] [TRT] Tactic: 0x999e005e3b016ea6 Time: 1.074
[08/10/2023-11:18:26] [V] [TRT] Conv_158 + Relu_159 || Conv_161 + Relu_162 || Conv_164 + Relu_165 || Conv_167 + Relu_168 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize256x64x32_stage3_warpsize4x2x1_g1_tensor16x8x8_t1r3s3 Tactic: 0xa9a06d0633580c0c
[08/10/2023-11:18:26] [V] [TRT] Tactic: 0xa9a06d0633580c0c Time: 1.24795
[08/10/2023-11:18:26] [V] [TRT] Conv_158 + Relu_159 || Conv_161 + Relu_162 || Conv_164 + Relu_165 || Conv_167 + Relu_168 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0xb443c221fcb1565b
[08/10/2023-11:18:26] [V] [TRT] Tactic: 0xb443c221fcb1565b Time: 1.10563
[08/10/2023-11:18:26] [V] [TRT] Conv_158 + Relu_159 || Conv_161 + Relu_162 || Conv_164 + Relu_165 || Conv_167 + Relu_168 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage3_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0xb47d7d926de02dee
[08/10/2023-11:18:26] [V] [TRT] Tactic: 0xb47d7d926de02dee Time: 1.63021
[08/10/2023-11:18:26] [V] [TRT] Conv_158 + Relu_159 || Conv_161 + Relu_162 || Conv_164 + Relu_165 || Conv_167 + Relu_168 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize256x64x32_stage3_warpsize4x2x1_g1_tensor16x8x8 Tactic: 0xba0185279ccb2b85
[08/10/2023-11:18:26] [V] [TRT] Tactic: 0xba0185279ccb2b85 Time: 1.40462
[08/10/2023-11:18:26] [V] [TRT] Conv_158 + Relu_159 || Conv_161 + Relu_162 || Conv_164 + Relu_165 || Conv_167 + Relu_168 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage3_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0xc0a715d897e240bb
[08/10/2023-11:18:26] [V] [TRT] Tactic: 0xc0a715d897e240bb Time: 1.61997
[08/10/2023-11:18:26] [V] [TRT] Conv_158 + Relu_159 || Conv_161 + Relu_162 || Conv_164 + Relu_165 || Conv_167 + Relu_168 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize256x64x32_stage3_warpsize4x2x1_g1_tensor16x8x8 Tactic: 0xcedbed6d66c946d0
[08/10/2023-11:18:26] [V] [TRT] Tactic: 0xcedbed6d66c946d0 Time: 1.30491
[08/10/2023-11:18:26] [V] [TRT] Conv_158 + Relu_159 || Conv_161 + Relu_162 || Conv_164 + Relu_165 || Conv_167 + Relu_168 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize64x128x32_stage5_warpsize2x2x1_g1_tensor16x8x8_t1r3s3 Tactic: 0xd4428a7355769d7d
[08/10/2023-11:18:26] [V] [TRT] Tactic: 0xd4428a7355769d7d Time: 1.41621
[08/10/2023-11:18:26] [V] [TRT] Conv_158 + Relu_159 || Conv_161 + Relu_162 || Conv_164 + Relu_165 || Conv_167 + Relu_168 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize64x32x64_stage5_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0xd7f5d575d49a4bc7
[08/10/2023-11:18:26] [V] [TRT] Tactic: 0xd7f5d575d49a4bc7 Time: 3.5334
[08/10/2023-11:18:26] [V] [TRT] Conv_158 + Relu_159 || Conv_161 + Relu_162 || Conv_164 + Relu_165 || Conv_167 + Relu_168 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x128x32_stage4_warpsize2x2x1_g1_tensor16x8x8_t1r3s3 Tactic: 0xd920b33c9bd27143
[08/10/2023-11:18:26] [V] [TRT] Tactic: 0xd920b33c9bd27143 Time: 1.08033
[08/10/2023-11:18:26] [V] [TRT] Conv_158 + Relu_159 || Conv_161 + Relu_162 || Conv_164 + Relu_165 || Conv_167 + Relu_168 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize256x128x32_stage3_warpsize4x2x1_g1_tensor16x8x8 Tactic: 0xe797e099911c0624
[08/10/2023-11:18:26] [V] [TRT] Tactic: 0xe797e099911c0624 Time: 1.08914
[08/10/2023-11:18:26] [V] [TRT] Conv_158 + Relu_159 || Conv_161 + Relu_162 || Conv_164 + Relu_165 || Conv_167 + Relu_168 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage3_warpsize2x2x1_g1_tensor16x8x8_t1r3s3 Tactic: 0xeb65d4c1e8fa2294
[08/10/2023-11:18:26] [V] [TRT] Tactic: 0xeb65d4c1e8fa2294 Time: 1.55549
[08/10/2023-11:18:26] [V] [TRT] Conv_158 + Relu_159 || Conv_161 + Relu_162 || Conv_164 + Relu_165 || Conv_167 + Relu_168 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x128x32_stage4_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0xf4156675c5f728d4
[08/10/2023-11:18:26] [V] [TRT] Tactic: 0xf4156675c5f728d4 Time: 1.59645
[08/10/2023-11:18:26] [V] [TRT] Fastest Tactic: 0x614e89f7852edbc3 Time: 1.02487
[08/10/2023-11:18:26] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x614e89f7852edbc3
[08/10/2023-11:18:26] [V] [TRT] *************** Autotuning format combination: Half(2073600,32400,180,1) -> Half(8294400,32400,180,1) ***************
[08/10/2023-11:18:26] [W] [TRT] Weights [name=Conv_158 + Relu_159 || Conv_161 + Relu_162 || Conv_164 + Relu_165 || Conv_167 + Relu_168.weight] had the following issues when converted to FP16:
[08/10/2023-11:18:26] [W] [TRT]  - Subnormal FP16 values detected. 
[08/10/2023-11:18:26] [W] [TRT]  - Values less than smallest positive FP16 Subnormal value detected. Converting to FP16 minimum subnormalized value. 
[08/10/2023-11:18:26] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.
[08/10/2023-11:18:26] [V] [TRT] --------------- Timing Runner: Conv_158 + Relu_159 || Conv_161 + Relu_162 || Conv_164 + Relu_165 || Conv_167 + Relu_168 (CudnnConvolution)
[08/10/2023-11:18:26] [V] [TRT] Tactic: 0x0000000000000000 Time: 7.62114
[08/10/2023-11:18:26] [V] [TRT] Tactic: 0x0000000000000001 Time: 6.39419
[08/10/2023-11:18:26] [V] [TRT] Tactic: 0x0000000000000002 Time: 7.7937
[08/10/2023-11:18:26] [V] [TRT] Tactic: 0x0000000000000004 skipped. Scratch requested: 8691449856, available: 4294967296
[08/10/2023-11:18:26] [V] [TRT] Tactic: 0x0000000000000005 Time: 25.0845
[08/10/2023-11:18:26] [V] [TRT] Tactic: 0x0000000000000006 Time: 5.19648
[08/10/2023-11:18:27] [V] [TRT] Tactic: 0x0000000000000038 Time: 7.52686
[08/10/2023-11:18:27] [V] [TRT] Tactic: 0x000000000000003a Time: 7.76931
[08/10/2023-11:18:27] [V] [TRT] Tactic: 0x000000000000003c skipped. Scratch requested: 8691449856, available: 4294967296
[08/10/2023-11:18:27] [V] [TRT] Tactic: 0x000000000000003d Time: 24.7462
[08/10/2023-11:18:27] [V] [TRT] Tactic: 0x000000000000003e Time: 4.81783
[08/10/2023-11:18:27] [V] [TRT] Fastest Tactic: 0x000000000000003e Time: 4.81783
[08/10/2023-11:18:27] [V] [TRT] Setting workspace to 8691449856enables more tactics for profiling
[08/10/2023-11:18:27] [W] [TRT] Weights [name=Conv_158 + Relu_159 || Conv_161 + Relu_162 || Conv_164 + Relu_165 || Conv_167 + Relu_168.weight] had the following issues when converted to FP16:
[08/10/2023-11:18:27] [W] [TRT]  - Subnormal FP16 values detected. 
[08/10/2023-11:18:27] [W] [TRT]  - Values less than smallest positive FP16 Subnormal value detected. Converting to FP16 minimum subnormalized value. 
[08/10/2023-11:18:27] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.
[08/10/2023-11:18:27] [V] [TRT] --------------- Timing Runner: Conv_158 + Relu_159 || Conv_161 + Relu_162 || Conv_164 + Relu_165 || Conv_167 + Relu_168 (CaskConvolution)
[08/10/2023-11:18:27] [V] [TRT] CaskConvolution has no valid tactics for this config, skipping
[08/10/2023-11:18:27] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CudnnConvolution Tactic: 0x000000000000003e
[08/10/2023-11:18:27] [V] [TRT] *************** Autotuning format combination: Half(1036800,32400:2,180,1) -> Half(4147200,32400:2,180,1) ***************
[08/10/2023-11:18:27] [W] [TRT] Weights [name=Conv_158 + Relu_159 || Conv_161 + Relu_162 || Conv_164 + Relu_165 || Conv_167 + Relu_168.weight] had the following issues when converted to FP16:
[08/10/2023-11:18:27] [W] [TRT]  - Subnormal FP16 values detected. 
[08/10/2023-11:18:27] [W] [TRT]  - Values less than smallest positive FP16 Subnormal value detected. Converting to FP16 minimum subnormalized value. 
[08/10/2023-11:18:27] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.
[08/10/2023-11:18:27] [V] [TRT] --------------- Timing Runner: Conv_158 + Relu_159 || Conv_161 + Relu_162 || Conv_164 + Relu_165 || Conv_167 + Relu_168 (FusedConvActConvolution)
[08/10/2023-11:18:27] [V] [TRT] FusedConvActConvolution has no valid tactics for this config, skipping
[08/10/2023-11:18:27] [W] [TRT] Weights [name=Conv_158 + Relu_159 || Conv_161 + Relu_162 || Conv_164 + Relu_165 || Conv_167 + Relu_168.weight] had the following issues when converted to FP16:
[08/10/2023-11:18:27] [W] [TRT]  - Subnormal FP16 values detected. 
[08/10/2023-11:18:27] [W] [TRT]  - Values less than smallest positive FP16 Subnormal value detected. Converting to FP16 minimum subnormalized value. 
[08/10/2023-11:18:27] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.
[08/10/2023-11:18:27] [V] [TRT] --------------- Timing Runner: Conv_158 + Relu_159 || Conv_161 + Relu_162 || Conv_164 + Relu_165 || Conv_167 + Relu_168 (CaskConvolution)
[08/10/2023-11:18:27] [V] [TRT] Conv_158 + Relu_159 || Conv_161 + Relu_162 || Conv_164 + Relu_165 || Conv_167 + Relu_168 Set Tactic Name: ampere_fp16x2_hcudnn_fp16x2_128x32_relu_medium_nn_v1 Tactic: 0x1e7896ba71ef1635
[08/10/2023-11:18:27] [V] [TRT] Tactic: 0x1e7896ba71ef1635 Time: 3.23812
[08/10/2023-11:18:27] [V] [TRT] Conv_158 + Relu_159 || Conv_161 + Relu_162 || Conv_164 + Relu_165 || Conv_167 + Relu_168 Set Tactic Name: ampere_fp16x2_hcudnn_fp16x2_128x64_relu_large_nn_v1 Tactic: 0x2f735ffbb05a30fd
[08/10/2023-11:18:27] [V] [TRT] Tactic: 0x2f735ffbb05a30fd Time: 3.14332
[08/10/2023-11:18:27] [V] [TRT] Conv_158 + Relu_159 || Conv_161 + Relu_162 || Conv_164 + Relu_165 || Conv_167 + Relu_168 Set Tactic Name: ampere_fp16x2_hcudnn_fp16x2_128x128_relu_medium_nn_v1 Tactic: 0x360278e347d63410
[08/10/2023-11:18:27] [V] [TRT] Tactic: 0x360278e347d63410 Time: 3.32129
[08/10/2023-11:18:27] [V] [TRT] Conv_158 + Relu_159 || Conv_161 + Relu_162 || Conv_164 + Relu_165 || Conv_167 + Relu_168 Set Tactic Name: ampere_fp16x2_hcudnn_fp16x2_128x64_relu_medium_nn_v1 Tactic: 0x4cfee77ea8c324db
[08/10/2023-11:18:27] [V] [TRT] Tactic: 0x4cfee77ea8c324db Time: 3.153
[08/10/2023-11:18:27] [V] [TRT] Conv_158 + Relu_159 || Conv_161 + Relu_162 || Conv_164 + Relu_165 || Conv_167 + Relu_168 Set Tactic Name: ampere_fp16x2_hcudnn_fp16x2_128x64_relu_small_nn_v1 Tactic: 0x540fde3a7bee53dc
[08/10/2023-11:18:27] [V] [TRT] Tactic: 0x540fde3a7bee53dc Time: 3.17184
[08/10/2023-11:18:27] [V] [TRT] Conv_158 + Relu_159 || Conv_161 + Relu_162 || Conv_164 + Relu_165 || Conv_167 + Relu_168 Set Tactic Name: ampere_fp16x2_hcudnn_fp16x2_128x128_relu_large_nn_v1 Tactic: 0x91f7e9c0851ad67c
[08/10/2023-11:18:27] [V] [TRT] Tactic: 0x91f7e9c0851ad67c Time: 3.4398
[08/10/2023-11:18:27] [V] [TRT] Conv_158 + Relu_159 || Conv_161 + Relu_162 || Conv_164 + Relu_165 || Conv_167 + Relu_168 Set Tactic Name: ampere_fp16x2_hcudnn_fp16x2_128x32_relu_large_nn_v1 Tactic: 0xb837f96ef306f686
[08/10/2023-11:18:27] [V] [TRT] Tactic: 0xb837f96ef306f686 Time: 3.25815
[08/10/2023-11:18:27] [V] [TRT] Conv_158 + Relu_159 || Conv_161 + Relu_162 || Conv_164 + Relu_165 || Conv_167 + Relu_168 Set Tactic Name: ampere_fp16x2_hcudnn_fp16x2_128x32_relu_small_nn_v1 Tactic: 0xc34b78af38b295a7
[08/10/2023-11:18:27] [V] [TRT] Tactic: 0xc34b78af38b295a7 Time: 3.2299
[08/10/2023-11:18:27] [V] [TRT] Conv_158 + Relu_159 || Conv_161 + Relu_162 || Conv_164 + Relu_165 || Conv_167 + Relu_168 Set Tactic Name: ampere_fp16x2_hcudnn_winograd_fp16x2_128x128_ldg1_ldg4_relu_tile148t_nt_v1 Tactic: 0xc754debea88ae0b7
[08/10/2023-11:18:27] [V] [TRT] Tactic: 0xc754debea88ae0b7 Time: 1.8666
[08/10/2023-11:18:27] [V] [TRT] Conv_158 + Relu_159 || Conv_161 + Relu_162 || Conv_164 + Relu_165 || Conv_167 + Relu_168 Set Tactic Name: ampere_fp16x2_hcudnn_fp16x2_128x128_relu_small_nn_v1 Tactic: 0xea8b68014eaeb55d
[08/10/2023-11:18:27] [V] [TRT] Tactic: 0xea8b68014eaeb55d Time: 3.35767
[08/10/2023-11:18:27] [V] [TRT] Fastest Tactic: 0xc754debea88ae0b7 Time: 1.8666
[08/10/2023-11:18:27] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0xc754debea88ae0b7
[08/10/2023-11:18:27] [V] [TRT] *************** Autotuning format combination: Half(259200,1:8,1440,8) -> Float(8294400,32400,180,1) ***************
[08/10/2023-11:18:27] [W] [TRT] Weights [name=Conv_158 + Relu_159 || Conv_161 + Relu_162 || Conv_164 + Relu_165 || Conv_167 + Relu_168.weight] had the following issues when converted to FP16:
[08/10/2023-11:18:27] [W] [TRT]  - Subnormal FP16 values detected. 
[08/10/2023-11:18:27] [W] [TRT]  - Values less than smallest positive FP16 Subnormal value detected. Converting to FP16 minimum subnormalized value. 
[08/10/2023-11:18:27] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.
[08/10/2023-11:18:27] [V] [TRT] --------------- Timing Runner: Conv_158 + Relu_159 || Conv_161 + Relu_162 || Conv_164 + Relu_165 || Conv_167 + Relu_168 (CaskConvolution)
[08/10/2023-11:18:27] [V] [TRT] CaskConvolution has no valid tactics for this config, skipping
[08/10/2023-11:18:27] [V] [TRT] *************** Autotuning format combination: Half(259200,1:8,1440,8) -> Half(1036800,1:8,5760,32) ***************
[08/10/2023-11:18:27] [W] [TRT] Weights [name=Conv_158 + Relu_159 || Conv_161 + Relu_162 || Conv_164 + Relu_165 || Conv_167 + Relu_168.weight] had the following issues when converted to FP16:
[08/10/2023-11:18:27] [W] [TRT]  - Subnormal FP16 values detected. 
[08/10/2023-11:18:27] [W] [TRT]  - Values less than smallest positive FP16 Subnormal value detected. Converting to FP16 minimum subnormalized value. 
[08/10/2023-11:18:27] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.
[08/10/2023-11:18:27] [V] [TRT] --------------- Timing Runner: Conv_158 + Relu_159 || Conv_161 + Relu_162 || Conv_164 + Relu_165 || Conv_167 + Relu_168 (CudaDepthwiseConvolution)
[08/10/2023-11:18:27] [V] [TRT] CudaDepthwiseConvolution has no valid tactics for this config, skipping
[08/10/2023-11:18:27] [W] [TRT] Weights [name=Conv_158 + Relu_159 || Conv_161 + Relu_162 || Conv_164 + Relu_165 || Conv_167 + Relu_168.weight] had the following issues when converted to FP16:
[08/10/2023-11:18:27] [W] [TRT]  - Subnormal FP16 values detected. 
[08/10/2023-11:18:27] [W] [TRT]  - Values less than smallest positive FP16 Subnormal value detected. Converting to FP16 minimum subnormalized value. 
[08/10/2023-11:18:27] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.
[08/10/2023-11:18:27] [V] [TRT] --------------- Timing Runner: Conv_158 + Relu_159 || Conv_161 + Relu_162 || Conv_164 + Relu_165 || Conv_167 + Relu_168 (CaskConvolution)
[08/10/2023-11:18:27] [V] [TRT] Conv_158 + Relu_159 || Conv_161 + Relu_162 || Conv_164 + Relu_165 || Conv_167 + Relu_168 Set Tactic Name: ampere_h16816cudnn_256x128_ldg8_relu_exp_medium_nhwc_tn_v1 Tactic: 0x03896956a39a1203
[08/10/2023-11:18:27] [V] [TRT] Tactic: 0x03896956a39a1203 Time: 0.62576
[08/10/2023-11:18:27] [V] [TRT] Conv_158 + Relu_159 || Conv_161 + Relu_162 || Conv_164 + Relu_165 || Conv_167 + Relu_168 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x128x32_stage3_warpsize4x2x1_g1_tensor16x8x16_t1r3s3 Tactic: 0x048d6d0400f33439
[08/10/2023-11:18:27] [V] [TRT] Tactic: 0x048d6d0400f33439 Time: 0.561102
[08/10/2023-11:18:27] [V] [TRT] Conv_158 + Relu_159 || Conv_161 + Relu_162 || Conv_164 + Relu_165 || Conv_167 + Relu_168 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x64x64_stage3_warpsize2x2x1_g1_tensor16x8x16_t1r3s3 Tactic: 0x05b6220f243edacd
[08/10/2023-11:18:27] [V] [TRT] Tactic: 0x05b6220f243edacd Time: 0.617184
[08/10/2023-11:18:27] [V] [TRT] Conv_158 + Relu_159 || Conv_161 + Relu_162 || Conv_164 + Relu_165 || Conv_167 + Relu_168 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x64x32_stage3_warpsize4x1x1_g1_tensor16x8x16_t1r3s3_aACCESS Tactic: 0x0866ddee325d07a6
[08/10/2023-11:18:27] [V] [TRT] Tactic: 0x0866ddee325d07a6 Time: 0.563269
[08/10/2023-11:18:27] [V] [TRT] Conv_158 + Relu_159 || Conv_161 + Relu_162 || Conv_164 + Relu_165 || Conv_167 + Relu_168 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x128x32_stage4_warpsize2x2x1_g1_tensor16x8x16 Tactic: 0x0e07ff4f4f7c1ac9
[08/10/2023-11:18:27] [V] [TRT] Tactic: 0x0e07ff4f4f7c1ac9 Time: 0.523602
[08/10/2023-11:18:27] [V] [TRT] Conv_158 + Relu_159 || Conv_161 + Relu_162 || Conv_164 + Relu_165 || Conv_167 + Relu_168 Set Tactic Name: ampere_h16816cudnn_256x128_ldg8_relu_exp_stages_64x3_small_nhwc_tn_v1 Tactic: 0x0e199750c30c6928
[08/10/2023-11:18:27] [V] [TRT] Tactic: 0x0e199750c30c6928 Time: 0.609888
[08/10/2023-11:18:27] [V] [TRT] Conv_158 + Relu_159 || Conv_161 + Relu_162 || Conv_164 + Relu_165 || Conv_167 + Relu_168 Set Tactic Name: ampere_h1688cudnn_128x128_ldg8_relu_exp_medium_nhwc_tn_v1 Tactic: 0x100c1ad308e08d35
[08/10/2023-11:18:27] [V] [TRT] Tactic: 0x100c1ad308e08d35 Time: 0.680645
[08/10/2023-11:18:27] [V] [TRT] Conv_158 + Relu_159 || Conv_161 + Relu_162 || Conv_164 + Relu_165 || Conv_167 + Relu_168 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x128x32_stage5_warpsize2x2x1_g1_tensor16x8x16_t1r3s3 Tactic: 0x17ebf0c9f418f10a
[08/10/2023-11:18:27] [V] [TRT] Tactic: 0x17ebf0c9f418f10a Time: 0.588146
[08/10/2023-11:18:27] [V] [TRT] Conv_158 + Relu_159 || Conv_161 + Relu_162 || Conv_164 + Relu_165 || Conv_167 + Relu_168 Set Tactic Name: ampere_h16816cudnn_128x64_ldg8_relu_exp_stages_64x4_medium_nhwc_tn_v1 Tactic: 0x196cbc423a9bb69e
[08/10/2023-11:18:27] [V] [TRT] Tactic: 0x196cbc423a9bb69e Time: 0.944361
[08/10/2023-11:18:27] [V] [TRT] Conv_158 + Relu_159 || Conv_161 + Relu_162 || Conv_164 + Relu_165 || Conv_167 + Relu_168 Set Tactic Name: ampere_h16816cudnn_128x64_sliced1x2_ldg8_relu_exp_stages_64x3_small_nhwc_tn_v1 Tactic: 0x19822de884f42a6a
[08/10/2023-11:18:27] [V] [TRT] Tactic: 0x19822de884f42a6a Time: 0.660745
[08/10/2023-11:18:27] [V] [TRT] Conv_158 + Relu_159 || Conv_161 + Relu_162 || Conv_164 + Relu_165 || Conv_167 + Relu_168 Set Tactic Name: ampere_h16816cudnn_64x64_sliced1x2_ldg8_relu_exp_stages_64x5_small_nhwc_tn_v1 Tactic: 0x1a5ba808ad4cc5a8
[08/10/2023-11:18:27] [V] [TRT] Tactic: 0x1a5ba808ad4cc5a8 Time: 1.03691
[08/10/2023-11:18:27] [V] [TRT] Conv_158 + Relu_159 || Conv_161 + Relu_162 || Conv_164 + Relu_165 || Conv_167 + Relu_168 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x64x64_stage3_warpsize2x2x1_g1_tensor16x8x16 Tactic: 0x21b295c0c8f6c95a
[08/10/2023-11:18:27] [V] [TRT] Tactic: 0x21b295c0c8f6c95a Time: 0.608718
[08/10/2023-11:18:27] [V] [TRT] Conv_158 + Relu_159 || Conv_161 + Relu_162 || Conv_164 + Relu_165 || Conv_167 + Relu_168 Set Tactic Name: ampere_h16816cudnn_128x128_ldg8_relu_exp_medium_nhwc_tn_v1 Tactic: 0x234580e8a194335c
[08/10/2023-11:18:27] [V] [TRT] Tactic: 0x234580e8a194335c Time: 0.522482
[08/10/2023-11:18:27] [V] [TRT] Conv_158 + Relu_159 || Conv_161 + Relu_162 || Conv_164 + Relu_165 || Conv_167 + Relu_168 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x128x32_stage4_warpsize2x2x1_g1_tensor16x8x16_t1r3s3_aACCESS Tactic: 0x245530c34bd6090f
[08/10/2023-11:18:28] [V] [TRT] Tactic: 0x245530c34bd6090f Time: 0.491739
[08/10/2023-11:18:28] [V] [TRT] Conv_158 + Relu_159 || Conv_161 + Relu_162 || Conv_164 + Relu_165 || Conv_167 + Relu_168 Set Tactic Name: ampere_h16816cudnn_64x64_sliced1x2_ldg8_relu_exp_stages_64x6_small_nhwc_tn_v1 Tactic: 0x24e01e7405cfdfe9
[08/10/2023-11:18:28] [V] [TRT] Tactic: 0x24e01e7405cfdfe9 Time: 1.40095
[08/10/2023-11:18:28] [V] [TRT] Conv_158 + Relu_159 || Conv_161 + Relu_162 || Conv_164 + Relu_165 || Conv_167 + Relu_168 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x64x64_stage3_warpsize2x2x1_g1_tensor16x8x16_t1r3s3_aACCESS Tactic: 0x263a38afd75e3a43
[08/10/2023-11:18:28] [V] [TRT] Tactic: 0x263a38afd75e3a43 Time: 0.577806
[08/10/2023-11:18:28] [V] [TRT] Conv_158 + Relu_159 || Conv_161 + Relu_162 || Conv_164 + Relu_165 || Conv_167 + Relu_168 Set Tactic Name: ampere_h16816cudnn_64x128_sliced1x2_ldg8_relu_exp_stages_64x4_large_nhwc_tn_v1 Tactic: 0x2721a7f18c2700db
[08/10/2023-11:18:28] [V] [TRT] Tactic: 0x2721a7f18c2700db Time: 0.980727
[08/10/2023-11:18:28] [V] [TRT] Conv_158 + Relu_159 || Conv_161 + Relu_162 || Conv_164 + Relu_165 || Conv_167 + Relu_168 Set Tactic Name: ampere_h16816cudnn_128x64_sliced1x2_ldg8_relu_exp_stages_64x3_medium_nhwc_tn_v1 Tactic: 0x29329b5741ea05f2
[08/10/2023-11:18:28] [V] [TRT] Tactic: 0x29329b5741ea05f2 Time: 0.68347
[08/10/2023-11:18:28] [V] [TRT] Conv_158 + Relu_159 || Conv_161 + Relu_162 || Conv_164 + Relu_165 || Conv_167 + Relu_168 Set Tactic Name: ampere_h16816cudnn_128x128_ldg8_relu_exp_large_nhwc_tn_v1 Tactic: 0x2970ae65966d569d
[08/10/2023-11:18:28] [V] [TRT] Tactic: 0x2970ae65966d569d Time: 0.540471
[08/10/2023-11:18:28] [V] [TRT] Conv_158 + Relu_159 || Conv_161 + Relu_162 || Conv_164 + Relu_165 || Conv_167 + Relu_168 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x256x64_stage3_warpsize1x4x1_g1_tensor16x8x16 Tactic: 0x29dc14520e0e4eb6
[08/10/2023-11:18:28] [V] [TRT] Tactic: 0x29dc14520e0e4eb6 Time: 0.658071
[08/10/2023-11:18:28] [V] [TRT] Conv_158 + Relu_159 || Conv_161 + Relu_162 || Conv_164 + Relu_165 || Conv_167 + Relu_168 Set Tactic Name: ampere_h16816cudnn_256x64_ldg8_relu_exp_stages_64x3_large_nhwc_tn_v1 Tactic: 0x2cb4a662694e89d3
[08/10/2023-11:18:28] [V] [TRT] Tactic: 0x2cb4a662694e89d3 Time: 0.744859
[08/10/2023-11:18:28] [V] [TRT] Conv_158 + Relu_159 || Conv_161 + Relu_162 || Conv_164 + Relu_165 || Conv_167 + Relu_168 Set Tactic Name: ampere_h16816cudnn_128x128_ldg8_relu_exp_stages_64x3_medium_nhwc_tn_v1 Tactic: 0x2eaa2202de9404d6
[08/10/2023-11:18:28] [V] [TRT] Tactic: 0x2eaa2202de9404d6 Time: 0.691918
[08/10/2023-11:18:28] [V] [TRT] Conv_158 + Relu_159 || Conv_161 + Relu_162 || Conv_164 + Relu_165 || Conv_167 + Relu_168 Set Tactic Name: ampere_h1688cudnn_256x128_ldg8_relu_exp_medium_nhwc_tn_v1 Tactic: 0x30c0f36d0aeeac6a
[08/10/2023-11:18:28] [V] [TRT] Tactic: 0x30c0f36d0aeeac6a Time: 0.715803
[08/10/2023-11:18:28] [V] [TRT] Conv_158 + Relu_159 || Conv_161 + Relu_162 || Conv_164 + Relu_165 || Conv_167 + Relu_168 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x64x64_stage4_warpsize2x2x1_g1_tensor16x8x16_t1r3s3_aACCESS Tactic: 0x30e8a8d7a953e5e9
[08/10/2023-11:18:28] [V] [TRT] Tactic: 0x30e8a8d7a953e5e9 Time: 0.726757
[08/10/2023-11:18:28] [V] [TRT] Conv_158 + Relu_159 || Conv_161 + Relu_162 || Conv_164 + Relu_165 || Conv_167 + Relu_168 Set Tactic Name: ampere_h16816cudnn_128x64_ldg8_relu_exp_stages_64x4_large_nhwc_tn_v1 Tactic: 0x3197d5044d71e98e
[08/10/2023-11:18:28] [V] [TRT] Tactic: 0x3197d5044d71e98e Time: 0.979205
[08/10/2023-11:18:28] [V] [TRT] Conv_158 + Relu_159 || Conv_161 + Relu_162 || Conv_164 + Relu_165 || Conv_167 + Relu_168 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x64x64_stage4_warpsize2x2x1_g1_tensor16x8x16_t1r3s3 Tactic: 0x31d93dc22d2af081
[08/10/2023-11:18:28] [V] [TRT] Tactic: 0x31d93dc22d2af081 Time: 0.765728
[08/10/2023-11:18:28] [V] [TRT] Conv_158 + Relu_159 || Conv_161 + Relu_162 || Conv_164 + Relu_165 || Conv_167 + Relu_168 Set Tactic Name: ampere_h1688cudnn_128x128_ldg8_relu_exp_small_nhwc_tn_v1 Tactic: 0x3369260c04f9ad73
[08/10/2023-11:18:28] [V] [TRT] Tactic: 0x3369260c04f9ad73 Time: 0.651954
[08/10/2023-11:18:28] [V] [TRT] Conv_158 + Relu_159 || Conv_161 + Relu_162 || Conv_164 + Relu_165 || Conv_167 + Relu_168 Set Tactic Name: ampere_h16816cudnn_64x128_ldg8_relu_exp_stages_64x4_small_nhwc_tn_v1 Tactic: 0x3b5fa0f2a8fc2410
[08/10/2023-11:18:28] [V] [TRT] Tactic: 0x3b5fa0f2a8fc2410 Time: 0.880654
[08/10/2023-11:18:28] [V] [TRT] Conv_158 + Relu_159 || Conv_161 + Relu_162 || Conv_164 + Relu_165 || Conv_167 + Relu_168 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x32x32_stage4_warpsize4x1x1_g1_tensor16x8x16_t1r3s3 Tactic: 0x3e7eb35b91b9fa63
[08/10/2023-11:18:28] [V] [TRT] Tactic: 0x3e7eb35b91b9fa63 Time: 0.905102
[08/10/2023-11:18:28] [V] [TRT] Conv_158 + Relu_159 || Conv_161 + Relu_162 || Conv_164 + Relu_165 || Conv_167 + Relu_168 Set Tactic Name: ampere_h16816cudnn_256x64_ldg8_relu_exp_stages_64x3_medium_nhwc_tn_v1 Tactic: 0x3f031df0e579d52c
[08/10/2023-11:18:28] [V] [TRT] Tactic: 0x3f031df0e579d52c Time: 0.725669
[08/10/2023-11:18:28] [V] [TRT] Conv_158 + Relu_159 || Conv_161 + Relu_162 || Conv_164 + Relu_165 || Conv_167 + Relu_168 Set Tactic Name: ampere_h16816cudnn_256x64_sliced1x2_ldg8_relu_exp_small_nhwc_tn_v1 Tactic: 0x42a5b8709d0039be
[08/10/2023-11:18:28] [V] [TRT] Tactic: 0x42a5b8709d0039be Time: 0.819598
[08/10/2023-11:18:28] [V] [TRT] Conv_158 + Relu_159 || Conv_161 + Relu_162 || Conv_164 + Relu_165 || Conv_167 + Relu_168 Set Tactic Name: ampere_h16816cudnn_256x128_ldg8_relu_exp_small_nhwc_tn_v1 Tactic: 0x463794ee4acffd1f
[08/10/2023-11:18:28] [V] [TRT] Tactic: 0x463794ee4acffd1f Time: 0.606048
[08/10/2023-11:18:28] [V] [TRT] Conv_158 + Relu_159 || Conv_161 + Relu_162 || Conv_164 + Relu_165 || Conv_167 + Relu_168 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x64x64_stage4_warpsize2x2x1_g1_tensor16x8x16 Tactic: 0x4a81ea1e51436a30
[08/10/2023-11:18:28] [V] [TRT] Tactic: 0x4a81ea1e51436a30 Time: 0.785957
[08/10/2023-11:18:28] [V] [TRT] Conv_158 + Relu_159 || Conv_161 + Relu_162 || Conv_164 + Relu_165 || Conv_167 + Relu_168 Set Tactic Name: ampere_h1688cudnn_256x64_sliced1x2_ldg8_relu_exp_small_nhwc_tn_v1 Tactic: 0x4aed1956bd10a795
[08/10/2023-11:18:28] [V] [TRT] Tactic: 0x4aed1956bd10a795 Time: 0.878807
[08/10/2023-11:18:28] [V] [TRT] Conv_158 + Relu_159 || Conv_161 + Relu_162 || Conv_164 + Relu_165 || Conv_167 + Relu_168 Set Tactic Name: ampere_h16816cudnn_64x128_sliced1x2_ldg8_relu_exp_stages_64x3_large_nhwc_tn_v1 Tactic: 0x4fc05923455fc266
[08/10/2023-11:18:28] [V] [TRT] Tactic: 0x4fc05923455fc266 Time: 0.671237
[08/10/2023-11:18:28] [V] [TRT] Conv_158 + Relu_159 || Conv_161 + Relu_162 || Conv_164 + Relu_165 || Conv_167 + Relu_168 Set Tactic Name: ampere_h16816cudnn_256x64_ldg8_relu_exp_large_nhwc_tn_v1 Tactic: 0x5013c38f55afa9ba
[08/10/2023-11:18:28] [V] [TRT] Tactic: 0x5013c38f55afa9ba Time: 0.634199
[08/10/2023-11:18:28] [V] [TRT] Conv_158 + Relu_159 || Conv_161 + Relu_162 || Conv_164 + Relu_165 || Conv_167 + Relu_168 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x64x32_stage3_warpsize4x1x1_g1_tensor16x8x16_t1r3s3 Tactic: 0x529f4431bdae94f5
[08/10/2023-11:18:28] [V] [TRT] Tactic: 0x529f4431bdae94f5 Time: 0.516814
[08/10/2023-11:18:28] [V] [TRT] Conv_158 + Relu_159 || Conv_161 + Relu_162 || Conv_164 + Relu_165 || Conv_167 + Relu_168 Set Tactic Name: ampere_h16816cudnn_64x128_ldg8_relu_exp_stages_64x3_small_nhwc_tn_v1 Tactic: 0x53be5e206184e6ad
[08/10/2023-11:18:28] [V] [TRT] Tactic: 0x53be5e206184e6ad Time: 0.691232
[08/10/2023-11:18:28] [V] [TRT] Conv_158 + Relu_159 || Conv_161 + Relu_162 || Conv_164 + Relu_165 || Conv_167 + Relu_168 Set Tactic Name: ampere_h16816cudnn_64x64_ldg8_relu_exp_stages_64x5_large_nhwc_tn_v1 Tactic: 0x544bff7ff9c5d908
[08/10/2023-11:18:28] [V] [TRT] Tactic: 0x544bff7ff9c5d908 Time: 1.09987
[08/10/2023-11:18:28] [V] [TRT] Conv_158 + Relu_159 || Conv_161 + Relu_162 || Conv_164 + Relu_165 || Conv_167 + Relu_168 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x64x64_stage3_warpsize2x2x1_g1_tensor16x8x16 Tactic: 0x5568fd8a32f4a40f
[08/10/2023-11:18:28] [V] [TRT] Tactic: 0x5568fd8a32f4a40f Time: 0.617582
[08/10/2023-11:18:28] [V] [TRT] Conv_158 + Relu_159 || Conv_161 + Relu_162 || Conv_164 + Relu_165 || Conv_167 + Relu_168 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x128x64_stage3_warpsize2x2x1_g1_tensor16x8x16 Tactic: 0x57c9a5ff682354a6
[08/10/2023-11:18:28] [V] [TRT] Tactic: 0x57c9a5ff682354a6 Time: 0.653687
[08/10/2023-11:18:28] [V] [TRT] Conv_158 + Relu_159 || Conv_161 + Relu_162 || Conv_164 + Relu_165 || Conv_167 + Relu_168 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x32x64_stage5_warpsize2x2x1_g1_tensor16x8x16 Tactic: 0x5820b3dda403c4d0
[08/10/2023-11:18:28] [V] [TRT] Tactic: 0x5820b3dda403c4d0 Time: 1.26175
[08/10/2023-11:18:28] [V] [TRT] Conv_158 + Relu_159 || Conv_161 + Relu_162 || Conv_164 + Relu_165 || Conv_167 + Relu_168 Set Tactic Name: ampere_h16816cudnn_128x128_ldg8_relu_exp_stages_64x3_small_nhwc_tn_v1 Tactic: 0x58816bf2e9c36afb
[08/10/2023-11:18:28] [V] [TRT] Tactic: 0x58816bf2e9c36afb Time: 0.689765
[08/10/2023-11:18:28] [V] [TRT] Conv_158 + Relu_159 || Conv_161 + Relu_162 || Conv_164 + Relu_165 || Conv_167 + Relu_168 Set Tactic Name: ampere_h16816cudnn_128x64_ldg8_relu_exp_stages_64x3_large_nhwc_tn_v1 Tactic: 0x59762bd684092b33
[08/10/2023-11:18:28] [V] [TRT] Tactic: 0x59762bd684092b33 Time: 0.683963
[08/10/2023-11:18:28] [V] [TRT] Conv_158 + Relu_159 || Conv_161 + Relu_162 || Conv_164 + Relu_165 || Conv_167 + Relu_168 Set Tactic Name: ampere_h16816cudnn_128x64_sliced1x2_ldg8_relu_exp_stages_64x4_medium_nhwc_tn_v1 Tactic: 0x59c5c647b4c76593
[08/10/2023-11:18:28] [V] [TRT] Tactic: 0x59c5c647b4c76593 Time: 0.968567
[08/10/2023-11:18:28] [V] [TRT] Conv_158 + Relu_159 || Conv_161 + Relu_162 || Conv_164 + Relu_165 || Conv_167 + Relu_168 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x128x64_stage3_warpsize4x2x1_g1_tensor16x8x16 Tactic: 0x5a55274960724ba8
[08/10/2023-11:18:28] [V] [TRT] Tactic: 0x5a55274960724ba8 Time: 0.571867
[08/10/2023-11:18:28] [V] [TRT] Conv_158 + Relu_159 || Conv_161 + Relu_162 || Conv_164 + Relu_165 || Conv_167 + Relu_168 Set Tactic Name: ampere_h1688cudnn_256x128_ldg8_relu_exp_large_nhwc_tn_v1 Tactic: 0x5c2e1c87d85b06f1
[08/10/2023-11:18:28] [V] [TRT] Tactic: 0x5c2e1c87d85b06f1 Time: 0.722231
[08/10/2023-11:18:28] [V] [TRT] Conv_158 + Relu_159 || Conv_161 + Relu_162 || Conv_164 + Relu_165 || Conv_167 + Relu_168 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x256x64_stage3_warpsize1x4x1_g1_tensor16x8x16 Tactic: 0x5d067c18f40c23e3
[08/10/2023-11:18:28] [V] [TRT] Tactic: 0x5d067c18f40c23e3 Time: 0.626272
[08/10/2023-11:18:28] [V] [TRT] Conv_158 + Relu_159 || Conv_161 + Relu_162 || Conv_164 + Relu_165 || Conv_167 + Relu_168 Set Tactic Name: ampere_h16816cudnn_64x64_ldg8_relu_exp_stages_64x5_medium_nhwc_tn_v1 Tactic: 0x5f31c22ec167f384
[08/10/2023-11:18:28] [V] [TRT] Tactic: 0x5f31c22ec167f384 Time: 1.07704
[08/10/2023-11:18:28] [V] [TRT] Conv_158 + Relu_159 || Conv_161 + Relu_162 || Conv_164 + Relu_165 || Conv_167 + Relu_168 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x128x32_stage4_warpsize2x2x1_g1_tensor16x8x16_t1r3s3 Tactic: 0x60c3421152ef8e10
[08/10/2023-11:18:28] [V] [TRT] Tactic: 0x60c3421152ef8e10 Time: 0.480256
[08/10/2023-11:18:28] [V] [TRT] Conv_158 + Relu_159 || Conv_161 + Relu_162 || Conv_164 + Relu_165 || Conv_167 + Relu_168 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x128x32_stage3_warpsize4x2x1_g1_tensor16x8x16 Tactic: 0x60da8c7151d91e47
[08/10/2023-11:18:28] [V] [TRT] Tactic: 0x60da8c7151d91e47 Time: 0.551621
[08/10/2023-11:18:28] [V] [TRT] Conv_158 + Relu_159 || Conv_161 + Relu_162 || Conv_164 + Relu_165 || Conv_167 + Relu_168 Set Tactic Name: ampere_h1688cudnn_256x64_ldg8_relu_exp_medium_nhwc_tn_v1 Tactic: 0x6426696f872a3b13
[08/10/2023-11:18:28] [V] [TRT] Tactic: 0x6426696f872a3b13 Time: 0.818816
[08/10/2023-11:18:28] [V] [TRT] Conv_158 + Relu_159 || Conv_161 + Relu_162 || Conv_164 + Relu_165 || Conv_167 + Relu_168 Set Tactic Name: ampere_h16816cudnn_128x64_ldg8_relu_exp_stages_64x3_medium_nhwc_tn_v1 Tactic: 0x699be152cfb6d6ff
[08/10/2023-11:18:28] [V] [TRT] Tactic: 0x699be152cfb6d6ff Time: 0.684494
[08/10/2023-11:18:28] [V] [TRT] Conv_158 + Relu_159 || Conv_161 + Relu_162 || Conv_164 + Relu_165 || Conv_167 + Relu_168 Set Tactic Name: ampere_h16816cudnn_64x64_ldg8_relu_exp_stages_64x6_large_nhwc_tn_v1 Tactic: 0x6af049035146c349
[08/10/2023-11:18:28] [V] [TRT] Tactic: 0x6af049035146c349 Time: 1.49093
[08/10/2023-11:18:28] [V] [TRT] Conv_158 + Relu_159 || Conv_161 + Relu_162 || Conv_164 + Relu_165 || Conv_167 + Relu_168 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x256x32_stage3_warpsize2x4x1_g1_tensor16x8x16 Tactic: 0x7005d10718f6c22d
[08/10/2023-11:18:28] [V] [TRT] Tactic: 0x7005d10718f6c22d Time: 0.554386
[08/10/2023-11:18:28] [V] [TRT] Conv_158 + Relu_159 || Conv_161 + Relu_162 || Conv_164 + Relu_165 || Conv_167 + Relu_168 Set Tactic Name: ampere_h16816cudnn_64x64_sliced1x2_ldg8_relu_exp_stages_64x5_medium_nhwc_tn_v1 Tactic: 0x706f08da35c795a5
[08/10/2023-11:18:28] [V] [TRT] Tactic: 0x706f08da35c795a5 Time: 1.06755
[08/10/2023-11:18:28] [V] [TRT] Conv_158 + Relu_159 || Conv_161 + Relu_162 || Conv_164 + Relu_165 || Conv_167 + Relu_168 Set Tactic Name: ampere_h16816cudnn_128x64_sliced1x2_ldg8_relu_exp_stages_64x4_small_nhwc_tn_v1 Tactic: 0x7163d33a4d8ce8d7
[08/10/2023-11:18:28] [V] [TRT] Tactic: 0x7163d33a4d8ce8d7 Time: 0.956823
[08/10/2023-11:18:28] [V] [TRT] Conv_158 + Relu_159 || Conv_161 + Relu_162 || Conv_164 + Relu_165 || Conv_167 + Relu_168 Set Tactic Name: ampere_h1688cudnn_256x64_ldg8_relu_exp_small_nhwc_tn_v1 Tactic: 0x7aad3976677d7155
[08/10/2023-11:18:28] [V] [TRT] Tactic: 0x7aad3976677d7155 Time: 0.770226
[08/10/2023-11:18:28] [V] [TRT] Conv_158 + Relu_159 || Conv_161 + Relu_162 || Conv_164 + Relu_165 || Conv_167 + Relu_168 Set Tactic Name: ampere_h16816cudnn_64x64_sliced1x2_ldg8_relu_exp_stages_64x5_large_nhwc_tn_v1 Tactic: 0x8015519605ab9963
[08/10/2023-11:18:28] [V] [TRT] Tactic: 0x8015519605ab9963 Time: 1.04677
[08/10/2023-11:18:28] [V] [TRT] Conv_158 + Relu_159 || Conv_161 + Relu_162 || Conv_164 + Relu_165 || Conv_167 + Relu_168 Set Tactic Name: ampere_h16816cudnn_256x64_sliced1x2_ldg8_relu_exp_medium_nhwc_tn_v1 Tactic: 0x82f8a2214b0b4178
[08/10/2023-11:18:28] [V] [TRT] Tactic: 0x82f8a2214b0b4178 Time: 0.890999
[08/10/2023-11:18:28] [V] [TRT] Conv_158 + Relu_159 || Conv_161 + Relu_162 || Conv_164 + Relu_165 || Conv_167 + Relu_168 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x32x32_stage4_warpsize4x1x1_g1_tensor16x8x16 Tactic: 0x834e11ecd4ab9454
[08/10/2023-11:18:28] [V] [TRT] Tactic: 0x834e11ecd4ab9454 Time: 0.907808
[08/10/2023-11:18:28] [V] [TRT] Conv_158 + Relu_159 || Conv_161 + Relu_162 || Conv_164 + Relu_165 || Conv_167 + Relu_168 Set Tactic Name: ampere_h16816cudnn_128x64_sliced1x2_ldg8_relu_exp_stages_64x3_large_nhwc_tn_v1 Tactic: 0x83ccd4762c1376a1
[08/10/2023-11:18:28] [V] [TRT] Tactic: 0x83ccd4762c1376a1 Time: 0.675355
[08/10/2023-11:18:28] [V] [TRT] Conv_158 + Relu_159 || Conv_161 + Relu_162 || Conv_164 + Relu_165 || Conv_167 + Relu_168 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x64x64_stage3_warpsize4x1x1_g1_tensor16x8x16_t1r3s3 Tactic: 0x84ee897dd1f4d2aa
[08/10/2023-11:18:28] [V] [TRT] Tactic: 0x84ee897dd1f4d2aa Time: 0.667141
[08/10/2023-11:18:28] [V] [TRT] Conv_158 + Relu_159 || Conv_161 + Relu_162 || Conv_164 + Relu_165 || Conv_167 + Relu_168 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x128x64_stage3_warpsize2x2x1_g1_tensor16x8x16 Tactic: 0x866e7a5f6401b67f
[08/10/2023-11:18:28] [V] [TRT] Tactic: 0x866e7a5f6401b67f Time: 0.616722
[08/10/2023-11:18:28] [V] [TRT] Conv_158 + Relu_159 || Conv_161 + Relu_162 || Conv_164 + Relu_165 || Conv_167 + Relu_168 Set Tactic Name: ampere_h16816cudnn_64x64_ldg8_relu_exp_stages_64x6_medium_nhwc_tn_v1 Tactic: 0x88971eec55aba850
[08/10/2023-11:18:28] [V] [TRT] Tactic: 0x88971eec55aba850 Time: 1.42728
[08/10/2023-11:18:28] [V] [TRT] Conv_158 + Relu_159 || Conv_161 + Relu_162 || Conv_164 + Relu_165 || Conv_167 + Relu_168 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x128x64_stage3_warpsize4x2x1_g1_tensor16x8x16 Tactic: 0x8bf2f8e96c50a971
[08/10/2023-11:18:28] [V] [TRT] Tactic: 0x8bf2f8e96c50a971 Time: 0.55461
[08/10/2023-11:18:28] [V] [TRT] Conv_158 + Relu_159 || Conv_161 + Relu_162 || Conv_164 + Relu_165 || Conv_167 + Relu_168 Set Tactic Name: ampere_h16816cudnn_256x128_ldg8_relu_exp_stages_64x3_large_nhwc_tn_v1 Tactic: 0x94576ece6beb35e3
[08/10/2023-11:18:28] [V] [TRT] Tactic: 0x94576ece6beb35e3 Time: 0.613339
[08/10/2023-11:18:28] [V] [TRT] Conv_158 + Relu_159 || Conv_161 + Relu_162 || Conv_164 + Relu_165 || Conv_167 + Relu_168 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x64x64_stage3_warpsize4x1x1_g1_tensor16x8x16 Tactic: 0x9fff6e65019cc310
[08/10/2023-11:18:28] [V] [TRT] Tactic: 0x9fff6e65019cc310 Time: 0.652466
[08/10/2023-11:18:28] [V] [TRT] Conv_158 + Relu_159 || Conv_161 + Relu_162 || Conv_164 + Relu_165 || Conv_167 + Relu_168 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x32x64_stage5_warpsize2x2x1_g1_tensor16x8x16_t1r3s3 Tactic: 0xa033e20ae9f412b2
[08/10/2023-11:18:28] [V] [TRT] Tactic: 0xa033e20ae9f412b2 Time: 1.15756
[08/10/2023-11:18:28] [V] [TRT] Conv_158 + Relu_159 || Conv_161 + Relu_162 || Conv_164 + Relu_165 || Conv_167 + Relu_168 Set Tactic Name: ampere_h16816cudnn_64x128_ldg8_relu_exp_stages_64x4_large_nhwc_tn_v1 Tactic: 0xa111596c001b78db
[08/10/2023-11:18:28] [V] [TRT] Tactic: 0xa111596c001b78db Time: 0.897541
[08/10/2023-11:18:28] [V] [TRT] Conv_158 + Relu_159 || Conv_161 + Relu_162 || Conv_164 + Relu_165 || Conv_167 + Relu_168 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x256x32_stage3_warpsize2x4x1_g1_tensor16x8x16 Tactic: 0xa1a20ea714d420f4
[08/10/2023-11:18:28] [V] [TRT] Tactic: 0xa1a20ea714d420f4 Time: 0.544165
[08/10/2023-11:18:28] [V] [TRT] Conv_158 + Relu_159 || Conv_161 + Relu_162 || Conv_164 + Relu_165 || Conv_167 + Relu_168 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x32x32_stage4_warpsize4x1x1_g1_tensor16x8x16_t1r3s3 Tactic: 0xa40cb43c296a36a8
[08/10/2023-11:18:28] [V] [TRT] Tactic: 0xa40cb43c296a36a8 Time: 0.844466
[08/10/2023-11:18:28] [V] [TRT] Conv_158 + Relu_159 || Conv_161 + Relu_162 || Conv_164 + Relu_165 || Conv_167 + Relu_168 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x64x32_stage3_warpsize4x1x1_g1_tensor16x8x16 Tactic: 0xa570c55d303796ff
[08/10/2023-11:18:28] [V] [TRT] Tactic: 0xa570c55d303796ff Time: 0.558761
[08/10/2023-11:18:28] [V] [TRT] Conv_158 + Relu_159 || Conv_161 + Relu_162 || Conv_164 + Relu_165 || Conv_167 + Relu_168 Set Tactic Name: ampere_h16816cudnn_64x64_sliced1x2_ldg8_relu_exp_stages_64x6_medium_nhwc_tn_v1 Tactic: 0xa7c9d418a10bce71
[08/10/2023-11:18:28] [V] [TRT] Tactic: 0xa7c9d418a10bce71 Time: 1.54156
[08/10/2023-11:18:28] [V] [TRT] Conv_158 + Relu_159 || Conv_161 + Relu_162 || Conv_164 + Relu_165 || Conv_167 + Relu_168 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x32x32_stage4_warpsize4x1x1_g1_tensor16x8x16 Tactic: 0xa83b68f30462f971
[08/10/2023-11:18:28] [V] [TRT] Tactic: 0xa83b68f30462f971 Time: 0.855598
[08/10/2023-11:18:28] [V] [TRT] Conv_158 + Relu_159 || Conv_161 + Relu_162 || Conv_164 + Relu_165 || Conv_167 + Relu_168 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x256x32_stage3_warpsize2x4x1_g1_tensor16x8x16_t1r3s3 Tactic: 0xa9177bbe4e767df8
[08/10/2023-11:18:28] [V] [TRT] Tactic: 0xa9177bbe4e767df8 Time: 0.540553
[08/10/2023-11:18:28] [V] [TRT] Conv_158 + Relu_159 || Conv_161 + Relu_162 || Conv_164 + Relu_165 || Conv_167 + Relu_168 Set Tactic Name: ampere_h1688cudnn_128x128_ldg8_relu_exp_large_nhwc_tn_v1 Tactic: 0xa927df92ac1ef1b8
[08/10/2023-11:18:28] [V] [TRT] Tactic: 0xa927df92ac1ef1b8 Time: 0.68229
[08/10/2023-11:18:28] [V] [TRT] Conv_158 + Relu_159 || Conv_161 + Relu_162 || Conv_164 + Relu_165 || Conv_167 + Relu_168 Set Tactic Name: ampere_h16816cudnn_128x64_ldg8_relu_exp_stages_64x4_small_nhwc_tn_v1 Tactic: 0xabd92c9ae596b545
[08/10/2023-11:18:28] [V] [TRT] Tactic: 0xabd92c9ae596b545 Time: 0.936489
[08/10/2023-11:18:28] [V] [TRT] Conv_158 + Relu_159 || Conv_161 + Relu_162 || Conv_164 + Relu_165 || Conv_167 + Relu_168 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x128x32_stage5_warpsize2x2x1_g1_tensor16x8x16 Tactic: 0xafd1e8bf6bd3d638
[08/10/2023-11:18:28] [V] [TRT] Tactic: 0xafd1e8bf6bd3d638 Time: 0.61611
[08/10/2023-11:18:28] [V] [TRT] Conv_158 + Relu_159 || Conv_161 + Relu_162 || Conv_164 + Relu_165 || Conv_167 + Relu_168 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x128x32_stage3_warpsize4x2x1_g1_tensor16x8x16 Tactic: 0xb17d53d15dfbfc9e
[08/10/2023-11:18:28] [V] [TRT] Tactic: 0xb17d53d15dfbfc9e Time: 0.557888
[08/10/2023-11:18:28] [V] [TRT] Conv_158 + Relu_159 || Conv_161 + Relu_162 || Conv_164 + Relu_165 || Conv_167 + Relu_168 Set Tactic Name: ampere_h16816cudnn_128x128_ldg8_relu_exp_small_nhwc_tn_v1 Tactic: 0xb33e57fb3e8a0a56
[08/10/2023-11:18:28] [V] [TRT] Tactic: 0xb33e57fb3e8a0a56 Time: 0.514583
[08/10/2023-11:18:28] [V] [TRT] Conv_158 + Relu_159 || Conv_161 + Relu_162 || Conv_164 + Relu_165 || Conv_167 + Relu_168 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x64x32_stage5_warpsize2x2x1_g1_tensor16x8x16_t1r3s3 Tactic: 0xb4bec086187edcfc
[08/10/2023-11:18:28] [V] [TRT] Tactic: 0xb4bec086187edcfc Time: 0.597691
[08/10/2023-11:18:28] [V] [TRT] Conv_158 + Relu_159 || Conv_161 + Relu_162 || Conv_164 + Relu_165 || Conv_167 + Relu_168 Set Tactic Name: ampere_h16816cudnn_256x64_ldg8_relu_exp_stages_64x3_small_nhwc_tn_v1 Tactic: 0xb6fa5ffcc1a9d518
[08/10/2023-11:18:28] [V] [TRT] Tactic: 0xb6fa5ffcc1a9d518 Time: 0.716576
[08/10/2023-11:18:28] [V] [TRT] Conv_158 + Relu_159 || Conv_161 + Relu_162 || Conv_164 + Relu_165 || Conv_167 + Relu_168 Set Tactic Name: ampere_h16816cudnn_64x128_sliced1x2_ldg8_relu_exp_stages_64x3_medium_nhwc_tn_v1 Tactic: 0xb85e52e87caf60a2
[08/10/2023-11:18:28] [V] [TRT] Tactic: 0xb85e52e87caf60a2 Time: 0.671881
[08/10/2023-11:18:28] [V] [TRT] Conv_158 + Relu_159 || Conv_161 + Relu_162 || Conv_164 + Relu_165 || Conv_167 + Relu_168 Set Tactic Name: ampere_h16816cudnn_64x128_ldg8_relu_exp_stages_64x3_medium_nhwc_tn_v1 Tactic: 0xb8d86216e1235cda
[08/10/2023-11:18:28] [V] [TRT] Tactic: 0xb8d86216e1235cda Time: 0.673006
[08/10/2023-11:18:28] [V] [TRT] Conv_158 + Relu_159 || Conv_161 + Relu_162 || Conv_164 + Relu_165 || Conv_167 + Relu_168 Set Tactic Name: ampere_h16816cudnn_256x128_ldg8_relu_exp_stages_64x3_medium_nhwc_tn_v1 Tactic: 0xb9171d70ba2eda4b
[08/10/2023-11:18:28] [V] [TRT] Tactic: 0xb9171d70ba2eda4b Time: 0.611963
[08/10/2023-11:18:28] [V] [TRT] Conv_158 + Relu_159 || Conv_161 + Relu_162 || Conv_164 + Relu_165 || Conv_167 + Relu_168 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x64x32_stage5_warpsize2x2x1_g1_tensor16x8x16 Tactic: 0xbd08239a9317f2fd
[08/10/2023-11:18:28] [V] [TRT] Tactic: 0xbd08239a9317f2fd Time: 0.618267
[08/10/2023-11:18:28] [V] [TRT] Conv_158 + Relu_159 || Conv_161 + Relu_162 || Conv_164 + Relu_165 || Conv_167 + Relu_168 Set Tactic Name: ampere_h16816cudnn_64x128_sliced1x2_ldg8_relu_exp_stages_64x4_small_nhwc_tn_v1 Tactic: 0xbd6f5e6f24c05c10
[08/10/2023-11:18:28] [V] [TRT] Tactic: 0xbd6f5e6f24c05c10 Time: 0.944251
[08/10/2023-11:18:28] [V] [TRT] Conv_158 + Relu_159 || Conv_161 + Relu_162 || Conv_164 + Relu_165 || Conv_167 + Relu_168 Set Tactic Name: ampere_h16816cudnn_64x64_sliced1x2_ldg8_relu_exp_stages_64x6_large_nhwc_tn_v1 Tactic: 0xbeaee7eaad288322
[08/10/2023-11:18:28] [V] [TRT] Tactic: 0xbeaee7eaad288322 Time: 1.5589
[08/10/2023-11:18:28] [V] [TRT] Conv_158 + Relu_159 || Conv_161 + Relu_162 || Conv_164 + Relu_165 || Conv_167 + Relu_168 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x128x64_stage3_warpsize2x2x1_g1_tensor16x8x16_t1r3s3 Tactic: 0xc0a02dc6095497cc
[08/10/2023-11:18:28] [V] [TRT] Tactic: 0xc0a02dc6095497cc Time: 0.646331
[08/10/2023-11:18:28] [V] [TRT] Conv_158 + Relu_159 || Conv_161 + Relu_162 || Conv_164 + Relu_165 || Conv_167 + Relu_168 Set Tactic Name: ampere_h16816cudnn_128x128_ldg8_relu_exp_stages_64x3_large_nhwc_tn_v1 Tactic: 0xc2cf926c41243630
[08/10/2023-11:18:28] [V] [TRT] Tactic: 0xc2cf926c41243630 Time: 0.706537
[08/10/2023-11:18:28] [V] [TRT] Conv_158 + Relu_159 || Conv_161 + Relu_162 || Conv_164 + Relu_165 || Conv_167 + Relu_168 Set Tactic Name: ampere_h16816cudnn_128x64_ldg8_relu_exp_stages_64x3_small_nhwc_tn_v1 Tactic: 0xc338d2482cee77f8
[08/10/2023-11:18:28] [V] [TRT] Tactic: 0xc338d2482cee77f8 Time: 0.676
[08/10/2023-11:18:29] [V] [TRT] Conv_158 + Relu_159 || Conv_161 + Relu_162 || Conv_164 + Relu_165 || Conv_167 + Relu_168 Set Tactic Name: ampere_h1688cudnn_256x128_ldg8_relu_exp_small_nhwc_tn_v1 Tactic: 0xc660e51970bc5a3a
[08/10/2023-11:18:29] [V] [TRT] Tactic: 0xc660e51970bc5a3a Time: 0.678139
[08/10/2023-11:18:29] [V] [TRT] Conv_158 + Relu_159 || Conv_161 + Relu_162 || Conv_164 + Relu_165 || Conv_167 + Relu_168 Set Tactic Name: ampere_h16816cudnn_64x128_ldg8_relu_exp_stages_64x4_medium_nhwc_tn_v1 Tactic: 0xc82f3f06140e3cbb
[08/10/2023-11:18:29] [V] [TRT] Tactic: 0xc82f3f06140e3cbb Time: 0.885733
[08/10/2023-11:18:29] [V] [TRT] Conv_158 + Relu_159 || Conv_161 + Relu_162 || Conv_164 + Relu_165 || Conv_167 + Relu_168 Set Tactic Name: ampere_h16816cudnn_64x128_sliced1x2_ldg8_relu_exp_stages_64x4_medium_nhwc_tn_v1 Tactic: 0xc8a90ff8898200c3
[08/10/2023-11:18:29] [V] [TRT] Tactic: 0xc8a90ff8898200c3 Time: 0.936201
[08/10/2023-11:18:29] [V] [TRT] Conv_158 + Relu_159 || Conv_161 + Relu_162 || Conv_164 + Relu_165 || Conv_167 + Relu_168 Set Tactic Name: ampere_h1688cudnn_256x64_sliced1x2_ldg8_relu_exp_medium_nhwc_tn_v1 Tactic: 0xc9cc55109bb4de26
[08/10/2023-11:18:29] [V] [TRT] Tactic: 0xc9cc55109bb4de26 Time: 0.951817
[08/10/2023-11:18:29] [V] [TRT] Conv_158 + Relu_159 || Conv_161 + Relu_162 || Conv_164 + Relu_165 || Conv_167 + Relu_168 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x64x32_stage5_warpsize2x2x1_g1_tensor16x8x16 Tactic: 0xc9d24bd069159fa8
[08/10/2023-11:18:29] [V] [TRT] Tactic: 0xc9d24bd069159fa8 Time: 0.673536
[08/10/2023-11:18:29] [V] [TRT] Conv_158 + Relu_159 || Conv_161 + Relu_162 || Conv_164 + Relu_165 || Conv_167 + Relu_168 Set Tactic Name: ampere_h16816cudnn_64x128_ldg8_relu_exp_stages_64x3_large_nhwc_tn_v1 Tactic: 0xc9f0a7bec963ba66
[08/10/2023-11:18:29] [V] [TRT] Tactic: 0xc9f0a7bec963ba66 Time: 0.680073
[08/10/2023-11:18:29] [V] [TRT] Conv_158 + Relu_159 || Conv_161 + Relu_162 || Conv_164 + Relu_165 || Conv_167 + Relu_168 Set Tactic Name: ampere_h16816cudnn_256x64_ldg8_relu_exp_small_nhwc_tn_v1 Tactic: 0xca5d3a11fd48f571
[08/10/2023-11:18:29] [V] [TRT] Tactic: 0xca5d3a11fd48f571 Time: 0.60427
[08/10/2023-11:18:29] [V] [TRT] Conv_158 + Relu_159 || Conv_161 + Relu_162 || Conv_164 + Relu_165 || Conv_167 + Relu_168 Set Tactic Name: ampere_h16816cudnn_64x64_ldg8_relu_exp_stages_64x5_small_nhwc_tn_v1 Tactic: 0xce0506e1512285c3
[08/10/2023-11:18:29] [V] [TRT] Tactic: 0xce0506e1512285c3 Time: 1.07507
[08/10/2023-11:18:29] [V] [TRT] Conv_158 + Relu_159 || Conv_161 + Relu_162 || Conv_164 + Relu_165 || Conv_167 + Relu_168 Set Tactic Name: ampere_h1688cudnn_256x64_sliced1x2_ldg8_relu_exp_large_nhwc_tn_v1 Tactic: 0xd0a3e0c815f7fb5e
[08/10/2023-11:18:29] [V] [TRT] Tactic: 0xd0a3e0c815f7fb5e Time: 0.95253
[08/10/2023-11:18:29] [V] [TRT] Conv_158 + Relu_159 || Conv_161 + Relu_162 || Conv_164 + Relu_165 || Conv_167 + Relu_168 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x64x32_stage3_warpsize4x1x1_g1_tensor16x8x16 Tactic: 0xd1aaad17ca35fbaa
[08/10/2023-11:18:29] [V] [TRT] Tactic: 0xd1aaad17ca35fbaa Time: 0.517929
[08/10/2023-11:18:29] [V] [TRT] Conv_158 + Relu_159 || Conv_161 + Relu_162 || Conv_164 + Relu_165 || Conv_167 + Relu_168 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x128x64_stage3_warpsize4x2x1_g1_tensor16x8x16_t1r3s3 Tactic: 0xd2fca0486ca97266
[08/10/2023-11:18:29] [V] [TRT] Tactic: 0xd2fca0486ca97266 Time: 0.572151
[08/10/2023-11:18:29] [V] [TRT] Conv_158 + Relu_159 || Conv_161 + Relu_162 || Conv_164 + Relu_165 || Conv_167 + Relu_168 Set Tactic Name: ampere_h16816cudnn_64x128_sliced1x2_ldg8_relu_exp_stages_64x3_small_nhwc_tn_v1 Tactic: 0xd58ea0bdedb89ead
[08/10/2023-11:18:29] [V] [TRT] Tactic: 0xd58ea0bdedb89ead Time: 0.670245
[08/10/2023-11:18:29] [V] [TRT] Conv_158 + Relu_159 || Conv_161 + Relu_162 || Conv_164 + Relu_165 || Conv_167 + Relu_168 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x256x64_stage3_warpsize1x4x1_g1_tensor16x8x16_t1r3s3 Tactic: 0xd7c261fa01db2af9
[08/10/2023-11:18:29] [V] [TRT] Tactic: 0xd7c261fa01db2af9 Time: 0.624795
[08/10/2023-11:18:29] [V] [TRT] Conv_158 + Relu_159 || Conv_161 + Relu_162 || Conv_164 + Relu_165 || Conv_167 + Relu_168 Set Tactic Name: ampere_h16816cudnn_256x64_sliced1x2_ldg8_relu_exp_large_nhwc_tn_v1 Tactic: 0xd8eb41ee35e76575
[08/10/2023-11:18:29] [V] [TRT] Tactic: 0xd8eb41ee35e76575 Time: 0.902386
[08/10/2023-11:18:29] [V] [TRT] Conv_158 + Relu_159 || Conv_161 + Relu_162 || Conv_164 + Relu_165 || Conv_167 + Relu_168 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x128x32_stage5_warpsize2x2x1_g1_tensor16x8x16 Tactic: 0xdb0b80f591d1bb6d
[08/10/2023-11:18:29] [V] [TRT] Tactic: 0xdb0b80f591d1bb6d Time: 0.598949
[08/10/2023-11:18:29] [V] [TRT] Conv_158 + Relu_159 || Conv_161 + Relu_162 || Conv_164 + Relu_165 || Conv_167 + Relu_168 Set Tactic Name: ampere_h16816cudnn_256x128_ldg8_relu_exp_large_nhwc_tn_v1 Tactic: 0xdc796d70e228a1d4
[08/10/2023-11:18:29] [V] [TRT] Tactic: 0xdc796d70e228a1d4 Time: 0.626697
[08/10/2023-11:18:29] [V] [TRT] Conv_158 + Relu_159 || Conv_161 + Relu_162 || Conv_164 + Relu_165 || Conv_167 + Relu_168 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x32x32_stage4_warpsize4x1x1_g1_tensor16x8x16 Tactic: 0xdce100b9fe609424
[08/10/2023-11:18:29] [V] [TRT] Tactic: 0xdce100b9fe609424 Time: 0.879369
[08/10/2023-11:18:29] [V] [TRT] Conv_158 + Relu_159 || Conv_161 + Relu_162 || Conv_164 + Relu_165 || Conv_167 + Relu_168 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x128x32_stage4_warpsize2x2x1_g1_tensor16x8x16 Tactic: 0xdfa020ef435ef810
[08/10/2023-11:18:29] [V] [TRT] Tactic: 0xdfa020ef435ef810 Time: 0.493614
[08/10/2023-11:18:29] [V] [TRT] Conv_158 + Relu_159 || Conv_161 + Relu_162 || Conv_164 + Relu_165 || Conv_167 + Relu_168 Set Tactic Name: ampere_h1688cudnn_256x64_ldg8_relu_exp_large_nhwc_tn_v1 Tactic: 0xe0e3c0e8cf9a2d9e
[08/10/2023-11:18:29] [V] [TRT] Tactic: 0xe0e3c0e8cf9a2d9e Time: 0.829911
[08/10/2023-11:18:29] [V] [TRT] Conv_158 + Relu_159 || Conv_161 + Relu_162 || Conv_164 + Relu_165 || Conv_167 + Relu_168 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x32x64_stage5_warpsize2x2x1_g1_tensor16x8x16 Tactic: 0xe1ff5ad20f5c6bf6
[08/10/2023-11:18:29] [V] [TRT] Tactic: 0xe1ff5ad20f5c6bf6 Time: 1.32748
[08/10/2023-11:18:29] [V] [TRT] Conv_158 + Relu_159 || Conv_161 + Relu_162 || Conv_164 + Relu_165 || Conv_167 + Relu_168 Set Tactic Name: ampere_h16816cudnn_256x64_ldg8_relu_exp_medium_nhwc_tn_v1 Tactic: 0xe4711898bd599c36
[08/10/2023-11:18:29] [V] [TRT] Tactic: 0xe4711898bd599c36 Time: 0.627269
[08/10/2023-11:18:29] [V] [TRT] Conv_158 + Relu_159 || Conv_161 + Relu_162 || Conv_164 + Relu_165 || Conv_167 + Relu_168 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x64x64_stage3_warpsize4x1x1_g1_tensor16x8x16 Tactic: 0xeb25062ffb9eae45
[08/10/2023-11:18:29] [V] [TRT] Tactic: 0xeb25062ffb9eae45 Time: 0.685079
[08/10/2023-11:18:29] [V] [TRT] Conv_158 + Relu_159 || Conv_161 + Relu_162 || Conv_164 + Relu_165 || Conv_167 + Relu_168 Set Tactic Name: ampere_h16816cudnn_128x64_sliced1x2_ldg8_relu_exp_stages_64x4_large_nhwc_tn_v1 Tactic: 0xeb2d2aa4e56bb41c
[08/10/2023-11:18:29] [V] [TRT] Tactic: 0xeb2d2aa4e56bb41c Time: 0.997079
[08/10/2023-11:18:29] [V] [TRT] Conv_158 + Relu_159 || Conv_161 + Relu_162 || Conv_164 + Relu_165 || Conv_167 + Relu_168 Set Tactic Name: ampere_h16816cudnn_64x64_ldg8_relu_exp_stages_64x6_small_nhwc_tn_v1 Tactic: 0xf0beb09df9a19f82
[08/10/2023-11:18:29] [V] [TRT] Tactic: 0xf0beb09df9a19f82 Time: 1.34925
[08/10/2023-11:18:29] [V] [TRT] Conv_158 + Relu_159 || Conv_161 + Relu_162 || Conv_164 + Relu_165 || Conv_167 + Relu_168 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x64x64_stage4_warpsize2x2x1_g1_tensor16x8x16 Tactic: 0xf35e0311fa1cc516
[08/10/2023-11:18:29] [V] [TRT] Tactic: 0xf35e0311fa1cc516 Time: 0.784773
[08/10/2023-11:18:29] [V] [TRT] Conv_158 + Relu_159 || Conv_161 + Relu_162 || Conv_164 + Relu_165 || Conv_167 + Relu_168 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x32x32_stage4_warpsize4x1x1_g1_tensor16x8x16 Tactic: 0xf79479a62ea9f901
[08/10/2023-11:18:29] [V] [TRT] Tactic: 0xf79479a62ea9f901 Time: 0.909737
[08/10/2023-11:18:29] [V] [TRT] Fastest Tactic: 0x60c3421152ef8e10 Time: 0.480256
[08/10/2023-11:18:29] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x60c3421152ef8e10
[08/10/2023-11:18:29] [V] [TRT] *************** Autotuning format combination: Half(129600,1:16,720,4) -> Half(518400,1:16,2880,16) ***************
[08/10/2023-11:18:29] [W] [TRT] Weights [name=Conv_158 + Relu_159 || Conv_161 + Relu_162 || Conv_164 + Relu_165 || Conv_167 + Relu_168.weight] had the following issues when converted to FP16:
[08/10/2023-11:18:29] [W] [TRT]  - Subnormal FP16 values detected. 
[08/10/2023-11:18:29] [W] [TRT]  - Values less than smallest positive FP16 Subnormal value detected. Converting to FP16 minimum subnormalized value. 
[08/10/2023-11:18:29] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.
[08/10/2023-11:18:29] [V] [TRT] --------------- Timing Runner: Conv_158 + Relu_159 || Conv_161 + Relu_162 || Conv_164 + Relu_165 || Conv_167 + Relu_168 (CaskConvolution)
[08/10/2023-11:18:29] [V] [TRT] Conv_158 + Relu_159 || Conv_161 + Relu_162 || Conv_164 + Relu_165 || Conv_167 + Relu_168 Set Tactic Name: ampere_h16816cudnn_256x128_ldg8_relu_exp_medium_nhwc_tn_v1 Tactic: 0x03896956a39a1203
[08/10/2023-11:18:29] [V] [TRT] Tactic: 0x03896956a39a1203 Time: 0.613669
[08/10/2023-11:18:29] [V] [TRT] Conv_158 + Relu_159 || Conv_161 + Relu_162 || Conv_164 + Relu_165 || Conv_167 + Relu_168 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x128x32_stage3_warpsize4x2x1_g1_tensor16x8x16_t1r3s3 Tactic: 0x048d6d0400f33439
[08/10/2023-11:18:29] [V] [TRT] Tactic: 0x048d6d0400f33439 Time: 0.552293
[08/10/2023-11:18:29] [V] [TRT] Conv_158 + Relu_159 || Conv_161 + Relu_162 || Conv_164 + Relu_165 || Conv_167 + Relu_168 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x64x64_stage3_warpsize2x2x1_g1_tensor16x8x16_t1r3s3 Tactic: 0x05b6220f243edacd
[08/10/2023-11:18:29] [V] [TRT] Tactic: 0x05b6220f243edacd Time: 0.603383
[08/10/2023-11:18:29] [V] [TRT] Conv_158 + Relu_159 || Conv_161 + Relu_162 || Conv_164 + Relu_165 || Conv_167 + Relu_168 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x64x32_stage3_warpsize4x1x1_g1_tensor16x8x16_t1r3s3_aACCESS Tactic: 0x0866ddee325d07a6
[08/10/2023-11:18:29] [V] [TRT] Tactic: 0x0866ddee325d07a6 Time: 0.562999
[08/10/2023-11:18:29] [V] [TRT] Conv_158 + Relu_159 || Conv_161 + Relu_162 || Conv_164 + Relu_165 || Conv_167 + Relu_168 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x128x32_stage4_warpsize2x2x1_g1_tensor16x8x16 Tactic: 0x0e07ff4f4f7c1ac9
[08/10/2023-11:18:29] [V] [TRT] Tactic: 0x0e07ff4f4f7c1ac9 Time: 0.534921
[08/10/2023-11:18:29] [V] [TRT] Conv_158 + Relu_159 || Conv_161 + Relu_162 || Conv_164 + Relu_165 || Conv_167 + Relu_168 Set Tactic Name: ampere_h16816cudnn_256x128_ldg8_relu_exp_stages_64x3_small_nhwc_tn_v1 Tactic: 0x0e199750c30c6928
[08/10/2023-11:18:29] [V] [TRT] Tactic: 0x0e199750c30c6928 Time: 0.622821
[08/10/2023-11:18:29] [V] [TRT] Conv_158 + Relu_159 || Conv_161 + Relu_162 || Conv_164 + Relu_165 || Conv_167 + Relu_168 Set Tactic Name: ampere_h1688cudnn_128x128_ldg8_relu_exp_medium_nhwc_tn_v1 Tactic: 0x100c1ad308e08d35
[08/10/2023-11:18:29] [V] [TRT] Tactic: 0x100c1ad308e08d35 Time: 0.694843
[08/10/2023-11:18:29] [V] [TRT] Conv_158 + Relu_159 || Conv_161 + Relu_162 || Conv_164 + Relu_165 || Conv_167 + Relu_168 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x128x32_stage5_warpsize2x2x1_g1_tensor16x8x16_t1r3s3 Tactic: 0x17ebf0c9f418f10a
[08/10/2023-11:18:29] [V] [TRT] Tactic: 0x17ebf0c9f418f10a Time: 0.587831
[08/10/2023-11:18:29] [V] [TRT] Conv_158 + Relu_159 || Conv_161 + Relu_162 || Conv_164 + Relu_165 || Conv_167 + Relu_168 Set Tactic Name: ampere_h16816cudnn_128x64_ldg8_relu_exp_stages_64x4_medium_nhwc_tn_v1 Tactic: 0x196cbc423a9bb69e
[08/10/2023-11:18:29] [V] [TRT] Tactic: 0x196cbc423a9bb69e Time: 0.947867
[08/10/2023-11:18:29] [V] [TRT] Conv_158 + Relu_159 || Conv_161 + Relu_162 || Conv_164 + Relu_165 || Conv_167 + Relu_168 Set Tactic Name: ampere_h16816cudnn_128x64_sliced1x2_ldg8_relu_exp_stages_64x3_small_nhwc_tn_v1 Tactic: 0x19822de884f42a6a
[08/10/2023-11:18:29] [V] [TRT] Tactic: 0x19822de884f42a6a Time: 0.661422
[08/10/2023-11:18:29] [V] [TRT] Conv_158 + Relu_159 || Conv_161 + Relu_162 || Conv_164 + Relu_165 || Conv_167 + Relu_168 Set Tactic Name: ampere_h16816cudnn_64x64_sliced1x2_ldg8_relu_exp_stages_64x5_small_nhwc_tn_v1 Tactic: 0x1a5ba808ad4cc5a8
[08/10/2023-11:18:29] [V] [TRT] Tactic: 0x1a5ba808ad4cc5a8 Time: 1.04287
[08/10/2023-11:18:29] [V] [TRT] Conv_158 + Relu_159 || Conv_161 + Relu_162 || Conv_164 + Relu_165 || Conv_167 + Relu_168 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x64x64_stage3_warpsize2x2x1_g1_tensor16x8x16 Tactic: 0x21b295c0c8f6c95a
[08/10/2023-11:18:29] [V] [TRT] Tactic: 0x21b295c0c8f6c95a Time: 0.611369
[08/10/2023-11:18:29] [V] [TRT] Conv_158 + Relu_159 || Conv_161 + Relu_162 || Conv_164 + Relu_165 || Conv_167 + Relu_168 Set Tactic Name: ampere_h16816cudnn_128x128_ldg8_relu_exp_medium_nhwc_tn_v1 Tactic: 0x234580e8a194335c
[08/10/2023-11:18:29] [V] [TRT] Tactic: 0x234580e8a194335c Time: 0.523762
[08/10/2023-11:18:29] [V] [TRT] Conv_158 + Relu_159 || Conv_161 + Relu_162 || Conv_164 + Relu_165 || Conv_167 + Relu_168 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x128x32_stage4_warpsize2x2x1_g1_tensor16x8x16_t1r3s3_aACCESS Tactic: 0x245530c34bd6090f
[08/10/2023-11:18:29] [V] [TRT] Tactic: 0x245530c34bd6090f Time: 0.492183
[08/10/2023-11:18:29] [V] [TRT] Conv_158 + Relu_159 || Conv_161 + Relu_162 || Conv_164 + Relu_165 || Conv_167 + Relu_168 Set Tactic Name: ampere_h16816cudnn_64x64_sliced1x2_ldg8_relu_exp_stages_64x6_small_nhwc_tn_v1 Tactic: 0x24e01e7405cfdfe9
[08/10/2023-11:18:29] [V] [TRT] Tactic: 0x24e01e7405cfdfe9 Time: 1.4027
[08/10/2023-11:18:29] [V] [TRT] Conv_158 + Relu_159 || Conv_161 + Relu_162 || Conv_164 + Relu_165 || Conv_167 + Relu_168 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x64x64_stage3_warpsize2x2x1_g1_tensor16x8x16_t1r3s3_aACCESS Tactic: 0x263a38afd75e3a43
[08/10/2023-11:18:29] [V] [TRT] Tactic: 0x263a38afd75e3a43 Time: 0.580667
[08/10/2023-11:18:29] [V] [TRT] Conv_158 + Relu_159 || Conv_161 + Relu_162 || Conv_164 + Relu_165 || Conv_167 + Relu_168 Set Tactic Name: ampere_h16816cudnn_64x128_sliced1x2_ldg8_relu_exp_stages_64x4_large_nhwc_tn_v1 Tactic: 0x2721a7f18c2700db
[08/10/2023-11:18:29] [V] [TRT] Tactic: 0x2721a7f18c2700db Time: 0.966939
[08/10/2023-11:18:29] [V] [TRT] Conv_158 + Relu_159 || Conv_161 + Relu_162 || Conv_164 + Relu_165 || Conv_167 + Relu_168 Set Tactic Name: ampere_h16816cudnn_128x64_sliced1x2_ldg8_relu_exp_stages_64x3_medium_nhwc_tn_v1 Tactic: 0x29329b5741ea05f2
[08/10/2023-11:18:29] [V] [TRT] Tactic: 0x29329b5741ea05f2 Time: 0.665097
[08/10/2023-11:18:29] [V] [TRT] Conv_158 + Relu_159 || Conv_161 + Relu_162 || Conv_164 + Relu_165 || Conv_167 + Relu_168 Set Tactic Name: ampere_h16816cudnn_128x128_ldg8_relu_exp_large_nhwc_tn_v1 Tactic: 0x2970ae65966d569d
[08/10/2023-11:18:29] [V] [TRT] Tactic: 0x2970ae65966d569d Time: 0.527726
[08/10/2023-11:18:29] [V] [TRT] Conv_158 + Relu_159 || Conv_161 + Relu_162 || Conv_164 + Relu_165 || Conv_167 + Relu_168 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x256x64_stage3_warpsize1x4x1_g1_tensor16x8x16 Tactic: 0x29dc14520e0e4eb6
[08/10/2023-11:18:29] [V] [TRT] Tactic: 0x29dc14520e0e4eb6 Time: 0.643323
[08/10/2023-11:18:29] [V] [TRT] Conv_158 + Relu_159 || Conv_161 + Relu_162 || Conv_164 + Relu_165 || Conv_167 + Relu_168 Set Tactic Name: ampere_h16816cudnn_256x64_ldg8_relu_exp_stages_64x3_large_nhwc_tn_v1 Tactic: 0x2cb4a662694e89d3
[08/10/2023-11:18:29] [V] [TRT] Tactic: 0x2cb4a662694e89d3 Time: 0.731154
[08/10/2023-11:18:29] [V] [TRT] Conv_158 + Relu_159 || Conv_161 + Relu_162 || Conv_164 + Relu_165 || Conv_167 + Relu_168 Set Tactic Name: ampere_h16816cudnn_128x128_ldg8_relu_exp_stages_64x3_medium_nhwc_tn_v1 Tactic: 0x2eaa2202de9404d6
[08/10/2023-11:18:29] [V] [TRT] Tactic: 0x2eaa2202de9404d6 Time: 0.690213
[08/10/2023-11:18:29] [V] [TRT] Conv_158 + Relu_159 || Conv_161 + Relu_162 || Conv_164 + Relu_165 || Conv_167 + Relu_168 Set Tactic Name: ampere_h1688cudnn_256x128_ldg8_relu_exp_medium_nhwc_tn_v1 Tactic: 0x30c0f36d0aeeac6a
[08/10/2023-11:18:29] [V] [TRT] Tactic: 0x30c0f36d0aeeac6a Time: 0.714121
[08/10/2023-11:18:29] [V] [TRT] Conv_158 + Relu_159 || Conv_161 + Relu_162 || Conv_164 + Relu_165 || Conv_167 + Relu_168 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x64x64_stage4_warpsize2x2x1_g1_tensor16x8x16_t1r3s3_aACCESS Tactic: 0x30e8a8d7a953e5e9
[08/10/2023-11:18:29] [V] [TRT] Tactic: 0x30e8a8d7a953e5e9 Time: 0.743643
[08/10/2023-11:18:29] [V] [TRT] Conv_158 + Relu_159 || Conv_161 + Relu_162 || Conv_164 + Relu_165 || Conv_167 + Relu_168 Set Tactic Name: ampere_h16816cudnn_128x64_ldg8_relu_exp_stages_64x4_large_nhwc_tn_v1 Tactic: 0x3197d5044d71e98e
[08/10/2023-11:18:29] [V] [TRT] Tactic: 0x3197d5044d71e98e Time: 0.989797
[08/10/2023-11:18:29] [V] [TRT] Conv_158 + Relu_159 || Conv_161 + Relu_162 || Conv_164 + Relu_165 || Conv_167 + Relu_168 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x64x64_stage4_warpsize2x2x1_g1_tensor16x8x16_t1r3s3 Tactic: 0x31d93dc22d2af081
[08/10/2023-11:18:29] [V] [TRT] Tactic: 0x31d93dc22d2af081 Time: 0.780878
[08/10/2023-11:18:29] [V] [TRT] Conv_158 + Relu_159 || Conv_161 + Relu_162 || Conv_164 + Relu_165 || Conv_167 + Relu_168 Set Tactic Name: ampere_h1688cudnn_128x128_ldg8_relu_exp_small_nhwc_tn_v1 Tactic: 0x3369260c04f9ad73
[08/10/2023-11:18:29] [V] [TRT] Tactic: 0x3369260c04f9ad73 Time: 0.666706
[08/10/2023-11:18:29] [V] [TRT] Conv_158 + Relu_159 || Conv_161 + Relu_162 || Conv_164 + Relu_165 || Conv_167 + Relu_168 Set Tactic Name: ampere_h16816cudnn_64x128_ldg8_relu_exp_stages_64x4_small_nhwc_tn_v1 Tactic: 0x3b5fa0f2a8fc2410
[08/10/2023-11:18:29] [V] [TRT] Tactic: 0x3b5fa0f2a8fc2410 Time: 0.875954
[08/10/2023-11:18:29] [V] [TRT] Conv_158 + Relu_159 || Conv_161 + Relu_162 || Conv_164 + Relu_165 || Conv_167 + Relu_168 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x32x32_stage4_warpsize4x1x1_g1_tensor16x8x16_t1r3s3 Tactic: 0x3e7eb35b91b9fa63
[08/10/2023-11:18:29] [V] [TRT] Tactic: 0x3e7eb35b91b9fa63 Time: 0.906043
[08/10/2023-11:18:29] [V] [TRT] Conv_158 + Relu_159 || Conv_161 + Relu_162 || Conv_164 + Relu_165 || Conv_167 + Relu_168 Set Tactic Name: ampere_h16816cudnn_256x64_ldg8_relu_exp_stages_64x3_medium_nhwc_tn_v1 Tactic: 0x3f031df0e579d52c
[08/10/2023-11:18:29] [V] [TRT] Tactic: 0x3f031df0e579d52c Time: 0.725509
[08/10/2023-11:18:29] [V] [TRT] Conv_158 + Relu_159 || Conv_161 + Relu_162 || Conv_164 + Relu_165 || Conv_167 + Relu_168 Set Tactic Name: ampere_h16816cudnn_256x64_sliced1x2_ldg8_relu_exp_small_nhwc_tn_v1 Tactic: 0x42a5b8709d0039be
[08/10/2023-11:18:29] [V] [TRT] Tactic: 0x42a5b8709d0039be Time: 0.825033
[08/10/2023-11:18:29] [V] [TRT] Conv_158 + Relu_159 || Conv_161 + Relu_162 || Conv_164 + Relu_165 || Conv_167 + Relu_168 Set Tactic Name: ampere_h16816cudnn_256x128_ldg8_relu_exp_small_nhwc_tn_v1 Tactic: 0x463794ee4acffd1f
[08/10/2023-11:18:29] [V] [TRT] Tactic: 0x463794ee4acffd1f Time: 0.605394
[08/10/2023-11:18:29] [V] [TRT] Conv_158 + Relu_159 || Conv_161 + Relu_162 || Conv_164 + Relu_165 || Conv_167 + Relu_168 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x64x64_stage4_warpsize2x2x1_g1_tensor16x8x16 Tactic: 0x4a81ea1e51436a30
[08/10/2023-11:18:29] [V] [TRT] Tactic: 0x4a81ea1e51436a30 Time: 0.786345
[08/10/2023-11:18:29] [V] [TRT] Conv_158 + Relu_159 || Conv_161 + Relu_162 || Conv_164 + Relu_165 || Conv_167 + Relu_168 Set Tactic Name: ampere_h1688cudnn_256x64_sliced1x2_ldg8_relu_exp_small_nhwc_tn_v1 Tactic: 0x4aed1956bd10a795
[08/10/2023-11:18:29] [V] [TRT] Tactic: 0x4aed1956bd10a795 Time: 0.883643
[08/10/2023-11:18:29] [V] [TRT] Conv_158 + Relu_159 || Conv_161 + Relu_162 || Conv_164 + Relu_165 || Conv_167 + Relu_168 Set Tactic Name: ampere_h16816cudnn_64x128_sliced1x2_ldg8_relu_exp_stages_64x3_large_nhwc_tn_v1 Tactic: 0x4fc05923455fc266
[08/10/2023-11:18:29] [V] [TRT] Tactic: 0x4fc05923455fc266 Time: 0.671776
[08/10/2023-11:18:29] [V] [TRT] Conv_158 + Relu_159 || Conv_161 + Relu_162 || Conv_164 + Relu_165 || Conv_167 + Relu_168 Set Tactic Name: ampere_h16816cudnn_256x64_ldg8_relu_exp_large_nhwc_tn_v1 Tactic: 0x5013c38f55afa9ba
[08/10/2023-11:18:29] [V] [TRT] Tactic: 0x5013c38f55afa9ba Time: 0.634711
[08/10/2023-11:18:29] [V] [TRT] Conv_158 + Relu_159 || Conv_161 + Relu_162 || Conv_164 + Relu_165 || Conv_167 + Relu_168 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x64x32_stage3_warpsize4x1x1_g1_tensor16x8x16_t1r3s3 Tactic: 0x529f4431bdae94f5
[08/10/2023-11:18:29] [V] [TRT] Tactic: 0x529f4431bdae94f5 Time: 0.51792
[08/10/2023-11:18:29] [V] [TRT] Conv_158 + Relu_159 || Conv_161 + Relu_162 || Conv_164 + Relu_165 || Conv_167 + Relu_168 Set Tactic Name: ampere_h16816cudnn_64x128_ldg8_relu_exp_stages_64x3_small_nhwc_tn_v1 Tactic: 0x53be5e206184e6ad
[08/10/2023-11:18:29] [V] [TRT] Tactic: 0x53be5e206184e6ad Time: 0.674693
[08/10/2023-11:18:29] [V] [TRT] Conv_158 + Relu_159 || Conv_161 + Relu_162 || Conv_164 + Relu_165 || Conv_167 + Relu_168 Set Tactic Name: ampere_h16816cudnn_64x64_ldg8_relu_exp_stages_64x5_large_nhwc_tn_v1 Tactic: 0x544bff7ff9c5d908
[08/10/2023-11:18:29] [V] [TRT] Tactic: 0x544bff7ff9c5d908 Time: 1.08613
[08/10/2023-11:18:29] [V] [TRT] Conv_158 + Relu_159 || Conv_161 + Relu_162 || Conv_164 + Relu_165 || Conv_167 + Relu_168 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x64x64_stage3_warpsize2x2x1_g1_tensor16x8x16 Tactic: 0x5568fd8a32f4a40f
[08/10/2023-11:18:29] [V] [TRT] Tactic: 0x5568fd8a32f4a40f Time: 0.620919
[08/10/2023-11:18:29] [V] [TRT] Conv_158 + Relu_159 || Conv_161 + Relu_162 || Conv_164 + Relu_165 || Conv_167 + Relu_168 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x128x64_stage3_warpsize2x2x1_g1_tensor16x8x16 Tactic: 0x57c9a5ff682354a6
[08/10/2023-11:18:29] [V] [TRT] Tactic: 0x57c9a5ff682354a6 Time: 0.652311
[08/10/2023-11:18:29] [V] [TRT] Conv_158 + Relu_159 || Conv_161 + Relu_162 || Conv_164 + Relu_165 || Conv_167 + Relu_168 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x32x64_stage5_warpsize2x2x1_g1_tensor16x8x16 Tactic: 0x5820b3dda403c4d0
[08/10/2023-11:18:29] [V] [TRT] Tactic: 0x5820b3dda403c4d0 Time: 1.28981
[08/10/2023-11:18:29] [V] [TRT] Conv_158 + Relu_159 || Conv_161 + Relu_162 || Conv_164 + Relu_165 || Conv_167 + Relu_168 Set Tactic Name: ampere_h16816cudnn_128x128_ldg8_relu_exp_stages_64x3_small_nhwc_tn_v1 Tactic: 0x58816bf2e9c36afb
[08/10/2023-11:18:29] [V] [TRT] Tactic: 0x58816bf2e9c36afb Time: 0.690034
[08/10/2023-11:18:29] [V] [TRT] Conv_158 + Relu_159 || Conv_161 + Relu_162 || Conv_164 + Relu_165 || Conv_167 + Relu_168 Set Tactic Name: ampere_h16816cudnn_128x64_ldg8_relu_exp_stages_64x3_large_nhwc_tn_v1 Tactic: 0x59762bd684092b33
[08/10/2023-11:18:29] [V] [TRT] Tactic: 0x59762bd684092b33 Time: 0.684334
[08/10/2023-11:18:29] [V] [TRT] Conv_158 + Relu_159 || Conv_161 + Relu_162 || Conv_164 + Relu_165 || Conv_167 + Relu_168 Set Tactic Name: ampere_h16816cudnn_128x64_sliced1x2_ldg8_relu_exp_stages_64x4_medium_nhwc_tn_v1 Tactic: 0x59c5c647b4c76593
[08/10/2023-11:18:29] [V] [TRT] Tactic: 0x59c5c647b4c76593 Time: 0.971442
[08/10/2023-11:18:29] [V] [TRT] Conv_158 + Relu_159 || Conv_161 + Relu_162 || Conv_164 + Relu_165 || Conv_167 + Relu_168 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x128x64_stage3_warpsize4x2x1_g1_tensor16x8x16 Tactic: 0x5a55274960724ba8
[08/10/2023-11:18:29] [V] [TRT] Tactic: 0x5a55274960724ba8 Time: 0.574176
[08/10/2023-11:18:29] [V] [TRT] Conv_158 + Relu_159 || Conv_161 + Relu_162 || Conv_164 + Relu_165 || Conv_167 + Relu_168 Set Tactic Name: ampere_h1688cudnn_256x128_ldg8_relu_exp_large_nhwc_tn_v1 Tactic: 0x5c2e1c87d85b06f1
[08/10/2023-11:18:29] [V] [TRT] Tactic: 0x5c2e1c87d85b06f1 Time: 0.723113
[08/10/2023-11:18:29] [V] [TRT] Conv_158 + Relu_159 || Conv_161 + Relu_162 || Conv_164 + Relu_165 || Conv_167 + Relu_168 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x256x64_stage3_warpsize1x4x1_g1_tensor16x8x16 Tactic: 0x5d067c18f40c23e3
[08/10/2023-11:18:29] [V] [TRT] Tactic: 0x5d067c18f40c23e3 Time: 0.627808
[08/10/2023-11:18:29] [V] [TRT] Conv_158 + Relu_159 || Conv_161 + Relu_162 || Conv_164 + Relu_165 || Conv_167 + Relu_168 Set Tactic Name: ampere_h16816cudnn_64x64_ldg8_relu_exp_stages_64x5_medium_nhwc_tn_v1 Tactic: 0x5f31c22ec167f384
[08/10/2023-11:18:29] [V] [TRT] Tactic: 0x5f31c22ec167f384 Time: 1.07818
[08/10/2023-11:18:29] [V] [TRT] Conv_158 + Relu_159 || Conv_161 + Relu_162 || Conv_164 + Relu_165 || Conv_167 + Relu_168 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x128x32_stage4_warpsize2x2x1_g1_tensor16x8x16_t1r3s3 Tactic: 0x60c3421152ef8e10
[08/10/2023-11:18:29] [V] [TRT] Tactic: 0x60c3421152ef8e10 Time: 0.4824
[08/10/2023-11:18:29] [V] [TRT] Conv_158 + Relu_159 || Conv_161 + Relu_162 || Conv_164 + Relu_165 || Conv_167 + Relu_168 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x128x32_stage3_warpsize4x2x1_g1_tensor16x8x16 Tactic: 0x60da8c7151d91e47
[08/10/2023-11:18:29] [V] [TRT] Tactic: 0x60da8c7151d91e47 Time: 0.555383
[08/10/2023-11:18:29] [V] [TRT] Conv_158 + Relu_159 || Conv_161 + Relu_162 || Conv_164 + Relu_165 || Conv_167 + Relu_168 Set Tactic Name: ampere_h1688cudnn_256x64_ldg8_relu_exp_medium_nhwc_tn_v1 Tactic: 0x6426696f872a3b13
[08/10/2023-11:18:30] [V] [TRT] Tactic: 0x6426696f872a3b13 Time: 0.817271
[08/10/2023-11:18:30] [V] [TRT] Conv_158 + Relu_159 || Conv_161 + Relu_162 || Conv_164 + Relu_165 || Conv_167 + Relu_168 Set Tactic Name: ampere_h16816cudnn_128x64_ldg8_relu_exp_stages_64x3_medium_nhwc_tn_v1 Tactic: 0x699be152cfb6d6ff
[08/10/2023-11:18:30] [V] [TRT] Tactic: 0x699be152cfb6d6ff Time: 0.687689
[08/10/2023-11:18:30] [V] [TRT] Conv_158 + Relu_159 || Conv_161 + Relu_162 || Conv_164 + Relu_165 || Conv_167 + Relu_168 Set Tactic Name: ampere_h16816cudnn_64x64_ldg8_relu_exp_stages_64x6_large_nhwc_tn_v1 Tactic: 0x6af049035146c349
[08/10/2023-11:18:30] [V] [TRT] Tactic: 0x6af049035146c349 Time: 1.46154
[08/10/2023-11:18:30] [V] [TRT] Conv_158 + Relu_159 || Conv_161 + Relu_162 || Conv_164 + Relu_165 || Conv_167 + Relu_168 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x256x32_stage3_warpsize2x4x1_g1_tensor16x8x16 Tactic: 0x7005d10718f6c22d
[08/10/2023-11:18:30] [V] [TRT] Tactic: 0x7005d10718f6c22d Time: 0.541975
[08/10/2023-11:18:30] [V] [TRT] Conv_158 + Relu_159 || Conv_161 + Relu_162 || Conv_164 + Relu_165 || Conv_167 + Relu_168 Set Tactic Name: ampere_h16816cudnn_64x64_sliced1x2_ldg8_relu_exp_stages_64x5_medium_nhwc_tn_v1 Tactic: 0x706f08da35c795a5
[08/10/2023-11:18:30] [V] [TRT] Tactic: 0x706f08da35c795a5 Time: 1.04688
[08/10/2023-11:18:30] [V] [TRT] Conv_158 + Relu_159 || Conv_161 + Relu_162 || Conv_164 + Relu_165 || Conv_167 + Relu_168 Set Tactic Name: ampere_h16816cudnn_128x64_sliced1x2_ldg8_relu_exp_stages_64x4_small_nhwc_tn_v1 Tactic: 0x7163d33a4d8ce8d7
[08/10/2023-11:18:30] [V] [TRT] Tactic: 0x7163d33a4d8ce8d7 Time: 0.944873
[08/10/2023-11:18:30] [V] [TRT] Conv_158 + Relu_159 || Conv_161 + Relu_162 || Conv_164 + Relu_165 || Conv_167 + Relu_168 Set Tactic Name: ampere_h1688cudnn_256x64_ldg8_relu_exp_small_nhwc_tn_v1 Tactic: 0x7aad3976677d7155
[08/10/2023-11:18:30] [V] [TRT] Tactic: 0x7aad3976677d7155 Time: 0.782048
[08/10/2023-11:18:30] [V] [TRT] Conv_158 + Relu_159 || Conv_161 + Relu_162 || Conv_164 + Relu_165 || Conv_167 + Relu_168 Set Tactic Name: ampere_h16816cudnn_64x64_sliced1x2_ldg8_relu_exp_stages_64x5_large_nhwc_tn_v1 Tactic: 0x8015519605ab9963
[08/10/2023-11:18:30] [V] [TRT] Tactic: 0x8015519605ab9963 Time: 1.06701
[08/10/2023-11:18:30] [V] [TRT] Conv_158 + Relu_159 || Conv_161 + Relu_162 || Conv_164 + Relu_165 || Conv_167 + Relu_168 Set Tactic Name: ampere_h16816cudnn_256x64_sliced1x2_ldg8_relu_exp_medium_nhwc_tn_v1 Tactic: 0x82f8a2214b0b4178
[08/10/2023-11:18:30] [V] [TRT] Tactic: 0x82f8a2214b0b4178 Time: 0.911854
[08/10/2023-11:18:30] [V] [TRT] Conv_158 + Relu_159 || Conv_161 + Relu_162 || Conv_164 + Relu_165 || Conv_167 + Relu_168 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x32x32_stage4_warpsize4x1x1_g1_tensor16x8x16 Tactic: 0x834e11ecd4ab9454
[08/10/2023-11:18:30] [V] [TRT] Tactic: 0x834e11ecd4ab9454 Time: 0.929614
[08/10/2023-11:18:30] [V] [TRT] Conv_158 + Relu_159 || Conv_161 + Relu_162 || Conv_164 + Relu_165 || Conv_167 + Relu_168 Set Tactic Name: ampere_h16816cudnn_128x64_sliced1x2_ldg8_relu_exp_stages_64x3_large_nhwc_tn_v1 Tactic: 0x83ccd4762c1376a1
[08/10/2023-11:18:30] [V] [TRT] Tactic: 0x83ccd4762c1376a1 Time: 0.675822
[08/10/2023-11:18:30] [V] [TRT] Conv_158 + Relu_159 || Conv_161 + Relu_162 || Conv_164 + Relu_165 || Conv_167 + Relu_168 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x64x64_stage3_warpsize4x1x1_g1_tensor16x8x16_t1r3s3 Tactic: 0x84ee897dd1f4d2aa
[08/10/2023-11:18:30] [V] [TRT] Tactic: 0x84ee897dd1f4d2aa Time: 0.669609
[08/10/2023-11:18:30] [V] [TRT] Conv_158 + Relu_159 || Conv_161 + Relu_162 || Conv_164 + Relu_165 || Conv_167 + Relu_168 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x128x64_stage3_warpsize2x2x1_g1_tensor16x8x16 Tactic: 0x866e7a5f6401b67f
[08/10/2023-11:18:30] [V] [TRT] Tactic: 0x866e7a5f6401b67f Time: 0.616795
[08/10/2023-11:18:30] [V] [TRT] Conv_158 + Relu_159 || Conv_161 + Relu_162 || Conv_164 + Relu_165 || Conv_167 + Relu_168 Set Tactic Name: ampere_h16816cudnn_64x64_ldg8_relu_exp_stages_64x6_medium_nhwc_tn_v1 Tactic: 0x88971eec55aba850
[08/10/2023-11:18:30] [V] [TRT] Tactic: 0x88971eec55aba850 Time: 1.42871
[08/10/2023-11:18:30] [V] [TRT] Conv_158 + Relu_159 || Conv_161 + Relu_162 || Conv_164 + Relu_165 || Conv_167 + Relu_168 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x128x64_stage3_warpsize4x2x1_g1_tensor16x8x16 Tactic: 0x8bf2f8e96c50a971
[08/10/2023-11:18:30] [V] [TRT] Tactic: 0x8bf2f8e96c50a971 Time: 0.555337
[08/10/2023-11:18:30] [V] [TRT] Conv_158 + Relu_159 || Conv_161 + Relu_162 || Conv_164 + Relu_165 || Conv_167 + Relu_168 Set Tactic Name: ampere_h16816cudnn_256x128_ldg8_relu_exp_stages_64x3_large_nhwc_tn_v1 Tactic: 0x94576ece6beb35e3
[08/10/2023-11:18:30] [V] [TRT] Tactic: 0x94576ece6beb35e3 Time: 0.615008
[08/10/2023-11:18:30] [V] [TRT] Conv_158 + Relu_159 || Conv_161 + Relu_162 || Conv_164 + Relu_165 || Conv_167 + Relu_168 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x64x64_stage3_warpsize4x1x1_g1_tensor16x8x16 Tactic: 0x9fff6e65019cc310
[08/10/2023-11:18:30] [V] [TRT] Tactic: 0x9fff6e65019cc310 Time: 0.652219
[08/10/2023-11:18:30] [V] [TRT] Conv_158 + Relu_159 || Conv_161 + Relu_162 || Conv_164 + Relu_165 || Conv_167 + Relu_168 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x32x64_stage5_warpsize2x2x1_g1_tensor16x8x16_t1r3s3 Tactic: 0xa033e20ae9f412b2
[08/10/2023-11:18:30] [V] [TRT] Tactic: 0xa033e20ae9f412b2 Time: 1.15813
[08/10/2023-11:18:30] [V] [TRT] Conv_158 + Relu_159 || Conv_161 + Relu_162 || Conv_164 + Relu_165 || Conv_167 + Relu_168 Set Tactic Name: ampere_h16816cudnn_64x128_ldg8_relu_exp_stages_64x4_large_nhwc_tn_v1 Tactic: 0xa111596c001b78db
[08/10/2023-11:18:30] [V] [TRT] Tactic: 0xa111596c001b78db Time: 0.898638
[08/10/2023-11:18:30] [V] [TRT] Conv_158 + Relu_159 || Conv_161 + Relu_162 || Conv_164 + Relu_165 || Conv_167 + Relu_168 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x256x32_stage3_warpsize2x4x1_g1_tensor16x8x16 Tactic: 0xa1a20ea714d420f4
[08/10/2023-11:18:30] [V] [TRT] Tactic: 0xa1a20ea714d420f4 Time: 0.544261
[08/10/2023-11:18:30] [V] [TRT] Conv_158 + Relu_159 || Conv_161 + Relu_162 || Conv_164 + Relu_165 || Conv_167 + Relu_168 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x32x32_stage4_warpsize4x1x1_g1_tensor16x8x16_t1r3s3 Tactic: 0xa40cb43c296a36a8
[08/10/2023-11:18:30] [V] [TRT] Tactic: 0xa40cb43c296a36a8 Time: 0.828343
[08/10/2023-11:18:30] [V] [TRT] Conv_158 + Relu_159 || Conv_161 + Relu_162 || Conv_164 + Relu_165 || Conv_167 + Relu_168 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x64x32_stage3_warpsize4x1x1_g1_tensor16x8x16 Tactic: 0xa570c55d303796ff
[08/10/2023-11:18:30] [V] [TRT] Tactic: 0xa570c55d303796ff Time: 0.559616
[08/10/2023-11:18:30] [V] [TRT] Conv_158 + Relu_159 || Conv_161 + Relu_162 || Conv_164 + Relu_165 || Conv_167 + Relu_168 Set Tactic Name: ampere_h16816cudnn_64x64_sliced1x2_ldg8_relu_exp_stages_64x6_medium_nhwc_tn_v1 Tactic: 0xa7c9d418a10bce71
[08/10/2023-11:18:30] [V] [TRT] Tactic: 0xa7c9d418a10bce71 Time: 1.54153
[08/10/2023-11:18:30] [V] [TRT] Conv_158 + Relu_159 || Conv_161 + Relu_162 || Conv_164 + Relu_165 || Conv_167 + Relu_168 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x32x32_stage4_warpsize4x1x1_g1_tensor16x8x16 Tactic: 0xa83b68f30462f971
[08/10/2023-11:18:30] [V] [TRT] Tactic: 0xa83b68f30462f971 Time: 0.85643
[08/10/2023-11:18:30] [V] [TRT] Conv_158 + Relu_159 || Conv_161 + Relu_162 || Conv_164 + Relu_165 || Conv_167 + Relu_168 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x256x32_stage3_warpsize2x4x1_g1_tensor16x8x16_t1r3s3 Tactic: 0xa9177bbe4e767df8
[08/10/2023-11:18:30] [V] [TRT] Tactic: 0xa9177bbe4e767df8 Time: 0.548507
[08/10/2023-11:18:30] [V] [TRT] Conv_158 + Relu_159 || Conv_161 + Relu_162 || Conv_164 + Relu_165 || Conv_167 + Relu_168 Set Tactic Name: ampere_h1688cudnn_128x128_ldg8_relu_exp_large_nhwc_tn_v1 Tactic: 0xa927df92ac1ef1b8
[08/10/2023-11:18:30] [V] [TRT] Tactic: 0xa927df92ac1ef1b8 Time: 0.681861
[08/10/2023-11:18:30] [V] [TRT] Conv_158 + Relu_159 || Conv_161 + Relu_162 || Conv_164 + Relu_165 || Conv_167 + Relu_168 Set Tactic Name: ampere_h16816cudnn_128x64_ldg8_relu_exp_stages_64x4_small_nhwc_tn_v1 Tactic: 0xabd92c9ae596b545
[08/10/2023-11:18:30] [V] [TRT] Tactic: 0xabd92c9ae596b545 Time: 0.936507
[08/10/2023-11:18:30] [V] [TRT] Conv_158 + Relu_159 || Conv_161 + Relu_162 || Conv_164 + Relu_165 || Conv_167 + Relu_168 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x128x32_stage5_warpsize2x2x1_g1_tensor16x8x16 Tactic: 0xafd1e8bf6bd3d638
[08/10/2023-11:18:30] [V] [TRT] Tactic: 0xafd1e8bf6bd3d638 Time: 0.61637
[08/10/2023-11:18:30] [V] [TRT] Conv_158 + Relu_159 || Conv_161 + Relu_162 || Conv_164 + Relu_165 || Conv_167 + Relu_168 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x128x32_stage3_warpsize4x2x1_g1_tensor16x8x16 Tactic: 0xb17d53d15dfbfc9e
[08/10/2023-11:18:30] [V] [TRT] Tactic: 0xb17d53d15dfbfc9e Time: 0.559739
[08/10/2023-11:18:30] [V] [TRT] Conv_158 + Relu_159 || Conv_161 + Relu_162 || Conv_164 + Relu_165 || Conv_167 + Relu_168 Set Tactic Name: ampere_h16816cudnn_128x128_ldg8_relu_exp_small_nhwc_tn_v1 Tactic: 0xb33e57fb3e8a0a56
[08/10/2023-11:18:30] [V] [TRT] Tactic: 0xb33e57fb3e8a0a56 Time: 0.517376
[08/10/2023-11:18:30] [V] [TRT] Conv_158 + Relu_159 || Conv_161 + Relu_162 || Conv_164 + Relu_165 || Conv_167 + Relu_168 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x64x32_stage5_warpsize2x2x1_g1_tensor16x8x16_t1r3s3 Tactic: 0xb4bec086187edcfc
[08/10/2023-11:18:30] [V] [TRT] Tactic: 0xb4bec086187edcfc Time: 0.599013
[08/10/2023-11:18:30] [V] [TRT] Conv_158 + Relu_159 || Conv_161 + Relu_162 || Conv_164 + Relu_165 || Conv_167 + Relu_168 Set Tactic Name: ampere_h16816cudnn_256x64_ldg8_relu_exp_stages_64x3_small_nhwc_tn_v1 Tactic: 0xb6fa5ffcc1a9d518
[08/10/2023-11:18:30] [V] [TRT] Tactic: 0xb6fa5ffcc1a9d518 Time: 0.715269
[08/10/2023-11:18:30] [V] [TRT] Conv_158 + Relu_159 || Conv_161 + Relu_162 || Conv_164 + Relu_165 || Conv_167 + Relu_168 Set Tactic Name: ampere_h16816cudnn_64x128_sliced1x2_ldg8_relu_exp_stages_64x3_medium_nhwc_tn_v1 Tactic: 0xb85e52e87caf60a2
[08/10/2023-11:18:30] [V] [TRT] Tactic: 0xb85e52e87caf60a2 Time: 0.673646
[08/10/2023-11:18:30] [V] [TRT] Conv_158 + Relu_159 || Conv_161 + Relu_162 || Conv_164 + Relu_165 || Conv_167 + Relu_168 Set Tactic Name: ampere_h16816cudnn_64x128_ldg8_relu_exp_stages_64x3_medium_nhwc_tn_v1 Tactic: 0xb8d86216e1235cda
[08/10/2023-11:18:30] [V] [TRT] Tactic: 0xb8d86216e1235cda Time: 0.673243
[08/10/2023-11:18:30] [V] [TRT] Conv_158 + Relu_159 || Conv_161 + Relu_162 || Conv_164 + Relu_165 || Conv_167 + Relu_168 Set Tactic Name: ampere_h16816cudnn_256x128_ldg8_relu_exp_stages_64x3_medium_nhwc_tn_v1 Tactic: 0xb9171d70ba2eda4b
[08/10/2023-11:18:30] [V] [TRT] Tactic: 0xb9171d70ba2eda4b Time: 0.611369
[08/10/2023-11:18:30] [V] [TRT] Conv_158 + Relu_159 || Conv_161 + Relu_162 || Conv_164 + Relu_165 || Conv_167 + Relu_168 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x64x32_stage5_warpsize2x2x1_g1_tensor16x8x16 Tactic: 0xbd08239a9317f2fd
[08/10/2023-11:18:30] [V] [TRT] Tactic: 0xbd08239a9317f2fd Time: 0.617682
[08/10/2023-11:18:30] [V] [TRT] Conv_158 + Relu_159 || Conv_161 + Relu_162 || Conv_164 + Relu_165 || Conv_167 + Relu_168 Set Tactic Name: ampere_h16816cudnn_64x128_sliced1x2_ldg8_relu_exp_stages_64x4_small_nhwc_tn_v1 Tactic: 0xbd6f5e6f24c05c10
[08/10/2023-11:18:30] [V] [TRT] Tactic: 0xbd6f5e6f24c05c10 Time: 0.930459
[08/10/2023-11:18:30] [V] [TRT] Conv_158 + Relu_159 || Conv_161 + Relu_162 || Conv_164 + Relu_165 || Conv_167 + Relu_168 Set Tactic Name: ampere_h16816cudnn_64x64_sliced1x2_ldg8_relu_exp_stages_64x6_large_nhwc_tn_v1 Tactic: 0xbeaee7eaad288322
[08/10/2023-11:18:30] [V] [TRT] Tactic: 0xbeaee7eaad288322 Time: 1.52122
[08/10/2023-11:18:30] [V] [TRT] Conv_158 + Relu_159 || Conv_161 + Relu_162 || Conv_164 + Relu_165 || Conv_167 + Relu_168 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x128x64_stage3_warpsize2x2x1_g1_tensor16x8x16_t1r3s3 Tactic: 0xc0a02dc6095497cc
[08/10/2023-11:18:30] [V] [TRT] Tactic: 0xc0a02dc6095497cc Time: 0.629714
[08/10/2023-11:18:30] [V] [TRT] Conv_158 + Relu_159 || Conv_161 + Relu_162 || Conv_164 + Relu_165 || Conv_167 + Relu_168 Set Tactic Name: ampere_h16816cudnn_128x128_ldg8_relu_exp_stages_64x3_large_nhwc_tn_v1 Tactic: 0xc2cf926c41243630
[08/10/2023-11:18:30] [V] [TRT] Tactic: 0xc2cf926c41243630 Time: 0.692878
[08/10/2023-11:18:30] [V] [TRT] Conv_158 + Relu_159 || Conv_161 + Relu_162 || Conv_164 + Relu_165 || Conv_167 + Relu_168 Set Tactic Name: ampere_h16816cudnn_128x64_ldg8_relu_exp_stages_64x3_small_nhwc_tn_v1 Tactic: 0xc338d2482cee77f8
[08/10/2023-11:18:30] [V] [TRT] Tactic: 0xc338d2482cee77f8 Time: 0.690752
[08/10/2023-11:18:30] [V] [TRT] Conv_158 + Relu_159 || Conv_161 + Relu_162 || Conv_164 + Relu_165 || Conv_167 + Relu_168 Set Tactic Name: ampere_h1688cudnn_256x128_ldg8_relu_exp_small_nhwc_tn_v1 Tactic: 0xc660e51970bc5a3a
[08/10/2023-11:18:30] [V] [TRT] Tactic: 0xc660e51970bc5a3a Time: 0.69275
[08/10/2023-11:18:30] [V] [TRT] Conv_158 + Relu_159 || Conv_161 + Relu_162 || Conv_164 + Relu_165 || Conv_167 + Relu_168 Set Tactic Name: ampere_h16816cudnn_64x128_ldg8_relu_exp_stages_64x4_medium_nhwc_tn_v1 Tactic: 0xc82f3f06140e3cbb
[08/10/2023-11:18:30] [V] [TRT] Tactic: 0xc82f3f06140e3cbb Time: 0.903854
[08/10/2023-11:18:30] [V] [TRT] Conv_158 + Relu_159 || Conv_161 + Relu_162 || Conv_164 + Relu_165 || Conv_167 + Relu_168 Set Tactic Name: ampere_h16816cudnn_64x128_sliced1x2_ldg8_relu_exp_stages_64x4_medium_nhwc_tn_v1 Tactic: 0xc8a90ff8898200c3
[08/10/2023-11:18:30] [V] [TRT] Tactic: 0xc8a90ff8898200c3 Time: 0.953289
[08/10/2023-11:18:30] [V] [TRT] Conv_158 + Relu_159 || Conv_161 + Relu_162 || Conv_164 + Relu_165 || Conv_167 + Relu_168 Set Tactic Name: ampere_h1688cudnn_256x64_sliced1x2_ldg8_relu_exp_medium_nhwc_tn_v1 Tactic: 0xc9cc55109bb4de26
[08/10/2023-11:18:30] [V] [TRT] Tactic: 0xc9cc55109bb4de26 Time: 0.941559
[08/10/2023-11:18:30] [V] [TRT] Conv_158 + Relu_159 || Conv_161 + Relu_162 || Conv_164 + Relu_165 || Conv_167 + Relu_168 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x64x32_stage5_warpsize2x2x1_g1_tensor16x8x16 Tactic: 0xc9d24bd069159fa8
[08/10/2023-11:18:30] [V] [TRT] Tactic: 0xc9d24bd069159fa8 Time: 0.675333
[08/10/2023-11:18:30] [V] [TRT] Conv_158 + Relu_159 || Conv_161 + Relu_162 || Conv_164 + Relu_165 || Conv_167 + Relu_168 Set Tactic Name: ampere_h16816cudnn_64x128_ldg8_relu_exp_stages_64x3_large_nhwc_tn_v1 Tactic: 0xc9f0a7bec963ba66
[08/10/2023-11:18:30] [V] [TRT] Tactic: 0xc9f0a7bec963ba66 Time: 0.681152
[08/10/2023-11:18:30] [V] [TRT] Conv_158 + Relu_159 || Conv_161 + Relu_162 || Conv_164 + Relu_165 || Conv_167 + Relu_168 Set Tactic Name: ampere_h16816cudnn_256x64_ldg8_relu_exp_small_nhwc_tn_v1 Tactic: 0xca5d3a11fd48f571
[08/10/2023-11:18:30] [V] [TRT] Tactic: 0xca5d3a11fd48f571 Time: 0.606341
[08/10/2023-11:18:30] [V] [TRT] Conv_158 + Relu_159 || Conv_161 + Relu_162 || Conv_164 + Relu_165 || Conv_167 + Relu_168 Set Tactic Name: ampere_h16816cudnn_64x64_ldg8_relu_exp_stages_64x5_small_nhwc_tn_v1 Tactic: 0xce0506e1512285c3
[08/10/2023-11:18:30] [V] [TRT] Tactic: 0xce0506e1512285c3 Time: 1.07761
[08/10/2023-11:18:30] [V] [TRT] Conv_158 + Relu_159 || Conv_161 + Relu_162 || Conv_164 + Relu_165 || Conv_167 + Relu_168 Set Tactic Name: ampere_h1688cudnn_256x64_sliced1x2_ldg8_relu_exp_large_nhwc_tn_v1 Tactic: 0xd0a3e0c815f7fb5e
[08/10/2023-11:18:30] [V] [TRT] Tactic: 0xd0a3e0c815f7fb5e Time: 0.952608
[08/10/2023-11:18:30] [V] [TRT] Conv_158 + Relu_159 || Conv_161 + Relu_162 || Conv_164 + Relu_165 || Conv_167 + Relu_168 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x64x32_stage3_warpsize4x1x1_g1_tensor16x8x16 Tactic: 0xd1aaad17ca35fbaa
[08/10/2023-11:18:30] [V] [TRT] Tactic: 0xd1aaad17ca35fbaa Time: 0.518345
[08/10/2023-11:18:30] [V] [TRT] Conv_158 + Relu_159 || Conv_161 + Relu_162 || Conv_164 + Relu_165 || Conv_167 + Relu_168 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x128x64_stage3_warpsize4x2x1_g1_tensor16x8x16_t1r3s3 Tactic: 0xd2fca0486ca97266
[08/10/2023-11:18:30] [V] [TRT] Tactic: 0xd2fca0486ca97266 Time: 0.570519
[08/10/2023-11:18:30] [V] [TRT] Conv_158 + Relu_159 || Conv_161 + Relu_162 || Conv_164 + Relu_165 || Conv_167 + Relu_168 Set Tactic Name: ampere_h16816cudnn_64x128_sliced1x2_ldg8_relu_exp_stages_64x3_small_nhwc_tn_v1 Tactic: 0xd58ea0bdedb89ead
[08/10/2023-11:18:30] [V] [TRT] Tactic: 0xd58ea0bdedb89ead Time: 0.668475
[08/10/2023-11:18:30] [V] [TRT] Conv_158 + Relu_159 || Conv_161 + Relu_162 || Conv_164 + Relu_165 || Conv_167 + Relu_168 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x256x64_stage3_warpsize1x4x1_g1_tensor16x8x16_t1r3s3 Tactic: 0xd7c261fa01db2af9
[08/10/2023-11:18:30] [V] [TRT] Tactic: 0xd7c261fa01db2af9 Time: 0.625499
[08/10/2023-11:18:30] [V] [TRT] Conv_158 + Relu_159 || Conv_161 + Relu_162 || Conv_164 + Relu_165 || Conv_167 + Relu_168 Set Tactic Name: ampere_h16816cudnn_256x64_sliced1x2_ldg8_relu_exp_large_nhwc_tn_v1 Tactic: 0xd8eb41ee35e76575
[08/10/2023-11:18:30] [V] [TRT] Tactic: 0xd8eb41ee35e76575 Time: 0.902496
[08/10/2023-11:18:30] [V] [TRT] Conv_158 + Relu_159 || Conv_161 + Relu_162 || Conv_164 + Relu_165 || Conv_167 + Relu_168 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x128x32_stage5_warpsize2x2x1_g1_tensor16x8x16 Tactic: 0xdb0b80f591d1bb6d
[08/10/2023-11:18:30] [V] [TRT] Tactic: 0xdb0b80f591d1bb6d Time: 0.599406
[08/10/2023-11:18:30] [V] [TRT] Conv_158 + Relu_159 || Conv_161 + Relu_162 || Conv_164 + Relu_165 || Conv_167 + Relu_168 Set Tactic Name: ampere_h16816cudnn_256x128_ldg8_relu_exp_large_nhwc_tn_v1 Tactic: 0xdc796d70e228a1d4
[08/10/2023-11:18:30] [V] [TRT] Tactic: 0xdc796d70e228a1d4 Time: 0.628096
[08/10/2023-11:18:30] [V] [TRT] Conv_158 + Relu_159 || Conv_161 + Relu_162 || Conv_164 + Relu_165 || Conv_167 + Relu_168 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x32x32_stage4_warpsize4x1x1_g1_tensor16x8x16 Tactic: 0xdce100b9fe609424
[08/10/2023-11:18:30] [V] [TRT] Tactic: 0xdce100b9fe609424 Time: 0.881545
[08/10/2023-11:18:30] [V] [TRT] Conv_158 + Relu_159 || Conv_161 + Relu_162 || Conv_164 + Relu_165 || Conv_167 + Relu_168 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x128x32_stage4_warpsize2x2x1_g1_tensor16x8x16 Tactic: 0xdfa020ef435ef810
[08/10/2023-11:18:30] [V] [TRT] Tactic: 0xdfa020ef435ef810 Time: 0.492891
[08/10/2023-11:18:30] [V] [TRT] Conv_158 + Relu_159 || Conv_161 + Relu_162 || Conv_164 + Relu_165 || Conv_167 + Relu_168 Set Tactic Name: ampere_h1688cudnn_256x64_ldg8_relu_exp_large_nhwc_tn_v1 Tactic: 0xe0e3c0e8cf9a2d9e
[08/10/2023-11:18:30] [V] [TRT] Tactic: 0xe0e3c0e8cf9a2d9e Time: 0.831538
[08/10/2023-11:18:30] [V] [TRT] Conv_158 + Relu_159 || Conv_161 + Relu_162 || Conv_164 + Relu_165 || Conv_167 + Relu_168 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x32x64_stage5_warpsize2x2x1_g1_tensor16x8x16 Tactic: 0xe1ff5ad20f5c6bf6
[08/10/2023-11:18:30] [V] [TRT] Tactic: 0xe1ff5ad20f5c6bf6 Time: 1.32761
[08/10/2023-11:18:30] [V] [TRT] Conv_158 + Relu_159 || Conv_161 + Relu_162 || Conv_164 + Relu_165 || Conv_167 + Relu_168 Set Tactic Name: ampere_h16816cudnn_256x64_ldg8_relu_exp_medium_nhwc_tn_v1 Tactic: 0xe4711898bd599c36
[08/10/2023-11:18:30] [V] [TRT] Tactic: 0xe4711898bd599c36 Time: 0.627666
[08/10/2023-11:18:30] [V] [TRT] Conv_158 + Relu_159 || Conv_161 + Relu_162 || Conv_164 + Relu_165 || Conv_167 + Relu_168 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x64x64_stage3_warpsize4x1x1_g1_tensor16x8x16 Tactic: 0xeb25062ffb9eae45
[08/10/2023-11:18:30] [V] [TRT] Tactic: 0xeb25062ffb9eae45 Time: 0.684197
[08/10/2023-11:18:30] [V] [TRT] Conv_158 + Relu_159 || Conv_161 + Relu_162 || Conv_164 + Relu_165 || Conv_167 + Relu_168 Set Tactic Name: ampere_h16816cudnn_128x64_sliced1x2_ldg8_relu_exp_stages_64x4_large_nhwc_tn_v1 Tactic: 0xeb2d2aa4e56bb41c
[08/10/2023-11:18:30] [V] [TRT] Tactic: 0xeb2d2aa4e56bb41c Time: 1.00061
[08/10/2023-11:18:30] [V] [TRT] Conv_158 + Relu_159 || Conv_161 + Relu_162 || Conv_164 + Relu_165 || Conv_167 + Relu_168 Set Tactic Name: ampere_h16816cudnn_64x64_ldg8_relu_exp_stages_64x6_small_nhwc_tn_v1 Tactic: 0xf0beb09df9a19f82
[08/10/2023-11:18:30] [V] [TRT] Tactic: 0xf0beb09df9a19f82 Time: 1.35224
[08/10/2023-11:18:30] [V] [TRT] Conv_158 + Relu_159 || Conv_161 + Relu_162 || Conv_164 + Relu_165 || Conv_167 + Relu_168 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x64x64_stage4_warpsize2x2x1_g1_tensor16x8x16 Tactic: 0xf35e0311fa1cc516
[08/10/2023-11:18:30] [V] [TRT] Tactic: 0xf35e0311fa1cc516 Time: 0.785198
[08/10/2023-11:18:30] [V] [TRT] Conv_158 + Relu_159 || Conv_161 + Relu_162 || Conv_164 + Relu_165 || Conv_167 + Relu_168 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x32x32_stage4_warpsize4x1x1_g1_tensor16x8x16 Tactic: 0xf79479a62ea9f901
[08/10/2023-11:18:30] [V] [TRT] Tactic: 0xf79479a62ea9f901 Time: 0.912357
[08/10/2023-11:18:30] [V] [TRT] Fastest Tactic: 0x60c3421152ef8e10 Time: 0.4824
[08/10/2023-11:18:30] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x60c3421152ef8e10
[08/10/2023-11:18:30] [V] [TRT] =============== Computing costs for 
[08/10/2023-11:18:30] [V] [TRT] *************** Autotuning format combination: Float(8294400,32400,180,1) -> Float(97200,32400,180,1) ***************
[08/10/2023-11:18:30] [V] [TRT] --------------- Timing Runner: Conv_160 (CudaDepthwiseConvolution)
[08/10/2023-11:18:30] [V] [TRT] CudaDepthwiseConvolution has no valid tactics for this config, skipping
[08/10/2023-11:18:30] [V] [TRT] --------------- Timing Runner: Conv_160 (FusedConvActConvolution)
[08/10/2023-11:18:30] [V] [TRT] Tactic: 0x000000000007ffff Time: 0.679922
[08/10/2023-11:18:30] [V] [TRT] Tactic: 0x00000000000fffff Time: 0.651328
[08/10/2023-11:18:30] [V] [TRT] Tactic: 0x000000000027ffff Time: 0.697609
[08/10/2023-11:18:30] [V] [TRT] Tactic: 0x000000000062ffff Time: 0.627886
[08/10/2023-11:18:30] [V] [TRT] Tactic: 0x000000000086ffff Time: 0.990866
[08/10/2023-11:18:30] [V] [TRT] Tactic: 0x000000000089ffff Time: 0.640306
[08/10/2023-11:18:30] [V] [TRT] Tactic: 0x00000000009fffff Time: 0.664206
[08/10/2023-11:18:31] [V] [TRT] Tactic: 0x0000000000a2ffff Time: 0.705161
[08/10/2023-11:18:31] [V] [TRT] Fastest Tactic: 0x000000000062ffff Time: 0.627886
[08/10/2023-11:18:31] [V] [TRT] --------------- Timing Runner: Conv_160 (CudnnConvolution)
[08/10/2023-11:18:31] [V] [TRT] Tactic: 0x0000000000000000 Time: 1.08391
[08/10/2023-11:18:31] [V] [TRT] Tactic: 0x0000000000000001 Time: 0.479218
[08/10/2023-11:18:31] [V] [TRT] Tactic: 0x0000000000000002 Time: 1.84377
[08/10/2023-11:18:31] [V] [TRT] Tactic: 0x0000000000000004 Time: 4.86184
[08/10/2023-11:18:31] [V] [TRT] Tactic: 0x0000000000000005 Time: 1.0895
[08/10/2023-11:18:31] [V] [TRT] Tactic: 0x0000000000000006 Time: 0.449518
[08/10/2023-11:18:31] [V] [TRT] Tactic: 0x0000000000000038 Time: 1.06248
[08/10/2023-11:18:31] [V] [TRT] Tactic: 0x0000000000000039 Time: 0.487611
[08/10/2023-11:18:31] [V] [TRT] Tactic: 0x000000000000003a Time: 1.85047
[08/10/2023-11:18:31] [V] [TRT] Tactic: 0x000000000000003c Time: 5.13749
[08/10/2023-11:18:31] [V] [TRT] Tactic: 0x000000000000003d Time: 1.0657
[08/10/2023-11:18:31] [V] [TRT] Tactic: 0x000000000000003e Time: 0.449257
[08/10/2023-11:18:31] [V] [TRT] Tactic: 0x0000000000000070 Time: 1.08471
[08/10/2023-11:18:31] [V] [TRT] Tactic: 0x0000000000000071 Time: 0.856448
[08/10/2023-11:18:31] [V] [TRT] Tactic: 0x0000000000000072 Time: 1.85988
[08/10/2023-11:18:31] [V] [TRT] Tactic: 0x0000000000000074 Time: 5.00709
[08/10/2023-11:18:31] [V] [TRT] Tactic: 0x0000000000000075 Time: 1.09573
[08/10/2023-11:18:31] [V] [TRT] Tactic: 0x0000000000000076 Time: 0.450313
[08/10/2023-11:18:31] [V] [TRT] Fastest Tactic: 0x000000000000003e Time: 0.449257
[08/10/2023-11:18:31] [V] [TRT] --------------- Timing Runner: Conv_160 (CaskConvolution)
[08/10/2023-11:18:31] [V] [TRT] Conv_160 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x128x8_stage3_warpsize1x4x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0x01cf8ce2da913006
[08/10/2023-11:18:31] [V] [TRT] Tactic: 0x01cf8ce2da913006 Time: 2.73755
[08/10/2023-11:18:31] [V] [TRT] Conv_160 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x64x8_stage3_warpsize2x2x1_g1_ffma_aligna4_alignc4 Tactic: 0x12dbf7d94ee3696d
[08/10/2023-11:18:31] [V] [TRT] Tactic: 0x12dbf7d94ee3696d Time: 1.47621
[08/10/2023-11:18:31] [V] [TRT] Conv_160 Set Tactic Name: ampere_scudnn_128x64_relu_small_nn_v1 Tactic: 0x3f243c490d502deb
[08/10/2023-11:18:31] [V] [TRT] Tactic: 0x3f243c490d502deb Time: 1.38945
[08/10/2023-11:18:31] [V] [TRT] Conv_160 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0x4727434768e46395
[08/10/2023-11:18:31] [V] [TRT] Tactic: 0x4727434768e46395 Time: 1.55911
[08/10/2023-11:18:31] [V] [TRT] Conv_160 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize256x128x8_stage3_warpsize4x2x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0x4efce38acc876f5c
[08/10/2023-11:18:31] [V] [TRT] Tactic: 0x4efce38acc876f5c Time: 2.72945
[08/10/2023-11:18:31] [V] [TRT] Conv_160 Set Tactic Name: ampere_scudnn_128x128_relu_small_nn_v1 Tactic: 0x503619c69ae500ff
[08/10/2023-11:18:31] [V] [TRT] Tactic: 0x503619c69ae500ff Time: 2.42793
[08/10/2023-11:18:31] [V] [TRT] Conv_160 Set Tactic Name: ampere_scudnn_128x128_relu_xregs_large_nn_v1 Tactic: 0x5403ad713f811a18
[08/10/2023-11:18:31] [V] [TRT] Tactic: 0x5403ad713f811a18 Time: 2.62543
[08/10/2023-11:18:31] [V] [TRT] Conv_160 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x128x8_stage3_warpsize1x4x1_g1_ffma_aligna4_alignc4 Tactic: 0x5aa723e0481da855
[08/10/2023-11:18:31] [V] [TRT] Tactic: 0x5aa723e0481da855 Time: 2.7575
[08/10/2023-11:18:31] [V] [TRT] Conv_160 Set Tactic Name: ampere_scudnn_128x64_relu_xregs_large_nn_v1 Tactic: 0x5deb29b7a8e275f7
[08/10/2023-11:18:31] [V] [TRT] Tactic: 0x5deb29b7a8e275f7 Time: 1.36539
[08/10/2023-11:18:31] [V] [TRT] Conv_160 Set Tactic Name: ampere_scudnn_winograd_128x128_ldg1_ldg4_relu_tile148t_nt_v1 Tactic: 0x94119b4c514b211a
[08/10/2023-11:18:31] [V] [TRT] Tactic: 0x94119b4c514b211a Time: 0.418322
[08/10/2023-11:18:31] [V] [TRT] Conv_160 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_aligna4_alignc4 Tactic: 0xa31d27de74b895ff
[08/10/2023-11:18:31] [V] [TRT] Tactic: 0xa31d27de74b895ff Time: 1.55313
[08/10/2023-11:18:31] [V] [TRT] Conv_160 Set Tactic Name: ampere_scudnn_128x32_relu_small_nn_v1 Tactic: 0xa8609adc4e0ceb90
[08/10/2023-11:18:31] [V] [TRT] Tactic: 0xa8609adc4e0ceb90 Time: 0.817824
[08/10/2023-11:18:31] [V] [TRT] Conv_160 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize256x128x8_stage3_warpsize4x2x1_g1_ffma_aligna4_alignc4 Tactic: 0xbb8c3889c7eacd30
[08/10/2023-11:18:32] [V] [TRT] Tactic: 0xbb8c3889c7eacd30 Time: 2.74803
[08/10/2023-11:18:32] [V] [TRT] Conv_160 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_aligna4_alignc4 Tactic: 0xd828f024626fa982
[08/10/2023-11:18:32] [V] [TRT] Tactic: 0xd828f024626fa982 Time: 2.15504
[08/10/2023-11:18:32] [V] [TRT] Conv_160 Set Tactic Name: ampere_scudnn_128x128_relu_medium_nn_v1 Tactic: 0xf067e6205da31c2e
[08/10/2023-11:18:32] [V] [TRT] Tactic: 0xf067e6205da31c2e Time: 2.46296
[08/10/2023-11:18:32] [V] [TRT] Conv_160 Set Tactic Name: ampere_scudnn_128x64_relu_medium_nn_v1 Tactic: 0xf64396b97c889179
[08/10/2023-11:18:32] [V] [TRT] Tactic: 0xf64396b97c889179 Time: 1.43894
[08/10/2023-11:18:32] [V] [TRT] Fastest Tactic: 0x94119b4c514b211a Time: 0.418322
[08/10/2023-11:18:32] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x94119b4c514b211a
[08/10/2023-11:18:32] [V] [TRT] *************** Autotuning format combination: Float(8294400,1,46080,256) -> Float(97200,1,540,3) ***************
[08/10/2023-11:18:32] [V] [TRT] --------------- Timing Runner: Conv_160 (CaskConvolution)
[08/10/2023-11:18:32] [V] [TRT] Conv_160 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_aligna4_alignc4 Tactic: 0x19b688348f983aa0
[08/10/2023-11:18:32] [V] [TRT] Tactic: 0x19b688348f983aa0 Time: 1.45331
[08/10/2023-11:18:32] [V] [TRT] Conv_160 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x64x8_stage3_warpsize2x2x1_g1_ffma_aligna4_alignc4 Tactic: 0x1da91d865428f237
[08/10/2023-11:18:32] [V] [TRT] Tactic: 0x1da91d865428f237 Time: 1.16197
[08/10/2023-11:18:32] [V] [TRT] Conv_160 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x256x8_stage3_warpsize1x4x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0x3f0c846d6379bc98
[08/10/2023-11:18:32] [V] [TRT] Tactic: 0x3f0c846d6379bc98 Time: 4.28928
[08/10/2023-11:18:32] [V] [TRT] Conv_160 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_aligna4_alignc4 Tactic: 0x62835fce994f06dd
[08/10/2023-11:18:32] [V] [TRT] Tactic: 0x62835fce994f06dd Time: 1.23389
[08/10/2023-11:18:32] [V] [TRT] Conv_160 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize256x64x8_stage3_warpsize2x2x1_g1_ffma_aligna4_alignc4 Tactic: 0x8014228ec08b4d49
[08/10/2023-11:18:32] [V] [TRT] Tactic: 0x8014228ec08b4d49 Time: 1.20072
[08/10/2023-11:18:32] [V] [TRT] Conv_160 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0x94a7db94ba744c45
[08/10/2023-11:18:32] [V] [TRT] Tactic: 0x94a7db94ba744c45 Time: 1.22265
[08/10/2023-11:18:32] [V] [TRT] Conv_160 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x128x8_stage3_warpsize1x4x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0xd15dd11d64344e83
[08/10/2023-11:18:32] [V] [TRT] Tactic: 0xd15dd11d64344e83 Time: 2.20243
[08/10/2023-11:18:32] [V] [TRT] Conv_160 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x64x8_stage3_warpsize2x2x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0xf48db81f02eca9ee
[08/10/2023-11:18:32] [V] [TRT] Tactic: 0xf48db81f02eca9ee Time: 1.1585
[08/10/2023-11:18:32] [V] [TRT] Fastest Tactic: 0xf48db81f02eca9ee Time: 1.1585
[08/10/2023-11:18:32] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0xf48db81f02eca9ee
[08/10/2023-11:18:32] [V] [TRT] *************** Autotuning format combination: Float(2073600,1:4,11520,64) -> Float(32400,1:4,180,1) ***************
[08/10/2023-11:18:32] [V] [TRT] --------------- Timing Runner: Conv_160 (CaskConvolution)
[08/10/2023-11:18:32] [V] [TRT] Conv_160 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize64x128x32_stage5_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0x112d7e4d280f396d
[08/10/2023-11:18:32] [V] [TRT] Tactic: 0x112d7e4d280f396d Time: 0.847671
[08/10/2023-11:18:32] [V] [TRT] Conv_160 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x64x32_stage5_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0x13cc2ad8dcd32ba2
[08/10/2023-11:18:32] [V] [TRT] Tactic: 0x13cc2ad8dcd32ba2 Time: 0.477445
[08/10/2023-11:18:32] [V] [TRT] Conv_160 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize256x128x32_stage3_warpsize4x2x1_g1_tensor16x8x8_t1r3s3 Tactic: 0x21246c8544eff903
[08/10/2023-11:18:32] [V] [TRT] Tactic: 0x21246c8544eff903 Time: 0.537289
[08/10/2023-11:18:32] [V] [TRT] Conv_160 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x128x32_stage4_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0x25b2b9d5c9d5ca0d
[08/10/2023-11:18:32] [V] [TRT] Tactic: 0x25b2b9d5c9d5ca0d Time: 0.600073
[08/10/2023-11:18:32] [V] [TRT] Conv_160 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage4_warpsize2x2x1_g1_tensor16x8x8_t1r3s3 Tactic: 0x2d80d2b384ab13f1
[08/10/2023-11:18:32] [V] [TRT] Tactic: 0x2d80d2b384ab13f1 Time: 0.512608
[08/10/2023-11:18:32] [V] [TRT] Conv_160 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize256x128x32_stage3_warpsize4x2x1_g1_tensor16x8x8 Tactic: 0x36303f399d3ee4fd
[08/10/2023-11:18:32] [V] [TRT] Tactic: 0x36303f399d3ee4fd Time: 0.636699
[08/10/2023-11:18:32] [V] [TRT] Conv_160 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage4_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0x385bfab9332b1413
[08/10/2023-11:18:32] [V] [TRT] Tactic: 0x385bfab9332b1413 Time: 0.5704
[08/10/2023-11:18:32] [V] [TRT] Conv_160 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x256x32_stage3_warpsize2x4x1_g1_tensor16x8x8 Tactic: 0x482b48242255b8ce
[08/10/2023-11:18:32] [V] [TRT] Tactic: 0x482b48242255b8ce Time: 1.00229
[08/10/2023-11:18:32] [V] [TRT] Conv_160 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x256x32_stage3_warpsize2x4x1_g1_tensor16x8x8_t1r3s3 Tactic: 0x614e89f7852edbc3
[08/10/2023-11:18:32] [V] [TRT] Tactic: 0x614e89f7852edbc3 Time: 0.973266
[08/10/2023-11:18:32] [V] [TRT] Conv_160 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0x65e41d81f093b482
[08/10/2023-11:18:32] [V] [TRT] Tactic: 0x65e41d81f093b482 Time: 0.597591
[08/10/2023-11:18:32] [V] [TRT] Conv_160 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize64x128x32_stage5_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0x65f71607d20d5438
[08/10/2023-11:18:32] [V] [TRT] Tactic: 0x65f71607d20d5438 Time: 0.795808
[08/10/2023-11:18:32] [V] [TRT] Conv_160 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x64x32_stage5_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0x6716429226d146f7
[08/10/2023-11:18:32] [V] [TRT] Tactic: 0x6716429226d146f7 Time: 0.421582
[08/10/2023-11:18:32] [V] [TRT] Conv_160 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize64x32x64_stage5_warpsize2x2x1_g1_tensor16x8x8_t1r3s3 Tactic: 0x679a281f7660c436
[08/10/2023-11:18:32] [V] [TRT] Tactic: 0x679a281f7660c436 Time: 0.397435
[08/10/2023-11:18:32] [V] [TRT] Conv_160 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize64x32x64_stage5_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0x6e2a3c7a7fc5e4e1
[08/10/2023-11:18:32] [V] [TRT] Tactic: 0x6e2a3c7a7fc5e4e1 Time: 0.421929
[08/10/2023-11:18:32] [V] [TRT] Conv_160 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x64x32_stage5_warpsize2x2x1_g1_tensor16x8x8_t1r3s3 Tactic: 0x7aa21282feb15f1a
[08/10/2023-11:18:32] [V] [TRT] Tactic: 0x7aa21282feb15f1a Time: 0.429454
[08/10/2023-11:18:32] [V] [TRT] Conv_160 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage4_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0x818413b69874bb35
[08/10/2023-11:18:32] [V] [TRT] Tactic: 0x818413b69874bb35 Time: 0.570149
[08/10/2023-11:18:32] [V] [TRT] Conv_160 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x256x32_stage3_warpsize2x4x1_g1_tensor16x8x8 Tactic: 0x998c97842e775a17
[08/10/2023-11:18:32] [V] [TRT] Tactic: 0x998c97842e775a17 Time: 1.0371
[08/10/2023-11:18:32] [V] [TRT] Conv_160 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8_t1r3s3 Tactic: 0x999e005e3b016ea6
[08/10/2023-11:18:32] [V] [TRT] Tactic: 0x999e005e3b016ea6 Time: 0.535616
[08/10/2023-11:18:32] [V] [TRT] Conv_160 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize256x64x32_stage3_warpsize4x2x1_g1_tensor16x8x8_t1r3s3 Tactic: 0xa9a06d0633580c0c
[08/10/2023-11:18:32] [V] [TRT] Tactic: 0xa9a06d0633580c0c Time: 0.337522
[08/10/2023-11:18:32] [V] [TRT] Conv_160 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0xb443c221fcb1565b
[08/10/2023-11:18:32] [V] [TRT] Tactic: 0xb443c221fcb1565b Time: 0.554162
[08/10/2023-11:18:32] [V] [TRT] Conv_160 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage3_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0xb47d7d926de02dee
[08/10/2023-11:18:32] [V] [TRT] Tactic: 0xb47d7d926de02dee Time: 0.424887
[08/10/2023-11:18:32] [V] [TRT] Conv_160 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize256x64x32_stage3_warpsize4x2x1_g1_tensor16x8x8 Tactic: 0xba0185279ccb2b85
[08/10/2023-11:18:32] [V] [TRT] Tactic: 0xba0185279ccb2b85 Time: 0.377861
[08/10/2023-11:18:32] [V] [TRT] Conv_160 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage3_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0xc0a715d897e240bb
[08/10/2023-11:18:32] [V] [TRT] Tactic: 0xc0a715d897e240bb Time: 0.424293
[08/10/2023-11:18:32] [V] [TRT] Conv_160 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize256x64x32_stage3_warpsize4x2x1_g1_tensor16x8x8 Tactic: 0xcedbed6d66c946d0
[08/10/2023-11:18:32] [V] [TRT] Tactic: 0xcedbed6d66c946d0 Time: 0.354304
[08/10/2023-11:18:32] [V] [TRT] Conv_160 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize64x128x32_stage5_warpsize2x2x1_g1_tensor16x8x8_t1r3s3 Tactic: 0xd4428a7355769d7d
[08/10/2023-11:18:32] [V] [TRT] Tactic: 0xd4428a7355769d7d Time: 0.766085
[08/10/2023-11:18:32] [V] [TRT] Conv_160 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize64x32x64_stage5_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0xd7f5d575d49a4bc7
[08/10/2023-11:18:32] [V] [TRT] Tactic: 0xd7f5d575d49a4bc7 Time: 0.465248
[08/10/2023-11:18:32] [V] [TRT] Conv_160 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x128x32_stage4_warpsize2x2x1_g1_tensor16x8x8_t1r3s3 Tactic: 0xd920b33c9bd27143
[08/10/2023-11:18:32] [V] [TRT] Tactic: 0xd920b33c9bd27143 Time: 0.566574
[08/10/2023-11:18:32] [V] [TRT] Conv_160 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize256x128x32_stage3_warpsize4x2x1_g1_tensor16x8x8 Tactic: 0xe797e099911c0624
[08/10/2023-11:18:32] [V] [TRT] Tactic: 0xe797e099911c0624 Time: 0.554903
[08/10/2023-11:18:32] [V] [TRT] Conv_160 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage3_warpsize2x2x1_g1_tensor16x8x8_t1r3s3 Tactic: 0xeb65d4c1e8fa2294
[08/10/2023-11:18:32] [V] [TRT] Tactic: 0xeb65d4c1e8fa2294 Time: 0.403616
[08/10/2023-11:18:32] [V] [TRT] Conv_160 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x128x32_stage4_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0xf4156675c5f728d4
[08/10/2023-11:18:32] [V] [TRT] Tactic: 0xf4156675c5f728d4 Time: 0.831173
[08/10/2023-11:18:32] [V] [TRT] Fastest Tactic: 0xa9a06d0633580c0c Time: 0.337522
[08/10/2023-11:18:32] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0xa9a06d0633580c0c
[08/10/2023-11:18:32] [V] [TRT] *************** Autotuning format combination: Half(8294400,32400,180,1) -> Half(97200,32400,180,1) ***************
[08/10/2023-11:18:32] [V] [TRT] --------------- Timing Runner: Conv_160 (CudnnConvolution)
[08/10/2023-11:18:32] [V] [TRT] Tactic: 0x0000000000000000 Time: 2.97102
[08/10/2023-11:18:32] [V] [TRT] Tactic: 0x0000000000000001 Time: 0.865394
[08/10/2023-11:18:32] [V] [TRT] Tactic: 0x0000000000000002 Time: 3.36795
[08/10/2023-11:18:32] [V] [TRT] Tactic: 0x0000000000000004 Time: 4.92468
[08/10/2023-11:18:32] [V] [TRT] Tactic: 0x0000000000000005 Time: 1.07453
[08/10/2023-11:18:33] [V] [TRT] Tactic: 0x0000000000000006 Time: 4.13645
[08/10/2023-11:18:33] [V] [TRT] Tactic: 0x0000000000000038 Time: 2.98445
[08/10/2023-11:18:33] [V] [TRT] Tactic: 0x000000000000003a Time: 3.36689
[08/10/2023-11:18:33] [V] [TRT] Tactic: 0x000000000000003c Time: 5.05247
[08/10/2023-11:18:33] [V] [TRT] Tactic: 0x000000000000003d Time: 1.06917
[08/10/2023-11:18:33] [V] [TRT] Tactic: 0x000000000000003e Time: 4.08205
[08/10/2023-11:18:33] [V] [TRT] Fastest Tactic: 0x0000000000000001 Time: 0.865394
[08/10/2023-11:18:33] [V] [TRT] --------------- Timing Runner: Conv_160 (CaskConvolution)
[08/10/2023-11:18:33] [V] [TRT] CaskConvolution has no valid tactics for this config, skipping
[08/10/2023-11:18:33] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CudnnConvolution Tactic: 0x0000000000000001
[08/10/2023-11:18:33] [V] [TRT] *************** Autotuning format combination: Half(4147200,32400:2,180,1) -> Half(64800,32400:2,180,1) ***************
[08/10/2023-11:18:33] [V] [TRT] --------------- Timing Runner: Conv_160 (FusedConvActConvolution)
[08/10/2023-11:18:33] [V] [TRT] FusedConvActConvolution has no valid tactics for this config, skipping
[08/10/2023-11:18:33] [V] [TRT] --------------- Timing Runner: Conv_160 (CaskConvolution)
[08/10/2023-11:18:33] [V] [TRT] Conv_160 Set Tactic Name: ampere_fp16x2_hcudnn_fp16x2_128x32_relu_medium_nn_v1 Tactic: 0x1e7896ba71ef1635
[08/10/2023-11:18:33] [V] [TRT] Tactic: 0x1e7896ba71ef1635 Time: 0.470048
[08/10/2023-11:18:33] [V] [TRT] Conv_160 Set Tactic Name: ampere_fp16x2_hcudnn_fp16x2_128x64_relu_large_nn_v1 Tactic: 0x2f735ffbb05a30fd
[08/10/2023-11:18:33] [V] [TRT] Tactic: 0x2f735ffbb05a30fd Time: 0.852352
[08/10/2023-11:18:33] [V] [TRT] Conv_160 Set Tactic Name: ampere_fp16x2_hcudnn_fp16x2_128x128_relu_medium_nn_v1 Tactic: 0x360278e347d63410
[08/10/2023-11:18:33] [V] [TRT] Tactic: 0x360278e347d63410 Time: 1.72886
[08/10/2023-11:18:33] [V] [TRT] Conv_160 Set Tactic Name: ampere_fp16x2_hcudnn_fp16x2_128x64_relu_medium_nn_v1 Tactic: 0x4cfee77ea8c324db
[08/10/2023-11:18:33] [V] [TRT] Tactic: 0x4cfee77ea8c324db Time: 0.849408
[08/10/2023-11:18:33] [V] [TRT] Conv_160 Set Tactic Name: ampere_fp16x2_hcudnn_fp16x2_128x64_relu_small_nn_v1 Tactic: 0x540fde3a7bee53dc
[08/10/2023-11:18:33] [V] [TRT] Tactic: 0x540fde3a7bee53dc Time: 0.822098
[08/10/2023-11:18:33] [V] [TRT] Conv_160 Set Tactic Name: ampere_fp16x2_hcudnn_fp16x2_128x128_relu_large_nn_v1 Tactic: 0x91f7e9c0851ad67c
[08/10/2023-11:18:33] [V] [TRT] Tactic: 0x91f7e9c0851ad67c Time: 1.74648
[08/10/2023-11:18:33] [V] [TRT] Conv_160 Set Tactic Name: ampere_fp16x2_hcudnn_fp16x2_128x32_relu_large_nn_v1 Tactic: 0xb837f96ef306f686
[08/10/2023-11:18:33] [V] [TRT] Tactic: 0xb837f96ef306f686 Time: 0.464905
[08/10/2023-11:18:33] [V] [TRT] Conv_160 Set Tactic Name: ampere_fp16x2_hcudnn_fp16x2_128x32_relu_small_nn_v1 Tactic: 0xc34b78af38b295a7
[08/10/2023-11:18:33] [V] [TRT] Tactic: 0xc34b78af38b295a7 Time: 0.456741
[08/10/2023-11:18:33] [V] [TRT] Conv_160 Set Tactic Name: ampere_fp16x2_hcudnn_winograd_fp16x2_128x128_ldg1_ldg4_relu_tile148t_nt_v1 Tactic: 0xc754debea88ae0b7
[08/10/2023-11:18:33] [V] [TRT] Tactic: 0xc754debea88ae0b7 Time: 0.244375
[08/10/2023-11:18:33] [V] [TRT] Conv_160 Set Tactic Name: ampere_fp16x2_hcudnn_fp16x2_128x128_relu_small_nn_v1 Tactic: 0xea8b68014eaeb55d
[08/10/2023-11:18:33] [V] [TRT] Tactic: 0xea8b68014eaeb55d Time: 1.71157
[08/10/2023-11:18:33] [V] [TRT] Fastest Tactic: 0xc754debea88ae0b7 Time: 0.244375
[08/10/2023-11:18:33] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0xc754debea88ae0b7
[08/10/2023-11:18:33] [V] [TRT] *************** Autotuning format combination: Half(1036800,1:8,5760,32) -> Float(97200,32400,180,1) ***************
[08/10/2023-11:18:33] [V] [TRT] --------------- Timing Runner: Conv_160 (CaskConvolution)
[08/10/2023-11:18:33] [V] [TRT] CaskConvolution has no valid tactics for this config, skipping
[08/10/2023-11:18:33] [V] [TRT] *************** Autotuning format combination: Half(1036800,1:8,5760,32) -> Half(32400,1:8,180,1) ***************
[08/10/2023-11:18:33] [V] [TRT] --------------- Timing Runner: Conv_160 (CudaDepthwiseConvolution)
[08/10/2023-11:18:33] [V] [TRT] CudaDepthwiseConvolution has no valid tactics for this config, skipping
[08/10/2023-11:18:33] [V] [TRT] --------------- Timing Runner: Conv_160 (CaskConvolution)
[08/10/2023-11:18:33] [V] [TRT] Conv_160 Set Tactic Name: ampere_h16816cudnn_256x128_ldg8_relu_exp_medium_nhwc_tn_v1 Tactic: 0x03896956a39a1203
[08/10/2023-11:18:33] [V] [TRT] Tactic: 0x03896956a39a1203 Time: 0.301902
[08/10/2023-11:18:33] [V] [TRT] Conv_160 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x128x32_stage3_warpsize4x2x1_g1_tensor16x8x16_t1r3s3 Tactic: 0x048d6d0400f33439
[08/10/2023-11:18:33] [V] [TRT] Tactic: 0x048d6d0400f33439 Time: 0.283278
[08/10/2023-11:18:33] [V] [TRT] Conv_160 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x64x64_stage3_warpsize2x2x1_g1_tensor16x8x16_t1r3s3 Tactic: 0x05b6220f243edacd
[08/10/2023-11:18:33] [V] [TRT] Tactic: 0x05b6220f243edacd Time: 0.150002
[08/10/2023-11:18:33] [V] [TRT] Conv_160 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x64x32_stage3_warpsize4x1x1_g1_tensor16x8x16_t1r3s3_aACCESS Tactic: 0x0866ddee325d07a6
[08/10/2023-11:18:33] [V] [TRT] Tactic: 0x0866ddee325d07a6 Time: 0.159858
[08/10/2023-11:18:33] [V] [TRT] Conv_160 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x128x32_stage4_warpsize2x2x1_g1_tensor16x8x16 Tactic: 0x0e07ff4f4f7c1ac9
[08/10/2023-11:18:33] [V] [TRT] Tactic: 0x0e07ff4f4f7c1ac9 Time: 0.269815
[08/10/2023-11:18:33] [V] [TRT] Conv_160 Set Tactic Name: ampere_h16816cudnn_256x128_ldg8_relu_exp_stages_64x3_small_nhwc_tn_v1 Tactic: 0x0e199750c30c6928
[08/10/2023-11:18:33] [V] [TRT] Tactic: 0x0e199750c30c6928 Time: 0.299081
[08/10/2023-11:18:33] [V] [TRT] Conv_160 Set Tactic Name: ampere_h1688cudnn_128x128_ldg8_relu_exp_medium_nhwc_tn_v1 Tactic: 0x100c1ad308e08d35
[08/10/2023-11:18:33] [V] [TRT] Tactic: 0x100c1ad308e08d35 Time: 0.345865
[08/10/2023-11:18:33] [V] [TRT] Conv_160 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x128x32_stage5_warpsize2x2x1_g1_tensor16x8x16_t1r3s3 Tactic: 0x17ebf0c9f418f10a
[08/10/2023-11:18:33] [V] [TRT] Tactic: 0x17ebf0c9f418f10a Time: 0.295342
[08/10/2023-11:18:33] [V] [TRT] Conv_160 Set Tactic Name: ampere_h16816cudnn_128x64_ldg8_relu_exp_stages_64x4_medium_nhwc_tn_v1 Tactic: 0x196cbc423a9bb69e
[08/10/2023-11:18:33] [V] [TRT] Tactic: 0x196cbc423a9bb69e Time: 0.25445
[08/10/2023-11:18:33] [V] [TRT] Conv_160 Set Tactic Name: ampere_h16816cudnn_128x64_sliced1x2_ldg8_relu_exp_stages_64x3_small_nhwc_tn_v1 Tactic: 0x19822de884f42a6a
[08/10/2023-11:18:33] [V] [TRT] Tactic: 0x19822de884f42a6a Time: 0.166501
[08/10/2023-11:18:33] [V] [TRT] Conv_160 Set Tactic Name: ampere_h16816cudnn_64x64_sliced1x2_ldg8_relu_exp_stages_64x5_small_nhwc_tn_v1 Tactic: 0x1a5ba808ad4cc5a8
[08/10/2023-11:18:33] [V] [TRT] Tactic: 0x1a5ba808ad4cc5a8 Time: 0.245198
[08/10/2023-11:18:33] [V] [TRT] Conv_160 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x64x64_stage3_warpsize2x2x1_g1_tensor16x8x16 Tactic: 0x21b295c0c8f6c95a
[08/10/2023-11:18:33] [V] [TRT] Tactic: 0x21b295c0c8f6c95a Time: 0.175122
[08/10/2023-11:18:33] [V] [TRT] Conv_160 Set Tactic Name: ampere_h16816cudnn_128x128_ldg8_relu_exp_medium_nhwc_tn_v1 Tactic: 0x234580e8a194335c
[08/10/2023-11:18:33] [V] [TRT] Tactic: 0x234580e8a194335c Time: 0.256581
[08/10/2023-11:18:33] [V] [TRT] Conv_160 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x128x32_stage4_warpsize2x2x1_g1_tensor16x8x16_t1r3s3_aACCESS Tactic: 0x245530c34bd6090f
[08/10/2023-11:18:33] [V] [TRT] Tactic: 0x245530c34bd6090f Time: 0.262194
[08/10/2023-11:18:33] [V] [TRT] Conv_160 Set Tactic Name: ampere_h16816cudnn_64x64_sliced1x2_ldg8_relu_exp_stages_64x6_small_nhwc_tn_v1 Tactic: 0x24e01e7405cfdfe9
[08/10/2023-11:18:33] [V] [TRT] Tactic: 0x24e01e7405cfdfe9 Time: 0.360416
[08/10/2023-11:18:33] [V] [TRT] Conv_160 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x64x64_stage3_warpsize2x2x1_g1_tensor16x8x16_t1r3s3_aACCESS Tactic: 0x263a38afd75e3a43
[08/10/2023-11:18:33] [V] [TRT] Tactic: 0x263a38afd75e3a43 Time: 0.16341
[08/10/2023-11:18:33] [V] [TRT] Conv_160 Set Tactic Name: ampere_h16816cudnn_64x128_sliced1x2_ldg8_relu_exp_stages_64x4_large_nhwc_tn_v1 Tactic: 0x2721a7f18c2700db
[08/10/2023-11:18:33] [V] [TRT] Tactic: 0x2721a7f18c2700db Time: 0.489298
[08/10/2023-11:18:33] [V] [TRT] Conv_160 Set Tactic Name: ampere_h16816cudnn_128x64_sliced1x2_ldg8_relu_exp_stages_64x3_medium_nhwc_tn_v1 Tactic: 0x29329b5741ea05f2
[08/10/2023-11:18:33] [V] [TRT] Tactic: 0x29329b5741ea05f2 Time: 0.165851
[08/10/2023-11:18:33] [V] [TRT] Conv_160 Set Tactic Name: ampere_h16816cudnn_128x128_ldg8_relu_exp_large_nhwc_tn_v1 Tactic: 0x2970ae65966d569d
[08/10/2023-11:18:33] [V] [TRT] Tactic: 0x2970ae65966d569d Time: 0.251163
[08/10/2023-11:18:33] [V] [TRT] Conv_160 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x256x64_stage3_warpsize1x4x1_g1_tensor16x8x16 Tactic: 0x29dc14520e0e4eb6
[08/10/2023-11:18:33] [V] [TRT] Tactic: 0x29dc14520e0e4eb6 Time: 0.626871
[08/10/2023-11:18:33] [V] [TRT] Conv_160 Set Tactic Name: ampere_h16816cudnn_256x64_ldg8_relu_exp_stages_64x3_large_nhwc_tn_v1 Tactic: 0x2cb4a662694e89d3
[08/10/2023-11:18:33] [V] [TRT] Tactic: 0x2cb4a662694e89d3 Time: 0.196846
[08/10/2023-11:18:33] [V] [TRT] Conv_160 Set Tactic Name: ampere_h16816cudnn_128x128_ldg8_relu_exp_stages_64x3_medium_nhwc_tn_v1 Tactic: 0x2eaa2202de9404d6
[08/10/2023-11:18:33] [V] [TRT] Tactic: 0x2eaa2202de9404d6 Time: 0.344274
[08/10/2023-11:18:33] [V] [TRT] Conv_160 Set Tactic Name: ampere_h1688cudnn_256x128_ldg8_relu_exp_medium_nhwc_tn_v1 Tactic: 0x30c0f36d0aeeac6a
[08/10/2023-11:18:33] [V] [TRT] Tactic: 0x30c0f36d0aeeac6a Time: 0.365033
[08/10/2023-11:18:33] [V] [TRT] Conv_160 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x64x64_stage4_warpsize2x2x1_g1_tensor16x8x16_t1r3s3_aACCESS Tactic: 0x30e8a8d7a953e5e9
[08/10/2023-11:18:33] [V] [TRT] Tactic: 0x30e8a8d7a953e5e9 Time: 0.201047
[08/10/2023-11:18:33] [V] [TRT] Conv_160 Set Tactic Name: ampere_h16816cudnn_128x64_ldg8_relu_exp_stages_64x4_large_nhwc_tn_v1 Tactic: 0x3197d5044d71e98e
[08/10/2023-11:18:33] [V] [TRT] Tactic: 0x3197d5044d71e98e Time: 0.255525
[08/10/2023-11:18:33] [V] [TRT] Conv_160 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x64x64_stage4_warpsize2x2x1_g1_tensor16x8x16_t1r3s3 Tactic: 0x31d93dc22d2af081
[08/10/2023-11:18:33] [V] [TRT] Tactic: 0x31d93dc22d2af081 Time: 0.196914
[08/10/2023-11:18:33] [V] [TRT] Conv_160 Set Tactic Name: ampere_h1688cudnn_128x128_ldg8_relu_exp_small_nhwc_tn_v1 Tactic: 0x3369260c04f9ad73
[08/10/2023-11:18:33] [V] [TRT] Tactic: 0x3369260c04f9ad73 Time: 0.332937
[08/10/2023-11:18:33] [V] [TRT] Conv_160 Set Tactic Name: ampere_h16816cudnn_64x128_ldg8_relu_exp_stages_64x4_small_nhwc_tn_v1 Tactic: 0x3b5fa0f2a8fc2410
[08/10/2023-11:18:33] [V] [TRT] Tactic: 0x3b5fa0f2a8fc2410 Time: 0.427808
[08/10/2023-11:18:33] [V] [TRT] Conv_160 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x32x32_stage4_warpsize4x1x1_g1_tensor16x8x16_t1r3s3 Tactic: 0x3e7eb35b91b9fa63
[08/10/2023-11:18:33] [V] [TRT] Tactic: 0x3e7eb35b91b9fa63 Time: 0.111392
[08/10/2023-11:18:33] [V] [TRT] Conv_160 Set Tactic Name: ampere_h16816cudnn_256x64_ldg8_relu_exp_stages_64x3_medium_nhwc_tn_v1 Tactic: 0x3f031df0e579d52c
[08/10/2023-11:18:33] [V] [TRT] Tactic: 0x3f031df0e579d52c Time: 0.196169
[08/10/2023-11:18:33] [V] [TRT] Conv_160 Set Tactic Name: ampere_h16816cudnn_256x64_sliced1x2_ldg8_relu_exp_small_nhwc_tn_v1 Tactic: 0x42a5b8709d0039be
[08/10/2023-11:18:33] [V] [TRT] Tactic: 0x42a5b8709d0039be Time: 0.21045
[08/10/2023-11:18:33] [V] [TRT] Conv_160 Set Tactic Name: ampere_h16816cudnn_256x128_ldg8_relu_exp_small_nhwc_tn_v1 Tactic: 0x463794ee4acffd1f
[08/10/2023-11:18:33] [V] [TRT] Tactic: 0x463794ee4acffd1f Time: 0.298683
[08/10/2023-11:18:33] [V] [TRT] Conv_160 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x64x64_stage4_warpsize2x2x1_g1_tensor16x8x16 Tactic: 0x4a81ea1e51436a30
[08/10/2023-11:18:33] [V] [TRT] Tactic: 0x4a81ea1e51436a30 Time: 0.219296
[08/10/2023-11:18:33] [V] [TRT] Conv_160 Set Tactic Name: ampere_h1688cudnn_256x64_sliced1x2_ldg8_relu_exp_small_nhwc_tn_v1 Tactic: 0x4aed1956bd10a795
[08/10/2023-11:18:33] [V] [TRT] Tactic: 0x4aed1956bd10a795 Time: 0.232238
[08/10/2023-11:18:33] [V] [TRT] Conv_160 Set Tactic Name: ampere_h16816cudnn_64x128_sliced1x2_ldg8_relu_exp_stages_64x3_large_nhwc_tn_v1 Tactic: 0x4fc05923455fc266
[08/10/2023-11:18:33] [V] [TRT] Tactic: 0x4fc05923455fc266 Time: 0.295653
[08/10/2023-11:18:33] [V] [TRT] Conv_160 Set Tactic Name: ampere_h16816cudnn_256x64_ldg8_relu_exp_large_nhwc_tn_v1 Tactic: 0x5013c38f55afa9ba
[08/10/2023-11:18:33] [V] [TRT] Tactic: 0x5013c38f55afa9ba Time: 0.175616
[08/10/2023-11:18:33] [V] [TRT] Conv_160 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x64x32_stage3_warpsize4x1x1_g1_tensor16x8x16_t1r3s3 Tactic: 0x529f4431bdae94f5
[08/10/2023-11:18:33] [V] [TRT] Tactic: 0x529f4431bdae94f5 Time: 0.143739
[08/10/2023-11:18:33] [V] [TRT] Conv_160 Set Tactic Name: ampere_h16816cudnn_64x128_ldg8_relu_exp_stages_64x3_small_nhwc_tn_v1 Tactic: 0x53be5e206184e6ad
[08/10/2023-11:18:33] [V] [TRT] Tactic: 0x53be5e206184e6ad Time: 0.29051
[08/10/2023-11:18:33] [V] [TRT] Conv_160 Set Tactic Name: ampere_h16816cudnn_64x64_ldg8_relu_exp_stages_64x5_large_nhwc_tn_v1 Tactic: 0x544bff7ff9c5d908
[08/10/2023-11:18:33] [V] [TRT] Tactic: 0x544bff7ff9c5d908 Time: 0.257166
[08/10/2023-11:18:33] [V] [TRT] Conv_160 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x64x64_stage3_warpsize2x2x1_g1_tensor16x8x16 Tactic: 0x5568fd8a32f4a40f
[08/10/2023-11:18:33] [V] [TRT] Tactic: 0x5568fd8a32f4a40f Time: 0.159611
[08/10/2023-11:18:33] [V] [TRT] Conv_160 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x128x64_stage3_warpsize2x2x1_g1_tensor16x8x16 Tactic: 0x57c9a5ff682354a6
[08/10/2023-11:18:33] [V] [TRT] Tactic: 0x57c9a5ff682354a6 Time: 0.330638
[08/10/2023-11:18:33] [V] [TRT] Conv_160 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x32x64_stage5_warpsize2x2x1_g1_tensor16x8x16 Tactic: 0x5820b3dda403c4d0
[08/10/2023-11:18:33] [V] [TRT] Tactic: 0x5820b3dda403c4d0 Time: 0.184151
[08/10/2023-11:18:33] [V] [TRT] Conv_160 Set Tactic Name: ampere_h16816cudnn_128x128_ldg8_relu_exp_stages_64x3_small_nhwc_tn_v1 Tactic: 0x58816bf2e9c36afb
[08/10/2023-11:18:33] [V] [TRT] Tactic: 0x58816bf2e9c36afb Time: 0.347045
[08/10/2023-11:18:33] [V] [TRT] Conv_160 Set Tactic Name: ampere_h16816cudnn_128x64_ldg8_relu_exp_stages_64x3_large_nhwc_tn_v1 Tactic: 0x59762bd684092b33
[08/10/2023-11:18:33] [V] [TRT] Tactic: 0x59762bd684092b33 Time: 0.179401
[08/10/2023-11:18:33] [V] [TRT] Conv_160 Set Tactic Name: ampere_h16816cudnn_128x64_sliced1x2_ldg8_relu_exp_stages_64x4_medium_nhwc_tn_v1 Tactic: 0x59c5c647b4c76593
[08/10/2023-11:18:33] [V] [TRT] Tactic: 0x59c5c647b4c76593 Time: 0.258569
[08/10/2023-11:18:33] [V] [TRT] Conv_160 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x128x64_stage3_warpsize4x2x1_g1_tensor16x8x16 Tactic: 0x5a55274960724ba8
[08/10/2023-11:18:33] [V] [TRT] Tactic: 0x5a55274960724ba8 Time: 0.296978
[08/10/2023-11:18:33] [V] [TRT] Conv_160 Set Tactic Name: ampere_h1688cudnn_256x128_ldg8_relu_exp_large_nhwc_tn_v1 Tactic: 0x5c2e1c87d85b06f1
[08/10/2023-11:18:33] [V] [TRT] Tactic: 0x5c2e1c87d85b06f1 Time: 0.368366
[08/10/2023-11:18:33] [V] [TRT] Conv_160 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x256x64_stage3_warpsize1x4x1_g1_tensor16x8x16 Tactic: 0x5d067c18f40c23e3
[08/10/2023-11:18:33] [V] [TRT] Tactic: 0x5d067c18f40c23e3 Time: 0.626213
[08/10/2023-11:18:33] [V] [TRT] Conv_160 Set Tactic Name: ampere_h16816cudnn_64x64_ldg8_relu_exp_stages_64x5_medium_nhwc_tn_v1 Tactic: 0x5f31c22ec167f384
[08/10/2023-11:18:33] [V] [TRT] Tactic: 0x5f31c22ec167f384 Time: 0.258505
[08/10/2023-11:18:33] [V] [TRT] Conv_160 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x128x32_stage4_warpsize2x2x1_g1_tensor16x8x16_t1r3s3 Tactic: 0x60c3421152ef8e10
[08/10/2023-11:18:33] [V] [TRT] Tactic: 0x60c3421152ef8e10 Time: 0.248315
[08/10/2023-11:18:33] [V] [TRT] Conv_160 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x128x32_stage3_warpsize4x2x1_g1_tensor16x8x16 Tactic: 0x60da8c7151d91e47
[08/10/2023-11:18:33] [V] [TRT] Tactic: 0x60da8c7151d91e47 Time: 0.283685
[08/10/2023-11:18:33] [V] [TRT] Conv_160 Set Tactic Name: ampere_h1688cudnn_256x64_ldg8_relu_exp_medium_nhwc_tn_v1 Tactic: 0x6426696f872a3b13
[08/10/2023-11:18:33] [V] [TRT] Tactic: 0x6426696f872a3b13 Time: 0.221285
[08/10/2023-11:18:33] [V] [TRT] Conv_160 Set Tactic Name: ampere_h16816cudnn_128x64_ldg8_relu_exp_stages_64x3_medium_nhwc_tn_v1 Tactic: 0x699be152cfb6d6ff
[08/10/2023-11:18:33] [V] [TRT] Tactic: 0x699be152cfb6d6ff Time: 0.177472
[08/10/2023-11:18:33] [V] [TRT] Conv_160 Set Tactic Name: ampere_h16816cudnn_64x64_ldg8_relu_exp_stages_64x6_large_nhwc_tn_v1 Tactic: 0x6af049035146c349
[08/10/2023-11:18:33] [V] [TRT] Tactic: 0x6af049035146c349 Time: 0.379584
[08/10/2023-11:18:33] [V] [TRT] Conv_160 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x256x32_stage3_warpsize2x4x1_g1_tensor16x8x16 Tactic: 0x7005d10718f6c22d
[08/10/2023-11:18:33] [V] [TRT] Tactic: 0x7005d10718f6c22d Time: 0.522674
[08/10/2023-11:18:33] [V] [TRT] Conv_160 Set Tactic Name: ampere_h16816cudnn_64x64_sliced1x2_ldg8_relu_exp_stages_64x5_medium_nhwc_tn_v1 Tactic: 0x706f08da35c795a5
[08/10/2023-11:18:33] [V] [TRT] Tactic: 0x706f08da35c795a5 Time: 0.251913
[08/10/2023-11:18:33] [V] [TRT] Conv_160 Set Tactic Name: ampere_h16816cudnn_128x64_sliced1x2_ldg8_relu_exp_stages_64x4_small_nhwc_tn_v1 Tactic: 0x7163d33a4d8ce8d7
[08/10/2023-11:18:33] [V] [TRT] Tactic: 0x7163d33a4d8ce8d7 Time: 0.243374
[08/10/2023-11:18:33] [V] [TRT] Conv_160 Set Tactic Name: ampere_h1688cudnn_256x64_ldg8_relu_exp_small_nhwc_tn_v1 Tactic: 0x7aad3976677d7155
[08/10/2023-11:18:33] [V] [TRT] Tactic: 0x7aad3976677d7155 Time: 0.209751
[08/10/2023-11:18:33] [V] [TRT] Conv_160 Set Tactic Name: ampere_h16816cudnn_64x64_sliced1x2_ldg8_relu_exp_stages_64x5_large_nhwc_tn_v1 Tactic: 0x8015519605ab9963
[08/10/2023-11:18:33] [V] [TRT] Tactic: 0x8015519605ab9963 Time: 0.251429
[08/10/2023-11:18:33] [V] [TRT] Conv_160 Set Tactic Name: ampere_h16816cudnn_256x64_sliced1x2_ldg8_relu_exp_medium_nhwc_tn_v1 Tactic: 0x82f8a2214b0b4178
[08/10/2023-11:18:33] [V] [TRT] Tactic: 0x82f8a2214b0b4178 Time: 0.227351
[08/10/2023-11:18:33] [V] [TRT] Conv_160 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x32x32_stage4_warpsize4x1x1_g1_tensor16x8x16 Tactic: 0x834e11ecd4ab9454
[08/10/2023-11:18:33] [V] [TRT] Tactic: 0x834e11ecd4ab9454 Time: 0.115954
[08/10/2023-11:18:33] [V] [TRT] Conv_160 Set Tactic Name: ampere_h16816cudnn_128x64_sliced1x2_ldg8_relu_exp_stages_64x3_large_nhwc_tn_v1 Tactic: 0x83ccd4762c1376a1
[08/10/2023-11:18:33] [V] [TRT] Tactic: 0x83ccd4762c1376a1 Time: 0.167479
[08/10/2023-11:18:33] [V] [TRT] Conv_160 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x64x64_stage3_warpsize4x1x1_g1_tensor16x8x16_t1r3s3 Tactic: 0x84ee897dd1f4d2aa
[08/10/2023-11:18:33] [V] [TRT] Tactic: 0x84ee897dd1f4d2aa Time: 0.185056
[08/10/2023-11:18:33] [V] [TRT] Conv_160 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x128x64_stage3_warpsize2x2x1_g1_tensor16x8x16 Tactic: 0x866e7a5f6401b67f
[08/10/2023-11:18:33] [V] [TRT] Tactic: 0x866e7a5f6401b67f Time: 0.326542
[08/10/2023-11:18:33] [V] [TRT] Conv_160 Set Tactic Name: ampere_h16816cudnn_64x64_ldg8_relu_exp_stages_64x6_medium_nhwc_tn_v1 Tactic: 0x88971eec55aba850
[08/10/2023-11:18:33] [V] [TRT] Tactic: 0x88971eec55aba850 Time: 0.380535
[08/10/2023-11:18:33] [V] [TRT] Conv_160 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x128x64_stage3_warpsize4x2x1_g1_tensor16x8x16 Tactic: 0x8bf2f8e96c50a971
[08/10/2023-11:18:33] [V] [TRT] Tactic: 0x8bf2f8e96c50a971 Time: 0.29595
[08/10/2023-11:18:33] [V] [TRT] Conv_160 Set Tactic Name: ampere_h16816cudnn_256x128_ldg8_relu_exp_stages_64x3_large_nhwc_tn_v1 Tactic: 0x94576ece6beb35e3
[08/10/2023-11:18:33] [V] [TRT] Tactic: 0x94576ece6beb35e3 Time: 0.311282
[08/10/2023-11:18:33] [V] [TRT] Conv_160 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x64x64_stage3_warpsize4x1x1_g1_tensor16x8x16 Tactic: 0x9fff6e65019cc310
[08/10/2023-11:18:33] [V] [TRT] Tactic: 0x9fff6e65019cc310 Time: 0.187634
[08/10/2023-11:18:33] [V] [TRT] Conv_160 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x32x64_stage5_warpsize2x2x1_g1_tensor16x8x16_t1r3s3 Tactic: 0xa033e20ae9f412b2
[08/10/2023-11:18:33] [V] [TRT] Tactic: 0xa033e20ae9f412b2 Time: 0.168187
[08/10/2023-11:18:33] [V] [TRT] Conv_160 Set Tactic Name: ampere_h16816cudnn_64x128_ldg8_relu_exp_stages_64x4_large_nhwc_tn_v1 Tactic: 0xa111596c001b78db
[08/10/2023-11:18:33] [V] [TRT] Tactic: 0xa111596c001b78db Time: 0.449518
[08/10/2023-11:18:33] [V] [TRT] Conv_160 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x256x32_stage3_warpsize2x4x1_g1_tensor16x8x16 Tactic: 0xa1a20ea714d420f4
[08/10/2023-11:18:33] [V] [TRT] Tactic: 0xa1a20ea714d420f4 Time: 0.519479
[08/10/2023-11:18:33] [V] [TRT] Conv_160 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x32x32_stage4_warpsize4x1x1_g1_tensor16x8x16_t1r3s3 Tactic: 0xa40cb43c296a36a8
[08/10/2023-11:18:33] [V] [TRT] Tactic: 0xa40cb43c296a36a8 Time: 0.116914
[08/10/2023-11:18:33] [V] [TRT] Conv_160 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x64x32_stage3_warpsize4x1x1_g1_tensor16x8x16 Tactic: 0xa570c55d303796ff
[08/10/2023-11:18:33] [V] [TRT] Tactic: 0xa570c55d303796ff Time: 0.154043
[08/10/2023-11:18:33] [V] [TRT] Conv_160 Set Tactic Name: ampere_h16816cudnn_64x64_sliced1x2_ldg8_relu_exp_stages_64x6_medium_nhwc_tn_v1 Tactic: 0xa7c9d418a10bce71
[08/10/2023-11:18:33] [V] [TRT] Tactic: 0xa7c9d418a10bce71 Time: 0.390967
[08/10/2023-11:18:33] [V] [TRT] Conv_160 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x32x32_stage4_warpsize4x1x1_g1_tensor16x8x16 Tactic: 0xa83b68f30462f971
[08/10/2023-11:18:33] [V] [TRT] Tactic: 0xa83b68f30462f971 Time: 0.124846
[08/10/2023-11:18:33] [V] [TRT] Conv_160 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x256x32_stage3_warpsize2x4x1_g1_tensor16x8x16_t1r3s3 Tactic: 0xa9177bbe4e767df8
[08/10/2023-11:18:33] [V] [TRT] Tactic: 0xa9177bbe4e767df8 Time: 0.520251
[08/10/2023-11:18:33] [V] [TRT] Conv_160 Set Tactic Name: ampere_h1688cudnn_128x128_ldg8_relu_exp_large_nhwc_tn_v1 Tactic: 0xa927df92ac1ef1b8
[08/10/2023-11:18:33] [V] [TRT] Tactic: 0xa927df92ac1ef1b8 Time: 0.348027
[08/10/2023-11:18:33] [V] [TRT] Conv_160 Set Tactic Name: ampere_h16816cudnn_128x64_ldg8_relu_exp_stages_64x4_small_nhwc_tn_v1 Tactic: 0xabd92c9ae596b545
[08/10/2023-11:18:34] [V] [TRT] Tactic: 0xabd92c9ae596b545 Time: 0.244517
[08/10/2023-11:18:34] [V] [TRT] Conv_160 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x128x32_stage5_warpsize2x2x1_g1_tensor16x8x16 Tactic: 0xafd1e8bf6bd3d638
[08/10/2023-11:18:34] [V] [TRT] Tactic: 0xafd1e8bf6bd3d638 Time: 0.305463
[08/10/2023-11:18:34] [V] [TRT] Conv_160 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x128x32_stage3_warpsize4x2x1_g1_tensor16x8x16 Tactic: 0xb17d53d15dfbfc9e
[08/10/2023-11:18:34] [V] [TRT] Tactic: 0xb17d53d15dfbfc9e Time: 0.286551
[08/10/2023-11:18:34] [V] [TRT] Conv_160 Set Tactic Name: ampere_h16816cudnn_128x128_ldg8_relu_exp_small_nhwc_tn_v1 Tactic: 0xb33e57fb3e8a0a56
[08/10/2023-11:18:34] [V] [TRT] Tactic: 0xb33e57fb3e8a0a56 Time: 0.248375
[08/10/2023-11:18:34] [V] [TRT] Conv_160 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x64x32_stage5_warpsize2x2x1_g1_tensor16x8x16_t1r3s3 Tactic: 0xb4bec086187edcfc
[08/10/2023-11:18:34] [V] [TRT] Tactic: 0xb4bec086187edcfc Time: 0.162213
[08/10/2023-11:18:34] [V] [TRT] Conv_160 Set Tactic Name: ampere_h16816cudnn_256x64_ldg8_relu_exp_stages_64x3_small_nhwc_tn_v1 Tactic: 0xb6fa5ffcc1a9d518
[08/10/2023-11:18:34] [V] [TRT] Tactic: 0xb6fa5ffcc1a9d518 Time: 0.191717
[08/10/2023-11:18:34] [V] [TRT] Conv_160 Set Tactic Name: ampere_h16816cudnn_64x128_sliced1x2_ldg8_relu_exp_stages_64x3_medium_nhwc_tn_v1 Tactic: 0xb85e52e87caf60a2
[08/10/2023-11:18:34] [V] [TRT] Tactic: 0xb85e52e87caf60a2 Time: 0.291653
[08/10/2023-11:18:34] [V] [TRT] Conv_160 Set Tactic Name: ampere_h16816cudnn_64x128_ldg8_relu_exp_stages_64x3_medium_nhwc_tn_v1 Tactic: 0xb8d86216e1235cda
[08/10/2023-11:18:34] [V] [TRT] Tactic: 0xb8d86216e1235cda Time: 0.291813
[08/10/2023-11:18:34] [V] [TRT] Conv_160 Set Tactic Name: ampere_h16816cudnn_256x128_ldg8_relu_exp_stages_64x3_medium_nhwc_tn_v1 Tactic: 0xb9171d70ba2eda4b
[08/10/2023-11:18:34] [V] [TRT] Tactic: 0xb9171d70ba2eda4b Time: 0.30304
[08/10/2023-11:18:34] [V] [TRT] Conv_160 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x64x32_stage5_warpsize2x2x1_g1_tensor16x8x16 Tactic: 0xbd08239a9317f2fd
[08/10/2023-11:18:34] [V] [TRT] Tactic: 0xbd08239a9317f2fd Time: 0.166798
[08/10/2023-11:18:34] [V] [TRT] Conv_160 Set Tactic Name: ampere_h16816cudnn_64x128_sliced1x2_ldg8_relu_exp_stages_64x4_small_nhwc_tn_v1 Tactic: 0xbd6f5e6f24c05c10
[08/10/2023-11:18:34] [V] [TRT] Tactic: 0xbd6f5e6f24c05c10 Time: 0.468645
[08/10/2023-11:18:34] [V] [TRT] Conv_160 Set Tactic Name: ampere_h16816cudnn_64x64_sliced1x2_ldg8_relu_exp_stages_64x6_large_nhwc_tn_v1 Tactic: 0xbeaee7eaad288322
[08/10/2023-11:18:34] [V] [TRT] Tactic: 0xbeaee7eaad288322 Time: 0.392983
[08/10/2023-11:18:34] [V] [TRT] Conv_160 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x128x64_stage3_warpsize2x2x1_g1_tensor16x8x16_t1r3s3 Tactic: 0xc0a02dc6095497cc
[08/10/2023-11:18:34] [V] [TRT] Tactic: 0xc0a02dc6095497cc Time: 0.318802
[08/10/2023-11:18:34] [V] [TRT] Conv_160 Set Tactic Name: ampere_h16816cudnn_128x128_ldg8_relu_exp_stages_64x3_large_nhwc_tn_v1 Tactic: 0xc2cf926c41243630
[08/10/2023-11:18:34] [V] [TRT] Tactic: 0xc2cf926c41243630 Time: 0.343954
[08/10/2023-11:18:34] [V] [TRT] Conv_160 Set Tactic Name: ampere_h16816cudnn_128x64_ldg8_relu_exp_stages_64x3_small_nhwc_tn_v1 Tactic: 0xc338d2482cee77f8
[08/10/2023-11:18:34] [V] [TRT] Tactic: 0xc338d2482cee77f8 Time: 0.174395
[08/10/2023-11:18:34] [V] [TRT] Conv_160 Set Tactic Name: ampere_h1688cudnn_256x128_ldg8_relu_exp_small_nhwc_tn_v1 Tactic: 0xc660e51970bc5a3a
[08/10/2023-11:18:34] [V] [TRT] Tactic: 0xc660e51970bc5a3a Time: 0.341765
[08/10/2023-11:18:34] [V] [TRT] Conv_160 Set Tactic Name: ampere_h16816cudnn_64x128_ldg8_relu_exp_stages_64x4_medium_nhwc_tn_v1 Tactic: 0xc82f3f06140e3cbb
[08/10/2023-11:18:34] [V] [TRT] Tactic: 0xc82f3f06140e3cbb Time: 0.435319
[08/10/2023-11:18:34] [V] [TRT] Conv_160 Set Tactic Name: ampere_h16816cudnn_64x128_sliced1x2_ldg8_relu_exp_stages_64x4_medium_nhwc_tn_v1 Tactic: 0xc8a90ff8898200c3
[08/10/2023-11:18:34] [V] [TRT] Tactic: 0xc8a90ff8898200c3 Time: 0.48032
[08/10/2023-11:18:34] [V] [TRT] Conv_160 Set Tactic Name: ampere_h1688cudnn_256x64_sliced1x2_ldg8_relu_exp_medium_nhwc_tn_v1 Tactic: 0xc9cc55109bb4de26
[08/10/2023-11:18:34] [V] [TRT] Tactic: 0xc9cc55109bb4de26 Time: 0.255982
[08/10/2023-11:18:34] [V] [TRT] Conv_160 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x64x32_stage5_warpsize2x2x1_g1_tensor16x8x16 Tactic: 0xc9d24bd069159fa8
[08/10/2023-11:18:34] [V] [TRT] Tactic: 0xc9d24bd069159fa8 Time: 0.185824
[08/10/2023-11:18:34] [V] [TRT] Conv_160 Set Tactic Name: ampere_h16816cudnn_64x128_ldg8_relu_exp_stages_64x3_large_nhwc_tn_v1 Tactic: 0xc9f0a7bec963ba66
[08/10/2023-11:18:34] [V] [TRT] Tactic: 0xc9f0a7bec963ba66 Time: 0.299365
[08/10/2023-11:18:34] [V] [TRT] Conv_160 Set Tactic Name: ampere_h16816cudnn_256x64_ldg8_relu_exp_small_nhwc_tn_v1 Tactic: 0xca5d3a11fd48f571
[08/10/2023-11:18:34] [V] [TRT] Tactic: 0xca5d3a11fd48f571 Time: 0.168503
[08/10/2023-11:18:34] [V] [TRT] Conv_160 Set Tactic Name: ampere_h16816cudnn_64x64_ldg8_relu_exp_stages_64x5_small_nhwc_tn_v1 Tactic: 0xce0506e1512285c3
[08/10/2023-11:18:34] [V] [TRT] Tactic: 0xce0506e1512285c3 Time: 0.262935
[08/10/2023-11:18:34] [V] [TRT] Conv_160 Set Tactic Name: ampere_h1688cudnn_256x64_sliced1x2_ldg8_relu_exp_large_nhwc_tn_v1 Tactic: 0xd0a3e0c815f7fb5e
[08/10/2023-11:18:34] [V] [TRT] Tactic: 0xd0a3e0c815f7fb5e Time: 0.251561
[08/10/2023-11:18:34] [V] [TRT] Conv_160 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x64x32_stage3_warpsize4x1x1_g1_tensor16x8x16 Tactic: 0xd1aaad17ca35fbaa
[08/10/2023-11:18:34] [V] [TRT] Tactic: 0xd1aaad17ca35fbaa Time: 0.143314
[08/10/2023-11:18:34] [V] [TRT] Conv_160 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x128x64_stage3_warpsize4x2x1_g1_tensor16x8x16_t1r3s3 Tactic: 0xd2fca0486ca97266
[08/10/2023-11:18:34] [V] [TRT] Tactic: 0xd2fca0486ca97266 Time: 0.295246
[08/10/2023-11:18:34] [V] [TRT] Conv_160 Set Tactic Name: ampere_h16816cudnn_64x128_sliced1x2_ldg8_relu_exp_stages_64x3_small_nhwc_tn_v1 Tactic: 0xd58ea0bdedb89ead
[08/10/2023-11:18:34] [V] [TRT] Tactic: 0xd58ea0bdedb89ead Time: 0.290185
[08/10/2023-11:18:34] [V] [TRT] Conv_160 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x256x64_stage3_warpsize1x4x1_g1_tensor16x8x16_t1r3s3 Tactic: 0xd7c261fa01db2af9
[08/10/2023-11:18:34] [V] [TRT] Tactic: 0xd7c261fa01db2af9 Time: 0.612389
[08/10/2023-11:18:34] [V] [TRT] Conv_160 Set Tactic Name: ampere_h16816cudnn_256x64_sliced1x2_ldg8_relu_exp_large_nhwc_tn_v1 Tactic: 0xd8eb41ee35e76575
[08/10/2023-11:18:34] [V] [TRT] Tactic: 0xd8eb41ee35e76575 Time: 0.232535
[08/10/2023-11:18:34] [V] [TRT] Conv_160 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x128x32_stage5_warpsize2x2x1_g1_tensor16x8x16 Tactic: 0xdb0b80f591d1bb6d
[08/10/2023-11:18:34] [V] [TRT] Tactic: 0xdb0b80f591d1bb6d Time: 0.293339
[08/10/2023-11:18:34] [V] [TRT] Conv_160 Set Tactic Name: ampere_h16816cudnn_256x128_ldg8_relu_exp_large_nhwc_tn_v1 Tactic: 0xdc796d70e228a1d4
[08/10/2023-11:18:34] [V] [TRT] Tactic: 0xdc796d70e228a1d4 Time: 0.302377
[08/10/2023-11:18:34] [V] [TRT] Conv_160 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x32x32_stage4_warpsize4x1x1_g1_tensor16x8x16 Tactic: 0xdce100b9fe609424
[08/10/2023-11:18:34] [V] [TRT] Tactic: 0xdce100b9fe609424 Time: 0.128334
[08/10/2023-11:18:34] [V] [TRT] Conv_160 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x128x32_stage4_warpsize2x2x1_g1_tensor16x8x16 Tactic: 0xdfa020ef435ef810
[08/10/2023-11:18:34] [V] [TRT] Tactic: 0xdfa020ef435ef810 Time: 0.246921
[08/10/2023-11:18:34] [V] [TRT] Conv_160 Set Tactic Name: ampere_h1688cudnn_256x64_ldg8_relu_exp_large_nhwc_tn_v1 Tactic: 0xe0e3c0e8cf9a2d9e
[08/10/2023-11:18:34] [V] [TRT] Tactic: 0xe0e3c0e8cf9a2d9e Time: 0.222597
[08/10/2023-11:18:34] [V] [TRT] Conv_160 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x32x64_stage5_warpsize2x2x1_g1_tensor16x8x16 Tactic: 0xe1ff5ad20f5c6bf6
[08/10/2023-11:18:34] [V] [TRT] Tactic: 0xe1ff5ad20f5c6bf6 Time: 0.181175
[08/10/2023-11:18:34] [V] [TRT] Conv_160 Set Tactic Name: ampere_h16816cudnn_256x64_ldg8_relu_exp_medium_nhwc_tn_v1 Tactic: 0xe4711898bd599c36
[08/10/2023-11:18:34] [V] [TRT] Tactic: 0xe4711898bd599c36 Time: 0.17131
[08/10/2023-11:18:34] [V] [TRT] Conv_160 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x64x64_stage3_warpsize4x1x1_g1_tensor16x8x16 Tactic: 0xeb25062ffb9eae45
[08/10/2023-11:18:34] [V] [TRT] Tactic: 0xeb25062ffb9eae45 Time: 0.187584
[08/10/2023-11:18:34] [V] [TRT] Conv_160 Set Tactic Name: ampere_h16816cudnn_128x64_sliced1x2_ldg8_relu_exp_stages_64x4_large_nhwc_tn_v1 Tactic: 0xeb2d2aa4e56bb41c
[08/10/2023-11:18:34] [V] [TRT] Tactic: 0xeb2d2aa4e56bb41c Time: 0.26128
[08/10/2023-11:18:34] [V] [TRT] Conv_160 Set Tactic Name: ampere_h16816cudnn_64x64_ldg8_relu_exp_stages_64x6_small_nhwc_tn_v1 Tactic: 0xf0beb09df9a19f82
[08/10/2023-11:18:34] [V] [TRT] Tactic: 0xf0beb09df9a19f82 Time: 0.350697
[08/10/2023-11:18:34] [V] [TRT] Conv_160 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x64x64_stage4_warpsize2x2x1_g1_tensor16x8x16 Tactic: 0xf35e0311fa1cc516
[08/10/2023-11:18:34] [V] [TRT] Tactic: 0xf35e0311fa1cc516 Time: 0.208229
[08/10/2023-11:18:34] [V] [TRT] Conv_160 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x32x32_stage4_warpsize4x1x1_g1_tensor16x8x16 Tactic: 0xf79479a62ea9f901
[08/10/2023-11:18:34] [V] [TRT] Tactic: 0xf79479a62ea9f901 Time: 0.12531
[08/10/2023-11:18:34] [V] [TRT] Fastest Tactic: 0x3e7eb35b91b9fa63 Time: 0.111392
[08/10/2023-11:18:34] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x3e7eb35b91b9fa63
[08/10/2023-11:18:34] [V] [TRT] *************** Autotuning format combination: Half(518400,1:16,2880,16) -> Half(32400,1:16,180,1) ***************
[08/10/2023-11:18:34] [V] [TRT] --------------- Timing Runner: Conv_160 (CaskConvolution)
[08/10/2023-11:18:34] [V] [TRT] Conv_160 Set Tactic Name: ampere_h16816cudnn_256x128_ldg8_relu_exp_medium_nhwc_tn_v1 Tactic: 0x03896956a39a1203
[08/10/2023-11:18:34] [V] [TRT] Tactic: 0x03896956a39a1203 Time: 0.310967
[08/10/2023-11:18:34] [V] [TRT] Conv_160 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x128x32_stage3_warpsize4x2x1_g1_tensor16x8x16_t1r3s3 Tactic: 0x048d6d0400f33439
[08/10/2023-11:18:34] [V] [TRT] Tactic: 0x048d6d0400f33439 Time: 0.290647
[08/10/2023-11:18:34] [V] [TRT] Conv_160 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x64x64_stage3_warpsize2x2x1_g1_tensor16x8x16_t1r3s3 Tactic: 0x05b6220f243edacd
[08/10/2023-11:18:34] [V] [TRT] Tactic: 0x05b6220f243edacd Time: 0.154542
[08/10/2023-11:18:34] [V] [TRT] Conv_160 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x64x32_stage3_warpsize4x1x1_g1_tensor16x8x16_t1r3s3_aACCESS Tactic: 0x0866ddee325d07a6
[08/10/2023-11:18:34] [V] [TRT] Tactic: 0x0866ddee325d07a6 Time: 0.163488
[08/10/2023-11:18:34] [V] [TRT] Conv_160 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x128x32_stage4_warpsize2x2x1_g1_tensor16x8x16 Tactic: 0x0e07ff4f4f7c1ac9
[08/10/2023-11:18:34] [V] [TRT] Tactic: 0x0e07ff4f4f7c1ac9 Time: 0.276018
[08/10/2023-11:18:34] [V] [TRT] Conv_160 Set Tactic Name: ampere_h16816cudnn_256x128_ldg8_relu_exp_stages_64x3_small_nhwc_tn_v1 Tactic: 0x0e199750c30c6928
[08/10/2023-11:18:34] [V] [TRT] Tactic: 0x0e199750c30c6928 Time: 0.308059
[08/10/2023-11:18:34] [V] [TRT] Conv_160 Set Tactic Name: ampere_h1688cudnn_128x128_ldg8_relu_exp_medium_nhwc_tn_v1 Tactic: 0x100c1ad308e08d35
[08/10/2023-11:18:34] [V] [TRT] Tactic: 0x100c1ad308e08d35 Time: 0.348626
[08/10/2023-11:18:34] [V] [TRT] Conv_160 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x128x32_stage5_warpsize2x2x1_g1_tensor16x8x16_t1r3s3 Tactic: 0x17ebf0c9f418f10a
[08/10/2023-11:18:34] [V] [TRT] Tactic: 0x17ebf0c9f418f10a Time: 0.286016
[08/10/2023-11:18:34] [V] [TRT] Conv_160 Set Tactic Name: ampere_h16816cudnn_128x64_ldg8_relu_exp_stages_64x4_medium_nhwc_tn_v1 Tactic: 0x196cbc423a9bb69e
[08/10/2023-11:18:34] [V] [TRT] Tactic: 0x196cbc423a9bb69e Time: 0.250066
[08/10/2023-11:18:34] [V] [TRT] Conv_160 Set Tactic Name: ampere_h16816cudnn_128x64_sliced1x2_ldg8_relu_exp_stages_64x3_small_nhwc_tn_v1 Tactic: 0x19822de884f42a6a
[08/10/2023-11:18:34] [V] [TRT] Tactic: 0x19822de884f42a6a Time: 0.165399
[08/10/2023-11:18:34] [V] [TRT] Conv_160 Set Tactic Name: ampere_h16816cudnn_64x64_sliced1x2_ldg8_relu_exp_stages_64x5_small_nhwc_tn_v1 Tactic: 0x1a5ba808ad4cc5a8
[08/10/2023-11:18:34] [V] [TRT] Tactic: 0x1a5ba808ad4cc5a8 Time: 0.248727
[08/10/2023-11:18:34] [V] [TRT] Conv_160 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x64x64_stage3_warpsize2x2x1_g1_tensor16x8x16 Tactic: 0x21b295c0c8f6c95a
[08/10/2023-11:18:34] [V] [TRT] Tactic: 0x21b295c0c8f6c95a Time: 0.171099
[08/10/2023-11:18:34] [V] [TRT] Conv_160 Set Tactic Name: ampere_h16816cudnn_128x128_ldg8_relu_exp_medium_nhwc_tn_v1 Tactic: 0x234580e8a194335c
[08/10/2023-11:18:34] [V] [TRT] Tactic: 0x234580e8a194335c Time: 0.252987
[08/10/2023-11:18:34] [V] [TRT] Conv_160 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x128x32_stage4_warpsize2x2x1_g1_tensor16x8x16_t1r3s3_aACCESS Tactic: 0x245530c34bd6090f
[08/10/2023-11:18:34] [V] [TRT] Tactic: 0x245530c34bd6090f Time: 0.258153
[08/10/2023-11:18:34] [V] [TRT] Conv_160 Set Tactic Name: ampere_h16816cudnn_64x64_sliced1x2_ldg8_relu_exp_stages_64x6_small_nhwc_tn_v1 Tactic: 0x24e01e7405cfdfe9
[08/10/2023-11:18:34] [V] [TRT] Tactic: 0x24e01e7405cfdfe9 Time: 0.361655
[08/10/2023-11:18:34] [V] [TRT] Conv_160 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x64x64_stage3_warpsize2x2x1_g1_tensor16x8x16_t1r3s3_aACCESS Tactic: 0x263a38afd75e3a43
[08/10/2023-11:18:34] [V] [TRT] Tactic: 0x263a38afd75e3a43 Time: 0.165545
[08/10/2023-11:18:34] [V] [TRT] Conv_160 Set Tactic Name: ampere_h16816cudnn_64x128_sliced1x2_ldg8_relu_exp_stages_64x4_large_nhwc_tn_v1 Tactic: 0x2721a7f18c2700db
[08/10/2023-11:18:34] [V] [TRT] Tactic: 0x2721a7f18c2700db Time: 0.490761
[08/10/2023-11:18:34] [V] [TRT] Conv_160 Set Tactic Name: ampere_h16816cudnn_128x64_sliced1x2_ldg8_relu_exp_stages_64x3_medium_nhwc_tn_v1 Tactic: 0x29329b5741ea05f2
[08/10/2023-11:18:34] [V] [TRT] Tactic: 0x29329b5741ea05f2 Time: 0.168256
[08/10/2023-11:18:34] [V] [TRT] Conv_160 Set Tactic Name: ampere_h16816cudnn_128x128_ldg8_relu_exp_large_nhwc_tn_v1 Tactic: 0x2970ae65966d569d
[08/10/2023-11:18:34] [V] [TRT] Tactic: 0x2970ae65966d569d Time: 0.252489
[08/10/2023-11:18:34] [V] [TRT] Conv_160 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x256x64_stage3_warpsize1x4x1_g1_tensor16x8x16 Tactic: 0x29dc14520e0e4eb6
[08/10/2023-11:18:34] [V] [TRT] Tactic: 0x29dc14520e0e4eb6 Time: 0.624562
[08/10/2023-11:18:34] [V] [TRT] Conv_160 Set Tactic Name: ampere_h16816cudnn_256x64_ldg8_relu_exp_stages_64x3_large_nhwc_tn_v1 Tactic: 0x2cb4a662694e89d3
[08/10/2023-11:18:34] [V] [TRT] Tactic: 0x2cb4a662694e89d3 Time: 0.197943
[08/10/2023-11:18:34] [V] [TRT] Conv_160 Set Tactic Name: ampere_h16816cudnn_128x128_ldg8_relu_exp_stages_64x3_medium_nhwc_tn_v1 Tactic: 0x2eaa2202de9404d6
[08/10/2023-11:18:34] [V] [TRT] Tactic: 0x2eaa2202de9404d6 Time: 0.343739
[08/10/2023-11:18:34] [V] [TRT] Conv_160 Set Tactic Name: ampere_h1688cudnn_256x128_ldg8_relu_exp_medium_nhwc_tn_v1 Tactic: 0x30c0f36d0aeeac6a
[08/10/2023-11:18:34] [V] [TRT] Tactic: 0x30c0f36d0aeeac6a Time: 0.364027
[08/10/2023-11:18:34] [V] [TRT] Conv_160 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x64x64_stage4_warpsize2x2x1_g1_tensor16x8x16_t1r3s3_aACCESS Tactic: 0x30e8a8d7a953e5e9
[08/10/2023-11:18:34] [V] [TRT] Tactic: 0x30e8a8d7a953e5e9 Time: 0.199538
[08/10/2023-11:18:34] [V] [TRT] Conv_160 Set Tactic Name: ampere_h16816cudnn_128x64_ldg8_relu_exp_stages_64x4_large_nhwc_tn_v1 Tactic: 0x3197d5044d71e98e
[08/10/2023-11:18:34] [V] [TRT] Tactic: 0x3197d5044d71e98e Time: 0.256887
[08/10/2023-11:18:34] [V] [TRT] Conv_160 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x64x64_stage4_warpsize2x2x1_g1_tensor16x8x16_t1r3s3 Tactic: 0x31d93dc22d2af081
[08/10/2023-11:18:34] [V] [TRT] Tactic: 0x31d93dc22d2af081 Time: 0.194921
[08/10/2023-11:18:34] [V] [TRT] Conv_160 Set Tactic Name: ampere_h1688cudnn_128x128_ldg8_relu_exp_small_nhwc_tn_v1 Tactic: 0x3369260c04f9ad73
[08/10/2023-11:18:34] [V] [TRT] Tactic: 0x3369260c04f9ad73 Time: 0.333033
[08/10/2023-11:18:34] [V] [TRT] Conv_160 Set Tactic Name: ampere_h16816cudnn_64x128_ldg8_relu_exp_stages_64x4_small_nhwc_tn_v1 Tactic: 0x3b5fa0f2a8fc2410
[08/10/2023-11:18:34] [V] [TRT] Tactic: 0x3b5fa0f2a8fc2410 Time: 0.428279
[08/10/2023-11:18:34] [V] [TRT] Conv_160 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x32x32_stage4_warpsize4x1x1_g1_tensor16x8x16_t1r3s3 Tactic: 0x3e7eb35b91b9fa63
[08/10/2023-11:18:34] [V] [TRT] Tactic: 0x3e7eb35b91b9fa63 Time: 0.119545
[08/10/2023-11:18:34] [V] [TRT] Conv_160 Set Tactic Name: ampere_h16816cudnn_256x64_ldg8_relu_exp_stages_64x3_medium_nhwc_tn_v1 Tactic: 0x3f031df0e579d52c
[08/10/2023-11:18:34] [V] [TRT] Tactic: 0x3f031df0e579d52c Time: 0.196782
[08/10/2023-11:18:34] [V] [TRT] Conv_160 Set Tactic Name: ampere_h16816cudnn_256x64_sliced1x2_ldg8_relu_exp_small_nhwc_tn_v1 Tactic: 0x42a5b8709d0039be
[08/10/2023-11:18:34] [V] [TRT] Tactic: 0x42a5b8709d0039be Time: 0.212119
[08/10/2023-11:18:34] [V] [TRT] Conv_160 Set Tactic Name: ampere_h16816cudnn_256x128_ldg8_relu_exp_small_nhwc_tn_v1 Tactic: 0x463794ee4acffd1f
[08/10/2023-11:18:34] [V] [TRT] Tactic: 0x463794ee4acffd1f Time: 0.306249
[08/10/2023-11:18:34] [V] [TRT] Conv_160 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x64x64_stage4_warpsize2x2x1_g1_tensor16x8x16 Tactic: 0x4a81ea1e51436a30
[08/10/2023-11:18:34] [V] [TRT] Tactic: 0x4a81ea1e51436a30 Time: 0.22325
[08/10/2023-11:18:34] [V] [TRT] Conv_160 Set Tactic Name: ampere_h1688cudnn_256x64_sliced1x2_ldg8_relu_exp_small_nhwc_tn_v1 Tactic: 0x4aed1956bd10a795
[08/10/2023-11:18:34] [V] [TRT] Tactic: 0x4aed1956bd10a795 Time: 0.24203
[08/10/2023-11:18:34] [V] [TRT] Conv_160 Set Tactic Name: ampere_h16816cudnn_64x128_sliced1x2_ldg8_relu_exp_stages_64x3_large_nhwc_tn_v1 Tactic: 0x4fc05923455fc266
[08/10/2023-11:18:34] [V] [TRT] Tactic: 0x4fc05923455fc266 Time: 0.305733
[08/10/2023-11:18:34] [V] [TRT] Conv_160 Set Tactic Name: ampere_h16816cudnn_256x64_ldg8_relu_exp_large_nhwc_tn_v1 Tactic: 0x5013c38f55afa9ba
[08/10/2023-11:18:34] [V] [TRT] Tactic: 0x5013c38f55afa9ba Time: 0.17947
[08/10/2023-11:18:34] [V] [TRT] Conv_160 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x64x32_stage3_warpsize4x1x1_g1_tensor16x8x16_t1r3s3 Tactic: 0x529f4431bdae94f5
[08/10/2023-11:18:34] [V] [TRT] Tactic: 0x529f4431bdae94f5 Time: 0.148439
[08/10/2023-11:18:34] [V] [TRT] Conv_160 Set Tactic Name: ampere_h16816cudnn_64x128_ldg8_relu_exp_stages_64x3_small_nhwc_tn_v1 Tactic: 0x53be5e206184e6ad
[08/10/2023-11:18:34] [V] [TRT] Tactic: 0x53be5e206184e6ad Time: 0.292782
[08/10/2023-11:18:34] [V] [TRT] Conv_160 Set Tactic Name: ampere_h16816cudnn_64x64_ldg8_relu_exp_stages_64x5_large_nhwc_tn_v1 Tactic: 0x544bff7ff9c5d908
[08/10/2023-11:18:34] [V] [TRT] Tactic: 0x544bff7ff9c5d908 Time: 0.262638
[08/10/2023-11:18:34] [V] [TRT] Conv_160 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x64x64_stage3_warpsize2x2x1_g1_tensor16x8x16 Tactic: 0x5568fd8a32f4a40f
[08/10/2023-11:18:34] [V] [TRT] Tactic: 0x5568fd8a32f4a40f Time: 0.156457
[08/10/2023-11:18:34] [V] [TRT] Conv_160 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x128x64_stage3_warpsize2x2x1_g1_tensor16x8x16 Tactic: 0x57c9a5ff682354a6
[08/10/2023-11:18:34] [V] [TRT] Tactic: 0x57c9a5ff682354a6 Time: 0.326944
[08/10/2023-11:18:34] [V] [TRT] Conv_160 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x32x64_stage5_warpsize2x2x1_g1_tensor16x8x16 Tactic: 0x5820b3dda403c4d0
[08/10/2023-11:18:34] [V] [TRT] Tactic: 0x5820b3dda403c4d0 Time: 0.18016
[08/10/2023-11:18:34] [V] [TRT] Conv_160 Set Tactic Name: ampere_h16816cudnn_128x128_ldg8_relu_exp_stages_64x3_small_nhwc_tn_v1 Tactic: 0x58816bf2e9c36afb
[08/10/2023-11:18:34] [V] [TRT] Tactic: 0x58816bf2e9c36afb Time: 0.341961
[08/10/2023-11:18:34] [V] [TRT] Conv_160 Set Tactic Name: ampere_h16816cudnn_128x64_ldg8_relu_exp_stages_64x3_large_nhwc_tn_v1 Tactic: 0x59762bd684092b33
[08/10/2023-11:18:34] [V] [TRT] Tactic: 0x59762bd684092b33 Time: 0.180741
[08/10/2023-11:18:34] [V] [TRT] Conv_160 Set Tactic Name: ampere_h16816cudnn_128x64_sliced1x2_ldg8_relu_exp_stages_64x4_medium_nhwc_tn_v1 Tactic: 0x59c5c647b4c76593
[08/10/2023-11:18:34] [V] [TRT] Tactic: 0x59c5c647b4c76593 Time: 0.25435
[08/10/2023-11:18:34] [V] [TRT] Conv_160 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x128x64_stage3_warpsize4x2x1_g1_tensor16x8x16 Tactic: 0x5a55274960724ba8
[08/10/2023-11:18:34] [V] [TRT] Tactic: 0x5a55274960724ba8 Time: 0.29637
[08/10/2023-11:18:34] [V] [TRT] Conv_160 Set Tactic Name: ampere_h1688cudnn_256x128_ldg8_relu_exp_large_nhwc_tn_v1 Tactic: 0x5c2e1c87d85b06f1
[08/10/2023-11:18:34] [V] [TRT] Tactic: 0x5c2e1c87d85b06f1 Time: 0.367365
[08/10/2023-11:18:34] [V] [TRT] Conv_160 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x256x64_stage3_warpsize1x4x1_g1_tensor16x8x16 Tactic: 0x5d067c18f40c23e3
[08/10/2023-11:18:34] [V] [TRT] Tactic: 0x5d067c18f40c23e3 Time: 0.624923
[08/10/2023-11:18:34] [V] [TRT] Conv_160 Set Tactic Name: ampere_h16816cudnn_64x64_ldg8_relu_exp_stages_64x5_medium_nhwc_tn_v1 Tactic: 0x5f31c22ec167f384
[08/10/2023-11:18:34] [V] [TRT] Tactic: 0x5f31c22ec167f384 Time: 0.261495
[08/10/2023-11:18:34] [V] [TRT] Conv_160 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x128x32_stage4_warpsize2x2x1_g1_tensor16x8x16_t1r3s3 Tactic: 0x60c3421152ef8e10
[08/10/2023-11:18:34] [V] [TRT] Tactic: 0x60c3421152ef8e10 Time: 0.247739
[08/10/2023-11:18:34] [V] [TRT] Conv_160 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x128x32_stage3_warpsize4x2x1_g1_tensor16x8x16 Tactic: 0x60da8c7151d91e47
[08/10/2023-11:18:34] [V] [TRT] Tactic: 0x60da8c7151d91e47 Time: 0.284201
[08/10/2023-11:18:34] [V] [TRT] Conv_160 Set Tactic Name: ampere_h1688cudnn_256x64_ldg8_relu_exp_medium_nhwc_tn_v1 Tactic: 0x6426696f872a3b13
[08/10/2023-11:18:34] [V] [TRT] Tactic: 0x6426696f872a3b13 Time: 0.226002
[08/10/2023-11:18:34] [V] [TRT] Conv_160 Set Tactic Name: ampere_h16816cudnn_128x64_ldg8_relu_exp_stages_64x3_medium_nhwc_tn_v1 Tactic: 0x699be152cfb6d6ff
[08/10/2023-11:18:34] [V] [TRT] Tactic: 0x699be152cfb6d6ff Time: 0.182464
[08/10/2023-11:18:34] [V] [TRT] Conv_160 Set Tactic Name: ampere_h16816cudnn_64x64_ldg8_relu_exp_stages_64x6_large_nhwc_tn_v1 Tactic: 0x6af049035146c349
[08/10/2023-11:18:34] [V] [TRT] Tactic: 0x6af049035146c349 Time: 0.38032
[08/10/2023-11:18:34] [V] [TRT] Conv_160 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x256x32_stage3_warpsize2x4x1_g1_tensor16x8x16 Tactic: 0x7005d10718f6c22d
[08/10/2023-11:18:34] [V] [TRT] Tactic: 0x7005d10718f6c22d Time: 0.523575
[08/10/2023-11:18:34] [V] [TRT] Conv_160 Set Tactic Name: ampere_h16816cudnn_64x64_sliced1x2_ldg8_relu_exp_stages_64x5_medium_nhwc_tn_v1 Tactic: 0x706f08da35c795a5
[08/10/2023-11:18:34] [V] [TRT] Tactic: 0x706f08da35c795a5 Time: 0.256553
[08/10/2023-11:18:34] [V] [TRT] Conv_160 Set Tactic Name: ampere_h16816cudnn_128x64_sliced1x2_ldg8_relu_exp_stages_64x4_small_nhwc_tn_v1 Tactic: 0x7163d33a4d8ce8d7
[08/10/2023-11:18:34] [V] [TRT] Tactic: 0x7163d33a4d8ce8d7 Time: 0.249669
[08/10/2023-11:18:34] [V] [TRT] Conv_160 Set Tactic Name: ampere_h1688cudnn_256x64_ldg8_relu_exp_small_nhwc_tn_v1 Tactic: 0x7aad3976677d7155
[08/10/2023-11:18:34] [V] [TRT] Tactic: 0x7aad3976677d7155 Time: 0.217179
[08/10/2023-11:18:34] [V] [TRT] Conv_160 Set Tactic Name: ampere_h16816cudnn_64x64_sliced1x2_ldg8_relu_exp_stages_64x5_large_nhwc_tn_v1 Tactic: 0x8015519605ab9963
[08/10/2023-11:18:34] [V] [TRT] Tactic: 0x8015519605ab9963 Time: 0.25979
[08/10/2023-11:18:34] [V] [TRT] Conv_160 Set Tactic Name: ampere_h16816cudnn_256x64_sliced1x2_ldg8_relu_exp_medium_nhwc_tn_v1 Tactic: 0x82f8a2214b0b4178
[08/10/2023-11:18:34] [V] [TRT] Tactic: 0x82f8a2214b0b4178 Time: 0.238565
[08/10/2023-11:18:34] [V] [TRT] Conv_160 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x32x32_stage4_warpsize4x1x1_g1_tensor16x8x16 Tactic: 0x834e11ecd4ab9454
[08/10/2023-11:18:34] [V] [TRT] Tactic: 0x834e11ecd4ab9454 Time: 0.124782
[08/10/2023-11:18:34] [V] [TRT] Conv_160 Set Tactic Name: ampere_h16816cudnn_128x64_sliced1x2_ldg8_relu_exp_stages_64x3_large_nhwc_tn_v1 Tactic: 0x83ccd4762c1376a1
[08/10/2023-11:18:34] [V] [TRT] Tactic: 0x83ccd4762c1376a1 Time: 0.170117
[08/10/2023-11:18:34] [V] [TRT] Conv_160 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x64x64_stage3_warpsize4x1x1_g1_tensor16x8x16_t1r3s3 Tactic: 0x84ee897dd1f4d2aa
[08/10/2023-11:18:34] [V] [TRT] Tactic: 0x84ee897dd1f4d2aa Time: 0.185477
[08/10/2023-11:18:34] [V] [TRT] Conv_160 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x128x64_stage3_warpsize2x2x1_g1_tensor16x8x16 Tactic: 0x866e7a5f6401b67f
[08/10/2023-11:18:34] [V] [TRT] Tactic: 0x866e7a5f6401b67f Time: 0.326638
[08/10/2023-11:18:34] [V] [TRT] Conv_160 Set Tactic Name: ampere_h16816cudnn_64x64_ldg8_relu_exp_stages_64x6_medium_nhwc_tn_v1 Tactic: 0x88971eec55aba850
[08/10/2023-11:18:34] [V] [TRT] Tactic: 0x88971eec55aba850 Time: 0.374213
[08/10/2023-11:18:34] [V] [TRT] Conv_160 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x128x64_stage3_warpsize4x2x1_g1_tensor16x8x16 Tactic: 0x8bf2f8e96c50a971
[08/10/2023-11:18:34] [V] [TRT] Tactic: 0x8bf2f8e96c50a971 Time: 0.291287
[08/10/2023-11:18:34] [V] [TRT] Conv_160 Set Tactic Name: ampere_h16816cudnn_256x128_ldg8_relu_exp_stages_64x3_large_nhwc_tn_v1 Tactic: 0x94576ece6beb35e3
[08/10/2023-11:18:34] [V] [TRT] Tactic: 0x94576ece6beb35e3 Time: 0.304955
[08/10/2023-11:18:34] [V] [TRT] Conv_160 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x64x64_stage3_warpsize4x1x1_g1_tensor16x8x16 Tactic: 0x9fff6e65019cc310
[08/10/2023-11:18:34] [V] [TRT] Tactic: 0x9fff6e65019cc310 Time: 0.185422
[08/10/2023-11:18:34] [V] [TRT] Conv_160 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x32x64_stage5_warpsize2x2x1_g1_tensor16x8x16_t1r3s3 Tactic: 0xa033e20ae9f412b2
[08/10/2023-11:18:34] [V] [TRT] Tactic: 0xa033e20ae9f412b2 Time: 0.165225
[08/10/2023-11:18:34] [V] [TRT] Conv_160 Set Tactic Name: ampere_h16816cudnn_64x128_ldg8_relu_exp_stages_64x4_large_nhwc_tn_v1 Tactic: 0xa111596c001b78db
[08/10/2023-11:18:34] [V] [TRT] Tactic: 0xa111596c001b78db Time: 0.442181
[08/10/2023-11:18:34] [V] [TRT] Conv_160 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x256x32_stage3_warpsize2x4x1_g1_tensor16x8x16 Tactic: 0xa1a20ea714d420f4
[08/10/2023-11:18:34] [V] [TRT] Tactic: 0xa1a20ea714d420f4 Time: 0.520187
[08/10/2023-11:18:34] [V] [TRT] Conv_160 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x32x32_stage4_warpsize4x1x1_g1_tensor16x8x16_t1r3s3 Tactic: 0xa40cb43c296a36a8
[08/10/2023-11:18:34] [V] [TRT] Tactic: 0xa40cb43c296a36a8 Time: 0.122971
[08/10/2023-11:18:34] [V] [TRT] Conv_160 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x64x32_stage3_warpsize4x1x1_g1_tensor16x8x16 Tactic: 0xa570c55d303796ff
[08/10/2023-11:18:34] [V] [TRT] Tactic: 0xa570c55d303796ff Time: 0.155808
[08/10/2023-11:18:34] [V] [TRT] Conv_160 Set Tactic Name: ampere_h16816cudnn_64x64_sliced1x2_ldg8_relu_exp_stages_64x6_medium_nhwc_tn_v1 Tactic: 0xa7c9d418a10bce71
[08/10/2023-11:18:34] [V] [TRT] Tactic: 0xa7c9d418a10bce71 Time: 0.389687
[08/10/2023-11:18:34] [V] [TRT] Conv_160 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x32x32_stage4_warpsize4x1x1_g1_tensor16x8x16 Tactic: 0xa83b68f30462f971
[08/10/2023-11:18:34] [V] [TRT] Tactic: 0xa83b68f30462f971 Time: 0.130384
[08/10/2023-11:18:34] [V] [TRT] Conv_160 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x256x32_stage3_warpsize2x4x1_g1_tensor16x8x16_t1r3s3 Tactic: 0xa9177bbe4e767df8
[08/10/2023-11:18:34] [V] [TRT] Tactic: 0xa9177bbe4e767df8 Time: 0.518638
[08/10/2023-11:18:34] [V] [TRT] Conv_160 Set Tactic Name: ampere_h1688cudnn_128x128_ldg8_relu_exp_large_nhwc_tn_v1 Tactic: 0xa927df92ac1ef1b8
[08/10/2023-11:18:34] [V] [TRT] Tactic: 0xa927df92ac1ef1b8 Time: 0.348786
[08/10/2023-11:18:34] [V] [TRT] Conv_160 Set Tactic Name: ampere_h16816cudnn_128x64_ldg8_relu_exp_stages_64x4_small_nhwc_tn_v1 Tactic: 0xabd92c9ae596b545
[08/10/2023-11:18:34] [V] [TRT] Tactic: 0xabd92c9ae596b545 Time: 0.243607
[08/10/2023-11:18:34] [V] [TRT] Conv_160 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x128x32_stage5_warpsize2x2x1_g1_tensor16x8x16 Tactic: 0xafd1e8bf6bd3d638
[08/10/2023-11:18:34] [V] [TRT] Tactic: 0xafd1e8bf6bd3d638 Time: 0.303392
[08/10/2023-11:18:34] [V] [TRT] Conv_160 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x128x32_stage3_warpsize4x2x1_g1_tensor16x8x16 Tactic: 0xb17d53d15dfbfc9e
[08/10/2023-11:18:34] [V] [TRT] Tactic: 0xb17d53d15dfbfc9e Time: 0.28917
[08/10/2023-11:18:34] [V] [TRT] Conv_160 Set Tactic Name: ampere_h16816cudnn_128x128_ldg8_relu_exp_small_nhwc_tn_v1 Tactic: 0xb33e57fb3e8a0a56
[08/10/2023-11:18:35] [V] [TRT] Tactic: 0xb33e57fb3e8a0a56 Time: 0.249056
[08/10/2023-11:18:35] [V] [TRT] Conv_160 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x64x32_stage5_warpsize2x2x1_g1_tensor16x8x16_t1r3s3 Tactic: 0xb4bec086187edcfc
[08/10/2023-11:18:35] [V] [TRT] Tactic: 0xb4bec086187edcfc Time: 0.162363
[08/10/2023-11:18:35] [V] [TRT] Conv_160 Set Tactic Name: ampere_h16816cudnn_256x64_ldg8_relu_exp_stages_64x3_small_nhwc_tn_v1 Tactic: 0xb6fa5ffcc1a9d518
[08/10/2023-11:18:35] [V] [TRT] Tactic: 0xb6fa5ffcc1a9d518 Time: 0.192654
[08/10/2023-11:18:35] [V] [TRT] Conv_160 Set Tactic Name: ampere_h16816cudnn_64x128_sliced1x2_ldg8_relu_exp_stages_64x3_medium_nhwc_tn_v1 Tactic: 0xb85e52e87caf60a2
[08/10/2023-11:18:35] [V] [TRT] Tactic: 0xb85e52e87caf60a2 Time: 0.293541
[08/10/2023-11:18:35] [V] [TRT] Conv_160 Set Tactic Name: ampere_h16816cudnn_64x128_ldg8_relu_exp_stages_64x3_medium_nhwc_tn_v1 Tactic: 0xb8d86216e1235cda
[08/10/2023-11:18:35] [V] [TRT] Tactic: 0xb8d86216e1235cda Time: 0.293422
[08/10/2023-11:18:35] [V] [TRT] Conv_160 Set Tactic Name: ampere_h16816cudnn_256x128_ldg8_relu_exp_stages_64x3_medium_nhwc_tn_v1 Tactic: 0xb9171d70ba2eda4b
[08/10/2023-11:18:35] [V] [TRT] Tactic: 0xb9171d70ba2eda4b Time: 0.311131
[08/10/2023-11:18:35] [V] [TRT] Conv_160 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x64x32_stage5_warpsize2x2x1_g1_tensor16x8x16 Tactic: 0xbd08239a9317f2fd
[08/10/2023-11:18:35] [V] [TRT] Tactic: 0xbd08239a9317f2fd Time: 0.171209
[08/10/2023-11:18:35] [V] [TRT] Conv_160 Set Tactic Name: ampere_h16816cudnn_64x128_sliced1x2_ldg8_relu_exp_stages_64x4_small_nhwc_tn_v1 Tactic: 0xbd6f5e6f24c05c10
[08/10/2023-11:18:35] [V] [TRT] Tactic: 0xbd6f5e6f24c05c10 Time: 0.480379
[08/10/2023-11:18:35] [V] [TRT] Conv_160 Set Tactic Name: ampere_h16816cudnn_64x64_sliced1x2_ldg8_relu_exp_stages_64x6_large_nhwc_tn_v1 Tactic: 0xbeaee7eaad288322
[08/10/2023-11:18:35] [V] [TRT] Tactic: 0xbeaee7eaad288322 Time: 0.402386
[08/10/2023-11:18:35] [V] [TRT] Conv_160 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x128x64_stage3_warpsize2x2x1_g1_tensor16x8x16_t1r3s3 Tactic: 0xc0a02dc6095497cc
[08/10/2023-11:18:35] [V] [TRT] Tactic: 0xc0a02dc6095497cc Time: 0.327557
[08/10/2023-11:18:35] [V] [TRT] Conv_160 Set Tactic Name: ampere_h16816cudnn_128x128_ldg8_relu_exp_stages_64x3_large_nhwc_tn_v1 Tactic: 0xc2cf926c41243630
[08/10/2023-11:18:35] [V] [TRT] Tactic: 0xc2cf926c41243630 Time: 0.352933
[08/10/2023-11:18:35] [V] [TRT] Conv_160 Set Tactic Name: ampere_h16816cudnn_128x64_ldg8_relu_exp_stages_64x3_small_nhwc_tn_v1 Tactic: 0xc338d2482cee77f8
[08/10/2023-11:18:35] [V] [TRT] Tactic: 0xc338d2482cee77f8 Time: 0.181874
[08/10/2023-11:18:35] [V] [TRT] Conv_160 Set Tactic Name: ampere_h1688cudnn_256x128_ldg8_relu_exp_small_nhwc_tn_v1 Tactic: 0xc660e51970bc5a3a
[08/10/2023-11:18:35] [V] [TRT] Tactic: 0xc660e51970bc5a3a Time: 0.343611
[08/10/2023-11:18:35] [V] [TRT] Conv_160 Set Tactic Name: ampere_h16816cudnn_64x128_ldg8_relu_exp_stages_64x4_medium_nhwc_tn_v1 Tactic: 0xc82f3f06140e3cbb
[08/10/2023-11:18:35] [V] [TRT] Tactic: 0xc82f3f06140e3cbb Time: 0.437271
[08/10/2023-11:18:35] [V] [TRT] Conv_160 Set Tactic Name: ampere_h16816cudnn_64x128_sliced1x2_ldg8_relu_exp_stages_64x4_medium_nhwc_tn_v1 Tactic: 0xc8a90ff8898200c3
[08/10/2023-11:18:35] [V] [TRT] Tactic: 0xc8a90ff8898200c3 Time: 0.47232
[08/10/2023-11:18:35] [V] [TRT] Conv_160 Set Tactic Name: ampere_h1688cudnn_256x64_sliced1x2_ldg8_relu_exp_medium_nhwc_tn_v1 Tactic: 0xc9cc55109bb4de26
[08/10/2023-11:18:35] [V] [TRT] Tactic: 0xc9cc55109bb4de26 Time: 0.253669
[08/10/2023-11:18:35] [V] [TRT] Conv_160 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x64x32_stage5_warpsize2x2x1_g1_tensor16x8x16 Tactic: 0xc9d24bd069159fa8
[08/10/2023-11:18:35] [V] [TRT] Tactic: 0xc9d24bd069159fa8 Time: 0.182976
[08/10/2023-11:18:35] [V] [TRT] Conv_160 Set Tactic Name: ampere_h16816cudnn_64x128_ldg8_relu_exp_stages_64x3_large_nhwc_tn_v1 Tactic: 0xc9f0a7bec963ba66
[08/10/2023-11:18:35] [V] [TRT] Tactic: 0xc9f0a7bec963ba66 Time: 0.294711
[08/10/2023-11:18:35] [V] [TRT] Conv_160 Set Tactic Name: ampere_h16816cudnn_256x64_ldg8_relu_exp_small_nhwc_tn_v1 Tactic: 0xca5d3a11fd48f571
[08/10/2023-11:18:35] [V] [TRT] Tactic: 0xca5d3a11fd48f571 Time: 0.166313
[08/10/2023-11:18:35] [V] [TRT] Conv_160 Set Tactic Name: ampere_h16816cudnn_64x64_ldg8_relu_exp_stages_64x5_small_nhwc_tn_v1 Tactic: 0xce0506e1512285c3
[08/10/2023-11:18:35] [V] [TRT] Tactic: 0xce0506e1512285c3 Time: 0.261934
[08/10/2023-11:18:35] [V] [TRT] Conv_160 Set Tactic Name: ampere_h1688cudnn_256x64_sliced1x2_ldg8_relu_exp_large_nhwc_tn_v1 Tactic: 0xd0a3e0c815f7fb5e
[08/10/2023-11:18:35] [V] [TRT] Tactic: 0xd0a3e0c815f7fb5e Time: 0.253783
[08/10/2023-11:18:35] [V] [TRT] Conv_160 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x64x32_stage3_warpsize4x1x1_g1_tensor16x8x16 Tactic: 0xd1aaad17ca35fbaa
[08/10/2023-11:18:35] [V] [TRT] Tactic: 0xd1aaad17ca35fbaa Time: 0.14597
[08/10/2023-11:18:35] [V] [TRT] Conv_160 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x128x64_stage3_warpsize4x2x1_g1_tensor16x8x16_t1r3s3 Tactic: 0xd2fca0486ca97266
[08/10/2023-11:18:35] [V] [TRT] Tactic: 0xd2fca0486ca97266 Time: 0.296608
[08/10/2023-11:18:35] [V] [TRT] Conv_160 Set Tactic Name: ampere_h16816cudnn_64x128_sliced1x2_ldg8_relu_exp_stages_64x3_small_nhwc_tn_v1 Tactic: 0xd58ea0bdedb89ead
[08/10/2023-11:18:35] [V] [TRT] Tactic: 0xd58ea0bdedb89ead Time: 0.292283
[08/10/2023-11:18:35] [V] [TRT] Conv_160 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x256x64_stage3_warpsize1x4x1_g1_tensor16x8x16_t1r3s3 Tactic: 0xd7c261fa01db2af9
[08/10/2023-11:18:35] [V] [TRT] Tactic: 0xd7c261fa01db2af9 Time: 0.610523
[08/10/2023-11:18:35] [V] [TRT] Conv_160 Set Tactic Name: ampere_h16816cudnn_256x64_sliced1x2_ldg8_relu_exp_large_nhwc_tn_v1 Tactic: 0xd8eb41ee35e76575
[08/10/2023-11:18:35] [V] [TRT] Tactic: 0xd8eb41ee35e76575 Time: 0.238327
[08/10/2023-11:18:35] [V] [TRT] Conv_160 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x128x32_stage5_warpsize2x2x1_g1_tensor16x8x16 Tactic: 0xdb0b80f591d1bb6d
[08/10/2023-11:18:35] [V] [TRT] Tactic: 0xdb0b80f591d1bb6d Time: 0.292846
[08/10/2023-11:18:35] [V] [TRT] Conv_160 Set Tactic Name: ampere_h16816cudnn_256x128_ldg8_relu_exp_large_nhwc_tn_v1 Tactic: 0xdc796d70e228a1d4
[08/10/2023-11:18:35] [V] [TRT] Tactic: 0xdc796d70e228a1d4 Time: 0.305216
[08/10/2023-11:18:35] [V] [TRT] Conv_160 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x32x32_stage4_warpsize4x1x1_g1_tensor16x8x16 Tactic: 0xdce100b9fe609424
[08/10/2023-11:18:35] [V] [TRT] Tactic: 0xdce100b9fe609424 Time: 0.130299
[08/10/2023-11:18:35] [V] [TRT] Conv_160 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x128x32_stage4_warpsize2x2x1_g1_tensor16x8x16 Tactic: 0xdfa020ef435ef810
[08/10/2023-11:18:35] [V] [TRT] Tactic: 0xdfa020ef435ef810 Time: 0.254482
[08/10/2023-11:18:35] [V] [TRT] Conv_160 Set Tactic Name: ampere_h1688cudnn_256x64_ldg8_relu_exp_large_nhwc_tn_v1 Tactic: 0xe0e3c0e8cf9a2d9e
[08/10/2023-11:18:35] [V] [TRT] Tactic: 0xe0e3c0e8cf9a2d9e Time: 0.231131
[08/10/2023-11:18:35] [V] [TRT] Conv_160 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x32x64_stage5_warpsize2x2x1_g1_tensor16x8x16 Tactic: 0xe1ff5ad20f5c6bf6
[08/10/2023-11:18:35] [V] [TRT] Tactic: 0xe1ff5ad20f5c6bf6 Time: 0.187122
[08/10/2023-11:18:35] [V] [TRT] Conv_160 Set Tactic Name: ampere_h16816cudnn_256x64_ldg8_relu_exp_medium_nhwc_tn_v1 Tactic: 0xe4711898bd599c36
[08/10/2023-11:18:35] [V] [TRT] Tactic: 0xe4711898bd599c36 Time: 0.177015
[08/10/2023-11:18:35] [V] [TRT] Conv_160 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x64x64_stage3_warpsize4x1x1_g1_tensor16x8x16 Tactic: 0xeb25062ffb9eae45
[08/10/2023-11:18:35] [V] [TRT] Tactic: 0xeb25062ffb9eae45 Time: 0.194066
[08/10/2023-11:18:35] [V] [TRT] Conv_160 Set Tactic Name: ampere_h16816cudnn_128x64_sliced1x2_ldg8_relu_exp_stages_64x4_large_nhwc_tn_v1 Tactic: 0xeb2d2aa4e56bb41c
[08/10/2023-11:18:35] [V] [TRT] Tactic: 0xeb2d2aa4e56bb41c Time: 0.267735
[08/10/2023-11:18:35] [V] [TRT] Conv_160 Set Tactic Name: ampere_h16816cudnn_64x64_ldg8_relu_exp_stages_64x6_small_nhwc_tn_v1 Tactic: 0xf0beb09df9a19f82
[08/10/2023-11:18:35] [V] [TRT] Tactic: 0xf0beb09df9a19f82 Time: 0.354962
[08/10/2023-11:18:35] [V] [TRT] Conv_160 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x64x64_stage4_warpsize2x2x1_g1_tensor16x8x16 Tactic: 0xf35e0311fa1cc516
[08/10/2023-11:18:35] [V] [TRT] Tactic: 0xf35e0311fa1cc516 Time: 0.205865
[08/10/2023-11:18:35] [V] [TRT] Conv_160 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x32x32_stage4_warpsize4x1x1_g1_tensor16x8x16 Tactic: 0xf79479a62ea9f901
[08/10/2023-11:18:35] [V] [TRT] Tactic: 0xf79479a62ea9f901 Time: 0.127723
[08/10/2023-11:18:35] [V] [TRT] Fastest Tactic: 0x3e7eb35b91b9fa63 Time: 0.119545
[08/10/2023-11:18:35] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x3e7eb35b91b9fa63
[08/10/2023-11:18:35] [V] [TRT] =============== Computing costs for 
[08/10/2023-11:18:35] [V] [TRT] *************** Autotuning format combination: Float(8294400,32400,180,1) -> Float(64800,32400,180,1) ***************
[08/10/2023-11:18:35] [V] [TRT] --------------- Timing Runner: Conv_163 (CudaDepthwiseConvolution)
[08/10/2023-11:18:35] [V] [TRT] CudaDepthwiseConvolution has no valid tactics for this config, skipping
[08/10/2023-11:18:35] [V] [TRT] --------------- Timing Runner: Conv_163 (FusedConvActConvolution)
[08/10/2023-11:18:35] [V] [TRT] Tactic: 0x000000000007ffff Time: 0.679863
[08/10/2023-11:18:35] [V] [TRT] Tactic: 0x00000000000affff Time: 1.16248
[08/10/2023-11:18:35] [V] [TRT] Tactic: 0x00000000000effff Time: 1.23928
[08/10/2023-11:18:35] [V] [TRT] Tactic: 0x00000000000fffff Time: 0.651977
[08/10/2023-11:18:35] [V] [TRT] Tactic: 0x00000000001effff Time: 1.22144
[08/10/2023-11:18:35] [V] [TRT] Tactic: 0x000000000027ffff Time: 0.696311
[08/10/2023-11:18:35] [V] [TRT] Tactic: 0x000000000062ffff Time: 0.628544
[08/10/2023-11:18:35] [V] [TRT] Tactic: 0x00000000006effff Time: 1.49291
[08/10/2023-11:18:35] [V] [TRT] Tactic: 0x000000000086ffff Time: 0.985838
[08/10/2023-11:18:35] [V] [TRT] Tactic: 0x000000000089ffff Time: 0.639511
[08/10/2023-11:18:35] [V] [TRT] Tactic: 0x00000000009fffff Time: 0.663593
[08/10/2023-11:18:35] [V] [TRT] Tactic: 0x0000000000a2ffff Time: 0.704457
[08/10/2023-11:18:35] [V] [TRT] Fastest Tactic: 0x000000000062ffff Time: 0.628544
[08/10/2023-11:18:35] [V] [TRT] --------------- Timing Runner: Conv_163 (CudnnConvolution)
[08/10/2023-11:18:35] [V] [TRT] Tactic: 0x0000000000000000 Time: 1.04198
[08/10/2023-11:18:35] [V] [TRT] Tactic: 0x0000000000000001 Time: 0.470107
[08/10/2023-11:18:35] [V] [TRT] Tactic: 0x0000000000000002 Time: 1.02149
[08/10/2023-11:18:35] [V] [TRT] Tactic: 0x0000000000000004 Time: 3.83949
[08/10/2023-11:18:35] [V] [TRT] Tactic: 0x0000000000000005 Time: 1.06566
[08/10/2023-11:18:35] [V] [TRT] Tactic: 0x0000000000000006 Time: 0.436535
[08/10/2023-11:18:35] [V] [TRT] Tactic: 0x0000000000000038 Time: 1.03882
[08/10/2023-11:18:35] [V] [TRT] Tactic: 0x0000000000000039 Time: 0.479314
[08/10/2023-11:18:35] [V] [TRT] Tactic: 0x000000000000003a Time: 1.02801
[08/10/2023-11:18:35] [V] [TRT] Tactic: 0x000000000000003c Time: 3.75096
[08/10/2023-11:18:35] [V] [TRT] Tactic: 0x000000000000003d Time: 1.05768
[08/10/2023-11:18:35] [V] [TRT] Tactic: 0x000000000000003e Time: 0.436037
[08/10/2023-11:18:35] [V] [TRT] Tactic: 0x0000000000000070 Time: 1.02328
[08/10/2023-11:18:35] [V] [TRT] Tactic: 0x0000000000000071 Time: 1.01782
[08/10/2023-11:18:35] [V] [TRT] Tactic: 0x0000000000000072 Time: 1.02235
[08/10/2023-11:18:35] [V] [TRT] Tactic: 0x0000000000000074 Time: 3.99642
[08/10/2023-11:18:36] [V] [TRT] Tactic: 0x0000000000000075 Time: 1.06361
[08/10/2023-11:18:36] [V] [TRT] Tactic: 0x0000000000000076 Time: 0.437541
[08/10/2023-11:18:36] [V] [TRT] Fastest Tactic: 0x000000000000003e Time: 0.436037
[08/10/2023-11:18:36] [V] [TRT] --------------- Timing Runner: Conv_163 (CaskConvolution)
[08/10/2023-11:18:36] [V] [TRT] Conv_163 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x128x8_stage3_warpsize1x4x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0x01cf8ce2da913006
[08/10/2023-11:18:36] [V] [TRT] Tactic: 0x01cf8ce2da913006 Time: 2.71132
[08/10/2023-11:18:36] [V] [TRT] Conv_163 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x64x8_stage3_warpsize2x2x1_g1_ffma_aligna4_alignc4 Tactic: 0x12dbf7d94ee3696d
[08/10/2023-11:18:36] [V] [TRT] Tactic: 0x12dbf7d94ee3696d Time: 1.47453
[08/10/2023-11:18:36] [V] [TRT] Conv_163 Set Tactic Name: ampere_scudnn_128x64_relu_small_nn_v1 Tactic: 0x3f243c490d502deb
[08/10/2023-11:18:36] [V] [TRT] Tactic: 0x3f243c490d502deb Time: 1.38708
[08/10/2023-11:18:36] [V] [TRT] Conv_163 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0x4727434768e46395
[08/10/2023-11:18:36] [V] [TRT] Tactic: 0x4727434768e46395 Time: 1.55636
[08/10/2023-11:18:36] [V] [TRT] Conv_163 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize256x128x8_stage3_warpsize4x2x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0x4efce38acc876f5c
[08/10/2023-11:18:36] [V] [TRT] Tactic: 0x4efce38acc876f5c Time: 2.79075
[08/10/2023-11:18:36] [V] [TRT] Conv_163 Set Tactic Name: ampere_scudnn_128x128_relu_small_nn_v1 Tactic: 0x503619c69ae500ff
[08/10/2023-11:18:36] [V] [TRT] Tactic: 0x503619c69ae500ff Time: 2.45431
[08/10/2023-11:18:36] [V] [TRT] Conv_163 Set Tactic Name: ampere_scudnn_128x128_relu_xregs_large_nn_v1 Tactic: 0x5403ad713f811a18
[08/10/2023-11:18:36] [V] [TRT] Tactic: 0x5403ad713f811a18 Time: 2.62695
[08/10/2023-11:18:36] [V] [TRT] Conv_163 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x128x8_stage3_warpsize1x4x1_g1_ffma_aligna4_alignc4 Tactic: 0x5aa723e0481da855
[08/10/2023-11:18:36] [V] [TRT] Tactic: 0x5aa723e0481da855 Time: 2.69621
[08/10/2023-11:18:36] [V] [TRT] Conv_163 Set Tactic Name: ampere_scudnn_128x64_relu_xregs_large_nn_v1 Tactic: 0x5deb29b7a8e275f7
[08/10/2023-11:18:36] [V] [TRT] Tactic: 0x5deb29b7a8e275f7 Time: 1.33678
[08/10/2023-11:18:36] [V] [TRT] Conv_163 Set Tactic Name: ampere_scudnn_winograd_128x128_ldg1_ldg4_relu_tile148t_nt_v1 Tactic: 0x94119b4c514b211a
[08/10/2023-11:18:36] [V] [TRT] Tactic: 0x94119b4c514b211a Time: 0.412201
[08/10/2023-11:18:36] [V] [TRT] Conv_163 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_aligna4_alignc4 Tactic: 0xa31d27de74b895ff
[08/10/2023-11:18:36] [V] [TRT] Tactic: 0xa31d27de74b895ff Time: 1.54919
[08/10/2023-11:18:36] [V] [TRT] Conv_163 Set Tactic Name: ampere_scudnn_128x32_relu_small_nn_v1 Tactic: 0xa8609adc4e0ceb90
[08/10/2023-11:18:36] [V] [TRT] Tactic: 0xa8609adc4e0ceb90 Time: 0.814866
[08/10/2023-11:18:36] [V] [TRT] Conv_163 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize256x128x8_stage3_warpsize4x2x1_g1_ffma_aligna4_alignc4 Tactic: 0xbb8c3889c7eacd30
[08/10/2023-11:18:36] [V] [TRT] Tactic: 0xbb8c3889c7eacd30 Time: 2.74759
[08/10/2023-11:18:36] [V] [TRT] Conv_163 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_aligna4_alignc4 Tactic: 0xd828f024626fa982
[08/10/2023-11:18:36] [V] [TRT] Tactic: 0xd828f024626fa982 Time: 2.20608
[08/10/2023-11:18:36] [V] [TRT] Conv_163 Set Tactic Name: ampere_scudnn_128x128_relu_medium_nn_v1 Tactic: 0xf067e6205da31c2e
[08/10/2023-11:18:36] [V] [TRT] Tactic: 0xf067e6205da31c2e Time: 2.5188
[08/10/2023-11:18:36] [V] [TRT] Conv_163 Set Tactic Name: ampere_scudnn_128x64_relu_medium_nn_v1 Tactic: 0xf64396b97c889179
[08/10/2023-11:18:36] [V] [TRT] Tactic: 0xf64396b97c889179 Time: 1.41584
[08/10/2023-11:18:36] [V] [TRT] Fastest Tactic: 0x94119b4c514b211a Time: 0.412201
[08/10/2023-11:18:36] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x94119b4c514b211a
[08/10/2023-11:18:36] [V] [TRT] *************** Autotuning format combination: Float(8294400,1,46080,256) -> Float(64800,1,360,2) ***************
[08/10/2023-11:18:36] [V] [TRT] --------------- Timing Runner: Conv_163 (CaskConvolution)
[08/10/2023-11:18:36] [V] [TRT] Conv_163 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_aligna4_alignc4 Tactic: 0x19b688348f983aa0
[08/10/2023-11:18:36] [V] [TRT] Tactic: 0x19b688348f983aa0 Time: 1.41991
[08/10/2023-11:18:36] [V] [TRT] Conv_163 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x64x8_stage3_warpsize2x2x1_g1_ffma_aligna4_alignc4 Tactic: 0x1da91d865428f237
[08/10/2023-11:18:36] [V] [TRT] Tactic: 0x1da91d865428f237 Time: 1.15932
[08/10/2023-11:18:36] [V] [TRT] Conv_163 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x256x8_stage3_warpsize1x4x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0x3f0c846d6379bc98
[08/10/2023-11:18:36] [V] [TRT] Tactic: 0x3f0c846d6379bc98 Time: 4.27838
[08/10/2023-11:18:36] [V] [TRT] Conv_163 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_aligna4_alignc4 Tactic: 0x62835fce994f06dd
[08/10/2023-11:18:36] [V] [TRT] Tactic: 0x62835fce994f06dd Time: 1.23197
[08/10/2023-11:18:36] [V] [TRT] Conv_163 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize256x64x8_stage3_warpsize2x2x1_g1_ffma_aligna4_alignc4 Tactic: 0x8014228ec08b4d49
[08/10/2023-11:18:36] [V] [TRT] Tactic: 0x8014228ec08b4d49 Time: 1.2257
[08/10/2023-11:18:36] [V] [TRT] Conv_163 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0x94a7db94ba744c45
[08/10/2023-11:18:36] [V] [TRT] Tactic: 0x94a7db94ba744c45 Time: 1.25029
[08/10/2023-11:18:36] [V] [TRT] Conv_163 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x128x8_stage3_warpsize1x4x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0xd15dd11d64344e83
[08/10/2023-11:18:36] [V] [TRT] Tactic: 0xd15dd11d64344e83 Time: 2.2115
[08/10/2023-11:18:36] [V] [TRT] Conv_163 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x64x8_stage3_warpsize2x2x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0xf48db81f02eca9ee
[08/10/2023-11:18:36] [V] [TRT] Tactic: 0xf48db81f02eca9ee Time: 1.16153
[08/10/2023-11:18:36] [V] [TRT] Fastest Tactic: 0x1da91d865428f237 Time: 1.15932
[08/10/2023-11:18:36] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x1da91d865428f237
[08/10/2023-11:18:36] [V] [TRT] *************** Autotuning format combination: Float(2073600,1:4,11520,64) -> Float(32400,1:4,180,1) ***************
[08/10/2023-11:18:36] [V] [TRT] --------------- Timing Runner: Conv_163 (CaskConvolution)
[08/10/2023-11:18:36] [V] [TRT] Conv_163 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize64x128x32_stage5_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0x112d7e4d280f396d
[08/10/2023-11:18:36] [V] [TRT] Tactic: 0x112d7e4d280f396d Time: 0.850053
[08/10/2023-11:18:36] [V] [TRT] Conv_163 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x64x32_stage5_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0x13cc2ad8dcd32ba2
[08/10/2023-11:18:36] [V] [TRT] Tactic: 0x13cc2ad8dcd32ba2 Time: 0.468471
[08/10/2023-11:18:36] [V] [TRT] Conv_163 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize256x128x32_stage3_warpsize4x2x1_g1_tensor16x8x8_t1r3s3 Tactic: 0x21246c8544eff903
[08/10/2023-11:18:36] [V] [TRT] Tactic: 0x21246c8544eff903 Time: 0.52587
[08/10/2023-11:18:36] [V] [TRT] Conv_163 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x128x32_stage4_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0x25b2b9d5c9d5ca0d
[08/10/2023-11:18:36] [V] [TRT] Tactic: 0x25b2b9d5c9d5ca0d Time: 0.586843
[08/10/2023-11:18:36] [V] [TRT] Conv_163 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage4_warpsize2x2x1_g1_tensor16x8x8_t1r3s3 Tactic: 0x2d80d2b384ab13f1
[08/10/2023-11:18:36] [V] [TRT] Tactic: 0x2d80d2b384ab13f1 Time: 0.511456
[08/10/2023-11:18:36] [V] [TRT] Conv_163 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize256x128x32_stage3_warpsize4x2x1_g1_tensor16x8x8 Tactic: 0x36303f399d3ee4fd
[08/10/2023-11:18:36] [V] [TRT] Tactic: 0x36303f399d3ee4fd Time: 0.635237
[08/10/2023-11:18:36] [V] [TRT] Conv_163 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage4_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0x385bfab9332b1413
[08/10/2023-11:18:36] [V] [TRT] Tactic: 0x385bfab9332b1413 Time: 0.568521
[08/10/2023-11:18:36] [V] [TRT] Conv_163 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x256x32_stage3_warpsize2x4x1_g1_tensor16x8x8 Tactic: 0x482b48242255b8ce
[08/10/2023-11:18:36] [V] [TRT] Tactic: 0x482b48242255b8ce Time: 1.00129
[08/10/2023-11:18:36] [V] [TRT] Conv_163 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x256x32_stage3_warpsize2x4x1_g1_tensor16x8x8_t1r3s3 Tactic: 0x614e89f7852edbc3
[08/10/2023-11:18:36] [V] [TRT] Tactic: 0x614e89f7852edbc3 Time: 0.970971
[08/10/2023-11:18:36] [V] [TRT] Conv_163 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0x65e41d81f093b482
[08/10/2023-11:18:36] [V] [TRT] Tactic: 0x65e41d81f093b482 Time: 0.611666
[08/10/2023-11:18:36] [V] [TRT] Conv_163 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize64x128x32_stage5_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0x65f71607d20d5438
[08/10/2023-11:18:36] [V] [TRT] Tactic: 0x65f71607d20d5438 Time: 0.812955
[08/10/2023-11:18:36] [V] [TRT] Conv_163 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x64x32_stage5_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0x6716429226d146f7
[08/10/2023-11:18:36] [V] [TRT] Tactic: 0x6716429226d146f7 Time: 0.430693
[08/10/2023-11:18:36] [V] [TRT] Conv_163 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize64x32x64_stage5_warpsize2x2x1_g1_tensor16x8x8_t1r3s3 Tactic: 0x679a281f7660c436
[08/10/2023-11:18:36] [V] [TRT] Tactic: 0x679a281f7660c436 Time: 0.405193
[08/10/2023-11:18:36] [V] [TRT] Conv_163 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize64x32x64_stage5_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0x6e2a3c7a7fc5e4e1
[08/10/2023-11:18:36] [V] [TRT] Tactic: 0x6e2a3c7a7fc5e4e1 Time: 0.424091
[08/10/2023-11:18:36] [V] [TRT] Conv_163 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x64x32_stage5_warpsize2x2x1_g1_tensor16x8x8_t1r3s3 Tactic: 0x7aa21282feb15f1a
[08/10/2023-11:18:36] [V] [TRT] Tactic: 0x7aa21282feb15f1a Time: 0.420119
[08/10/2023-11:18:36] [V] [TRT] Conv_163 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage4_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0x818413b69874bb35
[08/10/2023-11:18:36] [V] [TRT] Tactic: 0x818413b69874bb35 Time: 0.558213
[08/10/2023-11:18:36] [V] [TRT] Conv_163 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x256x32_stage3_warpsize2x4x1_g1_tensor16x8x8 Tactic: 0x998c97842e775a17
[08/10/2023-11:18:36] [V] [TRT] Tactic: 0x998c97842e775a17 Time: 1.01259
[08/10/2023-11:18:36] [V] [TRT] Conv_163 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8_t1r3s3 Tactic: 0x999e005e3b016ea6
[08/10/2023-11:18:36] [V] [TRT] Tactic: 0x999e005e3b016ea6 Time: 0.533952
[08/10/2023-11:18:36] [V] [TRT] Conv_163 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize256x64x32_stage3_warpsize4x2x1_g1_tensor16x8x8_t1r3s3 Tactic: 0xa9a06d0633580c0c
[08/10/2023-11:18:36] [V] [TRT] Tactic: 0xa9a06d0633580c0c Time: 0.337129
[08/10/2023-11:18:36] [V] [TRT] Conv_163 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0xb443c221fcb1565b
[08/10/2023-11:18:36] [V] [TRT] Tactic: 0xb443c221fcb1565b Time: 0.552379
[08/10/2023-11:18:36] [V] [TRT] Conv_163 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage3_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0xb47d7d926de02dee
[08/10/2023-11:18:37] [V] [TRT] Tactic: 0xb47d7d926de02dee Time: 0.423177
[08/10/2023-11:18:37] [V] [TRT] Conv_163 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize256x64x32_stage3_warpsize4x2x1_g1_tensor16x8x8 Tactic: 0xba0185279ccb2b85
[08/10/2023-11:18:37] [V] [TRT] Tactic: 0xba0185279ccb2b85 Time: 0.376219
[08/10/2023-11:18:37] [V] [TRT] Conv_163 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage3_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0xc0a715d897e240bb
[08/10/2023-11:18:37] [V] [TRT] Tactic: 0xc0a715d897e240bb Time: 0.423442
[08/10/2023-11:18:37] [V] [TRT] Conv_163 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize256x64x32_stage3_warpsize4x2x1_g1_tensor16x8x8 Tactic: 0xcedbed6d66c946d0
[08/10/2023-11:18:37] [V] [TRT] Tactic: 0xcedbed6d66c946d0 Time: 0.360571
[08/10/2023-11:18:37] [V] [TRT] Conv_163 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize64x128x32_stage5_warpsize2x2x1_g1_tensor16x8x8_t1r3s3 Tactic: 0xd4428a7355769d7d
[08/10/2023-11:18:37] [V] [TRT] Tactic: 0xd4428a7355769d7d Time: 0.782752
[08/10/2023-11:18:37] [V] [TRT] Conv_163 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize64x32x64_stage5_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0xd7f5d575d49a4bc7
[08/10/2023-11:18:37] [V] [TRT] Tactic: 0xd7f5d575d49a4bc7 Time: 0.475278
[08/10/2023-11:18:37] [V] [TRT] Conv_163 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x128x32_stage4_warpsize2x2x1_g1_tensor16x8x8_t1r3s3 Tactic: 0xd920b33c9bd27143
[08/10/2023-11:18:37] [V] [TRT] Tactic: 0xd920b33c9bd27143 Time: 0.568101
[08/10/2023-11:18:37] [V] [TRT] Conv_163 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize256x128x32_stage3_warpsize4x2x1_g1_tensor16x8x8 Tactic: 0xe797e099911c0624
[08/10/2023-11:18:37] [V] [TRT] Tactic: 0xe797e099911c0624 Time: 0.555877
[08/10/2023-11:18:37] [V] [TRT] Conv_163 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage3_warpsize2x2x1_g1_tensor16x8x8_t1r3s3 Tactic: 0xeb65d4c1e8fa2294
[08/10/2023-11:18:37] [V] [TRT] Tactic: 0xeb65d4c1e8fa2294 Time: 0.404073
[08/10/2023-11:18:37] [V] [TRT] Conv_163 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x128x32_stage4_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0xf4156675c5f728d4
[08/10/2023-11:18:37] [V] [TRT] Tactic: 0xf4156675c5f728d4 Time: 0.832969
[08/10/2023-11:18:37] [V] [TRT] Fastest Tactic: 0xa9a06d0633580c0c Time: 0.337129
[08/10/2023-11:18:37] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0xa9a06d0633580c0c
[08/10/2023-11:18:37] [V] [TRT] *************** Autotuning format combination: Half(8294400,32400,180,1) -> Half(64800,32400,180,1) ***************
[08/10/2023-11:18:37] [W] [TRT] Weights [name=Conv_163.weight] had the following issues when converted to FP16:
[08/10/2023-11:18:37] [W] [TRT]  - Subnormal FP16 values detected. 
[08/10/2023-11:18:37] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.
[08/10/2023-11:18:37] [V] [TRT] --------------- Timing Runner: Conv_163 (CudnnConvolution)
[08/10/2023-11:18:37] [V] [TRT] Tactic: 0x0000000000000000 Time: 2.92358
[08/10/2023-11:18:37] [V] [TRT] Tactic: 0x0000000000000001 Time: 0.844521
[08/10/2023-11:18:37] [V] [TRT] Tactic: 0x0000000000000002 Time: 3.35185
[08/10/2023-11:18:37] [V] [TRT] Tactic: 0x0000000000000004 Time: 4.0115
[08/10/2023-11:18:37] [V] [TRT] Tactic: 0x0000000000000005 Time: 1.0743
[08/10/2023-11:18:37] [V] [TRT] Tactic: 0x0000000000000006 Time: 4.12417
[08/10/2023-11:18:37] [V] [TRT] Tactic: 0x0000000000000038 Time: 2.89833
[08/10/2023-11:18:37] [V] [TRT] Tactic: 0x000000000000003a Time: 3.36349
[08/10/2023-11:18:37] [V] [TRT] Tactic: 0x000000000000003c Time: 4.09241
[08/10/2023-11:18:37] [V] [TRT] Tactic: 0x000000000000003d Time: 1.0758
[08/10/2023-11:18:37] [V] [TRT] Tactic: 0x000000000000003e Time: 4.1765
[08/10/2023-11:18:37] [V] [TRT] Fastest Tactic: 0x0000000000000001 Time: 0.844521
[08/10/2023-11:18:37] [W] [TRT] Weights [name=Conv_163.weight] had the following issues when converted to FP16:
[08/10/2023-11:18:37] [W] [TRT]  - Subnormal FP16 values detected. 
[08/10/2023-11:18:37] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.
[08/10/2023-11:18:37] [V] [TRT] --------------- Timing Runner: Conv_163 (CaskConvolution)
[08/10/2023-11:18:37] [V] [TRT] CaskConvolution has no valid tactics for this config, skipping
[08/10/2023-11:18:37] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CudnnConvolution Tactic: 0x0000000000000001
[08/10/2023-11:18:37] [V] [TRT] *************** Autotuning format combination: Half(4147200,32400:2,180,1) -> Half(32400,32400:2,180,1) ***************
[08/10/2023-11:18:37] [W] [TRT] Weights [name=Conv_163.weight] had the following issues when converted to FP16:
[08/10/2023-11:18:37] [W] [TRT]  - Subnormal FP16 values detected. 
[08/10/2023-11:18:37] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.
[08/10/2023-11:18:37] [V] [TRT] --------------- Timing Runner: Conv_163 (FusedConvActConvolution)
[08/10/2023-11:18:37] [V] [TRT] FusedConvActConvolution has no valid tactics for this config, skipping
[08/10/2023-11:18:37] [W] [TRT] Weights [name=Conv_163.weight] had the following issues when converted to FP16:
[08/10/2023-11:18:37] [W] [TRT]  - Subnormal FP16 values detected. 
[08/10/2023-11:18:37] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.
[08/10/2023-11:18:37] [V] [TRT] --------------- Timing Runner: Conv_163 (CaskConvolution)
[08/10/2023-11:18:37] [V] [TRT] Conv_163 Set Tactic Name: ampere_fp16x2_hcudnn_fp16x2_128x32_relu_medium_nn_v1 Tactic: 0x1e7896ba71ef1635
[08/10/2023-11:18:37] [V] [TRT] Tactic: 0x1e7896ba71ef1635 Time: 0.461417
[08/10/2023-11:18:37] [V] [TRT] Conv_163 Set Tactic Name: ampere_fp16x2_hcudnn_fp16x2_128x64_relu_large_nn_v1 Tactic: 0x2f735ffbb05a30fd
[08/10/2023-11:18:37] [V] [TRT] Tactic: 0x2f735ffbb05a30fd Time: 0.834857
[08/10/2023-11:18:37] [V] [TRT] Conv_163 Set Tactic Name: ampere_fp16x2_hcudnn_fp16x2_128x128_relu_medium_nn_v1 Tactic: 0x360278e347d63410
[08/10/2023-11:18:37] [V] [TRT] Tactic: 0x360278e347d63410 Time: 1.69082
[08/10/2023-11:18:37] [V] [TRT] Conv_163 Set Tactic Name: ampere_fp16x2_hcudnn_fp16x2_128x64_relu_medium_nn_v1 Tactic: 0x4cfee77ea8c324db
[08/10/2023-11:18:37] [V] [TRT] Tactic: 0x4cfee77ea8c324db Time: 0.826683
[08/10/2023-11:18:37] [V] [TRT] Conv_163 Set Tactic Name: ampere_fp16x2_hcudnn_fp16x2_128x64_relu_small_nn_v1 Tactic: 0x540fde3a7bee53dc
[08/10/2023-11:18:37] [V] [TRT] Tactic: 0x540fde3a7bee53dc Time: 0.820562
[08/10/2023-11:18:37] [V] [TRT] Conv_163 Set Tactic Name: ampere_fp16x2_hcudnn_fp16x2_128x128_relu_large_nn_v1 Tactic: 0x91f7e9c0851ad67c
[08/10/2023-11:18:37] [V] [TRT] Tactic: 0x91f7e9c0851ad67c Time: 1.74416
[08/10/2023-11:18:37] [V] [TRT] Conv_163 Set Tactic Name: ampere_fp16x2_hcudnn_fp16x2_128x32_relu_large_nn_v1 Tactic: 0xb837f96ef306f686
[08/10/2023-11:18:37] [V] [TRT] Tactic: 0xb837f96ef306f686 Time: 0.460503
[08/10/2023-11:18:37] [V] [TRT] Conv_163 Set Tactic Name: ampere_fp16x2_hcudnn_fp16x2_128x32_relu_small_nn_v1 Tactic: 0xc34b78af38b295a7
[08/10/2023-11:18:37] [V] [TRT] Tactic: 0xc34b78af38b295a7 Time: 0.457088
[08/10/2023-11:18:37] [V] [TRT] Conv_163 Set Tactic Name: ampere_fp16x2_hcudnn_winograd_fp16x2_128x128_ldg1_ldg4_relu_tile148t_nt_v1 Tactic: 0xc754debea88ae0b7
[08/10/2023-11:18:37] [V] [TRT] Tactic: 0xc754debea88ae0b7 Time: 0.244649
[08/10/2023-11:18:37] [V] [TRT] Conv_163 Set Tactic Name: ampere_fp16x2_hcudnn_fp16x2_128x128_relu_small_nn_v1 Tactic: 0xea8b68014eaeb55d
[08/10/2023-11:18:37] [V] [TRT] Tactic: 0xea8b68014eaeb55d Time: 1.71086
[08/10/2023-11:18:37] [V] [TRT] Fastest Tactic: 0xc754debea88ae0b7 Time: 0.244649
[08/10/2023-11:18:37] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0xc754debea88ae0b7
[08/10/2023-11:18:37] [V] [TRT] *************** Autotuning format combination: Half(1036800,1:8,5760,32) -> Float(64800,32400,180,1) ***************
[08/10/2023-11:18:37] [W] [TRT] Weights [name=Conv_163.weight] had the following issues when converted to FP16:
[08/10/2023-11:18:37] [W] [TRT]  - Subnormal FP16 values detected. 
[08/10/2023-11:18:37] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.
[08/10/2023-11:18:37] [V] [TRT] --------------- Timing Runner: Conv_163 (CaskConvolution)
[08/10/2023-11:18:37] [V] [TRT] CaskConvolution has no valid tactics for this config, skipping
[08/10/2023-11:18:37] [V] [TRT] *************** Autotuning format combination: Half(1036800,1:8,5760,32) -> Half(32400,1:8,180,1) ***************
[08/10/2023-11:18:37] [W] [TRT] Weights [name=Conv_163.weight] had the following issues when converted to FP16:
[08/10/2023-11:18:37] [W] [TRT]  - Subnormal FP16 values detected. 
[08/10/2023-11:18:37] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.
[08/10/2023-11:18:37] [V] [TRT] --------------- Timing Runner: Conv_163 (CudaDepthwiseConvolution)
[08/10/2023-11:18:37] [V] [TRT] CudaDepthwiseConvolution has no valid tactics for this config, skipping
[08/10/2023-11:18:37] [W] [TRT] Weights [name=Conv_163.weight] had the following issues when converted to FP16:
[08/10/2023-11:18:37] [W] [TRT]  - Subnormal FP16 values detected. 
[08/10/2023-11:18:37] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.
[08/10/2023-11:18:37] [V] [TRT] --------------- Timing Runner: Conv_163 (CaskConvolution)
[08/10/2023-11:18:37] [V] [TRT] Conv_163 Set Tactic Name: ampere_h16816cudnn_256x128_ldg8_relu_exp_medium_nhwc_tn_v1 Tactic: 0x03896956a39a1203
[08/10/2023-11:18:37] [V] [TRT] Tactic: 0x03896956a39a1203 Time: 0.302688
[08/10/2023-11:18:37] [V] [TRT] Conv_163 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x128x32_stage3_warpsize4x2x1_g1_tensor16x8x16_t1r3s3 Tactic: 0x048d6d0400f33439
[08/10/2023-11:18:37] [V] [TRT] Tactic: 0x048d6d0400f33439 Time: 0.28043
[08/10/2023-11:18:37] [V] [TRT] Conv_163 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x64x64_stage3_warpsize2x2x1_g1_tensor16x8x16_t1r3s3 Tactic: 0x05b6220f243edacd
[08/10/2023-11:18:37] [V] [TRT] Tactic: 0x05b6220f243edacd Time: 0.154423
[08/10/2023-11:18:37] [V] [TRT] Conv_163 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x64x32_stage3_warpsize4x1x1_g1_tensor16x8x16_t1r3s3_aACCESS Tactic: 0x0866ddee325d07a6
[08/10/2023-11:18:37] [V] [TRT] Tactic: 0x0866ddee325d07a6 Time: 0.160558
[08/10/2023-11:18:37] [V] [TRT] Conv_163 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x128x32_stage4_warpsize2x2x1_g1_tensor16x8x16 Tactic: 0x0e07ff4f4f7c1ac9
[08/10/2023-11:18:37] [V] [TRT] Tactic: 0x0e07ff4f4f7c1ac9 Time: 0.273902
[08/10/2023-11:18:37] [V] [TRT] Conv_163 Set Tactic Name: ampere_h16816cudnn_256x128_ldg8_relu_exp_stages_64x3_small_nhwc_tn_v1 Tactic: 0x0e199750c30c6928
[08/10/2023-11:18:37] [V] [TRT] Tactic: 0x0e199750c30c6928 Time: 0.307456
[08/10/2023-11:18:37] [V] [TRT] Conv_163 Set Tactic Name: ampere_h1688cudnn_128x128_ldg8_relu_exp_medium_nhwc_tn_v1 Tactic: 0x100c1ad308e08d35
[08/10/2023-11:18:37] [V] [TRT] Tactic: 0x100c1ad308e08d35 Time: 0.354299
[08/10/2023-11:18:37] [V] [TRT] Conv_163 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x128x32_stage5_warpsize2x2x1_g1_tensor16x8x16_t1r3s3 Tactic: 0x17ebf0c9f418f10a
[08/10/2023-11:18:37] [V] [TRT] Tactic: 0x17ebf0c9f418f10a Time: 0.292814
[08/10/2023-11:18:37] [V] [TRT] Conv_163 Set Tactic Name: ampere_h16816cudnn_128x64_ldg8_relu_exp_stages_64x4_medium_nhwc_tn_v1 Tactic: 0x196cbc423a9bb69e
[08/10/2023-11:18:37] [V] [TRT] Tactic: 0x196cbc423a9bb69e Time: 0.250459
[08/10/2023-11:18:37] [V] [TRT] Conv_163 Set Tactic Name: ampere_h16816cudnn_128x64_sliced1x2_ldg8_relu_exp_stages_64x3_small_nhwc_tn_v1 Tactic: 0x19822de884f42a6a
[08/10/2023-11:18:37] [V] [TRT] Tactic: 0x19822de884f42a6a Time: 0.162656
[08/10/2023-11:18:37] [V] [TRT] Conv_163 Set Tactic Name: ampere_h16816cudnn_64x64_sliced1x2_ldg8_relu_exp_stages_64x5_small_nhwc_tn_v1 Tactic: 0x1a5ba808ad4cc5a8
[08/10/2023-11:18:37] [V] [TRT] Tactic: 0x1a5ba808ad4cc5a8 Time: 0.242638
[08/10/2023-11:18:37] [V] [TRT] Conv_163 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x64x64_stage3_warpsize2x2x1_g1_tensor16x8x16 Tactic: 0x21b295c0c8f6c95a
[08/10/2023-11:18:37] [V] [TRT] Tactic: 0x21b295c0c8f6c95a Time: 0.172
[08/10/2023-11:18:37] [V] [TRT] Conv_163 Set Tactic Name: ampere_h16816cudnn_128x128_ldg8_relu_exp_medium_nhwc_tn_v1 Tactic: 0x234580e8a194335c
[08/10/2023-11:18:37] [V] [TRT] Tactic: 0x234580e8a194335c Time: 0.253029
[08/10/2023-11:18:37] [V] [TRT] Conv_163 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x128x32_stage4_warpsize2x2x1_g1_tensor16x8x16_t1r3s3_aACCESS Tactic: 0x245530c34bd6090f
[08/10/2023-11:18:37] [V] [TRT] Tactic: 0x245530c34bd6090f Time: 0.257198
[08/10/2023-11:18:37] [V] [TRT] Conv_163 Set Tactic Name: ampere_h16816cudnn_64x64_sliced1x2_ldg8_relu_exp_stages_64x6_small_nhwc_tn_v1 Tactic: 0x24e01e7405cfdfe9
[08/10/2023-11:18:37] [V] [TRT] Tactic: 0x24e01e7405cfdfe9 Time: 0.360233
[08/10/2023-11:18:37] [V] [TRT] Conv_163 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x64x64_stage3_warpsize2x2x1_g1_tensor16x8x16_t1r3s3_aACCESS Tactic: 0x263a38afd75e3a43
[08/10/2023-11:18:37] [V] [TRT] Tactic: 0x263a38afd75e3a43 Time: 0.163835
[08/10/2023-11:18:37] [V] [TRT] Conv_163 Set Tactic Name: ampere_h16816cudnn_64x128_sliced1x2_ldg8_relu_exp_stages_64x4_large_nhwc_tn_v1 Tactic: 0x2721a7f18c2700db
[08/10/2023-11:18:37] [V] [TRT] Tactic: 0x2721a7f18c2700db Time: 0.487575
[08/10/2023-11:18:37] [V] [TRT] Conv_163 Set Tactic Name: ampere_h16816cudnn_128x64_sliced1x2_ldg8_relu_exp_stages_64x3_medium_nhwc_tn_v1 Tactic: 0x29329b5741ea05f2
[08/10/2023-11:18:37] [V] [TRT] Tactic: 0x29329b5741ea05f2 Time: 0.165394
[08/10/2023-11:18:37] [V] [TRT] Conv_163 Set Tactic Name: ampere_h16816cudnn_128x128_ldg8_relu_exp_large_nhwc_tn_v1 Tactic: 0x2970ae65966d569d
[08/10/2023-11:18:37] [V] [TRT] Tactic: 0x2970ae65966d569d Time: 0.249911
[08/10/2023-11:18:37] [V] [TRT] Conv_163 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x256x64_stage3_warpsize1x4x1_g1_tensor16x8x16 Tactic: 0x29dc14520e0e4eb6
[08/10/2023-11:18:37] [V] [TRT] Tactic: 0x29dc14520e0e4eb6 Time: 0.626011
[08/10/2023-11:18:37] [V] [TRT] Conv_163 Set Tactic Name: ampere_h16816cudnn_256x64_ldg8_relu_exp_stages_64x3_large_nhwc_tn_v1 Tactic: 0x2cb4a662694e89d3
[08/10/2023-11:18:37] [V] [TRT] Tactic: 0x2cb4a662694e89d3 Time: 0.196027
[08/10/2023-11:18:37] [V] [TRT] Conv_163 Set Tactic Name: ampere_h16816cudnn_128x128_ldg8_relu_exp_stages_64x3_medium_nhwc_tn_v1 Tactic: 0x2eaa2202de9404d6
[08/10/2023-11:18:37] [V] [TRT] Tactic: 0x2eaa2202de9404d6 Time: 0.342962
[08/10/2023-11:18:37] [V] [TRT] Conv_163 Set Tactic Name: ampere_h1688cudnn_256x128_ldg8_relu_exp_medium_nhwc_tn_v1 Tactic: 0x30c0f36d0aeeac6a
[08/10/2023-11:18:37] [V] [TRT] Tactic: 0x30c0f36d0aeeac6a Time: 0.361856
[08/10/2023-11:18:37] [V] [TRT] Conv_163 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x64x64_stage4_warpsize2x2x1_g1_tensor16x8x16_t1r3s3_aACCESS Tactic: 0x30e8a8d7a953e5e9
[08/10/2023-11:18:37] [V] [TRT] Tactic: 0x30e8a8d7a953e5e9 Time: 0.201093
[08/10/2023-11:18:37] [V] [TRT] Conv_163 Set Tactic Name: ampere_h16816cudnn_128x64_ldg8_relu_exp_stages_64x4_large_nhwc_tn_v1 Tactic: 0x3197d5044d71e98e
[08/10/2023-11:18:37] [V] [TRT] Tactic: 0x3197d5044d71e98e Time: 0.255506
[08/10/2023-11:18:37] [V] [TRT] Conv_163 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x64x64_stage4_warpsize2x2x1_g1_tensor16x8x16_t1r3s3 Tactic: 0x31d93dc22d2af081
[08/10/2023-11:18:37] [V] [TRT] Tactic: 0x31d93dc22d2af081 Time: 0.196073
[08/10/2023-11:18:37] [V] [TRT] Conv_163 Set Tactic Name: ampere_h1688cudnn_128x128_ldg8_relu_exp_small_nhwc_tn_v1 Tactic: 0x3369260c04f9ad73
[08/10/2023-11:18:37] [V] [TRT] Tactic: 0x3369260c04f9ad73 Time: 0.333381
[08/10/2023-11:18:37] [V] [TRT] Conv_163 Set Tactic Name: ampere_h16816cudnn_64x128_ldg8_relu_exp_stages_64x4_small_nhwc_tn_v1 Tactic: 0x3b5fa0f2a8fc2410
[08/10/2023-11:18:37] [V] [TRT] Tactic: 0x3b5fa0f2a8fc2410 Time: 0.43792
[08/10/2023-11:18:37] [V] [TRT] Conv_163 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x32x32_stage4_warpsize4x1x1_g1_tensor16x8x16_t1r3s3 Tactic: 0x3e7eb35b91b9fa63
[08/10/2023-11:18:37] [V] [TRT] Tactic: 0x3e7eb35b91b9fa63 Time: 0.114345
[08/10/2023-11:18:37] [V] [TRT] Conv_163 Set Tactic Name: ampere_h16816cudnn_256x64_ldg8_relu_exp_stages_64x3_medium_nhwc_tn_v1 Tactic: 0x3f031df0e579d52c
[08/10/2023-11:18:37] [V] [TRT] Tactic: 0x3f031df0e579d52c Time: 0.201129
[08/10/2023-11:18:37] [V] [TRT] Conv_163 Set Tactic Name: ampere_h16816cudnn_256x64_sliced1x2_ldg8_relu_exp_small_nhwc_tn_v1 Tactic: 0x42a5b8709d0039be
[08/10/2023-11:18:37] [V] [TRT] Tactic: 0x42a5b8709d0039be Time: 0.212951
[08/10/2023-11:18:37] [V] [TRT] Conv_163 Set Tactic Name: ampere_h16816cudnn_256x128_ldg8_relu_exp_small_nhwc_tn_v1 Tactic: 0x463794ee4acffd1f
[08/10/2023-11:18:37] [V] [TRT] Tactic: 0x463794ee4acffd1f Time: 0.305463
[08/10/2023-11:18:37] [V] [TRT] Conv_163 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x64x64_stage4_warpsize2x2x1_g1_tensor16x8x16 Tactic: 0x4a81ea1e51436a30
[08/10/2023-11:18:37] [V] [TRT] Tactic: 0x4a81ea1e51436a30 Time: 0.223136
[08/10/2023-11:18:37] [V] [TRT] Conv_163 Set Tactic Name: ampere_h1688cudnn_256x64_sliced1x2_ldg8_relu_exp_small_nhwc_tn_v1 Tactic: 0x4aed1956bd10a795
[08/10/2023-11:18:37] [V] [TRT] Tactic: 0x4aed1956bd10a795 Time: 0.235063
[08/10/2023-11:18:37] [V] [TRT] Conv_163 Set Tactic Name: ampere_h16816cudnn_64x128_sliced1x2_ldg8_relu_exp_stages_64x3_large_nhwc_tn_v1 Tactic: 0x4fc05923455fc266
[08/10/2023-11:18:38] [V] [TRT] Tactic: 0x4fc05923455fc266 Time: 0.296389
[08/10/2023-11:18:38] [V] [TRT] Conv_163 Set Tactic Name: ampere_h16816cudnn_256x64_ldg8_relu_exp_large_nhwc_tn_v1 Tactic: 0x5013c38f55afa9ba
[08/10/2023-11:18:38] [V] [TRT] Tactic: 0x5013c38f55afa9ba Time: 0.174382
[08/10/2023-11:18:38] [V] [TRT] Conv_163 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x64x32_stage3_warpsize4x1x1_g1_tensor16x8x16_t1r3s3 Tactic: 0x529f4431bdae94f5
[08/10/2023-11:18:38] [V] [TRT] Tactic: 0x529f4431bdae94f5 Time: 0.14195
[08/10/2023-11:18:38] [V] [TRT] Conv_163 Set Tactic Name: ampere_h16816cudnn_64x128_ldg8_relu_exp_stages_64x3_small_nhwc_tn_v1 Tactic: 0x53be5e206184e6ad
[08/10/2023-11:18:38] [V] [TRT] Tactic: 0x53be5e206184e6ad Time: 0.291703
[08/10/2023-11:18:38] [V] [TRT] Conv_163 Set Tactic Name: ampere_h16816cudnn_64x64_ldg8_relu_exp_stages_64x5_large_nhwc_tn_v1 Tactic: 0x544bff7ff9c5d908
[08/10/2023-11:18:38] [V] [TRT] Tactic: 0x544bff7ff9c5d908 Time: 0.257298
[08/10/2023-11:18:38] [V] [TRT] Conv_163 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x64x64_stage3_warpsize2x2x1_g1_tensor16x8x16 Tactic: 0x5568fd8a32f4a40f
[08/10/2023-11:18:38] [V] [TRT] Tactic: 0x5568fd8a32f4a40f Time: 0.156133
[08/10/2023-11:18:38] [V] [TRT] Conv_163 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x128x64_stage3_warpsize2x2x1_g1_tensor16x8x16 Tactic: 0x57c9a5ff682354a6
[08/10/2023-11:18:38] [V] [TRT] Tactic: 0x57c9a5ff682354a6 Time: 0.325312
[08/10/2023-11:18:38] [V] [TRT] Conv_163 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x32x64_stage5_warpsize2x2x1_g1_tensor16x8x16 Tactic: 0x5820b3dda403c4d0
[08/10/2023-11:18:38] [V] [TRT] Tactic: 0x5820b3dda403c4d0 Time: 0.179241
[08/10/2023-11:18:38] [V] [TRT] Conv_163 Set Tactic Name: ampere_h16816cudnn_128x128_ldg8_relu_exp_stages_64x3_small_nhwc_tn_v1 Tactic: 0x58816bf2e9c36afb
[08/10/2023-11:18:38] [V] [TRT] Tactic: 0x58816bf2e9c36afb Time: 0.341531
[08/10/2023-11:18:38] [V] [TRT] Conv_163 Set Tactic Name: ampere_h16816cudnn_128x64_ldg8_relu_exp_stages_64x3_large_nhwc_tn_v1 Tactic: 0x59762bd684092b33
[08/10/2023-11:18:38] [V] [TRT] Tactic: 0x59762bd684092b33 Time: 0.176594
[08/10/2023-11:18:38] [V] [TRT] Conv_163 Set Tactic Name: ampere_h16816cudnn_128x64_sliced1x2_ldg8_relu_exp_stages_64x4_medium_nhwc_tn_v1 Tactic: 0x59c5c647b4c76593
[08/10/2023-11:18:38] [V] [TRT] Tactic: 0x59c5c647b4c76593 Time: 0.254007
[08/10/2023-11:18:38] [V] [TRT] Conv_163 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x128x64_stage3_warpsize4x2x1_g1_tensor16x8x16 Tactic: 0x5a55274960724ba8
[08/10/2023-11:18:38] [V] [TRT] Tactic: 0x5a55274960724ba8 Time: 0.295776
[08/10/2023-11:18:38] [V] [TRT] Conv_163 Set Tactic Name: ampere_h1688cudnn_256x128_ldg8_relu_exp_large_nhwc_tn_v1 Tactic: 0x5c2e1c87d85b06f1
[08/10/2023-11:18:38] [V] [TRT] Tactic: 0x5c2e1c87d85b06f1 Time: 0.367067
[08/10/2023-11:18:38] [V] [TRT] Conv_163 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x256x64_stage3_warpsize1x4x1_g1_tensor16x8x16 Tactic: 0x5d067c18f40c23e3
[08/10/2023-11:18:38] [V] [TRT] Tactic: 0x5d067c18f40c23e3 Time: 0.62464
[08/10/2023-11:18:38] [V] [TRT] Conv_163 Set Tactic Name: ampere_h16816cudnn_64x64_ldg8_relu_exp_stages_64x5_medium_nhwc_tn_v1 Tactic: 0x5f31c22ec167f384
[08/10/2023-11:18:38] [V] [TRT] Tactic: 0x5f31c22ec167f384 Time: 0.25723
[08/10/2023-11:18:38] [V] [TRT] Conv_163 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x128x32_stage4_warpsize2x2x1_g1_tensor16x8x16_t1r3s3 Tactic: 0x60c3421152ef8e10
[08/10/2023-11:18:38] [V] [TRT] Tactic: 0x60c3421152ef8e10 Time: 0.247374
[08/10/2023-11:18:38] [V] [TRT] Conv_163 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x128x32_stage3_warpsize4x2x1_g1_tensor16x8x16 Tactic: 0x60da8c7151d91e47
[08/10/2023-11:18:38] [V] [TRT] Tactic: 0x60da8c7151d91e47 Time: 0.282551
[08/10/2023-11:18:38] [V] [TRT] Conv_163 Set Tactic Name: ampere_h1688cudnn_256x64_ldg8_relu_exp_medium_nhwc_tn_v1 Tactic: 0x6426696f872a3b13
[08/10/2023-11:18:38] [V] [TRT] Tactic: 0x6426696f872a3b13 Time: 0.224553
[08/10/2023-11:18:38] [V] [TRT] Conv_163 Set Tactic Name: ampere_h16816cudnn_128x64_ldg8_relu_exp_stages_64x3_medium_nhwc_tn_v1 Tactic: 0x699be152cfb6d6ff
[08/10/2023-11:18:38] [V] [TRT] Tactic: 0x699be152cfb6d6ff Time: 0.177536
[08/10/2023-11:18:38] [V] [TRT] Conv_163 Set Tactic Name: ampere_h16816cudnn_64x64_ldg8_relu_exp_stages_64x6_large_nhwc_tn_v1 Tactic: 0x6af049035146c349
[08/10/2023-11:18:38] [V] [TRT] Tactic: 0x6af049035146c349 Time: 0.379579
[08/10/2023-11:18:38] [V] [TRT] Conv_163 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x256x32_stage3_warpsize2x4x1_g1_tensor16x8x16 Tactic: 0x7005d10718f6c22d
[08/10/2023-11:18:38] [V] [TRT] Tactic: 0x7005d10718f6c22d Time: 0.522743
[08/10/2023-11:18:38] [V] [TRT] Conv_163 Set Tactic Name: ampere_h16816cudnn_64x64_sliced1x2_ldg8_relu_exp_stages_64x5_medium_nhwc_tn_v1 Tactic: 0x706f08da35c795a5
[08/10/2023-11:18:38] [V] [TRT] Tactic: 0x706f08da35c795a5 Time: 0.249358
[08/10/2023-11:18:38] [V] [TRT] Conv_163 Set Tactic Name: ampere_h16816cudnn_128x64_sliced1x2_ldg8_relu_exp_stages_64x4_small_nhwc_tn_v1 Tactic: 0x7163d33a4d8ce8d7
[08/10/2023-11:18:38] [V] [TRT] Tactic: 0x7163d33a4d8ce8d7 Time: 0.243131
[08/10/2023-11:18:38] [V] [TRT] Conv_163 Set Tactic Name: ampere_h1688cudnn_256x64_ldg8_relu_exp_small_nhwc_tn_v1 Tactic: 0x7aad3976677d7155
[08/10/2023-11:18:38] [V] [TRT] Tactic: 0x7aad3976677d7155 Time: 0.211323
[08/10/2023-11:18:38] [V] [TRT] Conv_163 Set Tactic Name: ampere_h16816cudnn_64x64_sliced1x2_ldg8_relu_exp_stages_64x5_large_nhwc_tn_v1 Tactic: 0x8015519605ab9963
[08/10/2023-11:18:38] [V] [TRT] Tactic: 0x8015519605ab9963 Time: 0.257179
[08/10/2023-11:18:38] [V] [TRT] Conv_163 Set Tactic Name: ampere_h16816cudnn_256x64_sliced1x2_ldg8_relu_exp_medium_nhwc_tn_v1 Tactic: 0x82f8a2214b0b4178
[08/10/2023-11:18:38] [V] [TRT] Tactic: 0x82f8a2214b0b4178 Time: 0.235808
[08/10/2023-11:18:38] [V] [TRT] Conv_163 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x32x32_stage4_warpsize4x1x1_g1_tensor16x8x16 Tactic: 0x834e11ecd4ab9454
[08/10/2023-11:18:38] [V] [TRT] Tactic: 0x834e11ecd4ab9454 Time: 0.117913
[08/10/2023-11:18:38] [V] [TRT] Conv_163 Set Tactic Name: ampere_h16816cudnn_128x64_sliced1x2_ldg8_relu_exp_stages_64x3_large_nhwc_tn_v1 Tactic: 0x83ccd4762c1376a1
[08/10/2023-11:18:38] [V] [TRT] Tactic: 0x83ccd4762c1376a1 Time: 0.171995
[08/10/2023-11:18:38] [V] [TRT] Conv_163 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x64x64_stage3_warpsize4x1x1_g1_tensor16x8x16_t1r3s3 Tactic: 0x84ee897dd1f4d2aa
[08/10/2023-11:18:38] [V] [TRT] Tactic: 0x84ee897dd1f4d2aa Time: 0.186363
[08/10/2023-11:18:38] [V] [TRT] Conv_163 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x128x64_stage3_warpsize2x2x1_g1_tensor16x8x16 Tactic: 0x866e7a5f6401b67f
[08/10/2023-11:18:38] [V] [TRT] Tactic: 0x866e7a5f6401b67f Time: 0.33019
[08/10/2023-11:18:38] [V] [TRT] Conv_163 Set Tactic Name: ampere_h16816cudnn_64x64_ldg8_relu_exp_stages_64x6_medium_nhwc_tn_v1 Tactic: 0x88971eec55aba850
[08/10/2023-11:18:38] [V] [TRT] Tactic: 0x88971eec55aba850 Time: 0.374089
[08/10/2023-11:18:38] [V] [TRT] Conv_163 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x128x64_stage3_warpsize4x2x1_g1_tensor16x8x16 Tactic: 0x8bf2f8e96c50a971
[08/10/2023-11:18:38] [V] [TRT] Tactic: 0x8bf2f8e96c50a971 Time: 0.289175
[08/10/2023-11:18:38] [V] [TRT] Conv_163 Set Tactic Name: ampere_h16816cudnn_256x128_ldg8_relu_exp_stages_64x3_large_nhwc_tn_v1 Tactic: 0x94576ece6beb35e3
[08/10/2023-11:18:38] [V] [TRT] Tactic: 0x94576ece6beb35e3 Time: 0.303945
[08/10/2023-11:18:38] [V] [TRT] Conv_163 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x64x64_stage3_warpsize4x1x1_g1_tensor16x8x16 Tactic: 0x9fff6e65019cc310
[08/10/2023-11:18:38] [V] [TRT] Tactic: 0x9fff6e65019cc310 Time: 0.183314
[08/10/2023-11:18:38] [V] [TRT] Conv_163 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x32x64_stage5_warpsize2x2x1_g1_tensor16x8x16_t1r3s3 Tactic: 0xa033e20ae9f412b2
[08/10/2023-11:18:38] [V] [TRT] Tactic: 0xa033e20ae9f412b2 Time: 0.16475
[08/10/2023-11:18:38] [V] [TRT] Conv_163 Set Tactic Name: ampere_h16816cudnn_64x128_ldg8_relu_exp_stages_64x4_large_nhwc_tn_v1 Tactic: 0xa111596c001b78db
[08/10/2023-11:18:38] [V] [TRT] Tactic: 0xa111596c001b78db Time: 0.441339
[08/10/2023-11:18:38] [V] [TRT] Conv_163 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x256x32_stage3_warpsize2x4x1_g1_tensor16x8x16 Tactic: 0xa1a20ea714d420f4
[08/10/2023-11:18:38] [V] [TRT] Tactic: 0xa1a20ea714d420f4 Time: 0.517449
[08/10/2023-11:18:38] [V] [TRT] Conv_163 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x32x32_stage4_warpsize4x1x1_g1_tensor16x8x16_t1r3s3 Tactic: 0xa40cb43c296a36a8
[08/10/2023-11:18:38] [V] [TRT] Tactic: 0xa40cb43c296a36a8 Time: 0.116181
[08/10/2023-11:18:38] [V] [TRT] Conv_163 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x64x32_stage3_warpsize4x1x1_g1_tensor16x8x16 Tactic: 0xa570c55d303796ff
[08/10/2023-11:18:38] [V] [TRT] Tactic: 0xa570c55d303796ff Time: 0.153998
[08/10/2023-11:18:38] [V] [TRT] Conv_163 Set Tactic Name: ampere_h16816cudnn_64x64_sliced1x2_ldg8_relu_exp_stages_64x6_medium_nhwc_tn_v1 Tactic: 0xa7c9d418a10bce71
[08/10/2023-11:18:38] [V] [TRT] Tactic: 0xa7c9d418a10bce71 Time: 0.384832
[08/10/2023-11:18:38] [V] [TRT] Conv_163 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x32x32_stage4_warpsize4x1x1_g1_tensor16x8x16 Tactic: 0xa83b68f30462f971
[08/10/2023-11:18:38] [V] [TRT] Tactic: 0xa83b68f30462f971 Time: 0.124923
[08/10/2023-11:18:38] [V] [TRT] Conv_163 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x256x32_stage3_warpsize2x4x1_g1_tensor16x8x16_t1r3s3 Tactic: 0xa9177bbe4e767df8
[08/10/2023-11:18:38] [V] [TRT] Tactic: 0xa9177bbe4e767df8 Time: 0.51893
[08/10/2023-11:18:38] [V] [TRT] Conv_163 Set Tactic Name: ampere_h1688cudnn_128x128_ldg8_relu_exp_large_nhwc_tn_v1 Tactic: 0xa927df92ac1ef1b8
[08/10/2023-11:18:38] [V] [TRT] Tactic: 0xa927df92ac1ef1b8 Time: 0.347195
[08/10/2023-11:18:38] [V] [TRT] Conv_163 Set Tactic Name: ampere_h16816cudnn_128x64_ldg8_relu_exp_stages_64x4_small_nhwc_tn_v1 Tactic: 0xabd92c9ae596b545
[08/10/2023-11:18:38] [V] [TRT] Tactic: 0xabd92c9ae596b545 Time: 0.243168
[08/10/2023-11:18:38] [V] [TRT] Conv_163 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x128x32_stage5_warpsize2x2x1_g1_tensor16x8x16 Tactic: 0xafd1e8bf6bd3d638
[08/10/2023-11:18:38] [V] [TRT] Tactic: 0xafd1e8bf6bd3d638 Time: 0.304466
[08/10/2023-11:18:38] [V] [TRT] Conv_163 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x128x32_stage3_warpsize4x2x1_g1_tensor16x8x16 Tactic: 0xb17d53d15dfbfc9e
[08/10/2023-11:18:38] [V] [TRT] Tactic: 0xb17d53d15dfbfc9e Time: 0.286048
[08/10/2023-11:18:38] [V] [TRT] Conv_163 Set Tactic Name: ampere_h16816cudnn_128x128_ldg8_relu_exp_small_nhwc_tn_v1 Tactic: 0xb33e57fb3e8a0a56
[08/10/2023-11:18:38] [V] [TRT] Tactic: 0xb33e57fb3e8a0a56 Time: 0.248073
[08/10/2023-11:18:38] [V] [TRT] Conv_163 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x64x32_stage5_warpsize2x2x1_g1_tensor16x8x16_t1r3s3 Tactic: 0xb4bec086187edcfc
[08/10/2023-11:18:38] [V] [TRT] Tactic: 0xb4bec086187edcfc Time: 0.163941
[08/10/2023-11:18:38] [V] [TRT] Conv_163 Set Tactic Name: ampere_h16816cudnn_256x64_ldg8_relu_exp_stages_64x3_small_nhwc_tn_v1 Tactic: 0xb6fa5ffcc1a9d518
[08/10/2023-11:18:38] [V] [TRT] Tactic: 0xb6fa5ffcc1a9d518 Time: 0.195758
[08/10/2023-11:18:38] [V] [TRT] Conv_163 Set Tactic Name: ampere_h16816cudnn_64x128_sliced1x2_ldg8_relu_exp_stages_64x3_medium_nhwc_tn_v1 Tactic: 0xb85e52e87caf60a2
[08/10/2023-11:18:38] [V] [TRT] Tactic: 0xb85e52e87caf60a2 Time: 0.29909
[08/10/2023-11:18:38] [V] [TRT] Conv_163 Set Tactic Name: ampere_h16816cudnn_64x128_ldg8_relu_exp_stages_64x3_medium_nhwc_tn_v1 Tactic: 0xb8d86216e1235cda
[08/10/2023-11:18:38] [V] [TRT] Tactic: 0xb8d86216e1235cda Time: 0.297819
[08/10/2023-11:18:38] [V] [TRT] Conv_163 Set Tactic Name: ampere_h16816cudnn_256x128_ldg8_relu_exp_stages_64x3_medium_nhwc_tn_v1 Tactic: 0xb9171d70ba2eda4b
[08/10/2023-11:18:38] [V] [TRT] Tactic: 0xb9171d70ba2eda4b Time: 0.310976
[08/10/2023-11:18:38] [V] [TRT] Conv_163 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x64x32_stage5_warpsize2x2x1_g1_tensor16x8x16 Tactic: 0xbd08239a9317f2fd
[08/10/2023-11:18:38] [V] [TRT] Tactic: 0xbd08239a9317f2fd Time: 0.169367
[08/10/2023-11:18:38] [V] [TRT] Conv_163 Set Tactic Name: ampere_h16816cudnn_64x128_sliced1x2_ldg8_relu_exp_stages_64x4_small_nhwc_tn_v1 Tactic: 0xbd6f5e6f24c05c10
[08/10/2023-11:18:38] [V] [TRT] Tactic: 0xbd6f5e6f24c05c10 Time: 0.476768
[08/10/2023-11:18:38] [V] [TRT] Conv_163 Set Tactic Name: ampere_h16816cudnn_64x64_sliced1x2_ldg8_relu_exp_stages_64x6_large_nhwc_tn_v1 Tactic: 0xbeaee7eaad288322
[08/10/2023-11:18:38] [V] [TRT] Tactic: 0xbeaee7eaad288322 Time: 0.394725
[08/10/2023-11:18:38] [V] [TRT] Conv_163 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x128x64_stage3_warpsize2x2x1_g1_tensor16x8x16_t1r3s3 Tactic: 0xc0a02dc6095497cc
[08/10/2023-11:18:38] [V] [TRT] Tactic: 0xc0a02dc6095497cc Time: 0.31909
[08/10/2023-11:18:38] [V] [TRT] Conv_163 Set Tactic Name: ampere_h16816cudnn_128x128_ldg8_relu_exp_stages_64x3_large_nhwc_tn_v1 Tactic: 0xc2cf926c41243630
[08/10/2023-11:18:38] [V] [TRT] Tactic: 0xc2cf926c41243630 Time: 0.345019
[08/10/2023-11:18:38] [V] [TRT] Conv_163 Set Tactic Name: ampere_h16816cudnn_128x64_ldg8_relu_exp_stages_64x3_small_nhwc_tn_v1 Tactic: 0xc338d2482cee77f8
[08/10/2023-11:18:38] [V] [TRT] Tactic: 0xc338d2482cee77f8 Time: 0.174423
[08/10/2023-11:18:38] [V] [TRT] Conv_163 Set Tactic Name: ampere_h1688cudnn_256x128_ldg8_relu_exp_small_nhwc_tn_v1 Tactic: 0xc660e51970bc5a3a
[08/10/2023-11:18:38] [V] [TRT] Tactic: 0xc660e51970bc5a3a Time: 0.341518
[08/10/2023-11:18:38] [V] [TRT] Conv_163 Set Tactic Name: ampere_h16816cudnn_64x128_ldg8_relu_exp_stages_64x4_medium_nhwc_tn_v1 Tactic: 0xc82f3f06140e3cbb
[08/10/2023-11:18:38] [V] [TRT] Tactic: 0xc82f3f06140e3cbb Time: 0.437157
[08/10/2023-11:18:38] [V] [TRT] Conv_163 Set Tactic Name: ampere_h16816cudnn_64x128_sliced1x2_ldg8_relu_exp_stages_64x4_medium_nhwc_tn_v1 Tactic: 0xc8a90ff8898200c3
[08/10/2023-11:18:38] [V] [TRT] Tactic: 0xc8a90ff8898200c3 Time: 0.471511
[08/10/2023-11:18:38] [V] [TRT] Conv_163 Set Tactic Name: ampere_h1688cudnn_256x64_sliced1x2_ldg8_relu_exp_medium_nhwc_tn_v1 Tactic: 0xc9cc55109bb4de26
[08/10/2023-11:18:38] [V] [TRT] Tactic: 0xc9cc55109bb4de26 Time: 0.252265
[08/10/2023-11:18:38] [V] [TRT] Conv_163 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x64x32_stage5_warpsize2x2x1_g1_tensor16x8x16 Tactic: 0xc9d24bd069159fa8
[08/10/2023-11:18:38] [V] [TRT] Tactic: 0xc9d24bd069159fa8 Time: 0.181248
[08/10/2023-11:18:38] [V] [TRT] Conv_163 Set Tactic Name: ampere_h16816cudnn_64x128_ldg8_relu_exp_stages_64x3_large_nhwc_tn_v1 Tactic: 0xc9f0a7bec963ba66
[08/10/2023-11:18:38] [V] [TRT] Tactic: 0xc9f0a7bec963ba66 Time: 0.292178
[08/10/2023-11:18:38] [V] [TRT] Conv_163 Set Tactic Name: ampere_h16816cudnn_256x64_ldg8_relu_exp_small_nhwc_tn_v1 Tactic: 0xca5d3a11fd48f571
[08/10/2023-11:18:38] [V] [TRT] Tactic: 0xca5d3a11fd48f571 Time: 0.164933
[08/10/2023-11:18:38] [V] [TRT] Conv_163 Set Tactic Name: ampere_h16816cudnn_64x64_ldg8_relu_exp_stages_64x5_small_nhwc_tn_v1 Tactic: 0xce0506e1512285c3
[08/10/2023-11:18:38] [V] [TRT] Tactic: 0xce0506e1512285c3 Time: 0.259008
[08/10/2023-11:18:38] [V] [TRT] Conv_163 Set Tactic Name: ampere_h1688cudnn_256x64_sliced1x2_ldg8_relu_exp_large_nhwc_tn_v1 Tactic: 0xd0a3e0c815f7fb5e
[08/10/2023-11:18:38] [V] [TRT] Tactic: 0xd0a3e0c815f7fb5e Time: 0.254277
[08/10/2023-11:18:38] [V] [TRT] Conv_163 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x64x32_stage3_warpsize4x1x1_g1_tensor16x8x16 Tactic: 0xd1aaad17ca35fbaa
[08/10/2023-11:18:38] [V] [TRT] Tactic: 0xd1aaad17ca35fbaa Time: 0.142926
[08/10/2023-11:18:38] [V] [TRT] Conv_163 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x128x64_stage3_warpsize4x2x1_g1_tensor16x8x16_t1r3s3 Tactic: 0xd2fca0486ca97266
[08/10/2023-11:18:38] [V] [TRT] Tactic: 0xd2fca0486ca97266 Time: 0.292087
[08/10/2023-11:18:38] [V] [TRT] Conv_163 Set Tactic Name: ampere_h16816cudnn_64x128_sliced1x2_ldg8_relu_exp_stages_64x3_small_nhwc_tn_v1 Tactic: 0xd58ea0bdedb89ead
[08/10/2023-11:18:38] [V] [TRT] Tactic: 0xd58ea0bdedb89ead Time: 0.289033
[08/10/2023-11:18:38] [V] [TRT] Conv_163 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x256x64_stage3_warpsize1x4x1_g1_tensor16x8x16_t1r3s3 Tactic: 0xd7c261fa01db2af9
[08/10/2023-11:18:38] [V] [TRT] Tactic: 0xd7c261fa01db2af9 Time: 0.610254
[08/10/2023-11:18:38] [V] [TRT] Conv_163 Set Tactic Name: ampere_h16816cudnn_256x64_sliced1x2_ldg8_relu_exp_large_nhwc_tn_v1 Tactic: 0xd8eb41ee35e76575
[08/10/2023-11:18:38] [V] [TRT] Tactic: 0xd8eb41ee35e76575 Time: 0.231232
[08/10/2023-11:18:38] [V] [TRT] Conv_163 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x128x32_stage5_warpsize2x2x1_g1_tensor16x8x16 Tactic: 0xdb0b80f591d1bb6d
[08/10/2023-11:18:38] [V] [TRT] Tactic: 0xdb0b80f591d1bb6d Time: 0.292571
[08/10/2023-11:18:38] [V] [TRT] Conv_163 Set Tactic Name: ampere_h16816cudnn_256x128_ldg8_relu_exp_large_nhwc_tn_v1 Tactic: 0xdc796d70e228a1d4
[08/10/2023-11:18:38] [V] [TRT] Tactic: 0xdc796d70e228a1d4 Time: 0.302395
[08/10/2023-11:18:38] [V] [TRT] Conv_163 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x32x32_stage4_warpsize4x1x1_g1_tensor16x8x16 Tactic: 0xdce100b9fe609424
[08/10/2023-11:18:38] [V] [TRT] Tactic: 0xdce100b9fe609424 Time: 0.128921
[08/10/2023-11:18:38] [V] [TRT] Conv_163 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x128x32_stage4_warpsize2x2x1_g1_tensor16x8x16 Tactic: 0xdfa020ef435ef810
[08/10/2023-11:18:38] [V] [TRT] Tactic: 0xdfa020ef435ef810 Time: 0.248361
[08/10/2023-11:18:38] [V] [TRT] Conv_163 Set Tactic Name: ampere_h1688cudnn_256x64_ldg8_relu_exp_large_nhwc_tn_v1 Tactic: 0xe0e3c0e8cf9a2d9e
[08/10/2023-11:18:38] [V] [TRT] Tactic: 0xe0e3c0e8cf9a2d9e Time: 0.222066
[08/10/2023-11:18:38] [V] [TRT] Conv_163 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x32x64_stage5_warpsize2x2x1_g1_tensor16x8x16 Tactic: 0xe1ff5ad20f5c6bf6
[08/10/2023-11:18:38] [V] [TRT] Tactic: 0xe1ff5ad20f5c6bf6 Time: 0.183154
[08/10/2023-11:18:38] [V] [TRT] Conv_163 Set Tactic Name: ampere_h16816cudnn_256x64_ldg8_relu_exp_medium_nhwc_tn_v1 Tactic: 0xe4711898bd599c36
[08/10/2023-11:18:38] [V] [TRT] Tactic: 0xe4711898bd599c36 Time: 0.175593
[08/10/2023-11:18:38] [V] [TRT] Conv_163 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x64x64_stage3_warpsize4x1x1_g1_tensor16x8x16 Tactic: 0xeb25062ffb9eae45
[08/10/2023-11:18:38] [V] [TRT] Tactic: 0xeb25062ffb9eae45 Time: 0.191351
[08/10/2023-11:18:38] [V] [TRT] Conv_163 Set Tactic Name: ampere_h16816cudnn_128x64_sliced1x2_ldg8_relu_exp_stages_64x4_large_nhwc_tn_v1 Tactic: 0xeb2d2aa4e56bb41c
[08/10/2023-11:18:38] [V] [TRT] Tactic: 0xeb2d2aa4e56bb41c Time: 0.267273
[08/10/2023-11:18:38] [V] [TRT] Conv_163 Set Tactic Name: ampere_h16816cudnn_64x64_ldg8_relu_exp_stages_64x6_small_nhwc_tn_v1 Tactic: 0xf0beb09df9a19f82
[08/10/2023-11:18:38] [V] [TRT] Tactic: 0xf0beb09df9a19f82 Time: 0.358423
[08/10/2023-11:18:38] [V] [TRT] Conv_163 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x64x64_stage4_warpsize2x2x1_g1_tensor16x8x16 Tactic: 0xf35e0311fa1cc516
[08/10/2023-11:18:38] [V] [TRT] Tactic: 0xf35e0311fa1cc516 Time: 0.21243
[08/10/2023-11:18:38] [V] [TRT] Conv_163 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x32x32_stage4_warpsize4x1x1_g1_tensor16x8x16 Tactic: 0xf79479a62ea9f901
[08/10/2023-11:18:38] [V] [TRT] Tactic: 0xf79479a62ea9f901 Time: 0.127792
[08/10/2023-11:18:38] [V] [TRT] Fastest Tactic: 0x3e7eb35b91b9fa63 Time: 0.114345
[08/10/2023-11:18:38] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x3e7eb35b91b9fa63
[08/10/2023-11:18:38] [V] [TRT] *************** Autotuning format combination: Half(518400,1:16,2880,16) -> Half(32400,1:16,180,1) ***************
[08/10/2023-11:18:38] [W] [TRT] Weights [name=Conv_163.weight] had the following issues when converted to FP16:
[08/10/2023-11:18:38] [W] [TRT]  - Subnormal FP16 values detected. 
[08/10/2023-11:18:38] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.
[08/10/2023-11:18:38] [V] [TRT] --------------- Timing Runner: Conv_163 (CaskConvolution)
[08/10/2023-11:18:38] [V] [TRT] Conv_163 Set Tactic Name: ampere_h16816cudnn_256x128_ldg8_relu_exp_medium_nhwc_tn_v1 Tactic: 0x03896956a39a1203
[08/10/2023-11:18:38] [V] [TRT] Tactic: 0x03896956a39a1203 Time: 0.302295
[08/10/2023-11:18:38] [V] [TRT] Conv_163 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x128x32_stage3_warpsize4x2x1_g1_tensor16x8x16_t1r3s3 Tactic: 0x048d6d0400f33439
[08/10/2023-11:18:38] [V] [TRT] Tactic: 0x048d6d0400f33439 Time: 0.284315
[08/10/2023-11:18:38] [V] [TRT] Conv_163 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x64x64_stage3_warpsize2x2x1_g1_tensor16x8x16_t1r3s3 Tactic: 0x05b6220f243edacd
[08/10/2023-11:18:38] [V] [TRT] Tactic: 0x05b6220f243edacd Time: 0.151081
[08/10/2023-11:18:38] [V] [TRT] Conv_163 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x64x32_stage3_warpsize4x1x1_g1_tensor16x8x16_t1r3s3_aACCESS Tactic: 0x0866ddee325d07a6
[08/10/2023-11:18:38] [V] [TRT] Tactic: 0x0866ddee325d07a6 Time: 0.161125
[08/10/2023-11:18:38] [V] [TRT] Conv_163 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x128x32_stage4_warpsize2x2x1_g1_tensor16x8x16 Tactic: 0x0e07ff4f4f7c1ac9
[08/10/2023-11:18:38] [V] [TRT] Tactic: 0x0e07ff4f4f7c1ac9 Time: 0.272165
[08/10/2023-11:18:38] [V] [TRT] Conv_163 Set Tactic Name: ampere_h16816cudnn_256x128_ldg8_relu_exp_stages_64x3_small_nhwc_tn_v1 Tactic: 0x0e199750c30c6928
[08/10/2023-11:18:38] [V] [TRT] Tactic: 0x0e199750c30c6928 Time: 0.301691
[08/10/2023-11:18:38] [V] [TRT] Conv_163 Set Tactic Name: ampere_h1688cudnn_128x128_ldg8_relu_exp_medium_nhwc_tn_v1 Tactic: 0x100c1ad308e08d35
[08/10/2023-11:18:38] [V] [TRT] Tactic: 0x100c1ad308e08d35 Time: 0.348635
[08/10/2023-11:18:38] [V] [TRT] Conv_163 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x128x32_stage5_warpsize2x2x1_g1_tensor16x8x16_t1r3s3 Tactic: 0x17ebf0c9f418f10a
[08/10/2023-11:18:38] [V] [TRT] Tactic: 0x17ebf0c9f418f10a Time: 0.285513
[08/10/2023-11:18:38] [V] [TRT] Conv_163 Set Tactic Name: ampere_h16816cudnn_128x64_ldg8_relu_exp_stages_64x4_medium_nhwc_tn_v1 Tactic: 0x196cbc423a9bb69e
[08/10/2023-11:18:38] [V] [TRT] Tactic: 0x196cbc423a9bb69e Time: 0.248247
[08/10/2023-11:18:38] [V] [TRT] Conv_163 Set Tactic Name: ampere_h16816cudnn_128x64_sliced1x2_ldg8_relu_exp_stages_64x3_small_nhwc_tn_v1 Tactic: 0x19822de884f42a6a
[08/10/2023-11:18:38] [V] [TRT] Tactic: 0x19822de884f42a6a Time: 0.164814
[08/10/2023-11:18:38] [V] [TRT] Conv_163 Set Tactic Name: ampere_h16816cudnn_64x64_sliced1x2_ldg8_relu_exp_stages_64x5_small_nhwc_tn_v1 Tactic: 0x1a5ba808ad4cc5a8
[08/10/2023-11:18:38] [V] [TRT] Tactic: 0x1a5ba808ad4cc5a8 Time: 0.248869
[08/10/2023-11:18:38] [V] [TRT] Conv_163 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x64x64_stage3_warpsize2x2x1_g1_tensor16x8x16 Tactic: 0x21b295c0c8f6c95a
[08/10/2023-11:18:38] [V] [TRT] Tactic: 0x21b295c0c8f6c95a Time: 0.17157
[08/10/2023-11:18:38] [V] [TRT] Conv_163 Set Tactic Name: ampere_h16816cudnn_128x128_ldg8_relu_exp_medium_nhwc_tn_v1 Tactic: 0x234580e8a194335c
[08/10/2023-11:18:38] [V] [TRT] Tactic: 0x234580e8a194335c Time: 0.251721
[08/10/2023-11:18:38] [V] [TRT] Conv_163 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x128x32_stage4_warpsize2x2x1_g1_tensor16x8x16_t1r3s3_aACCESS Tactic: 0x245530c34bd6090f
[08/10/2023-11:18:38] [V] [TRT] Tactic: 0x245530c34bd6090f Time: 0.257262
[08/10/2023-11:18:38] [V] [TRT] Conv_163 Set Tactic Name: ampere_h16816cudnn_64x64_sliced1x2_ldg8_relu_exp_stages_64x6_small_nhwc_tn_v1 Tactic: 0x24e01e7405cfdfe9
[08/10/2023-11:18:38] [V] [TRT] Tactic: 0x24e01e7405cfdfe9 Time: 0.360311
[08/10/2023-11:18:38] [V] [TRT] Conv_163 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x64x64_stage3_warpsize2x2x1_g1_tensor16x8x16_t1r3s3_aACCESS Tactic: 0x263a38afd75e3a43
[08/10/2023-11:18:38] [V] [TRT] Tactic: 0x263a38afd75e3a43 Time: 0.164782
[08/10/2023-11:18:38] [V] [TRT] Conv_163 Set Tactic Name: ampere_h16816cudnn_64x128_sliced1x2_ldg8_relu_exp_stages_64x4_large_nhwc_tn_v1 Tactic: 0x2721a7f18c2700db
[08/10/2023-11:18:38] [V] [TRT] Tactic: 0x2721a7f18c2700db Time: 0.488814
[08/10/2023-11:18:38] [V] [TRT] Conv_163 Set Tactic Name: ampere_h16816cudnn_128x64_sliced1x2_ldg8_relu_exp_stages_64x3_medium_nhwc_tn_v1 Tactic: 0x29329b5741ea05f2
[08/10/2023-11:18:38] [V] [TRT] Tactic: 0x29329b5741ea05f2 Time: 0.172695
[08/10/2023-11:18:38] [V] [TRT] Conv_163 Set Tactic Name: ampere_h16816cudnn_128x128_ldg8_relu_exp_large_nhwc_tn_v1 Tactic: 0x2970ae65966d569d
[08/10/2023-11:18:38] [V] [TRT] Tactic: 0x2970ae65966d569d Time: 0.258171
[08/10/2023-11:18:38] [V] [TRT] Conv_163 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x256x64_stage3_warpsize1x4x1_g1_tensor16x8x16 Tactic: 0x29dc14520e0e4eb6
[08/10/2023-11:18:38] [V] [TRT] Tactic: 0x29dc14520e0e4eb6 Time: 0.6384
[08/10/2023-11:18:38] [V] [TRT] Conv_163 Set Tactic Name: ampere_h16816cudnn_256x64_ldg8_relu_exp_stages_64x3_large_nhwc_tn_v1 Tactic: 0x2cb4a662694e89d3
[08/10/2023-11:18:38] [V] [TRT] Tactic: 0x2cb4a662694e89d3 Time: 0.20053
[08/10/2023-11:18:38] [V] [TRT] Conv_163 Set Tactic Name: ampere_h16816cudnn_128x128_ldg8_relu_exp_stages_64x3_medium_nhwc_tn_v1 Tactic: 0x2eaa2202de9404d6
[08/10/2023-11:18:38] [V] [TRT] Tactic: 0x2eaa2202de9404d6 Time: 0.351607
[08/10/2023-11:18:38] [V] [TRT] Conv_163 Set Tactic Name: ampere_h1688cudnn_256x128_ldg8_relu_exp_medium_nhwc_tn_v1 Tactic: 0x30c0f36d0aeeac6a
[08/10/2023-11:18:38] [V] [TRT] Tactic: 0x30c0f36d0aeeac6a Time: 0.372827
[08/10/2023-11:18:38] [V] [TRT] Conv_163 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x64x64_stage4_warpsize2x2x1_g1_tensor16x8x16_t1r3s3_aACCESS Tactic: 0x30e8a8d7a953e5e9
[08/10/2023-11:18:38] [V] [TRT] Tactic: 0x30e8a8d7a953e5e9 Time: 0.200018
[08/10/2023-11:18:38] [V] [TRT] Conv_163 Set Tactic Name: ampere_h16816cudnn_128x64_ldg8_relu_exp_stages_64x4_large_nhwc_tn_v1 Tactic: 0x3197d5044d71e98e
[08/10/2023-11:18:38] [V] [TRT] Tactic: 0x3197d5044d71e98e Time: 0.257367
[08/10/2023-11:18:38] [V] [TRT] Conv_163 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x64x64_stage4_warpsize2x2x1_g1_tensor16x8x16_t1r3s3 Tactic: 0x31d93dc22d2af081
[08/10/2023-11:18:38] [V] [TRT] Tactic: 0x31d93dc22d2af081 Time: 0.195191
[08/10/2023-11:18:38] [V] [TRT] Conv_163 Set Tactic Name: ampere_h1688cudnn_128x128_ldg8_relu_exp_small_nhwc_tn_v1 Tactic: 0x3369260c04f9ad73
[08/10/2023-11:18:38] [V] [TRT] Tactic: 0x3369260c04f9ad73 Time: 0.333568
[08/10/2023-11:18:38] [V] [TRT] Conv_163 Set Tactic Name: ampere_h16816cudnn_64x128_ldg8_relu_exp_stages_64x4_small_nhwc_tn_v1 Tactic: 0x3b5fa0f2a8fc2410
[08/10/2023-11:18:38] [V] [TRT] Tactic: 0x3b5fa0f2a8fc2410 Time: 0.42928
[08/10/2023-11:18:38] [V] [TRT] Conv_163 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x32x32_stage4_warpsize4x1x1_g1_tensor16x8x16_t1r3s3 Tactic: 0x3e7eb35b91b9fa63
[08/10/2023-11:18:38] [V] [TRT] Tactic: 0x3e7eb35b91b9fa63 Time: 0.120903
[08/10/2023-11:18:38] [V] [TRT] Conv_163 Set Tactic Name: ampere_h16816cudnn_256x64_ldg8_relu_exp_stages_64x3_medium_nhwc_tn_v1 Tactic: 0x3f031df0e579d52c
[08/10/2023-11:18:38] [V] [TRT] Tactic: 0x3f031df0e579d52c Time: 0.196846
[08/10/2023-11:18:38] [V] [TRT] Conv_163 Set Tactic Name: ampere_h16816cudnn_256x64_sliced1x2_ldg8_relu_exp_small_nhwc_tn_v1 Tactic: 0x42a5b8709d0039be
[08/10/2023-11:18:38] [V] [TRT] Tactic: 0x42a5b8709d0039be Time: 0.217093
[08/10/2023-11:18:38] [V] [TRT] Conv_163 Set Tactic Name: ampere_h16816cudnn_256x128_ldg8_relu_exp_small_nhwc_tn_v1 Tactic: 0x463794ee4acffd1f
[08/10/2023-11:18:38] [V] [TRT] Tactic: 0x463794ee4acffd1f Time: 0.300142
[08/10/2023-11:18:38] [V] [TRT] Conv_163 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x64x64_stage4_warpsize2x2x1_g1_tensor16x8x16 Tactic: 0x4a81ea1e51436a30
[08/10/2023-11:18:38] [V] [TRT] Tactic: 0x4a81ea1e51436a30 Time: 0.218702
[08/10/2023-11:18:38] [V] [TRT] Conv_163 Set Tactic Name: ampere_h1688cudnn_256x64_sliced1x2_ldg8_relu_exp_small_nhwc_tn_v1 Tactic: 0x4aed1956bd10a795
[08/10/2023-11:18:38] [V] [TRT] Tactic: 0x4aed1956bd10a795 Time: 0.238747
[08/10/2023-11:18:38] [V] [TRT] Conv_163 Set Tactic Name: ampere_h16816cudnn_64x128_sliced1x2_ldg8_relu_exp_stages_64x3_large_nhwc_tn_v1 Tactic: 0x4fc05923455fc266
[08/10/2023-11:18:38] [V] [TRT] Tactic: 0x4fc05923455fc266 Time: 0.297915
[08/10/2023-11:18:38] [V] [TRT] Conv_163 Set Tactic Name: ampere_h16816cudnn_256x64_ldg8_relu_exp_large_nhwc_tn_v1 Tactic: 0x5013c38f55afa9ba
[08/10/2023-11:18:38] [V] [TRT] Tactic: 0x5013c38f55afa9ba Time: 0.177719
[08/10/2023-11:18:38] [V] [TRT] Conv_163 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x64x32_stage3_warpsize4x1x1_g1_tensor16x8x16_t1r3s3 Tactic: 0x529f4431bdae94f5
[08/10/2023-11:18:38] [V] [TRT] Tactic: 0x529f4431bdae94f5 Time: 0.143803
[08/10/2023-11:18:39] [V] [TRT] Conv_163 Set Tactic Name: ampere_h16816cudnn_64x128_ldg8_relu_exp_stages_64x3_small_nhwc_tn_v1 Tactic: 0x53be5e206184e6ad
[08/10/2023-11:18:39] [V] [TRT] Tactic: 0x53be5e206184e6ad Time: 0.292306
[08/10/2023-11:18:39] [V] [TRT] Conv_163 Set Tactic Name: ampere_h16816cudnn_64x64_ldg8_relu_exp_stages_64x5_large_nhwc_tn_v1 Tactic: 0x544bff7ff9c5d908
[08/10/2023-11:18:39] [V] [TRT] Tactic: 0x544bff7ff9c5d908 Time: 0.261307
[08/10/2023-11:18:39] [V] [TRT] Conv_163 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x64x64_stage3_warpsize2x2x1_g1_tensor16x8x16 Tactic: 0x5568fd8a32f4a40f
[08/10/2023-11:18:39] [V] [TRT] Tactic: 0x5568fd8a32f4a40f Time: 0.156229
[08/10/2023-11:18:39] [V] [TRT] Conv_163 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x128x64_stage3_warpsize2x2x1_g1_tensor16x8x16 Tactic: 0x57c9a5ff682354a6
[08/10/2023-11:18:39] [V] [TRT] Tactic: 0x57c9a5ff682354a6 Time: 0.326757
[08/10/2023-11:18:39] [V] [TRT] Conv_163 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x32x64_stage5_warpsize2x2x1_g1_tensor16x8x16 Tactic: 0x5820b3dda403c4d0
[08/10/2023-11:18:39] [V] [TRT] Tactic: 0x5820b3dda403c4d0 Time: 0.17968
[08/10/2023-11:18:39] [V] [TRT] Conv_163 Set Tactic Name: ampere_h16816cudnn_128x128_ldg8_relu_exp_stages_64x3_small_nhwc_tn_v1 Tactic: 0x58816bf2e9c36afb
[08/10/2023-11:18:39] [V] [TRT] Tactic: 0x58816bf2e9c36afb Time: 0.340261
[08/10/2023-11:18:39] [V] [TRT] Conv_163 Set Tactic Name: ampere_h16816cudnn_128x64_ldg8_relu_exp_stages_64x3_large_nhwc_tn_v1 Tactic: 0x59762bd684092b33
[08/10/2023-11:18:39] [V] [TRT] Tactic: 0x59762bd684092b33 Time: 0.180242
[08/10/2023-11:18:39] [V] [TRT] Conv_163 Set Tactic Name: ampere_h16816cudnn_128x64_sliced1x2_ldg8_relu_exp_stages_64x4_medium_nhwc_tn_v1 Tactic: 0x59c5c647b4c76593
[08/10/2023-11:18:39] [V] [TRT] Tactic: 0x59c5c647b4c76593 Time: 0.255392
[08/10/2023-11:18:39] [V] [TRT] Conv_163 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x128x64_stage3_warpsize4x2x1_g1_tensor16x8x16 Tactic: 0x5a55274960724ba8
[08/10/2023-11:18:39] [V] [TRT] Tactic: 0x5a55274960724ba8 Time: 0.296759
[08/10/2023-11:18:39] [V] [TRT] Conv_163 Set Tactic Name: ampere_h1688cudnn_256x128_ldg8_relu_exp_large_nhwc_tn_v1 Tactic: 0x5c2e1c87d85b06f1
[08/10/2023-11:18:39] [V] [TRT] Tactic: 0x5c2e1c87d85b06f1 Time: 0.36677
[08/10/2023-11:18:39] [V] [TRT] Conv_163 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x256x64_stage3_warpsize1x4x1_g1_tensor16x8x16 Tactic: 0x5d067c18f40c23e3
[08/10/2023-11:18:39] [V] [TRT] Tactic: 0x5d067c18f40c23e3 Time: 0.639378
[08/10/2023-11:18:39] [V] [TRT] Conv_163 Set Tactic Name: ampere_h16816cudnn_64x64_ldg8_relu_exp_stages_64x5_medium_nhwc_tn_v1 Tactic: 0x5f31c22ec167f384
[08/10/2023-11:18:39] [V] [TRT] Tactic: 0x5f31c22ec167f384 Time: 0.265906
[08/10/2023-11:18:39] [V] [TRT] Conv_163 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x128x32_stage4_warpsize2x2x1_g1_tensor16x8x16_t1r3s3 Tactic: 0x60c3421152ef8e10
[08/10/2023-11:18:39] [V] [TRT] Tactic: 0x60c3421152ef8e10 Time: 0.254299
[08/10/2023-11:18:39] [V] [TRT] Conv_163 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x128x32_stage3_warpsize4x2x1_g1_tensor16x8x16 Tactic: 0x60da8c7151d91e47
[08/10/2023-11:18:39] [V] [TRT] Tactic: 0x60da8c7151d91e47 Time: 0.289673
[08/10/2023-11:18:39] [V] [TRT] Conv_163 Set Tactic Name: ampere_h1688cudnn_256x64_ldg8_relu_exp_medium_nhwc_tn_v1 Tactic: 0x6426696f872a3b13
[08/10/2023-11:18:39] [V] [TRT] Tactic: 0x6426696f872a3b13 Time: 0.233088
[08/10/2023-11:18:39] [V] [TRT] Conv_163 Set Tactic Name: ampere_h16816cudnn_128x64_ldg8_relu_exp_stages_64x3_medium_nhwc_tn_v1 Tactic: 0x699be152cfb6d6ff
[08/10/2023-11:18:39] [V] [TRT] Tactic: 0x699be152cfb6d6ff Time: 0.185705
[08/10/2023-11:18:39] [V] [TRT] Conv_163 Set Tactic Name: ampere_h16816cudnn_64x64_ldg8_relu_exp_stages_64x6_large_nhwc_tn_v1 Tactic: 0x6af049035146c349
[08/10/2023-11:18:39] [V] [TRT] Tactic: 0x6af049035146c349 Time: 0.382267
[08/10/2023-11:18:39] [V] [TRT] Conv_163 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x256x32_stage3_warpsize2x4x1_g1_tensor16x8x16 Tactic: 0x7005d10718f6c22d
[08/10/2023-11:18:39] [V] [TRT] Tactic: 0x7005d10718f6c22d Time: 0.523982
[08/10/2023-11:18:39] [V] [TRT] Conv_163 Set Tactic Name: ampere_h16816cudnn_64x64_sliced1x2_ldg8_relu_exp_stages_64x5_medium_nhwc_tn_v1 Tactic: 0x706f08da35c795a5
[08/10/2023-11:18:39] [V] [TRT] Tactic: 0x706f08da35c795a5 Time: 0.254999
[08/10/2023-11:18:39] [V] [TRT] Conv_163 Set Tactic Name: ampere_h16816cudnn_128x64_sliced1x2_ldg8_relu_exp_stages_64x4_small_nhwc_tn_v1 Tactic: 0x7163d33a4d8ce8d7
[08/10/2023-11:18:39] [V] [TRT] Tactic: 0x7163d33a4d8ce8d7 Time: 0.244594
[08/10/2023-11:18:39] [V] [TRT] Conv_163 Set Tactic Name: ampere_h1688cudnn_256x64_ldg8_relu_exp_small_nhwc_tn_v1 Tactic: 0x7aad3976677d7155
[08/10/2023-11:18:39] [V] [TRT] Tactic: 0x7aad3976677d7155 Time: 0.212146
[08/10/2023-11:18:39] [V] [TRT] Conv_163 Set Tactic Name: ampere_h16816cudnn_64x64_sliced1x2_ldg8_relu_exp_stages_64x5_large_nhwc_tn_v1 Tactic: 0x8015519605ab9963
[08/10/2023-11:18:39] [V] [TRT] Tactic: 0x8015519605ab9963 Time: 0.253088
[08/10/2023-11:18:39] [V] [TRT] Conv_163 Set Tactic Name: ampere_h16816cudnn_256x64_sliced1x2_ldg8_relu_exp_medium_nhwc_tn_v1 Tactic: 0x82f8a2214b0b4178
[08/10/2023-11:18:39] [V] [TRT] Tactic: 0x82f8a2214b0b4178 Time: 0.23541
[08/10/2023-11:18:39] [V] [TRT] Conv_163 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x32x32_stage4_warpsize4x1x1_g1_tensor16x8x16 Tactic: 0x834e11ecd4ab9454
[08/10/2023-11:18:39] [V] [TRT] Tactic: 0x834e11ecd4ab9454 Time: 0.121609
[08/10/2023-11:18:39] [V] [TRT] Conv_163 Set Tactic Name: ampere_h16816cudnn_128x64_sliced1x2_ldg8_relu_exp_stages_64x3_large_nhwc_tn_v1 Tactic: 0x83ccd4762c1376a1
[08/10/2023-11:18:39] [V] [TRT] Tactic: 0x83ccd4762c1376a1 Time: 0.169477
[08/10/2023-11:18:39] [V] [TRT] Conv_163 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x64x64_stage3_warpsize4x1x1_g1_tensor16x8x16_t1r3s3 Tactic: 0x84ee897dd1f4d2aa
[08/10/2023-11:18:39] [V] [TRT] Tactic: 0x84ee897dd1f4d2aa Time: 0.182894
[08/10/2023-11:18:39] [V] [TRT] Conv_163 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x128x64_stage3_warpsize2x2x1_g1_tensor16x8x16 Tactic: 0x866e7a5f6401b67f
[08/10/2023-11:18:39] [V] [TRT] Tactic: 0x866e7a5f6401b67f Time: 0.326496
[08/10/2023-11:18:39] [V] [TRT] Conv_163 Set Tactic Name: ampere_h16816cudnn_64x64_ldg8_relu_exp_stages_64x6_medium_nhwc_tn_v1 Tactic: 0x88971eec55aba850
[08/10/2023-11:18:39] [V] [TRT] Tactic: 0x88971eec55aba850 Time: 0.372398
[08/10/2023-11:18:39] [V] [TRT] Conv_163 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x128x64_stage3_warpsize4x2x1_g1_tensor16x8x16 Tactic: 0x8bf2f8e96c50a971
[08/10/2023-11:18:39] [V] [TRT] Tactic: 0x8bf2f8e96c50a971 Time: 0.290546
[08/10/2023-11:18:39] [V] [TRT] Conv_163 Set Tactic Name: ampere_h16816cudnn_256x128_ldg8_relu_exp_stages_64x3_large_nhwc_tn_v1 Tactic: 0x94576ece6beb35e3
[08/10/2023-11:18:39] [V] [TRT] Tactic: 0x94576ece6beb35e3 Time: 0.30432
[08/10/2023-11:18:39] [V] [TRT] Conv_163 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x64x64_stage3_warpsize4x1x1_g1_tensor16x8x16 Tactic: 0x9fff6e65019cc310
[08/10/2023-11:18:39] [V] [TRT] Tactic: 0x9fff6e65019cc310 Time: 0.184873
[08/10/2023-11:18:39] [V] [TRT] Conv_163 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x32x64_stage5_warpsize2x2x1_g1_tensor16x8x16_t1r3s3 Tactic: 0xa033e20ae9f412b2
[08/10/2023-11:18:39] [V] [TRT] Tactic: 0xa033e20ae9f412b2 Time: 0.165682
[08/10/2023-11:18:39] [V] [TRT] Conv_163 Set Tactic Name: ampere_h16816cudnn_64x128_ldg8_relu_exp_stages_64x4_large_nhwc_tn_v1 Tactic: 0xa111596c001b78db
[08/10/2023-11:18:39] [V] [TRT] Tactic: 0xa111596c001b78db Time: 0.441239
[08/10/2023-11:18:39] [V] [TRT] Conv_163 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x256x32_stage3_warpsize2x4x1_g1_tensor16x8x16 Tactic: 0xa1a20ea714d420f4
[08/10/2023-11:18:39] [V] [TRT] Tactic: 0xa1a20ea714d420f4 Time: 0.5184
[08/10/2023-11:18:39] [V] [TRT] Conv_163 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x32x32_stage4_warpsize4x1x1_g1_tensor16x8x16_t1r3s3 Tactic: 0xa40cb43c296a36a8
[08/10/2023-11:18:39] [V] [TRT] Tactic: 0xa40cb43c296a36a8 Time: 0.125879
[08/10/2023-11:18:39] [V] [TRT] Conv_163 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x64x32_stage3_warpsize4x1x1_g1_tensor16x8x16 Tactic: 0xa570c55d303796ff
[08/10/2023-11:18:39] [V] [TRT] Tactic: 0xa570c55d303796ff Time: 0.157262
[08/10/2023-11:18:39] [V] [TRT] Conv_163 Set Tactic Name: ampere_h16816cudnn_64x64_sliced1x2_ldg8_relu_exp_stages_64x6_medium_nhwc_tn_v1 Tactic: 0xa7c9d418a10bce71
[08/10/2023-11:18:39] [V] [TRT] Tactic: 0xa7c9d418a10bce71 Time: 0.397833
[08/10/2023-11:18:39] [V] [TRT] Conv_163 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x32x32_stage4_warpsize4x1x1_g1_tensor16x8x16 Tactic: 0xa83b68f30462f971
[08/10/2023-11:18:39] [V] [TRT] Tactic: 0xa83b68f30462f971 Time: 0.133673
[08/10/2023-11:18:39] [V] [TRT] Conv_163 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x256x32_stage3_warpsize2x4x1_g1_tensor16x8x16_t1r3s3 Tactic: 0xa9177bbe4e767df8
[08/10/2023-11:18:39] [V] [TRT] Tactic: 0xa9177bbe4e767df8 Time: 0.53237
[08/10/2023-11:18:39] [V] [TRT] Conv_163 Set Tactic Name: ampere_h1688cudnn_128x128_ldg8_relu_exp_large_nhwc_tn_v1 Tactic: 0xa927df92ac1ef1b8
[08/10/2023-11:18:39] [V] [TRT] Tactic: 0xa927df92ac1ef1b8 Time: 0.357481
[08/10/2023-11:18:39] [V] [TRT] Conv_163 Set Tactic Name: ampere_h16816cudnn_128x64_ldg8_relu_exp_stages_64x4_small_nhwc_tn_v1 Tactic: 0xabd92c9ae596b545
[08/10/2023-11:18:39] [V] [TRT] Tactic: 0xabd92c9ae596b545 Time: 0.246098
[08/10/2023-11:18:39] [V] [TRT] Conv_163 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x128x32_stage5_warpsize2x2x1_g1_tensor16x8x16 Tactic: 0xafd1e8bf6bd3d638
[08/10/2023-11:18:39] [V] [TRT] Tactic: 0xafd1e8bf6bd3d638 Time: 0.30437
[08/10/2023-11:18:39] [V] [TRT] Conv_163 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x128x32_stage3_warpsize4x2x1_g1_tensor16x8x16 Tactic: 0xb17d53d15dfbfc9e
[08/10/2023-11:18:39] [V] [TRT] Tactic: 0xb17d53d15dfbfc9e Time: 0.290158
[08/10/2023-11:18:39] [V] [TRT] Conv_163 Set Tactic Name: ampere_h16816cudnn_128x128_ldg8_relu_exp_small_nhwc_tn_v1 Tactic: 0xb33e57fb3e8a0a56
[08/10/2023-11:18:39] [V] [TRT] Tactic: 0xb33e57fb3e8a0a56 Time: 0.250208
[08/10/2023-11:18:39] [V] [TRT] Conv_163 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x64x32_stage5_warpsize2x2x1_g1_tensor16x8x16_t1r3s3 Tactic: 0xb4bec086187edcfc
[08/10/2023-11:18:39] [V] [TRT] Tactic: 0xb4bec086187edcfc Time: 0.163337
[08/10/2023-11:18:39] [V] [TRT] Conv_163 Set Tactic Name: ampere_h16816cudnn_256x64_ldg8_relu_exp_stages_64x3_small_nhwc_tn_v1 Tactic: 0xb6fa5ffcc1a9d518
[08/10/2023-11:18:39] [V] [TRT] Tactic: 0xb6fa5ffcc1a9d518 Time: 0.193175
[08/10/2023-11:18:39] [V] [TRT] Conv_163 Set Tactic Name: ampere_h16816cudnn_64x128_sliced1x2_ldg8_relu_exp_stages_64x3_medium_nhwc_tn_v1 Tactic: 0xb85e52e87caf60a2
[08/10/2023-11:18:39] [V] [TRT] Tactic: 0xb85e52e87caf60a2 Time: 0.294706
[08/10/2023-11:18:39] [V] [TRT] Conv_163 Set Tactic Name: ampere_h16816cudnn_64x128_ldg8_relu_exp_stages_64x3_medium_nhwc_tn_v1 Tactic: 0xb8d86216e1235cda
[08/10/2023-11:18:39] [V] [TRT] Tactic: 0xb8d86216e1235cda Time: 0.293774
[08/10/2023-11:18:39] [V] [TRT] Conv_163 Set Tactic Name: ampere_h16816cudnn_256x128_ldg8_relu_exp_stages_64x3_medium_nhwc_tn_v1 Tactic: 0xb9171d70ba2eda4b
[08/10/2023-11:18:39] [V] [TRT] Tactic: 0xb9171d70ba2eda4b Time: 0.305239
[08/10/2023-11:18:39] [V] [TRT] Conv_163 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x64x32_stage5_warpsize2x2x1_g1_tensor16x8x16 Tactic: 0xbd08239a9317f2fd
[08/10/2023-11:18:39] [V] [TRT] Tactic: 0xbd08239a9317f2fd Time: 0.169769
[08/10/2023-11:18:39] [V] [TRT] Conv_163 Set Tactic Name: ampere_h16816cudnn_64x128_sliced1x2_ldg8_relu_exp_stages_64x4_small_nhwc_tn_v1 Tactic: 0xbd6f5e6f24c05c10
[08/10/2023-11:18:39] [V] [TRT] Tactic: 0xbd6f5e6f24c05c10 Time: 0.471227
[08/10/2023-11:18:39] [V] [TRT] Conv_163 Set Tactic Name: ampere_h16816cudnn_64x64_sliced1x2_ldg8_relu_exp_stages_64x6_large_nhwc_tn_v1 Tactic: 0xbeaee7eaad288322
[08/10/2023-11:18:39] [V] [TRT] Tactic: 0xbeaee7eaad288322 Time: 0.395643
[08/10/2023-11:18:39] [V] [TRT] Conv_163 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x128x64_stage3_warpsize2x2x1_g1_tensor16x8x16_t1r3s3 Tactic: 0xc0a02dc6095497cc
[08/10/2023-11:18:39] [V] [TRT] Tactic: 0xc0a02dc6095497cc Time: 0.320617
[08/10/2023-11:18:39] [V] [TRT] Conv_163 Set Tactic Name: ampere_h16816cudnn_128x128_ldg8_relu_exp_stages_64x3_large_nhwc_tn_v1 Tactic: 0xc2cf926c41243630
[08/10/2023-11:18:39] [V] [TRT] Tactic: 0xc2cf926c41243630 Time: 0.344421
[08/10/2023-11:18:39] [V] [TRT] Conv_163 Set Tactic Name: ampere_h16816cudnn_128x64_ldg8_relu_exp_stages_64x3_small_nhwc_tn_v1 Tactic: 0xc338d2482cee77f8
[08/10/2023-11:18:39] [V] [TRT] Tactic: 0xc338d2482cee77f8 Time: 0.180581
[08/10/2023-11:18:39] [V] [TRT] Conv_163 Set Tactic Name: ampere_h1688cudnn_256x128_ldg8_relu_exp_small_nhwc_tn_v1 Tactic: 0xc660e51970bc5a3a
[08/10/2023-11:18:39] [V] [TRT] Tactic: 0xc660e51970bc5a3a Time: 0.343026
[08/10/2023-11:18:39] [V] [TRT] Conv_163 Set Tactic Name: ampere_h16816cudnn_64x128_ldg8_relu_exp_stages_64x4_medium_nhwc_tn_v1 Tactic: 0xc82f3f06140e3cbb
[08/10/2023-11:18:39] [V] [TRT] Tactic: 0xc82f3f06140e3cbb Time: 0.436014
[08/10/2023-11:18:39] [V] [TRT] Conv_163 Set Tactic Name: ampere_h16816cudnn_64x128_sliced1x2_ldg8_relu_exp_stages_64x4_medium_nhwc_tn_v1 Tactic: 0xc8a90ff8898200c3
[08/10/2023-11:18:39] [V] [TRT] Tactic: 0xc8a90ff8898200c3 Time: 0.470757
[08/10/2023-11:18:39] [V] [TRT] Conv_163 Set Tactic Name: ampere_h1688cudnn_256x64_sliced1x2_ldg8_relu_exp_medium_nhwc_tn_v1 Tactic: 0xc9cc55109bb4de26
[08/10/2023-11:18:39] [V] [TRT] Tactic: 0xc9cc55109bb4de26 Time: 0.252704
[08/10/2023-11:18:39] [V] [TRT] Conv_163 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x64x32_stage5_warpsize2x2x1_g1_tensor16x8x16 Tactic: 0xc9d24bd069159fa8
[08/10/2023-11:18:39] [V] [TRT] Tactic: 0xc9d24bd069159fa8 Time: 0.181929
[08/10/2023-11:18:39] [V] [TRT] Conv_163 Set Tactic Name: ampere_h16816cudnn_64x128_ldg8_relu_exp_stages_64x3_large_nhwc_tn_v1 Tactic: 0xc9f0a7bec963ba66
[08/10/2023-11:18:39] [V] [TRT] Tactic: 0xc9f0a7bec963ba66 Time: 0.294953
[08/10/2023-11:18:39] [V] [TRT] Conv_163 Set Tactic Name: ampere_h16816cudnn_256x64_ldg8_relu_exp_small_nhwc_tn_v1 Tactic: 0xca5d3a11fd48f571
[08/10/2023-11:18:39] [V] [TRT] Tactic: 0xca5d3a11fd48f571 Time: 0.167022
[08/10/2023-11:18:39] [V] [TRT] Conv_163 Set Tactic Name: ampere_h16816cudnn_64x64_ldg8_relu_exp_stages_64x5_small_nhwc_tn_v1 Tactic: 0xce0506e1512285c3
[08/10/2023-11:18:39] [V] [TRT] Tactic: 0xce0506e1512285c3 Time: 0.263922
[08/10/2023-11:18:39] [V] [TRT] Conv_163 Set Tactic Name: ampere_h1688cudnn_256x64_sliced1x2_ldg8_relu_exp_large_nhwc_tn_v1 Tactic: 0xd0a3e0c815f7fb5e
[08/10/2023-11:18:39] [V] [TRT] Tactic: 0xd0a3e0c815f7fb5e Time: 0.254277
[08/10/2023-11:18:39] [V] [TRT] Conv_163 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x64x32_stage3_warpsize4x1x1_g1_tensor16x8x16 Tactic: 0xd1aaad17ca35fbaa
[08/10/2023-11:18:39] [V] [TRT] Tactic: 0xd1aaad17ca35fbaa Time: 0.150949
[08/10/2023-11:18:39] [V] [TRT] Conv_163 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x128x64_stage3_warpsize4x2x1_g1_tensor16x8x16_t1r3s3 Tactic: 0xd2fca0486ca97266
[08/10/2023-11:18:39] [V] [TRT] Tactic: 0xd2fca0486ca97266 Time: 0.300517
[08/10/2023-11:18:39] [V] [TRT] Conv_163 Set Tactic Name: ampere_h16816cudnn_64x128_sliced1x2_ldg8_relu_exp_stages_64x3_small_nhwc_tn_v1 Tactic: 0xd58ea0bdedb89ead
[08/10/2023-11:18:39] [V] [TRT] Tactic: 0xd58ea0bdedb89ead Time: 0.298126
[08/10/2023-11:18:39] [V] [TRT] Conv_163 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x256x64_stage3_warpsize1x4x1_g1_tensor16x8x16_t1r3s3 Tactic: 0xd7c261fa01db2af9
[08/10/2023-11:18:39] [V] [TRT] Tactic: 0xd7c261fa01db2af9 Time: 0.624768
[08/10/2023-11:18:39] [V] [TRT] Conv_163 Set Tactic Name: ampere_h16816cudnn_256x64_sliced1x2_ldg8_relu_exp_large_nhwc_tn_v1 Tactic: 0xd8eb41ee35e76575
[08/10/2023-11:18:39] [V] [TRT] Tactic: 0xd8eb41ee35e76575 Time: 0.242299
[08/10/2023-11:18:39] [V] [TRT] Conv_163 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x128x32_stage5_warpsize2x2x1_g1_tensor16x8x16 Tactic: 0xdb0b80f591d1bb6d
[08/10/2023-11:18:39] [V] [TRT] Tactic: 0xdb0b80f591d1bb6d Time: 0.300046
[08/10/2023-11:18:39] [V] [TRT] Conv_163 Set Tactic Name: ampere_h16816cudnn_256x128_ldg8_relu_exp_large_nhwc_tn_v1 Tactic: 0xdc796d70e228a1d4
[08/10/2023-11:18:39] [V] [TRT] Tactic: 0xdc796d70e228a1d4 Time: 0.304402
[08/10/2023-11:18:39] [V] [TRT] Conv_163 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x32x32_stage4_warpsize4x1x1_g1_tensor16x8x16 Tactic: 0xdce100b9fe609424
[08/10/2023-11:18:39] [V] [TRT] Tactic: 0xdce100b9fe609424 Time: 0.132379
[08/10/2023-11:18:39] [V] [TRT] Conv_163 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x128x32_stage4_warpsize2x2x1_g1_tensor16x8x16 Tactic: 0xdfa020ef435ef810
[08/10/2023-11:18:39] [V] [TRT] Tactic: 0xdfa020ef435ef810 Time: 0.248274
[08/10/2023-11:18:39] [V] [TRT] Conv_163 Set Tactic Name: ampere_h1688cudnn_256x64_ldg8_relu_exp_large_nhwc_tn_v1 Tactic: 0xe0e3c0e8cf9a2d9e
[08/10/2023-11:18:39] [V] [TRT] Tactic: 0xe0e3c0e8cf9a2d9e Time: 0.223863
[08/10/2023-11:18:39] [V] [TRT] Conv_163 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x32x64_stage5_warpsize2x2x1_g1_tensor16x8x16 Tactic: 0xe1ff5ad20f5c6bf6
[08/10/2023-11:18:39] [V] [TRT] Tactic: 0xe1ff5ad20f5c6bf6 Time: 0.183227
[08/10/2023-11:18:39] [V] [TRT] Conv_163 Set Tactic Name: ampere_h16816cudnn_256x64_ldg8_relu_exp_medium_nhwc_tn_v1 Tactic: 0xe4711898bd599c36
[08/10/2023-11:18:39] [V] [TRT] Tactic: 0xe4711898bd599c36 Time: 0.171694
[08/10/2023-11:18:39] [V] [TRT] Conv_163 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x64x64_stage3_warpsize4x1x1_g1_tensor16x8x16 Tactic: 0xeb25062ffb9eae45
[08/10/2023-11:18:39] [V] [TRT] Tactic: 0xeb25062ffb9eae45 Time: 0.188645
[08/10/2023-11:18:39] [V] [TRT] Conv_163 Set Tactic Name: ampere_h16816cudnn_128x64_sliced1x2_ldg8_relu_exp_stages_64x4_large_nhwc_tn_v1 Tactic: 0xeb2d2aa4e56bb41c
[08/10/2023-11:18:39] [V] [TRT] Tactic: 0xeb2d2aa4e56bb41c Time: 0.262496
[08/10/2023-11:18:39] [V] [TRT] Conv_163 Set Tactic Name: ampere_h16816cudnn_64x64_ldg8_relu_exp_stages_64x6_small_nhwc_tn_v1 Tactic: 0xf0beb09df9a19f82
[08/10/2023-11:18:39] [V] [TRT] Tactic: 0xf0beb09df9a19f82 Time: 0.350386
[08/10/2023-11:18:39] [V] [TRT] Conv_163 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x64x64_stage4_warpsize2x2x1_g1_tensor16x8x16 Tactic: 0xf35e0311fa1cc516
[08/10/2023-11:18:39] [V] [TRT] Tactic: 0xf35e0311fa1cc516 Time: 0.207374
[08/10/2023-11:18:39] [V] [TRT] Conv_163 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x32x32_stage4_warpsize4x1x1_g1_tensor16x8x16 Tactic: 0xf79479a62ea9f901
[08/10/2023-11:18:39] [V] [TRT] Tactic: 0xf79479a62ea9f901 Time: 0.127794
[08/10/2023-11:18:39] [V] [TRT] Fastest Tactic: 0x3e7eb35b91b9fa63 Time: 0.120903
[08/10/2023-11:18:39] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x3e7eb35b91b9fa63
[08/10/2023-11:18:39] [V] [TRT] =============== Computing costs for 
[08/10/2023-11:18:39] [V] [TRT] *************** Autotuning format combination: Float(8294400,32400,180,1) -> Float(64800,32400,180,1) ***************
[08/10/2023-11:18:39] [V] [TRT] *************** Autotuning format combination: Float(8294400,1,46080,256) -> Float(64800,1,360,2) ***************
[08/10/2023-11:18:39] [V] [TRT] *************** Autotuning format combination: Float(2073600,1:4,11520,64) -> Float(32400,1:4,180,1) ***************
[08/10/2023-11:18:39] [V] [TRT] *************** Autotuning format combination: Half(8294400,32400,180,1) -> Half(64800,32400,180,1) ***************
[08/10/2023-11:18:39] [V] [TRT] *************** Autotuning format combination: Half(4147200,32400:2,180,1) -> Half(32400,32400:2,180,1) ***************
[08/10/2023-11:18:39] [V] [TRT] *************** Autotuning format combination: Half(1036800,1:8,5760,32) -> Float(64800,32400,180,1) ***************
[08/10/2023-11:18:39] [V] [TRT] --------------- Timing Runner: Conv_166 (CaskConvolution)
[08/10/2023-11:18:39] [V] [TRT] CaskConvolution has no valid tactics for this config, skipping
[08/10/2023-11:18:39] [V] [TRT] *************** Autotuning format combination: Half(1036800,1:8,5760,32) -> Half(32400,1:8,180,1) ***************
[08/10/2023-11:18:39] [V] [TRT] *************** Autotuning format combination: Half(518400,1:16,2880,16) -> Half(32400,1:16,180,1) ***************
[08/10/2023-11:18:39] [V] [TRT] =============== Computing costs for 
[08/10/2023-11:18:39] [V] [TRT] *************** Autotuning format combination: Float(8294400,32400,180,1) -> Float(64800,32400,180,1) ***************
[08/10/2023-11:18:39] [V] [TRT] *************** Autotuning format combination: Float(8294400,1,46080,256) -> Float(64800,1,360,2) ***************
[08/10/2023-11:18:39] [V] [TRT] *************** Autotuning format combination: Float(2073600,1:4,11520,64) -> Float(32400,1:4,180,1) ***************
[08/10/2023-11:18:39] [V] [TRT] *************** Autotuning format combination: Half(8294400,32400,180,1) -> Half(64800,32400,180,1) ***************
[08/10/2023-11:18:39] [V] [TRT] *************** Autotuning format combination: Half(4147200,32400:2,180,1) -> Half(32400,32400:2,180,1) ***************
[08/10/2023-11:18:39] [V] [TRT] *************** Autotuning format combination: Half(1036800,1:8,5760,32) -> Float(64800,32400,180,1) ***************
[08/10/2023-11:18:39] [V] [TRT] --------------- Timing Runner: Conv_169 (CaskConvolution)
[08/10/2023-11:18:39] [V] [TRT] CaskConvolution has no valid tactics for this config, skipping
[08/10/2023-11:18:39] [V] [TRT] *************** Autotuning format combination: Half(1036800,1:8,5760,32) -> Half(32400,1:8,180,1) ***************
[08/10/2023-11:18:39] [V] [TRT] *************** Autotuning format combination: Half(518400,1:16,2880,16) -> Half(32400,1:16,180,1) ***************
[08/10/2023-11:18:39] [V] [TRT] Adding reformat layer: Reformatted Input Tensor 0 to Conv_15 + Relu_16 (input) from Half(8294400,32400,180,1) to Half(1036800,1:8,5760,32)
[08/10/2023-11:18:39] [V] [TRT] Adding reformat layer: Reformatted Input Tensor 0 to Conv_44 + Relu_45 (input.72) from Half(518400,1:8,2880,16) to Half(259200,1:16,1440,8)
[08/10/2023-11:18:39] [V] [TRT] Adding reformat layer: Reformatted Input Tensor 0 to ConvTranspose_56 + BatchNormalization_57 + Relu_58 (onnx::ConvTranspose_644) from Half(129600,1:16,1440,16) to Half(259200,1:8,2880,32)
[08/10/2023-11:18:39] [V] [TRT] Adding reformat layer: Reformatted Output Tensor 0 to Conv_60 + Relu_61 (onnx::Conv_651) from Half(259200,1:8,1440,8) to Half(129600,1:16,720,4)
[08/10/2023-11:18:39] [V] [TRT] Adding reformat layer: Reformatted Output Tensor 0 to Conv_62 + Relu_63 || Conv_65 + Relu_66 || Conv_68 + Relu_69 || Conv_71 + Relu_72 || Conv_74 + Relu_75 || Conv_77 + Relu_78 || Conv_80 + Relu_81 || Conv_83 + Relu_84 (Conv_62 + Relu_63 || Conv_65 + Relu_66 || Conv_68 + Relu_69 || Conv_71 + Relu_72 || Conv_74 + Relu_75 || Conv_77 + Relu_78 || Conv_80 + Relu_81 || Conv_83 + Relu_84) from Half(1036800,1:16,5760,32) to Half(2073600,1:8,11520,64)
[08/10/2023-11:18:39] [V] [TRT] Adding reformat layer: Reformatted Output Tensor 0 to Conv_64 (reg_0) from Half(32400,1:8,180,1) to Half(64800,32400,180,1)
[08/10/2023-11:18:39] [V] [TRT] Adding reformat layer: Reformatted Output Tensor 0 to Conv_67 (height_0) from Half(32400,1:8,180,1) to Half(32400,32400,180,1)
[08/10/2023-11:18:39] [V] [TRT] Adding reformat layer: Reformatted Output Tensor 0 to Conv_70 (dim_0) from Half(32400,1:8,180,1) to Half(97200,32400,180,1)
[08/10/2023-11:18:39] [V] [TRT] Adding reformat layer: Reformatted Output Tensor 0 to Conv_73 (rot_0) from Half(32400,1:8,180,1) to Half(64800,32400,180,1)
[08/10/2023-11:18:39] [V] [TRT] Adding reformat layer: Reformatted Output Tensor 0 to Conv_76 (vel_0) from Half(32400,1:8,180,1) to Half(64800,32400,180,1)
[08/10/2023-11:18:39] [V] [TRT] Adding reformat layer: Reformatted Output Tensor 0 to Conv_79 (hm_0) from Half(32400,1:8,180,1) to Half(32400,32400,180,1)
[08/10/2023-11:18:39] [V] [TRT] Adding reformat layer: Reformatted Output Tensor 0 to Conv_82 (reg_1) from Half(32400,1:8,180,1) to Half(64800,32400,180,1)
[08/10/2023-11:18:39] [V] [TRT] Adding reformat layer: Reformatted Output Tensor 0 to Conv_85 (height_1) from Half(32400,1:8,180,1) to Half(32400,32400,180,1)
[08/10/2023-11:18:39] [V] [TRT] Adding reformat layer: Reformatted Output Tensor 0 to Conv_86 + Relu_87 || Conv_89 + Relu_90 || Conv_92 + Relu_93 || Conv_95 + Relu_96 || Conv_98 + Relu_99 || Conv_101 + Relu_102 || Conv_104 + Relu_105 || Conv_107 + Relu_108 (Conv_86 + Relu_87 || Conv_89 + Relu_90 || Conv_92 + Relu_93 || Conv_95 + Relu_96 || Conv_98 + Relu_99 || Conv_101 + Relu_102 || Conv_104 + Relu_105 || Conv_107 + Relu_108) from Half(1036800,1:16,5760,32) to Half(2073600,1:8,11520,64)
[08/10/2023-11:18:39] [V] [TRT] Adding reformat layer: Reformatted Output Tensor 0 to Conv_88 (dim_1) from Half(32400,1:8,180,1) to Half(97200,32400,180,1)
[08/10/2023-11:18:39] [V] [TRT] Adding reformat layer: Reformatted Output Tensor 0 to Conv_91 (rot_1) from Half(32400,1:8,180,1) to Half(64800,32400,180,1)
[08/10/2023-11:18:39] [V] [TRT] Adding reformat layer: Reformatted Output Tensor 0 to Conv_94 (vel_1) from Half(32400,1:8,180,1) to Half(64800,32400,180,1)
[08/10/2023-11:18:39] [V] [TRT] Adding reformat layer: Reformatted Output Tensor 0 to Conv_97 (hm_1) from Half(32400,1:8,180,1) to Half(64800,32400,180,1)
[08/10/2023-11:18:39] [V] [TRT] Adding reformat layer: Reformatted Output Tensor 0 to Conv_100 (reg_2) from Half(32400,1:8,180,1) to Half(64800,32400,180,1)
[08/10/2023-11:18:39] [V] [TRT] Adding reformat layer: Reformatted Output Tensor 0 to Conv_103 (height_2) from Half(32400,1:8,180,1) to Half(32400,32400,180,1)
[08/10/2023-11:18:39] [V] [TRT] Adding reformat layer: Reformatted Output Tensor 0 to Conv_106 (dim_2) from Half(32400,1:8,180,1) to Half(97200,32400,180,1)
[08/10/2023-11:18:39] [V] [TRT] Adding reformat layer: Reformatted Output Tensor 0 to Conv_109 (rot_2) from Half(32400,1:8,180,1) to Half(64800,32400,180,1)
[08/10/2023-11:18:39] [V] [TRT] Adding reformat layer: Reformatted Output Tensor 0 to Conv_110 + Relu_111 || Conv_113 + Relu_114 || Conv_116 + Relu_117 || Conv_119 + Relu_120 || Conv_122 + Relu_123 || Conv_125 + Relu_126 || Conv_128 + Relu_129 || Conv_131 + Relu_132 (Conv_110 + Relu_111 || Conv_113 + Relu_114 || Conv_116 + Relu_117 || Conv_119 + Relu_120 || Conv_122 + Relu_123 || Conv_125 + Relu_126 || Conv_128 + Relu_129 || Conv_131 + Relu_132) from Half(1036800,1:16,5760,32) to Half(2073600,1:8,11520,64)
[08/10/2023-11:18:39] [V] [TRT] Adding reformat layer: Reformatted Output Tensor 0 to Conv_112 (vel_2) from Half(32400,1:8,180,1) to Half(64800,32400,180,1)
[08/10/2023-11:18:39] [V] [TRT] Adding reformat layer: Reformatted Output Tensor 0 to Conv_115 (hm_2) from Half(32400,1:8,180,1) to Half(64800,32400,180,1)
[08/10/2023-11:18:39] [V] [TRT] Adding reformat layer: Reformatted Output Tensor 0 to Conv_118 (reg_3) from Half(32400,1:8,180,1) to Half(64800,32400,180,1)
[08/10/2023-11:18:39] [V] [TRT] Adding reformat layer: Reformatted Output Tensor 0 to Conv_121 (height_3) from Half(32400,1:8,180,1) to Half(32400,32400,180,1)
[08/10/2023-11:18:39] [V] [TRT] Adding reformat layer: Reformatted Output Tensor 0 to Conv_124 (dim_3) from Half(32400,1:8,180,1) to Half(97200,32400,180,1)
[08/10/2023-11:18:39] [V] [TRT] Adding reformat layer: Reformatted Output Tensor 0 to Conv_127 (rot_3) from Half(32400,1:8,180,1) to Half(64800,32400,180,1)
[08/10/2023-11:18:39] [V] [TRT] Adding reformat layer: Reformatted Output Tensor 0 to Conv_130 (vel_3) from Half(32400,1:8,180,1) to Half(64800,32400,180,1)
[08/10/2023-11:18:39] [V] [TRT] Adding reformat layer: Reformatted Output Tensor 0 to Conv_133 (hm_3) from Half(32400,1:8,180,1) to Half(32400,32400,180,1)
[08/10/2023-11:18:39] [V] [TRT] Adding reformat layer: Reformatted Output Tensor 0 to Conv_134 + Relu_135 || Conv_137 + Relu_138 || Conv_140 + Relu_141 || Conv_143 + Relu_144 || Conv_146 + Relu_147 || Conv_149 + Relu_150 || Conv_152 + Relu_153 || Conv_155 + Relu_156 (Conv_134 + Relu_135 || Conv_137 + Relu_138 || Conv_140 + Relu_141 || Conv_143 + Relu_144 || Conv_146 + Relu_147 || Conv_149 + Relu_150 || Conv_152 + Relu_153 || Conv_155 + Relu_156) from Half(1036800,1:16,5760,32) to Half(2073600,1:8,11520,64)
[08/10/2023-11:18:39] [V] [TRT] Adding reformat layer: Reformatted Output Tensor 0 to Conv_136 (reg_4) from Half(32400,1:8,180,1) to Half(64800,32400,180,1)
[08/10/2023-11:18:39] [V] [TRT] Adding reformat layer: Reformatted Output Tensor 0 to Conv_139 (height_4) from Half(32400,1:8,180,1) to Half(32400,32400,180,1)
[08/10/2023-11:18:39] [V] [TRT] Adding reformat layer: Reformatted Output Tensor 0 to Conv_142 (dim_4) from Half(32400,1:8,180,1) to Half(97200,32400,180,1)
[08/10/2023-11:18:39] [V] [TRT] Adding reformat layer: Reformatted Output Tensor 0 to Conv_145 (rot_4) from Half(32400,1:8,180,1) to Half(64800,32400,180,1)
[08/10/2023-11:18:39] [V] [TRT] Adding reformat layer: Reformatted Output Tensor 0 to Conv_148 (vel_4) from Half(32400,1:8,180,1) to Half(64800,32400,180,1)
[08/10/2023-11:18:39] [V] [TRT] Adding reformat layer: Reformatted Output Tensor 0 to Conv_151 (hm_4) from Half(32400,1:8,180,1) to Half(64800,32400,180,1)
[08/10/2023-11:18:39] [V] [TRT] Adding reformat layer: Reformatted Output Tensor 0 to Conv_154 (reg_5) from Half(32400,1:8,180,1) to Half(64800,32400,180,1)
[08/10/2023-11:18:39] [V] [TRT] Adding reformat layer: Reformatted Output Tensor 0 to Conv_157 (height_5) from Half(32400,1:8,180,1) to Half(32400,32400,180,1)
[08/10/2023-11:18:39] [V] [TRT] Adding reformat layer: Reformatted Input Tensor 0 to Conv_158 + Relu_159 || Conv_161 + Relu_162 || Conv_164 + Relu_165 || Conv_167 + Relu_168 (onnx::Conv_651) from Half(129600,1:16,720,4) to Half(259200,1:8,1440,8)
[08/10/2023-11:18:39] [V] [TRT] Adding reformat layer: Reformatted Output Tensor 0 to Conv_160 (dim_5) from Half(32400,1:8,180,1) to Half(97200,32400,180,1)
[08/10/2023-11:18:39] [V] [TRT] Adding reformat layer: Reformatted Output Tensor 0 to Conv_163 (rot_5) from Half(32400,1:8,180,1) to Half(64800,32400,180,1)
[08/10/2023-11:18:39] [V] [TRT] Adding reformat layer: Reformatted Output Tensor 0 to Conv_166 (vel_5) from Half(32400,1:8,180,1) to Half(64800,32400,180,1)
[08/10/2023-11:18:39] [V] [TRT] Adding reformat layer: Reformatted Output Tensor 0 to Conv_169 (hm_5) from Half(32400,1:8,180,1) to Half(64800,32400,180,1)
[08/10/2023-11:18:39] [V] [TRT] Formats and tactics selection completed in 135.844 seconds.
[08/10/2023-11:18:39] [V] [TRT] After reformat layers: 102 layers
[08/10/2023-11:18:39] [V] [TRT] Pre-optimized block assignment.
[08/10/2023-11:18:39] [V] [TRT] Block size 8294400
[08/10/2023-11:18:39] [V] [TRT] Block size 8294400
[08/10/2023-11:18:39] [V] [TRT] Block size 8294400
[08/10/2023-11:18:39] [V] [TRT] Block size 8294400
[08/10/2023-11:18:39] [V] [TRT] Block size 8294400
[08/10/2023-11:18:39] [V] [TRT] Block size 8294400
[08/10/2023-11:18:39] [V] [TRT] Block size 4147200
[08/10/2023-11:18:39] [V] [TRT] Block size 4147200
[08/10/2023-11:18:39] [V] [TRT] Block size 4147200
[08/10/2023-11:18:39] [V] [TRT] Block size 4147200
[08/10/2023-11:18:39] [V] [TRT] Block size 4147200
[08/10/2023-11:18:39] [V] [TRT] Block size 4147200
[08/10/2023-11:18:39] [V] [TRT] Block size 16588800
[08/10/2023-11:18:39] [V] [TRT] Block size 33177600
[08/10/2023-11:18:39] [V] [TRT] Block size 4
[08/10/2023-11:18:39] [V] [TRT] Block size 4
[08/10/2023-11:18:39] [V] [TRT] Block size 4
[08/10/2023-11:18:39] [V] [TRT] Block size 4
[08/10/2023-11:18:39] [V] [TRT] Block size 4
[08/10/2023-11:18:39] [V] [TRT] Block size 16588800
[08/10/2023-11:18:39] [V] [TRT] Block size 16588800
[08/10/2023-11:18:39] [V] [TRT] Block size 4
[08/10/2023-11:18:39] [V] [TRT] Block size 4
[08/10/2023-11:18:39] [V] [TRT] Block size 4147200
[08/10/2023-11:18:39] [V] [TRT] Block size 33177600
[08/10/2023-11:18:39] [V] [TRT] Block size 518656
[08/10/2023-11:18:39] [V] [TRT] Block size 518656
[08/10/2023-11:18:39] [V] [TRT] Block size 518656
[08/10/2023-11:18:39] [V] [TRT] Block size 518656
[08/10/2023-11:18:39] [V] [TRT] Block size 518656
[08/10/2023-11:18:39] [V] [TRT] Block size 518656
[08/10/2023-11:18:39] [V] [TRT] Block size 518656
[08/10/2023-11:18:39] [V] [TRT] Block size 518656
[08/10/2023-11:18:39] [V] [TRT] Block size 33177600
[08/10/2023-11:18:39] [V] [TRT] Block size 518656
[08/10/2023-11:18:39] [V] [TRT] Block size 518656
[08/10/2023-11:18:39] [V] [TRT] Block size 518656
[08/10/2023-11:18:39] [V] [TRT] Block size 518656
[08/10/2023-11:18:39] [V] [TRT] Block size 518656
[08/10/2023-11:18:39] [V] [TRT] Block size 518656
[08/10/2023-11:18:39] [V] [TRT] Block size 518656
[08/10/2023-11:18:39] [V] [TRT] Block size 518656
[08/10/2023-11:18:39] [V] [TRT] Block size 33177600
[08/10/2023-11:18:39] [V] [TRT] Block size 518656
[08/10/2023-11:18:39] [V] [TRT] Block size 518656
[08/10/2023-11:18:39] [V] [TRT] Block size 518656
[08/10/2023-11:18:39] [V] [TRT] Block size 518656
[08/10/2023-11:18:39] [V] [TRT] Block size 518656
[08/10/2023-11:18:39] [V] [TRT] Block size 518656
[08/10/2023-11:18:39] [V] [TRT] Block size 518656
[08/10/2023-11:18:39] [V] [TRT] Block size 518656
[08/10/2023-11:18:39] [V] [TRT] Block size 33177600
[08/10/2023-11:18:39] [V] [TRT] Block size 518656
[08/10/2023-11:18:39] [V] [TRT] Block size 518656
[08/10/2023-11:18:39] [V] [TRT] Block size 518656
[08/10/2023-11:18:39] [V] [TRT] Block size 518656
[08/10/2023-11:18:39] [V] [TRT] Block size 518656
[08/10/2023-11:18:39] [V] [TRT] Block size 518656
[08/10/2023-11:18:39] [V] [TRT] Block size 518656
[08/10/2023-11:18:39] [V] [TRT] Block size 518656
[08/10/2023-11:18:39] [V] [TRT] Block size 4
[08/10/2023-11:18:39] [V] [TRT] Block size 518656
[08/10/2023-11:18:39] [V] [TRT] Block size 518656
[08/10/2023-11:18:39] [V] [TRT] Block size 518656
[08/10/2023-11:18:39] [V] [TRT] Block size 518656
[08/10/2023-11:18:39] [V] [TRT] Block size 4294967296
[08/10/2023-11:18:39] [V] [TRT] Total Activation Memory: 4608090144
[08/10/2023-11:18:39] [I] [TRT] Detected 1 inputs and 36 output network tensors.
[08/10/2023-11:18:39] [W] [TRT] Weights [name=Conv_15 + Relu_16.weight] had the following issues when converted to FP16:
[08/10/2023-11:18:39] [W] [TRT]  - Subnormal FP16 values detected. 
[08/10/2023-11:18:39] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.
[08/10/2023-11:18:39] [V] [TRT] Conv_15 + Relu_16 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x128x32_stage4_warpsize2x2x1_g1_tensor16x8x16 Tactic: 0xdfa020ef435ef810
[08/10/2023-11:18:39] [W] [TRT] Weights [name=Conv_17 + Relu_18.weight] had the following issues when converted to FP16:
[08/10/2023-11:18:39] [W] [TRT]  - Subnormal FP16 values detected. 
[08/10/2023-11:18:39] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.
[08/10/2023-11:18:39] [V] [TRT] Conv_17 + Relu_18 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x128x32_stage4_warpsize2x2x1_g1_tensor16x8x16_t1r3s3 Tactic: 0x60c3421152ef8e10
[08/10/2023-11:18:39] [W] [TRT] Weights [name=Conv_19 + Relu_20.weight] had the following issues when converted to FP16:
[08/10/2023-11:18:39] [W] [TRT]  - Subnormal FP16 values detected. 
[08/10/2023-11:18:39] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.
[08/10/2023-11:18:39] [V] [TRT] Conv_19 + Relu_20 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x128x32_stage4_warpsize2x2x1_g1_tensor16x8x16_t1r3s3 Tactic: 0x60c3421152ef8e10
[08/10/2023-11:18:39] [W] [TRT] Weights [name=Conv_21 + Relu_22.weight] had the following issues when converted to FP16:
[08/10/2023-11:18:39] [W] [TRT]  - Subnormal FP16 values detected. 
[08/10/2023-11:18:39] [W] [TRT]  - Values less than smallest positive FP16 Subnormal value detected. Converting to FP16 minimum subnormalized value. 
[08/10/2023-11:18:39] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.
[08/10/2023-11:18:39] [V] [TRT] Conv_21 + Relu_22 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x128x32_stage4_warpsize2x2x1_g1_tensor16x8x16_t1r3s3 Tactic: 0x60c3421152ef8e10
[08/10/2023-11:18:39] [W] [TRT] Weights [name=Conv_23 + Relu_24.weight] had the following issues when converted to FP16:
[08/10/2023-11:18:39] [W] [TRT]  - Subnormal FP16 values detected. 
[08/10/2023-11:18:39] [W] [TRT]  - Values less than smallest positive FP16 Subnormal value detected. Converting to FP16 minimum subnormalized value. 
[08/10/2023-11:18:39] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.
[08/10/2023-11:18:39] [V] [TRT] Conv_23 + Relu_24 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x128x32_stage4_warpsize2x2x1_g1_tensor16x8x16_t1r3s3 Tactic: 0x60c3421152ef8e10
[08/10/2023-11:18:39] [W] [TRT] Weights [name=Conv_25 + Relu_26.weight] had the following issues when converted to FP16:
[08/10/2023-11:18:39] [W] [TRT]  - Subnormal FP16 values detected. 
[08/10/2023-11:18:39] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.
[08/10/2023-11:18:39] [V] [TRT] Conv_25 + Relu_26 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x128x32_stage4_warpsize2x2x1_g1_tensor16x8x16_t1r3s3 Tactic: 0x60c3421152ef8e10
[08/10/2023-11:18:39] [W] [TRT] Weights [name=Conv_27 + Relu_28.weight] had the following issues when converted to FP16:
[08/10/2023-11:18:39] [W] [TRT]  - Subnormal FP16 values detected. 
[08/10/2023-11:18:39] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.
[08/10/2023-11:18:39] [V] [TRT] Conv_27 + Relu_28 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x128x32_stage4_warpsize2x2x1_g1_tensor16x8x16_t1r1s1 Tactic: 0xea50b6d3d87bf5dd
[08/10/2023-11:18:39] [W] [TRT] Weights [name=Conv_44 + Relu_45.weight] had the following issues when converted to FP16:
[08/10/2023-11:18:39] [W] [TRT]  - Subnormal FP16 values detected. 
[08/10/2023-11:18:39] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.
[08/10/2023-11:18:39] [V] [TRT] Conv_44 + Relu_45 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x128x32_stage4_warpsize2x2x1_g1_tensor16x8x16 Tactic: 0xdfa020ef435ef810
[08/10/2023-11:18:39] [W] [TRT] Weights [name=Conv_46 + Relu_47.weight] had the following issues when converted to FP16:
[08/10/2023-11:18:39] [W] [TRT]  - Subnormal FP16 values detected. 
[08/10/2023-11:18:39] [W] [TRT]  - Values less than smallest positive FP16 Subnormal value detected. Converting to FP16 minimum subnormalized value. 
[08/10/2023-11:18:39] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.
[08/10/2023-11:18:39] [V] [TRT] Conv_46 + Relu_47 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x128x32_stage4_warpsize2x2x1_g1_tensor16x8x16 Tactic: 0xdfa020ef435ef810
[08/10/2023-11:18:39] [W] [TRT] Weights [name=Conv_48 + Relu_49.weight] had the following issues when converted to FP16:
[08/10/2023-11:18:39] [W] [TRT]  - Subnormal FP16 values detected. 
[08/10/2023-11:18:39] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.
[08/10/2023-11:18:39] [V] [TRT] Conv_48 + Relu_49 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x128x32_stage4_warpsize2x2x1_g1_tensor16x8x16 Tactic: 0xdfa020ef435ef810
[08/10/2023-11:18:39] [W] [TRT] Weights [name=Conv_50 + Relu_51.weight] had the following issues when converted to FP16:
[08/10/2023-11:18:39] [W] [TRT]  - Subnormal FP16 values detected. 
[08/10/2023-11:18:39] [W] [TRT]  - Values less than smallest positive FP16 Subnormal value detected. Converting to FP16 minimum subnormalized value. 
[08/10/2023-11:18:39] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.
[08/10/2023-11:18:39] [V] [TRT] Conv_50 + Relu_51 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x128x32_stage4_warpsize2x2x1_g1_tensor16x8x16 Tactic: 0xdfa020ef435ef810
[08/10/2023-11:18:39] [W] [TRT] Weights [name=Conv_52 + Relu_53.weight] had the following issues when converted to FP16:
[08/10/2023-11:18:39] [W] [TRT]  - Subnormal FP16 values detected. 
[08/10/2023-11:18:39] [W] [TRT]  - Values less than smallest positive FP16 Subnormal value detected. Converting to FP16 minimum subnormalized value. 
[08/10/2023-11:18:39] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.
[08/10/2023-11:18:39] [V] [TRT] Conv_52 + Relu_53 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x128x32_stage4_warpsize2x2x1_g1_tensor16x8x16 Tactic: 0xdfa020ef435ef810
[08/10/2023-11:18:39] [W] [TRT] Weights [name=Conv_54 + Relu_55.weight] had the following issues when converted to FP16:
[08/10/2023-11:18:39] [W] [TRT]  - Subnormal FP16 values detected. 
[08/10/2023-11:18:39] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.
[08/10/2023-11:18:39] [V] [TRT] Conv_54 + Relu_55 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x128x32_stage4_warpsize2x2x1_g1_tensor16x8x16 Tactic: 0xdfa020ef435ef810
[08/10/2023-11:18:39] [W] [TRT] Weights [name=ConvTranspose_56 + BatchNormalization_57 + Relu_58.weight] had the following issues when converted to FP16:
[08/10/2023-11:18:39] [W] [TRT]  - Subnormal FP16 values detected. 
[08/10/2023-11:18:39] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.
[08/10/2023-11:18:39] [V] [TRT] ConvTranspose_56 + BatchNormalization_57 + Relu_58 Set Tactic Name: sm80_xmma_deconv_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x128x32_stage4_warpsize2x2x1_g1_tensor16x8x16_strided Tactic: 0x611e899dccd3003a
[08/10/2023-11:18:39] [W] [TRT] Weights [name=Conv_60 + Relu_61.weight] had the following issues when converted to FP16:
[08/10/2023-11:18:39] [W] [TRT]  - Subnormal FP16 values detected. 
[08/10/2023-11:18:39] [W] [TRT]  - Values less than smallest positive FP16 Subnormal value detected. Converting to FP16 minimum subnormalized value. 
[08/10/2023-11:18:39] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.
[08/10/2023-11:18:39] [V] [TRT] Conv_60 + Relu_61 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x64x32_stage3_warpsize4x1x1_g1_tensor16x8x16_t1r3s3 Tactic: 0x529f4431bdae94f5
[08/10/2023-11:18:39] [W] [TRT] Weights [name=Conv_62 + Relu_63 || Conv_65 + Relu_66 || Conv_68 + Relu_69 || Conv_71 + Relu_72 || Conv_74 + Relu_75 || Conv_77 + Relu_78 || Conv_80 + Relu_81 || Conv_83 + Relu_84.weight] had the following issues when converted to FP16:
[08/10/2023-11:18:39] [W] [TRT]  - Subnormal FP16 values detected. 
[08/10/2023-11:18:39] [W] [TRT]  - Values less than smallest positive FP16 Subnormal value detected. Converting to FP16 minimum subnormalized value. 
[08/10/2023-11:18:39] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.
[08/10/2023-11:18:39] [W] [TRT] Weights [name=Conv_62 + Relu_63 || Conv_65 + Relu_66 || Conv_68 + Relu_69 || Conv_71 + Relu_72 || Conv_74 + Relu_75 || Conv_77 + Relu_78 || Conv_80 + Relu_81 || Conv_83 + Relu_84.bias] had the following issues when converted to FP16:
[08/10/2023-11:18:39] [W] [TRT]  - Subnormal FP16 values detected. 
[08/10/2023-11:18:39] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.
[08/10/2023-11:18:39] [V] [TRT] Conv_62 + Relu_63 || Conv_65 + Relu_66 || Conv_68 + Relu_69 || Conv_71 + Relu_72 || Conv_74 + Relu_75 || Conv_77 + Relu_78 || Conv_80 + Relu_81 || Conv_83 + Relu_84 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x128x32_stage4_warpsize2x2x1_g1_tensor16x8x16_t1r3s3 Tactic: 0x60c3421152ef8e10
[08/10/2023-11:18:39] [V] [TRT] Conv_64 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x32x32_stage4_warpsize4x1x1_g1_tensor16x8x16_t1r3s3 Tactic: 0x3e7eb35b91b9fa63
[08/10/2023-11:18:39] [V] [TRT] Conv_67 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x32x32_stage4_warpsize4x1x1_g1_tensor16x8x16_t1r3s3 Tactic: 0x3e7eb35b91b9fa63
[08/10/2023-11:18:39] [V] [TRT] Conv_70 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x32x32_stage4_warpsize4x1x1_g1_tensor16x8x16_t1r3s3 Tactic: 0x3e7eb35b91b9fa63
[08/10/2023-11:18:39] [W] [TRT] Weights [name=Conv_73.weight] had the following issues when converted to FP16:
[08/10/2023-11:18:39] [W] [TRT]  - Subnormal FP16 values detected. 
[08/10/2023-11:18:39] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.
[08/10/2023-11:18:39] [V] [TRT] Conv_73 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x32x32_stage4_warpsize4x1x1_g1_tensor16x8x16_t1r3s3 Tactic: 0x3e7eb35b91b9fa63
[08/10/2023-11:18:39] [V] [TRT] Conv_76 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x32x32_stage4_warpsize4x1x1_g1_tensor16x8x16_t1r3s3 Tactic: 0x3e7eb35b91b9fa63
[08/10/2023-11:18:39] [V] [TRT] Conv_79 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x32x32_stage4_warpsize4x1x1_g1_tensor16x8x16_t1r3s3 Tactic: 0x3e7eb35b91b9fa63
[08/10/2023-11:18:39] [V] [TRT] Conv_82 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x32x32_stage4_warpsize4x1x1_g1_tensor16x8x16_t1r3s3 Tactic: 0x3e7eb35b91b9fa63
[08/10/2023-11:18:39] [V] [TRT] Conv_85 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x32x32_stage4_warpsize4x1x1_g1_tensor16x8x16_t1r3s3 Tactic: 0x3e7eb35b91b9fa63
[08/10/2023-11:18:39] [W] [TRT] Weights [name=Conv_86 + Relu_87 || Conv_89 + Relu_90 || Conv_92 + Relu_93 || Conv_95 + Relu_96 || Conv_98 + Relu_99 || Conv_101 + Relu_102 || Conv_104 + Relu_105 || Conv_107 + Relu_108.weight] had the following issues when converted to FP16:
[08/10/2023-11:18:39] [W] [TRT]  - Subnormal FP16 values detected. 
[08/10/2023-11:18:39] [W] [TRT]  - Values less than smallest positive FP16 Subnormal value detected. Converting to FP16 minimum subnormalized value. 
[08/10/2023-11:18:39] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.
[08/10/2023-11:18:39] [V] [TRT] Conv_86 + Relu_87 || Conv_89 + Relu_90 || Conv_92 + Relu_93 || Conv_95 + Relu_96 || Conv_98 + Relu_99 || Conv_101 + Relu_102 || Conv_104 + Relu_105 || Conv_107 + Relu_108 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x128x32_stage4_warpsize2x2x1_g1_tensor16x8x16_t1r3s3 Tactic: 0x60c3421152ef8e10
[08/10/2023-11:18:39] [W] [TRT] Weights [name=Conv_88.weight] had the following issues when converted to FP16:
[08/10/2023-11:18:39] [W] [TRT]  - Subnormal FP16 values detected. 
[08/10/2023-11:18:39] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.
[08/10/2023-11:18:39] [V] [TRT] Conv_88 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x32x32_stage4_warpsize4x1x1_g1_tensor16x8x16_t1r3s3 Tactic: 0x3e7eb35b91b9fa63
[08/10/2023-11:18:39] [V] [TRT] Conv_91 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x32x32_stage4_warpsize4x1x1_g1_tensor16x8x16_t1r3s3 Tactic: 0x3e7eb35b91b9fa63
[08/10/2023-11:18:39] [V] [TRT] Conv_94 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x32x32_stage4_warpsize4x1x1_g1_tensor16x8x16_t1r3s3 Tactic: 0x3e7eb35b91b9fa63
[08/10/2023-11:18:39] [V] [TRT] Conv_97 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x32x32_stage4_warpsize4x1x1_g1_tensor16x8x16_t1r3s3 Tactic: 0x3e7eb35b91b9fa63
[08/10/2023-11:18:39] [W] [TRT] Weights [name=Conv_100.weight] had the following issues when converted to FP16:
[08/10/2023-11:18:39] [W] [TRT]  - Subnormal FP16 values detected. 
[08/10/2023-11:18:39] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.
[08/10/2023-11:18:39] [V] [TRT] Conv_100 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x32x32_stage4_warpsize4x1x1_g1_tensor16x8x16_t1r3s3 Tactic: 0x3e7eb35b91b9fa63
[08/10/2023-11:18:39] [V] [TRT] Conv_103 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x32x32_stage4_warpsize4x1x1_g1_tensor16x8x16_t1r3s3 Tactic: 0x3e7eb35b91b9fa63
[08/10/2023-11:18:39] [V] [TRT] Conv_106 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x32x32_stage4_warpsize4x1x1_g1_tensor16x8x16_t1r3s3 Tactic: 0x3e7eb35b91b9fa63
[08/10/2023-11:18:39] [V] [TRT] Conv_109 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x32x32_stage4_warpsize4x1x1_g1_tensor16x8x16_t1r3s3 Tactic: 0x3e7eb35b91b9fa63
[08/10/2023-11:18:39] [W] [TRT] Weights [name=Conv_110 + Relu_111 || Conv_113 + Relu_114 || Conv_116 + Relu_117 || Conv_119 + Relu_120 || Conv_122 + Relu_123 || Conv_125 + Relu_126 || Conv_128 + Relu_129 || Conv_131 + Relu_132.weight] had the following issues when converted to FP16:
[08/10/2023-11:18:39] [W] [TRT]  - Subnormal FP16 values detected. 
[08/10/2023-11:18:39] [W] [TRT]  - Values less than smallest positive FP16 Subnormal value detected. Converting to FP16 minimum subnormalized value. 
[08/10/2023-11:18:39] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.
[08/10/2023-11:18:39] [V] [TRT] Conv_110 + Relu_111 || Conv_113 + Relu_114 || Conv_116 + Relu_117 || Conv_119 + Relu_120 || Conv_122 + Relu_123 || Conv_125 + Relu_126 || Conv_128 + Relu_129 || Conv_131 + Relu_132 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x128x32_stage4_warpsize2x2x1_g1_tensor16x8x16_t1r3s3 Tactic: 0x60c3421152ef8e10
[08/10/2023-11:18:39] [V] [TRT] Conv_112 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x32x32_stage4_warpsize4x1x1_g1_tensor16x8x16_t1r3s3 Tactic: 0x3e7eb35b91b9fa63
[08/10/2023-11:18:39] [V] [TRT] Conv_115 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x32x32_stage4_warpsize4x1x1_g1_tensor16x8x16_t1r3s3 Tactic: 0x3e7eb35b91b9fa63
[08/10/2023-11:18:39] [V] [TRT] Conv_118 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x32x32_stage4_warpsize4x1x1_g1_tensor16x8x16_t1r3s3 Tactic: 0x3e7eb35b91b9fa63
[08/10/2023-11:18:39] [V] [TRT] Conv_121 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x32x32_stage4_warpsize4x1x1_g1_tensor16x8x16_t1r3s3 Tactic: 0x3e7eb35b91b9fa63
[08/10/2023-11:18:39] [W] [TRT] Weights [name=Conv_124.weight] had the following issues when converted to FP16:
[08/10/2023-11:18:39] [W] [TRT]  - Subnormal FP16 values detected. 
[08/10/2023-11:18:39] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.
[08/10/2023-11:18:39] [V] [TRT] Conv_124 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x32x32_stage4_warpsize4x1x1_g1_tensor16x8x16_t1r3s3 Tactic: 0x3e7eb35b91b9fa63
[08/10/2023-11:18:39] [W] [TRT] Weights [name=Conv_127.weight] had the following issues when converted to FP16:
[08/10/2023-11:18:39] [W] [TRT]  - Subnormal FP16 values detected. 
[08/10/2023-11:18:39] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.
[08/10/2023-11:18:39] [V] [TRT] Conv_127 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x32x32_stage4_warpsize4x1x1_g1_tensor16x8x16_t1r3s3 Tactic: 0x3e7eb35b91b9fa63
[08/10/2023-11:18:39] [V] [TRT] Conv_130 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x32x32_stage4_warpsize4x1x1_g1_tensor16x8x16_t1r3s3 Tactic: 0x3e7eb35b91b9fa63
[08/10/2023-11:18:39] [V] [TRT] Conv_133 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x32x32_stage4_warpsize4x1x1_g1_tensor16x8x16_t1r3s3 Tactic: 0x3e7eb35b91b9fa63
[08/10/2023-11:18:39] [W] [TRT] Weights [name=Conv_134 + Relu_135 || Conv_137 + Relu_138 || Conv_140 + Relu_141 || Conv_143 + Relu_144 || Conv_146 + Relu_147 || Conv_149 + Relu_150 || Conv_152 + Relu_153 || Conv_155 + Relu_156.weight] had the following issues when converted to FP16:
[08/10/2023-11:18:39] [W] [TRT]  - Subnormal FP16 values detected. 
[08/10/2023-11:18:39] [W] [TRT]  - Values less than smallest positive FP16 Subnormal value detected. Converting to FP16 minimum subnormalized value. 
[08/10/2023-11:18:39] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.
[08/10/2023-11:18:39] [V] [TRT] Conv_134 + Relu_135 || Conv_137 + Relu_138 || Conv_140 + Relu_141 || Conv_143 + Relu_144 || Conv_146 + Relu_147 || Conv_149 + Relu_150 || Conv_152 + Relu_153 || Conv_155 + Relu_156 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x128x32_stage4_warpsize2x2x1_g1_tensor16x8x16_t1r3s3 Tactic: 0x60c3421152ef8e10
[08/10/2023-11:18:39] [V] [TRT] Conv_136 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x32x32_stage4_warpsize4x1x1_g1_tensor16x8x16_t1r3s3 Tactic: 0x3e7eb35b91b9fa63
[08/10/2023-11:18:39] [V] [TRT] Conv_139 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x32x32_stage4_warpsize4x1x1_g1_tensor16x8x16_t1r3s3 Tactic: 0x3e7eb35b91b9fa63
[08/10/2023-11:18:39] [V] [TRT] Conv_142 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x32x32_stage4_warpsize4x1x1_g1_tensor16x8x16_t1r3s3 Tactic: 0x3e7eb35b91b9fa63
[08/10/2023-11:18:39] [W] [TRT] Weights [name=Conv_145.weight] had the following issues when converted to FP16:
[08/10/2023-11:18:39] [W] [TRT]  - Subnormal FP16 values detected. 
[08/10/2023-11:18:39] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.
[08/10/2023-11:18:39] [V] [TRT] Conv_145 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x32x32_stage4_warpsize4x1x1_g1_tensor16x8x16_t1r3s3 Tactic: 0x3e7eb35b91b9fa63
[08/10/2023-11:18:39] [V] [TRT] Conv_148 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x32x32_stage4_warpsize4x1x1_g1_tensor16x8x16_t1r3s3 Tactic: 0x3e7eb35b91b9fa63
[08/10/2023-11:18:39] [V] [TRT] Conv_151 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x32x32_stage4_warpsize4x1x1_g1_tensor16x8x16_t1r3s3 Tactic: 0x3e7eb35b91b9fa63
[08/10/2023-11:18:39] [V] [TRT] Conv_154 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x32x32_stage4_warpsize4x1x1_g1_tensor16x8x16_t1r3s3 Tactic: 0x3e7eb35b91b9fa63
[08/10/2023-11:18:39] [V] [TRT] Conv_157 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x32x32_stage4_warpsize4x1x1_g1_tensor16x8x16_t1r3s3 Tactic: 0x3e7eb35b91b9fa63
[08/10/2023-11:18:39] [W] [TRT] Weights [name=Conv_158 + Relu_159 || Conv_161 + Relu_162 || Conv_164 + Relu_165 || Conv_167 + Relu_168.weight] had the following issues when converted to FP16:
[08/10/2023-11:18:39] [W] [TRT]  - Subnormal FP16 values detected. 
[08/10/2023-11:18:39] [W] [TRT]  - Values less than smallest positive FP16 Subnormal value detected. Converting to FP16 minimum subnormalized value. 
[08/10/2023-11:18:39] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.
[08/10/2023-11:18:39] [V] [TRT] Conv_158 + Relu_159 || Conv_161 + Relu_162 || Conv_164 + Relu_165 || Conv_167 + Relu_168 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x128x32_stage4_warpsize2x2x1_g1_tensor16x8x16_t1r3s3 Tactic: 0x60c3421152ef8e10
[08/10/2023-11:18:39] [V] [TRT] Conv_160 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x32x32_stage4_warpsize4x1x1_g1_tensor16x8x16_t1r3s3 Tactic: 0x3e7eb35b91b9fa63
[08/10/2023-11:18:39] [W] [TRT] Weights [name=Conv_163.weight] had the following issues when converted to FP16:
[08/10/2023-11:18:39] [W] [TRT]  - Subnormal FP16 values detected. 
[08/10/2023-11:18:39] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.
[08/10/2023-11:18:39] [V] [TRT] Conv_163 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x32x32_stage4_warpsize4x1x1_g1_tensor16x8x16_t1r3s3 Tactic: 0x3e7eb35b91b9fa63
[08/10/2023-11:18:39] [V] [TRT] Conv_166 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x32x32_stage4_warpsize4x1x1_g1_tensor16x8x16_t1r3s3 Tactic: 0x3e7eb35b91b9fa63
[08/10/2023-11:18:39] [V] [TRT] Conv_169 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x32x32_stage4_warpsize4x1x1_g1_tensor16x8x16_t1r3s3 Tactic: 0x3e7eb35b91b9fa63
[08/10/2023-11:18:39] [V] [TRT] Layer: Reformatting CopyNode for Input Tensor 0 to Conv_15 + Relu_16 Host Persistent: 0 Device Persistent: 0 Scratch Memory: 0
[08/10/2023-11:18:39] [V] [TRT] Layer: Conv_15 + Relu_16 Host Persistent: 2784 Device Persistent: 0 Scratch Memory: 0
[08/10/2023-11:18:39] [V] [TRT] Layer: Conv_17 + Relu_18 Host Persistent: 2784 Device Persistent: 0 Scratch Memory: 0
[08/10/2023-11:18:39] [V] [TRT] Layer: Conv_19 + Relu_20 Host Persistent: 2784 Device Persistent: 0 Scratch Memory: 0
[08/10/2023-11:18:39] [V] [TRT] Layer: Conv_21 + Relu_22 Host Persistent: 2784 Device Persistent: 0 Scratch Memory: 0
[08/10/2023-11:18:39] [V] [TRT] Layer: Conv_23 + Relu_24 Host Persistent: 2784 Device Persistent: 0 Scratch Memory: 0
[08/10/2023-11:18:39] [V] [TRT] Layer: Conv_25 + Relu_26 Host Persistent: 2784 Device Persistent: 0 Scratch Memory: 0
[08/10/2023-11:18:39] [V] [TRT] Layer: Conv_27 + Relu_28 Host Persistent: 2784 Device Persistent: 0 Scratch Memory: 0
[08/10/2023-11:18:39] [V] [TRT] Layer: Reformatting CopyNode for Input Tensor 0 to Conv_44 + Relu_45 Host Persistent: 0 Device Persistent: 0 Scratch Memory: 0
[08/10/2023-11:18:39] [V] [TRT] Layer: Conv_44 + Relu_45 Host Persistent: 2784 Device Persistent: 0 Scratch Memory: 0
[08/10/2023-11:18:39] [V] [TRT] Layer: Conv_46 + Relu_47 Host Persistent: 2784 Device Persistent: 0 Scratch Memory: 0
[08/10/2023-11:18:39] [V] [TRT] Layer: Conv_48 + Relu_49 Host Persistent: 2784 Device Persistent: 0 Scratch Memory: 0
[08/10/2023-11:18:39] [V] [TRT] Layer: Conv_50 + Relu_51 Host Persistent: 2784 Device Persistent: 0 Scratch Memory: 0
[08/10/2023-11:18:39] [V] [TRT] Layer: Conv_52 + Relu_53 Host Persistent: 2784 Device Persistent: 0 Scratch Memory: 0
[08/10/2023-11:18:39] [V] [TRT] Layer: Conv_54 + Relu_55 Host Persistent: 2784 Device Persistent: 0 Scratch Memory: 0
[08/10/2023-11:18:39] [V] [TRT] Layer: Reformatting CopyNode for Input Tensor 0 to ConvTranspose_56 + BatchNormalization_57 + Relu_58 Host Persistent: 0 Device Persistent: 0 Scratch Memory: 0
[08/10/2023-11:18:39] [V] [TRT] Layer: ConvTranspose_56 + BatchNormalization_57 + Relu_58 Host Persistent: 2016 Device Persistent: 134144 Scratch Memory: 0
[08/10/2023-11:18:39] [V] [TRT] Layer: onnx::Concat_647 copy Host Persistent: 0 Device Persistent: 0 Scratch Memory: 0
[08/10/2023-11:18:39] [V] [TRT] Layer: Conv_60 + Relu_61 Host Persistent: 2784 Device Persistent: 0 Scratch Memory: 0
[08/10/2023-11:18:39] [V] [TRT] Layer: Reformatting CopyNode for Output Tensor 0 to Conv_60 + Relu_61 Host Persistent: 0 Device Persistent: 0 Scratch Memory: 0
[08/10/2023-11:18:39] [V] [TRT] Layer: Conv_62 + Relu_63 || Conv_65 + Relu_66 || Conv_68 + Relu_69 || Conv_71 + Relu_72 || Conv_74 + Relu_75 || Conv_77 + Relu_78 || Conv_80 + Relu_81 || Conv_83 + Relu_84 Host Persistent: 2784 Device Persistent: 0 Scratch Memory: 0
[08/10/2023-11:18:39] [V] [TRT] Layer: Reformatting CopyNode for Output Tensor 0 to Conv_62 + Relu_63 || Conv_65 + Relu_66 || Conv_68 + Relu_69 || Conv_71 + Relu_72 || Conv_74 + Relu_75 || Conv_77 + Relu_78 || Conv_80 + Relu_81 || Conv_83 + Relu_84 Host Persistent: 0 Device Persistent: 0 Scratch Memory: 0
[08/10/2023-11:18:39] [V] [TRT] Layer: Conv_64 Host Persistent: 2784 Device Persistent: 0 Scratch Memory: 0
[08/10/2023-11:18:39] [V] [TRT] Layer: Reformatting CopyNode for Output Tensor 0 to Conv_64 Host Persistent: 0 Device Persistent: 0 Scratch Memory: 0
[08/10/2023-11:18:39] [V] [TRT] Layer: Conv_67 Host Persistent: 2784 Device Persistent: 0 Scratch Memory: 0
[08/10/2023-11:18:39] [V] [TRT] Layer: Reformatting CopyNode for Output Tensor 0 to Conv_67 Host Persistent: 0 Device Persistent: 0 Scratch Memory: 0
[08/10/2023-11:18:39] [V] [TRT] Layer: Conv_70 Host Persistent: 2784 Device Persistent: 0 Scratch Memory: 0
[08/10/2023-11:18:39] [V] [TRT] Layer: Reformatting CopyNode for Output Tensor 0 to Conv_70 Host Persistent: 0 Device Persistent: 0 Scratch Memory: 0
[08/10/2023-11:18:39] [V] [TRT] Layer: Conv_73 Host Persistent: 2784 Device Persistent: 0 Scratch Memory: 0
[08/10/2023-11:18:39] [V] [TRT] Layer: Reformatting CopyNode for Output Tensor 0 to Conv_73 Host Persistent: 0 Device Persistent: 0 Scratch Memory: 0
[08/10/2023-11:18:39] [V] [TRT] Layer: Conv_76 Host Persistent: 2784 Device Persistent: 0 Scratch Memory: 0
[08/10/2023-11:18:39] [V] [TRT] Layer: Reformatting CopyNode for Output Tensor 0 to Conv_76 Host Persistent: 0 Device Persistent: 0 Scratch Memory: 0
[08/10/2023-11:18:39] [V] [TRT] Layer: Conv_79 Host Persistent: 2784 Device Persistent: 0 Scratch Memory: 0
[08/10/2023-11:18:39] [V] [TRT] Layer: Reformatting CopyNode for Output Tensor 0 to Conv_79 Host Persistent: 0 Device Persistent: 0 Scratch Memory: 0
[08/10/2023-11:18:39] [V] [TRT] Layer: Conv_82 Host Persistent: 2784 Device Persistent: 0 Scratch Memory: 0
[08/10/2023-11:18:39] [V] [TRT] Layer: Reformatting CopyNode for Output Tensor 0 to Conv_82 Host Persistent: 0 Device Persistent: 0 Scratch Memory: 0
[08/10/2023-11:18:39] [V] [TRT] Layer: Conv_85 Host Persistent: 2784 Device Persistent: 0 Scratch Memory: 0
[08/10/2023-11:18:39] [V] [TRT] Layer: Reformatting CopyNode for Output Tensor 0 to Conv_85 Host Persistent: 0 Device Persistent: 0 Scratch Memory: 0
[08/10/2023-11:18:39] [V] [TRT] Layer: Conv_86 + Relu_87 || Conv_89 + Relu_90 || Conv_92 + Relu_93 || Conv_95 + Relu_96 || Conv_98 + Relu_99 || Conv_101 + Relu_102 || Conv_104 + Relu_105 || Conv_107 + Relu_108 Host Persistent: 2784 Device Persistent: 0 Scratch Memory: 0
[08/10/2023-11:18:39] [V] [TRT] Layer: Reformatting CopyNode for Output Tensor 0 to Conv_86 + Relu_87 || Conv_89 + Relu_90 || Conv_92 + Relu_93 || Conv_95 + Relu_96 || Conv_98 + Relu_99 || Conv_101 + Relu_102 || Conv_104 + Relu_105 || Conv_107 + Relu_108 Host Persistent: 0 Device Persistent: 0 Scratch Memory: 0
[08/10/2023-11:18:39] [V] [TRT] Layer: Conv_88 Host Persistent: 2784 Device Persistent: 0 Scratch Memory: 0
[08/10/2023-11:18:39] [V] [TRT] Layer: Reformatting CopyNode for Output Tensor 0 to Conv_88 Host Persistent: 0 Device Persistent: 0 Scratch Memory: 0
[08/10/2023-11:18:39] [V] [TRT] Layer: Conv_91 Host Persistent: 2784 Device Persistent: 0 Scratch Memory: 0
[08/10/2023-11:18:39] [V] [TRT] Layer: Reformatting CopyNode for Output Tensor 0 to Conv_91 Host Persistent: 0 Device Persistent: 0 Scratch Memory: 0
[08/10/2023-11:18:39] [V] [TRT] Layer: Conv_94 Host Persistent: 2784 Device Persistent: 0 Scratch Memory: 0
[08/10/2023-11:18:39] [V] [TRT] Layer: Reformatting CopyNode for Output Tensor 0 to Conv_94 Host Persistent: 0 Device Persistent: 0 Scratch Memory: 0
[08/10/2023-11:18:39] [V] [TRT] Layer: Conv_97 Host Persistent: 2784 Device Persistent: 0 Scratch Memory: 0
[08/10/2023-11:18:39] [V] [TRT] Layer: Reformatting CopyNode for Output Tensor 0 to Conv_97 Host Persistent: 0 Device Persistent: 0 Scratch Memory: 0
[08/10/2023-11:18:39] [V] [TRT] Layer: Conv_100 Host Persistent: 2784 Device Persistent: 0 Scratch Memory: 0
[08/10/2023-11:18:39] [V] [TRT] Layer: Reformatting CopyNode for Output Tensor 0 to Conv_100 Host Persistent: 0 Device Persistent: 0 Scratch Memory: 0
[08/10/2023-11:18:39] [V] [TRT] Layer: Conv_103 Host Persistent: 2784 Device Persistent: 0 Scratch Memory: 0
[08/10/2023-11:18:39] [V] [TRT] Layer: Reformatting CopyNode for Output Tensor 0 to Conv_103 Host Persistent: 0 Device Persistent: 0 Scratch Memory: 0
[08/10/2023-11:18:39] [V] [TRT] Layer: Conv_106 Host Persistent: 2784 Device Persistent: 0 Scratch Memory: 0
[08/10/2023-11:18:39] [V] [TRT] Layer: Reformatting CopyNode for Output Tensor 0 to Conv_106 Host Persistent: 0 Device Persistent: 0 Scratch Memory: 0
[08/10/2023-11:18:39] [V] [TRT] Layer: Conv_109 Host Persistent: 2784 Device Persistent: 0 Scratch Memory: 0
[08/10/2023-11:18:39] [V] [TRT] Layer: Reformatting CopyNode for Output Tensor 0 to Conv_109 Host Persistent: 0 Device Persistent: 0 Scratch Memory: 0
[08/10/2023-11:18:39] [V] [TRT] Layer: Conv_110 + Relu_111 || Conv_113 + Relu_114 || Conv_116 + Relu_117 || Conv_119 + Relu_120 || Conv_122 + Relu_123 || Conv_125 + Relu_126 || Conv_128 + Relu_129 || Conv_131 + Relu_132 Host Persistent: 2784 Device Persistent: 0 Scratch Memory: 0
[08/10/2023-11:18:39] [V] [TRT] Layer: Reformatting CopyNode for Output Tensor 0 to Conv_110 + Relu_111 || Conv_113 + Relu_114 || Conv_116 + Relu_117 || Conv_119 + Relu_120 || Conv_122 + Relu_123 || Conv_125 + Relu_126 || Conv_128 + Relu_129 || Conv_131 + Relu_132 Host Persistent: 0 Device Persistent: 0 Scratch Memory: 0
[08/10/2023-11:18:39] [V] [TRT] Layer: Conv_112 Host Persistent: 2784 Device Persistent: 0 Scratch Memory: 0
[08/10/2023-11:18:39] [V] [TRT] Layer: Reformatting CopyNode for Output Tensor 0 to Conv_112 Host Persistent: 0 Device Persistent: 0 Scratch Memory: 0
[08/10/2023-11:18:39] [V] [TRT] Layer: Conv_115 Host Persistent: 2784 Device Persistent: 0 Scratch Memory: 0
[08/10/2023-11:18:39] [V] [TRT] Layer: Reformatting CopyNode for Output Tensor 0 to Conv_115 Host Persistent: 0 Device Persistent: 0 Scratch Memory: 0
[08/10/2023-11:18:39] [V] [TRT] Layer: Conv_118 Host Persistent: 2784 Device Persistent: 0 Scratch Memory: 0
[08/10/2023-11:18:39] [V] [TRT] Layer: Reformatting CopyNode for Output Tensor 0 to Conv_118 Host Persistent: 0 Device Persistent: 0 Scratch Memory: 0
[08/10/2023-11:18:39] [V] [TRT] Layer: Conv_121 Host Persistent: 2784 Device Persistent: 0 Scratch Memory: 0
[08/10/2023-11:18:39] [V] [TRT] Layer: Reformatting CopyNode for Output Tensor 0 to Conv_121 Host Persistent: 0 Device Persistent: 0 Scratch Memory: 0
[08/10/2023-11:18:39] [V] [TRT] Layer: Conv_124 Host Persistent: 2784 Device Persistent: 0 Scratch Memory: 0
[08/10/2023-11:18:39] [V] [TRT] Layer: Reformatting CopyNode for Output Tensor 0 to Conv_124 Host Persistent: 0 Device Persistent: 0 Scratch Memory: 0
[08/10/2023-11:18:39] [V] [TRT] Layer: Conv_127 Host Persistent: 2784 Device Persistent: 0 Scratch Memory: 0
[08/10/2023-11:18:39] [V] [TRT] Layer: Reformatting CopyNode for Output Tensor 0 to Conv_127 Host Persistent: 0 Device Persistent: 0 Scratch Memory: 0
[08/10/2023-11:18:39] [V] [TRT] Layer: Conv_130 Host Persistent: 2784 Device Persistent: 0 Scratch Memory: 0
[08/10/2023-11:18:39] [V] [TRT] Layer: Reformatting CopyNode for Output Tensor 0 to Conv_130 Host Persistent: 0 Device Persistent: 0 Scratch Memory: 0
[08/10/2023-11:18:39] [V] [TRT] Layer: Conv_133 Host Persistent: 2784 Device Persistent: 0 Scratch Memory: 0
[08/10/2023-11:18:39] [V] [TRT] Layer: Reformatting CopyNode for Output Tensor 0 to Conv_133 Host Persistent: 0 Device Persistent: 0 Scratch Memory: 0
[08/10/2023-11:18:39] [V] [TRT] Layer: Conv_134 + Relu_135 || Conv_137 + Relu_138 || Conv_140 + Relu_141 || Conv_143 + Relu_144 || Conv_146 + Relu_147 || Conv_149 + Relu_150 || Conv_152 + Relu_153 || Conv_155 + Relu_156 Host Persistent: 2784 Device Persistent: 0 Scratch Memory: 0
[08/10/2023-11:18:39] [V] [TRT] Layer: Reformatting CopyNode for Output Tensor 0 to Conv_134 + Relu_135 || Conv_137 + Relu_138 || Conv_140 + Relu_141 || Conv_143 + Relu_144 || Conv_146 + Relu_147 || Conv_149 + Relu_150 || Conv_152 + Relu_153 || Conv_155 + Relu_156 Host Persistent: 0 Device Persistent: 0 Scratch Memory: 0
[08/10/2023-11:18:39] [V] [TRT] Layer: Conv_136 Host Persistent: 2784 Device Persistent: 0 Scratch Memory: 0
[08/10/2023-11:18:39] [V] [TRT] Layer: Reformatting CopyNode for Output Tensor 0 to Conv_136 Host Persistent: 0 Device Persistent: 0 Scratch Memory: 0
[08/10/2023-11:18:39] [V] [TRT] Layer: Conv_139 Host Persistent: 2784 Device Persistent: 0 Scratch Memory: 0
[08/10/2023-11:18:39] [V] [TRT] Layer: Reformatting CopyNode for Output Tensor 0 to Conv_139 Host Persistent: 0 Device Persistent: 0 Scratch Memory: 0
[08/10/2023-11:18:39] [V] [TRT] Layer: Conv_142 Host Persistent: 2784 Device Persistent: 0 Scratch Memory: 0
[08/10/2023-11:18:39] [V] [TRT] Layer: Reformatting CopyNode for Output Tensor 0 to Conv_142 Host Persistent: 0 Device Persistent: 0 Scratch Memory: 0
[08/10/2023-11:18:39] [V] [TRT] Layer: Conv_145 Host Persistent: 2784 Device Persistent: 0 Scratch Memory: 0
[08/10/2023-11:18:39] [V] [TRT] Layer: Reformatting CopyNode for Output Tensor 0 to Conv_145 Host Persistent: 0 Device Persistent: 0 Scratch Memory: 0
[08/10/2023-11:18:39] [V] [TRT] Layer: Conv_148 Host Persistent: 2784 Device Persistent: 0 Scratch Memory: 0
[08/10/2023-11:18:39] [V] [TRT] Layer: Reformatting CopyNode for Output Tensor 0 to Conv_148 Host Persistent: 0 Device Persistent: 0 Scratch Memory: 0
[08/10/2023-11:18:39] [V] [TRT] Layer: Conv_151 Host Persistent: 2784 Device Persistent: 0 Scratch Memory: 0
[08/10/2023-11:18:39] [V] [TRT] Layer: Reformatting CopyNode for Output Tensor 0 to Conv_151 Host Persistent: 0 Device Persistent: 0 Scratch Memory: 0
[08/10/2023-11:18:39] [V] [TRT] Layer: Conv_154 Host Persistent: 2784 Device Persistent: 0 Scratch Memory: 0
[08/10/2023-11:18:39] [V] [TRT] Layer: Reformatting CopyNode for Output Tensor 0 to Conv_154 Host Persistent: 0 Device Persistent: 0 Scratch Memory: 0
[08/10/2023-11:18:39] [V] [TRT] Layer: Conv_157 Host Persistent: 2784 Device Persistent: 0 Scratch Memory: 0
[08/10/2023-11:18:39] [V] [TRT] Layer: Reformatting CopyNode for Output Tensor 0 to Conv_157 Host Persistent: 0 Device Persistent: 0 Scratch Memory: 0
[08/10/2023-11:18:39] [V] [TRT] Layer: Reformatting CopyNode for Input Tensor 0 to Conv_158 + Relu_159 || Conv_161 + Relu_162 || Conv_164 + Relu_165 || Conv_167 + Relu_168 Host Persistent: 0 Device Persistent: 0 Scratch Memory: 0
[08/10/2023-11:18:39] [V] [TRT] Layer: Conv_158 + Relu_159 || Conv_161 + Relu_162 || Conv_164 + Relu_165 || Conv_167 + Relu_168 Host Persistent: 2784 Device Persistent: 0 Scratch Memory: 0
[08/10/2023-11:18:39] [V] [TRT] Layer: Conv_160 Host Persistent: 2784 Device Persistent: 0 Scratch Memory: 0
[08/10/2023-11:18:39] [V] [TRT] Layer: Reformatting CopyNode for Output Tensor 0 to Conv_160 Host Persistent: 0 Device Persistent: 0 Scratch Memory: 0
[08/10/2023-11:18:39] [V] [TRT] Layer: Conv_163 Host Persistent: 2784 Device Persistent: 0 Scratch Memory: 0
[08/10/2023-11:18:39] [V] [TRT] Layer: Reformatting CopyNode for Output Tensor 0 to Conv_163 Host Persistent: 0 Device Persistent: 0 Scratch Memory: 0
[08/10/2023-11:18:39] [V] [TRT] Layer: Conv_166 Host Persistent: 2784 Device Persistent: 0 Scratch Memory: 0
[08/10/2023-11:18:39] [V] [TRT] Layer: Reformatting CopyNode for Output Tensor 0 to Conv_166 Host Persistent: 0 Device Persistent: 0 Scratch Memory: 0
[08/10/2023-11:18:39] [V] [TRT] Layer: Conv_169 Host Persistent: 2784 Device Persistent: 0 Scratch Memory: 0
[08/10/2023-11:18:39] [V] [TRT] Layer: Reformatting CopyNode for Output Tensor 0 to Conv_169 Host Persistent: 0 Device Persistent: 0 Scratch Memory: 0
[08/10/2023-11:18:39] [I] [TRT] Total Host Persistent Memory: 155136
[08/10/2023-11:18:39] [I] [TRT] Total Device Persistent Memory: 134144
[08/10/2023-11:18:39] [I] [TRT] Total Scratch Memory: 0
[08/10/2023-11:18:39] [I] [TRT] [MemUsageStats] Peak memory usage of TRT CPU/GPU memory allocators: CPU 18 MiB, GPU 644 MiB
[08/10/2023-11:18:39] [I] [TRT] [BlockAssignment] Algorithm ShiftNTopDown took 3.38834ms to assign 5 blocks to 65 nodes requiring 58060808 bytes.
[08/10/2023-11:18:39] [V] [TRT] Optimized block assignment.
[08/10/2023-11:18:39] [V] [TRT] Block size 33177600
[08/10/2023-11:18:39] [V] [TRT] Block size 16588800
[08/10/2023-11:18:39] [V] [TRT] Block size 8294400
[08/10/2023-11:18:39] [V] [TRT] Block size 4
[08/10/2023-11:18:39] [V] [TRT] Block size 4
[08/10/2023-11:18:39] [I] [TRT] Total Activation Memory: 58060808
[08/10/2023-11:18:39] [V] [TRT] Disabling unused tactic source: CUDNN
[08/10/2023-11:18:39] [V] [TRT] Disabling unused tactic source: CUBLAS, CUBLAS_LT
[08/10/2023-11:18:39] [V] [TRT] Engine generation completed in 139.847 seconds.
[08/10/2023-11:18:39] [V] [TRT] Deleting timing cache: 626 entries, served 2668 hits since creation.
[08/10/2023-11:18:39] [V] [TRT] Engine Layer Information:
Layer(Reformat): Reformatting CopyNode for Input Tensor 0 to Conv_15 + Relu_16, Tactic: 0x00000000000003ea, input[Half(1,256,180,180)] -> Reformatted Input Tensor 0 to Conv_15 + Relu_16[Half(1,256,180,180)]
Layer(CaskConvolution): Conv_15 + Relu_16, Tactic: 0xdfa020ef435ef810, Reformatted Input Tensor 0 to Conv_15 + Relu_16[Half(1,256,180,180)] -> input.12[Half(1,128,180,180)]
Layer(CaskConvolution): Conv_17 + Relu_18, Tactic: 0x60c3421152ef8e10, input.12[Half(1,128,180,180)] -> input.24[Half(1,128,180,180)]
Layer(CaskConvolution): Conv_19 + Relu_20, Tactic: 0x60c3421152ef8e10, input.24[Half(1,128,180,180)] -> input.36[Half(1,128,180,180)]
Layer(CaskConvolution): Conv_21 + Relu_22, Tactic: 0x60c3421152ef8e10, input.36[Half(1,128,180,180)] -> input.48[Half(1,128,180,180)]
Layer(CaskConvolution): Conv_23 + Relu_24, Tactic: 0x60c3421152ef8e10, input.48[Half(1,128,180,180)] -> input.60[Half(1,128,180,180)]
Layer(CaskConvolution): Conv_25 + Relu_26, Tactic: 0x60c3421152ef8e10, input.60[Half(1,128,180,180)] -> input.72[Half(1,128,180,180)]
Layer(CaskConvolution): Conv_27 + Relu_28, Tactic: 0xea50b6d3d87bf5dd, input.72[Half(1,128,180,180)] -> input.164[Half(1,256,180,180)]
Layer(NoOp): Reformatting CopyNode for Input Tensor 0 to Conv_44 + Relu_45, Tactic: 0x0000000000000000, input.72[Half(1,128,180,180)] -> Reformatted Input Tensor 0 to Conv_44 + Relu_45[Half(1,128,180,180)]
Layer(CaskConvolution): Conv_44 + Relu_45, Tactic: 0xdfa020ef435ef810, Reformatted Input Tensor 0 to Conv_44 + Relu_45[Half(1,128,180,180)] -> input.96[Half(1,256,90,90)]
Layer(CaskConvolution): Conv_46 + Relu_47, Tactic: 0xdfa020ef435ef810, input.96[Half(1,256,90,90)] -> input.108[Half(1,256,90,90)]
Layer(CaskConvolution): Conv_48 + Relu_49, Tactic: 0xdfa020ef435ef810, input.108[Half(1,256,90,90)] -> input.120[Half(1,256,90,90)]
Layer(CaskConvolution): Conv_50 + Relu_51, Tactic: 0xdfa020ef435ef810, input.120[Half(1,256,90,90)] -> input.132[Half(1,256,90,90)]
Layer(CaskConvolution): Conv_52 + Relu_53, Tactic: 0xdfa020ef435ef810, input.132[Half(1,256,90,90)] -> input.144[Half(1,256,90,90)]
Layer(CaskConvolution): Conv_54 + Relu_55, Tactic: 0xdfa020ef435ef810, input.144[Half(1,256,90,90)] -> onnx::ConvTranspose_644[Half(1,256,90,90)]
Layer(NoOp): Reformatting CopyNode for Input Tensor 0 to ConvTranspose_56 + BatchNormalization_57 + Relu_58, Tactic: 0x0000000000000000, onnx::ConvTranspose_644[Half(1,256,90,90)] -> Reformatted Input Tensor 0 to ConvTranspose_56 + BatchNormalization_57 + Relu_58[Half(1,256,90,90)]
Layer(CaskDeconvolutionV2): ConvTranspose_56 + BatchNormalization_57 + Relu_58, Tactic: 0x611e899dccd3003a, Reformatted Input Tensor 0 to ConvTranspose_56 + BatchNormalization_57 + Relu_58[Half(1,256,90,90)] -> onnx::Concat_647[Half(1,256,180,180)]
Layer(Reformat): onnx::Concat_647 copy, Tactic: 0x00000000000003ea, onnx::Concat_647[Half(1,256,180,180)] -> input.164[Half(1,256,180,180)]
Layer(CaskConvolution): Conv_60 + Relu_61, Tactic: 0x529f4431bdae94f5, input.164[Half(1,512,180,180)] -> Reformatted Output Tensor 0 to Conv_60 + Relu_61[Half(1,64,180,180)]
Layer(NoOp): Reformatting CopyNode for Output Tensor 0 to Conv_60 + Relu_61, Tactic: 0x0000000000000000, Reformatted Output Tensor 0 to Conv_60 + Relu_61[Half(1,64,180,180)] -> onnx::Conv_651[Half(1,64,180,180)]
Layer(CaskConvolution): Conv_62 + Relu_63 || Conv_65 + Relu_66 || Conv_68 + Relu_69 || Conv_71 + Relu_72 || Conv_74 + Relu_75 || Conv_77 + Relu_78 || Conv_80 + Relu_81 || Conv_83 + Relu_84, Tactic: 0x60c3421152ef8e10, onnx::Conv_651[Half(1,64,180,180)] -> Reformatted Output Tensor 0 to Conv_62 + Relu_63 || Conv_65 + Relu_66 || Conv_68 + Relu_69 || Conv_71 + Relu_72 || Conv_74 + Relu_75 || Conv_77 + Relu_78 || Conv_80 + Relu_81 || Conv_83 + Relu_84[Half(1,512,180,180)]
Layer(NoOp): Reformatting CopyNode for Output Tensor 0 to Conv_62 + Relu_63 || Conv_65 + Relu_66 || Conv_68 + Relu_69 || Conv_71 + Relu_72 || Conv_74 + Relu_75 || Conv_77 + Relu_78 || Conv_80 + Relu_81 || Conv_83 + Relu_84, Tactic: 0x0000000000000000, Reformatted Output Tensor 0 to Conv_62 + Relu_63 || Conv_65 + Relu_66 || Conv_68 + Relu_69 || Conv_71 + Relu_72 || Conv_74 + Relu_75 || Conv_77 + Relu_78 || Conv_80 + Relu_81 || Conv_83 + Relu_84[Half(1,512,180,180)] -> Conv_62 + Relu_63 || Conv_65 + Relu_66 || Conv_68 + Relu_69 || Conv_71 + Relu_72 || Conv_74 + Relu_75 || Conv_77 + Relu_78 || Conv_80 + Relu_81 || Conv_83 + Relu_84[Half(1,512,180,180)]
Layer(CaskConvolution): Conv_64, Tactic: 0x3e7eb35b91b9fa63, Conv_62 + Relu_63 || Conv_65 + Relu_66 || Conv_68 + Relu_69 || Conv_71 + Relu_72 || Conv_74 + Relu_75 || Conv_77 + Relu_78 || Conv_80 + Relu_81 || Conv_83 + Relu_84[Half(1,64,180,180)] -> Reformatted Output Tensor 0 to Conv_64[Half(1,2,180,180)]
Layer(Reformat): Reformatting CopyNode for Output Tensor 0 to Conv_64, Tactic: 0x0000000000000000, Reformatted Output Tensor 0 to Conv_64[Half(1,2,180,180)] -> reg_0[Half(1,2,180,180)]
Layer(CaskConvolution): Conv_67, Tactic: 0x3e7eb35b91b9fa63, Conv_62 + Relu_63 || Conv_65 + Relu_66 || Conv_68 + Relu_69 || Conv_71 + Relu_72 || Conv_74 + Relu_75 || Conv_77 + Relu_78 || Conv_80 + Relu_81 || Conv_83 + Relu_84[Half(1,64,180,180)] -> Reformatted Output Tensor 0 to Conv_67[Half(1,1,180,180)]
Layer(Reformat): Reformatting CopyNode for Output Tensor 0 to Conv_67, Tactic: 0x00000000000003e8, Reformatted Output Tensor 0 to Conv_67[Half(1,1,180,180)] -> height_0[Half(1,1,180,180)]
Layer(CaskConvolution): Conv_70, Tactic: 0x3e7eb35b91b9fa63, Conv_62 + Relu_63 || Conv_65 + Relu_66 || Conv_68 + Relu_69 || Conv_71 + Relu_72 || Conv_74 + Relu_75 || Conv_77 + Relu_78 || Conv_80 + Relu_81 || Conv_83 + Relu_84[Half(1,64,180,180)] -> Reformatted Output Tensor 0 to Conv_70[Half(1,3,180,180)]
Layer(Reformat): Reformatting CopyNode for Output Tensor 0 to Conv_70, Tactic: 0x0000000000000000, Reformatted Output Tensor 0 to Conv_70[Half(1,3,180,180)] -> dim_0[Half(1,3,180,180)]
Layer(CaskConvolution): Conv_73, Tactic: 0x3e7eb35b91b9fa63, Conv_62 + Relu_63 || Conv_65 + Relu_66 || Conv_68 + Relu_69 || Conv_71 + Relu_72 || Conv_74 + Relu_75 || Conv_77 + Relu_78 || Conv_80 + Relu_81 || Conv_83 + Relu_84[Half(1,64,180,180)] -> Reformatted Output Tensor 0 to Conv_73[Half(1,2,180,180)]
Layer(Reformat): Reformatting CopyNode for Output Tensor 0 to Conv_73, Tactic: 0x0000000000000000, Reformatted Output Tensor 0 to Conv_73[Half(1,2,180,180)] -> rot_0[Half(1,2,180,180)]
Layer(CaskConvolution): Conv_76, Tactic: 0x3e7eb35b91b9fa63, Conv_62 + Relu_63 || Conv_65 + Relu_66 || Conv_68 + Relu_69 || Conv_71 + Relu_72 || Conv_74 + Relu_75 || Conv_77 + Relu_78 || Conv_80 + Relu_81 || Conv_83 + Relu_84[Half(1,64,180,180)] -> Reformatted Output Tensor 0 to Conv_76[Half(1,2,180,180)]
Layer(Reformat): Reformatting CopyNode for Output Tensor 0 to Conv_76, Tactic: 0x0000000000000000, Reformatted Output Tensor 0 to Conv_76[Half(1,2,180,180)] -> vel_0[Half(1,2,180,180)]
Layer(CaskConvolution): Conv_79, Tactic: 0x3e7eb35b91b9fa63, Conv_62 + Relu_63 || Conv_65 + Relu_66 || Conv_68 + Relu_69 || Conv_71 + Relu_72 || Conv_74 + Relu_75 || Conv_77 + Relu_78 || Conv_80 + Relu_81 || Conv_83 + Relu_84[Half(1,64,180,180)] -> Reformatted Output Tensor 0 to Conv_79[Half(1,1,180,180)]
Layer(Reformat): Reformatting CopyNode for Output Tensor 0 to Conv_79, Tactic: 0x00000000000003e8, Reformatted Output Tensor 0 to Conv_79[Half(1,1,180,180)] -> hm_0[Half(1,1,180,180)]
Layer(CaskConvolution): Conv_82, Tactic: 0x3e7eb35b91b9fa63, Conv_62 + Relu_63 || Conv_65 + Relu_66 || Conv_68 + Relu_69 || Conv_71 + Relu_72 || Conv_74 + Relu_75 || Conv_77 + Relu_78 || Conv_80 + Relu_81 || Conv_83 + Relu_84[Half(1,64,180,180)] -> Reformatted Output Tensor 0 to Conv_82[Half(1,2,180,180)]
Layer(Reformat): Reformatting CopyNode for Output Tensor 0 to Conv_82, Tactic: 0x0000000000000000, Reformatted Output Tensor 0 to Conv_82[Half(1,2,180,180)] -> reg_1[Half(1,2,180,180)]
Layer(CaskConvolution): Conv_85, Tactic: 0x3e7eb35b91b9fa63, Conv_62 + Relu_63 || Conv_65 + Relu_66 || Conv_68 + Relu_69 || Conv_71 + Relu_72 || Conv_74 + Relu_75 || Conv_77 + Relu_78 || Conv_80 + Relu_81 || Conv_83 + Relu_84[Half(1,64,180,180)] -> Reformatted Output Tensor 0 to Conv_85[Half(1,1,180,180)]
Layer(Reformat): Reformatting CopyNode for Output Tensor 0 to Conv_85, Tactic: 0x00000000000003e8, Reformatted Output Tensor 0 to Conv_85[Half(1,1,180,180)] -> height_1[Half(1,1,180,180)]
Layer(CaskConvolution): Conv_86 + Relu_87 || Conv_89 + Relu_90 || Conv_92 + Relu_93 || Conv_95 + Relu_96 || Conv_98 + Relu_99 || Conv_101 + Relu_102 || Conv_104 + Relu_105 || Conv_107 + Relu_108, Tactic: 0x60c3421152ef8e10, onnx::Conv_651[Half(1,64,180,180)] -> Reformatted Output Tensor 0 to Conv_86 + Relu_87 || Conv_89 + Relu_90 || Conv_92 + Relu_93 || Conv_95 + Relu_96 || Conv_98 + Relu_99 || Conv_101 + Relu_102 || Conv_104 + Relu_105 || Conv_107 + Relu_108[Half(1,512,180,180)]
Layer(NoOp): Reformatting CopyNode for Output Tensor 0 to Conv_86 + Relu_87 || Conv_89 + Relu_90 || Conv_92 + Relu_93 || Conv_95 + Relu_96 || Conv_98 + Relu_99 || Conv_101 + Relu_102 || Conv_104 + Relu_105 || Conv_107 + Relu_108, Tactic: 0x0000000000000000, Reformatted Output Tensor 0 to Conv_86 + Relu_87 || Conv_89 + Relu_90 || Conv_92 + Relu_93 || Conv_95 + Relu_96 || Conv_98 + Relu_99 || Conv_101 + Relu_102 || Conv_104 + Relu_105 || Conv_107 + Relu_108[Half(1,512,180,180)] -> Conv_86 + Relu_87 || Conv_89 + Relu_90 || Conv_92 + Relu_93 || Conv_95 + Relu_96 || Conv_98 + Relu_99 || Conv_101 + Relu_102 || Conv_104 + Relu_105 || Conv_107 + Relu_108[Half(1,512,180,180)]
Layer(CaskConvolution): Conv_88, Tactic: 0x3e7eb35b91b9fa63, Conv_86 + Relu_87 || Conv_89 + Relu_90 || Conv_92 + Relu_93 || Conv_95 + Relu_96 || Conv_98 + Relu_99 || Conv_101 + Relu_102 || Conv_104 + Relu_105 || Conv_107 + Relu_108[Half(1,64,180,180)] -> Reformatted Output Tensor 0 to Conv_88[Half(1,3,180,180)]
Layer(Reformat): Reformatting CopyNode for Output Tensor 0 to Conv_88, Tactic: 0x0000000000000000, Reformatted Output Tensor 0 to Conv_88[Half(1,3,180,180)] -> dim_1[Half(1,3,180,180)]
Layer(CaskConvolution): Conv_91, Tactic: 0x3e7eb35b91b9fa63, Conv_86 + Relu_87 || Conv_89 + Relu_90 || Conv_92 + Relu_93 || Conv_95 + Relu_96 || Conv_98 + Relu_99 || Conv_101 + Relu_102 || Conv_104 + Relu_105 || Conv_107 + Relu_108[Half(1,64,180,180)] -> Reformatted Output Tensor 0 to Conv_91[Half(1,2,180,180)]
Layer(Reformat): Reformatting CopyNode for Output Tensor 0 to Conv_91, Tactic: 0x0000000000000000, Reformatted Output Tensor 0 to Conv_91[Half(1,2,180,180)] -> rot_1[Half(1,2,180,180)]
Layer(CaskConvolution): Conv_94, Tactic: 0x3e7eb35b91b9fa63, Conv_86 + Relu_87 || Conv_89 + Relu_90 || Conv_92 + Relu_93 || Conv_95 + Relu_96 || Conv_98 + Relu_99 || Conv_101 + Relu_102 || Conv_104 + Relu_105 || Conv_107 + Relu_108[Half(1,64,180,180)] -> Reformatted Output Tensor 0 to Conv_94[Half(1,2,180,180)]
Layer(Reformat): Reformatting CopyNode for Output Tensor 0 to Conv_94, Tactic: 0x0000000000000000, Reformatted Output Tensor 0 to Conv_94[Half(1,2,180,180)] -> vel_1[Half(1,2,180,180)]
Layer(CaskConvolution): Conv_97, Tactic: 0x3e7eb35b91b9fa63, Conv_86 + Relu_87 || Conv_89 + Relu_90 || Conv_92 + Relu_93 || Conv_95 + Relu_96 || Conv_98 + Relu_99 || Conv_101 + Relu_102 || Conv_104 + Relu_105 || Conv_107 + Relu_108[Half(1,64,180,180)] -> Reformatted Output Tensor 0 to Conv_97[Half(1,2,180,180)]
Layer(Reformat): Reformatting CopyNode for Output Tensor 0 to Conv_97, Tactic: 0x0000000000000000, Reformatted Output Tensor 0 to Conv_97[Half(1,2,180,180)] -> hm_1[Half(1,2,180,180)]
Layer(CaskConvolution): Conv_100, Tactic: 0x3e7eb35b91b9fa63, Conv_86 + Relu_87 || Conv_89 + Relu_90 || Conv_92 + Relu_93 || Conv_95 + Relu_96 || Conv_98 + Relu_99 || Conv_101 + Relu_102 || Conv_104 + Relu_105 || Conv_107 + Relu_108[Half(1,64,180,180)] -> Reformatted Output Tensor 0 to Conv_100[Half(1,2,180,180)]
Layer(Reformat): Reformatting CopyNode for Output Tensor 0 to Conv_100, Tactic: 0x0000000000000000, Reformatted Output Tensor 0 to Conv_100[Half(1,2,180,180)] -> reg_2[Half(1,2,180,180)]
Layer(CaskConvolution): Conv_103, Tactic: 0x3e7eb35b91b9fa63, Conv_86 + Relu_87 || Conv_89 + Relu_90 || Conv_92 + Relu_93 || Conv_95 + Relu_96 || Conv_98 + Relu_99 || Conv_101 + Relu_102 || Conv_104 + Relu_105 || Conv_107 + Relu_108[Half(1,64,180,180)] -> Reformatted Output Tensor 0 to Conv_103[Half(1,1,180,180)]
Layer(Reformat): Reformatting CopyNode for Output Tensor 0 to Conv_103, Tactic: 0x00000000000003e8, Reformatted Output Tensor 0 to Conv_103[Half(1,1,180,180)] -> height_2[Half(1,1,180,180)]
Layer(CaskConvolution): Conv_106, Tactic: 0x3e7eb35b91b9fa63, Conv_86 + Relu_87 || Conv_89 + Relu_90 || Conv_92 + Relu_93 || Conv_95 + Relu_96 || Conv_98 + Relu_99 || Conv_101 + Relu_102 || Conv_104 + Relu_105 || Conv_107 + Relu_108[Half(1,64,180,180)] -> Reformatted Output Tensor 0 to Conv_106[Half(1,3,180,180)]
Layer(Reformat): Reformatting CopyNode for Output Tensor 0 to Conv_106, Tactic: 0x0000000000000000, Reformatted Output Tensor 0 to Conv_106[Half(1,3,180,180)] -> dim_2[Half(1,3,180,180)]
Layer(CaskConvolution): Conv_109, Tactic: 0x3e7eb35b91b9fa63, Conv_86 + Relu_87 || Conv_89 + Relu_90 || Conv_92 + Relu_93 || Conv_95 + Relu_96 || Conv_98 + Relu_99 || Conv_101 + Relu_102 || Conv_104 + Relu_105 || Conv_107 + Relu_108[Half(1,64,180,180)] -> Reformatted Output Tensor 0 to Conv_109[Half(1,2,180,180)]
Layer(Reformat): Reformatting CopyNode for Output Tensor 0 to Conv_109, Tactic: 0x0000000000000000, Reformatted Output Tensor 0 to Conv_109[Half(1,2,180,180)] -> rot_2[Half(1,2,180,180)]
Layer(CaskConvolution): Conv_110 + Relu_111 || Conv_113 + Relu_114 || Conv_116 + Relu_117 || Conv_119 + Relu_120 || Conv_122 + Relu_123 || Conv_125 + Relu_126 || Conv_128 + Relu_129 || Conv_131 + Relu_132, Tactic: 0x60c3421152ef8e10, onnx::Conv_651[Half(1,64,180,180)] -> Reformatted Output Tensor 0 to Conv_110 + Relu_111 || Conv_113 + Relu_114 || Conv_116 + Relu_117 || Conv_119 + Relu_120 || Conv_122 + Relu_123 || Conv_125 + Relu_126 || Conv_128 + Relu_129 || Conv_131 + Relu_132[Half(1,512,180,180)]
Layer(NoOp): Reformatting CopyNode for Output Tensor 0 to Conv_110 + Relu_111 || Conv_113 + Relu_114 || Conv_116 + Relu_117 || Conv_119 + Relu_120 || Conv_122 + Relu_123 || Conv_125 + Relu_126 || Conv_128 + Relu_129 || Conv_131 + Relu_132, Tactic: 0x0000000000000000, Reformatted Output Tensor 0 to Conv_110 + Relu_111 || Conv_113 + Relu_114 || Conv_116 + Relu_117 || Conv_119 + Relu_120 || Conv_122 + Relu_123 || Conv_125 + Relu_126 || Conv_128 + Relu_129 || Conv_131 + Relu_132[Half(1,512,180,180)] -> Conv_110 + Relu_111 || Conv_113 + Relu_114 || Conv_116 + Relu_117 || Conv_119 + Relu_120 || Conv_122 + Relu_123 || Conv_125 + Relu_126 || Conv_128 + Relu_129 || Conv_131 + Relu_132[Half(1,512,180,180)]
Layer(CaskConvolution): Conv_112, Tactic: 0x3e7eb35b91b9fa63, Conv_110 + Relu_111 || Conv_113 + Relu_114 || Conv_116 + Relu_117 || Conv_119 + Relu_120 || Conv_122 + Relu_123 || Conv_125 + Relu_126 || Conv_128 + Relu_129 || Conv_131 + Relu_132[Half(1,64,180,180)] -> Reformatted Output Tensor 0 to Conv_112[Half(1,2,180,180)]
Layer(Reformat): Reformatting CopyNode for Output Tensor 0 to Conv_112, Tactic: 0x0000000000000000, Reformatted Output Tensor 0 to Conv_112[Half(1,2,180,180)] -> vel_2[Half(1,2,180,180)]
Layer(CaskConvolution): Conv_115, Tactic: 0x3e7eb35b91b9fa63, Conv_110 + Relu_111 || Conv_113 + Relu_114 || Conv_116 + Relu_117 || Conv_119 + Relu_120 || Conv_122 + Relu_123 || Conv_125 + Relu_126 || Conv_128 + Relu_129 || Conv_131 + Relu_132[Half(1,64,180,180)] -> Reformatted Output Tensor 0 to Conv_115[Half(1,2,180,180)]
Layer(Reformat): Reformatting CopyNode for Output Tensor 0 to Conv_115, Tactic: 0x0000000000000000, Reformatted Output Tensor 0 to Conv_115[Half(1,2,180,180)] -> hm_2[Half(1,2,180,180)]
Layer(CaskConvolution): Conv_118, Tactic: 0x3e7eb35b91b9fa63, Conv_110 + Relu_111 || Conv_113 + Relu_114 || Conv_116 + Relu_117 || Conv_119 + Relu_120 || Conv_122 + Relu_123 || Conv_125 + Relu_126 || Conv_128 + Relu_129 || Conv_131 + Relu_132[Half(1,64,180,180)] -> Reformatted Output Tensor 0 to Conv_118[Half(1,2,180,180)]
Layer(Reformat): Reformatting CopyNode for Output Tensor 0 to Conv_118, Tactic: 0x0000000000000000, Reformatted Output Tensor 0 to Conv_118[Half(1,2,180,180)] -> reg_3[Half(1,2,180,180)]
Layer(CaskConvolution): Conv_121, Tactic: 0x3e7eb35b91b9fa63, Conv_110 + Relu_111 || Conv_113 + Relu_114 || Conv_116 + Relu_117 || Conv_119 + Relu_120 || Conv_122 + Relu_123 || Conv_125 + Relu_126 || Conv_128 + Relu_129 || Conv_131 + Relu_132[Half(1,64,180,180)] -> Reformatted Output Tensor 0 to Conv_121[Half(1,1,180,180)]
Layer(Reformat): Reformatting CopyNode for Output Tensor 0 to Conv_121, Tactic: 0x00000000000003e8, Reformatted Output Tensor 0 to Conv_121[Half(1,1,180,180)] -> height_3[Half(1,1,180,180)]
Layer(CaskConvolution): Conv_124, Tactic: 0x3e7eb35b91b9fa63, Conv_110 + Relu_111 || Conv_113 + Relu_114 || Conv_116 + Relu_117 || Conv_119 + Relu_120 || Conv_122 + Relu_123 || Conv_125 + Relu_126 || Conv_128 + Relu_129 || Conv_131 + Relu_132[Half(1,64,180,180)] -> Reformatted Output Tensor 0 to Conv_124[Half(1,3,180,180)]
Layer(Reformat): Reformatting CopyNode for Output Tensor 0 to Conv_124, Tactic: 0x0000000000000000, Reformatted Output Tensor 0 to Conv_124[Half(1,3,180,180)] -> dim_3[Half(1,3,180,180)]
Layer(CaskConvolution): Conv_127, Tactic: 0x3e7eb35b91b9fa63, Conv_110 + Relu_111 || Conv_113 + Relu_114 || Conv_116 + Relu_117 || Conv_119 + Relu_120 || Conv_122 + Relu_123 || Conv_125 + Relu_126 || Conv_128 + Relu_129 || Conv_131 + Relu_132[Half(1,64,180,180)] -> Reformatted Output Tensor 0 to Conv_127[Half(1,2,180,180)]
Layer(Reformat): Reformatting CopyNode for Output Tensor 0 to Conv_127, Tactic: 0x0000000000000000, Reformatted Output Tensor 0 to Conv_127[Half(1,2,180,180)] -> rot_3[Half(1,2,180,180)]
Layer(CaskConvolution): Conv_130, Tactic: 0x3e7eb35b91b9fa63, Conv_110 + Relu_111 || Conv_113 + Relu_114 || Conv_116 + Relu_117 || Conv_119 + Relu_120 || Conv_122 + Relu_123 || Conv_125 + Relu_126 || Conv_128 + Relu_129 || Conv_131 + Relu_132[Half(1,64,180,180)] -> Reformatted Output Tensor 0 to Conv_130[Half(1,2,180,180)]
Layer(Reformat): Reformatting CopyNode for Output Tensor 0 to Conv_130, Tactic: 0x0000000000000000, Reformatted Output Tensor 0 to Conv_130[Half(1,2,180,180)] -> vel_3[Half(1,2,180,180)]
Layer(CaskConvolution): Conv_133, Tactic: 0x3e7eb35b91b9fa63, Conv_110 + Relu_111 || Conv_113 + Relu_114 || Conv_116 + Relu_117 || Conv_119 + Relu_120 || Conv_122 + Relu_123 || Conv_125 + Relu_126 || Conv_128 + Relu_129 || Conv_131 + Relu_132[Half(1,64,180,180)] -> Reformatted Output Tensor 0 to Conv_133[Half(1,1,180,180)]
Layer(Reformat): Reformatting CopyNode for Output Tensor 0 to Conv_133, Tactic: 0x00000000000003e8, Reformatted Output Tensor 0 to Conv_133[Half(1,1,180,180)] -> hm_3[Half(1,1,180,180)]
Layer(CaskConvolution): Conv_134 + Relu_135 || Conv_137 + Relu_138 || Conv_140 + Relu_141 || Conv_143 + Relu_144 || Conv_146 + Relu_147 || Conv_149 + Relu_150 || Conv_152 + Relu_153 || Conv_155 + Relu_156, Tactic: 0x60c3421152ef8e10, onnx::Conv_651[Half(1,64,180,180)] -> Reformatted Output Tensor 0 to Conv_134 + Relu_135 || Conv_137 + Relu_138 || Conv_140 + Relu_141 || Conv_143 + Relu_144 || Conv_146 + Relu_147 || Conv_149 + Relu_150 || Conv_152 + Relu_153 || Conv_155 + Relu_156[Half(1,512,180,180)]
Layer(NoOp): Reformatting CopyNode for Output Tensor 0 to Conv_134 + Relu_135 || Conv_137 + Relu_138 || Conv_140 + Relu_141 || Conv_143 + Relu_144 || Conv_146 + Relu_147 || Conv_149 + Relu_150 || Conv_152 + Relu_153 || Conv_155 + Relu_156, Tactic: 0x0000000000000000, Reformatted Output Tensor 0 to Conv_134 + Relu_135 || Conv_137 + Relu_138 || Conv_140 + Relu_141 || Conv_143 + Relu_144 || Conv_146 + Relu_147 || Conv_149 + Relu_150 || Conv_152 + Relu_153 || Conv_155 + Relu_156[Half(1,512,180,180)] -> Conv_134 + Relu_135 || Conv_137 + Relu_138 || Conv_140 + Relu_141 || Conv_143 + Relu_144 || Conv_146 + Relu_147 || Conv_149 + Relu_150 || Conv_152 + Relu_153 || Conv_155 + Relu_156[Half(1,512,180,180)]
Layer(CaskConvolution): Conv_136, Tactic: 0x3e7eb35b91b9fa63, Conv_134 + Relu_135 || Conv_137 + Relu_138 || Conv_140 + Relu_141 || Conv_143 + Relu_144 || Conv_146 + Relu_147 || Conv_149 + Relu_150 || Conv_152 + Relu_153 || Conv_155 + Relu_156[Half(1,64,180,180)] -> Reformatted Output Tensor 0 to Conv_136[Half(1,2,180,180)]
Layer(Reformat): Reformatting CopyNode for Output Tensor 0 to Conv_136, Tactic: 0x0000000000000000, Reformatted Output Tensor 0 to Conv_136[Half(1,2,180,180)] -> reg_4[Half(1,2,180,180)]
Layer(CaskConvolution): Conv_139, Tactic: 0x3e7eb35b91b9fa63, Conv_134 + Relu_135 || Conv_137 + Relu_138 || Conv_140 + Relu_141 || Conv_143 + Relu_144 || Conv_146 + Relu_147 || Conv_149 + Relu_150 || Conv_152 + Relu_153 || Conv_155 + Relu_156[Half(1,64,180,180)] -> Reformatted Output Tensor 0 to Conv_139[Half(1,1,180,180)]
Layer(Reformat): Reformatting CopyNode for Output Tensor 0 to Conv_139, Tactic: 0x00000000000003e8, Reformatted Output Tensor 0 to Conv_139[Half(1,1,180,180)] -> height_4[Half(1,1,180,180)]
Layer(CaskConvolution): Conv_142, Tactic: 0x3e7eb35b91b9fa63, Conv_134 + Relu_135 || Conv_137 + Relu_138 || Conv_140 + Relu_141 || Conv_143 + Relu_144 || Conv_146 + Relu_147 || Conv_149 + Relu_150 || Conv_152 + Relu_153 || Conv_155 + Relu_156[Half(1,64,180,180)] -> Reformatted Output Tensor 0 to Conv_142[Half(1,3,180,180)]
Layer(Reformat): Reformatting CopyNode for Output Tensor 0 to Conv_142, Tactic: 0x0000000000000000, Reformatted Output Tensor 0 to Conv_142[Half(1,3,180,180)] -> dim_4[Half(1,3,180,180)]
Layer(CaskConvolution): Conv_145, Tactic: 0x3e7eb35b91b9fa63, Conv_134 + Relu_135 || Conv_137 + Relu_138 || Conv_140 + Relu_141 || Conv_143 + Relu_144 || Conv_146 + Relu_147 || Conv_149 + Relu_150 || Conv_152 + Relu_153 || Conv_155 + Relu_156[Half(1,64,180,180)] -> Reformatted Output Tensor 0 to Conv_145[Half(1,2,180,180)]
Layer(Reformat): Reformatting CopyNode for Output Tensor 0 to Conv_145, Tactic: 0x0000000000000000, Reformatted Output Tensor 0 to Conv_145[Half(1,2,180,180)] -> rot_4[Half(1,2,180,180)]
Layer(CaskConvolution): Conv_148, Tactic: 0x3e7eb35b91b9fa63, Conv_134 + Relu_135 || Conv_137 + Relu_138 || Conv_140 + Relu_141 || Conv_143 + Relu_144 || Conv_146 + Relu_147 || Conv_149 + Relu_150 || Conv_152 + Relu_153 || Conv_155 + Relu_156[Half(1,64,180,180)] -> Reformatted Output Tensor 0 to Conv_148[Half(1,2,180,180)]
Layer(Reformat): Reformatting CopyNode for Output Tensor 0 to Conv_148, Tactic: 0x0000000000000000, Reformatted Output Tensor 0 to Conv_148[Half(1,2,180,180)] -> vel_4[Half(1,2,180,180)]
Layer(CaskConvolution): Conv_151, Tactic: 0x3e7eb35b91b9fa63, Conv_134 + Relu_135 || Conv_137 + Relu_138 || Conv_140 + Relu_141 || Conv_143 + Relu_144 || Conv_146 + Relu_147 || Conv_149 + Relu_150 || Conv_152 + Relu_153 || Conv_155 + Relu_156[Half(1,64,180,180)] -> Reformatted Output Tensor 0 to Conv_151[Half(1,2,180,180)]
Layer(Reformat): Reformatting CopyNode for Output Tensor 0 to Conv_151, Tactic: 0x0000000000000000, Reformatted Output Tensor 0 to Conv_151[Half(1,2,180,180)] -> hm_4[Half(1,2,180,180)]
Layer(CaskConvolution): Conv_154, Tactic: 0x3e7eb35b91b9fa63, Conv_134 + Relu_135 || Conv_137 + Relu_138 || Conv_140 + Relu_141 || Conv_143 + Relu_144 || Conv_146 + Relu_147 || Conv_149 + Relu_150 || Conv_152 + Relu_153 || Conv_155 + Relu_156[Half(1,64,180,180)] -> Reformatted Output Tensor 0 to Conv_154[Half(1,2,180,180)]
Layer(Reformat): Reformatting CopyNode for Output Tensor 0 to Conv_154, Tactic: 0x0000000000000000, Reformatted Output Tensor 0 to Conv_154[Half(1,2,180,180)] -> reg_5[Half(1,2,180,180)]
Layer(CaskConvolution): Conv_157, Tactic: 0x3e7eb35b91b9fa63, Conv_134 + Relu_135 || Conv_137 + Relu_138 || Conv_140 + Relu_141 || Conv_143 + Relu_144 || Conv_146 + Relu_147 || Conv_149 + Relu_150 || Conv_152 + Relu_153 || Conv_155 + Relu_156[Half(1,64,180,180)] -> Reformatted Output Tensor 0 to Conv_157[Half(1,1,180,180)]
Layer(Reformat): Reformatting CopyNode for Output Tensor 0 to Conv_157, Tactic: 0x00000000000003e8, Reformatted Output Tensor 0 to Conv_157[Half(1,1,180,180)] -> height_5[Half(1,1,180,180)]
Layer(NoOp): Reformatting CopyNode for Input Tensor 0 to Conv_158 + Relu_159 || Conv_161 + Relu_162 || Conv_164 + Relu_165 || Conv_167 + Relu_168, Tactic: 0x0000000000000000, onnx::Conv_651[Half(1,64,180,180)] -> Reformatted Input Tensor 0 to Conv_158 + Relu_159 || Conv_161 + Relu_162 || Conv_164 + Relu_165 || Conv_167 + Relu_168[Half(1,64,180,180)]
Layer(CaskConvolution): Conv_158 + Relu_159 || Conv_161 + Relu_162 || Conv_164 + Relu_165 || Conv_167 + Relu_168, Tactic: 0x60c3421152ef8e10, Reformatted Input Tensor 0 to Conv_158 + Relu_159 || Conv_161 + Relu_162 || Conv_164 + Relu_165 || Conv_167 + Relu_168[Half(1,64,180,180)] -> Conv_158 + Relu_159 || Conv_161 + Relu_162 || Conv_164 + Relu_165 || Conv_167 + Relu_168[Half(1,256,180,180)]
Layer(CaskConvolution): Conv_160, Tactic: 0x3e7eb35b91b9fa63, Conv_158 + Relu_159 || Conv_161 + Relu_162 || Conv_164 + Relu_165 || Conv_167 + Relu_168[Half(1,64,180,180)] -> Reformatted Output Tensor 0 to Conv_160[Half(1,3,180,180)]
Layer(Reformat): Reformatting CopyNode for Output Tensor 0 to Conv_160, Tactic: 0x0000000000000000, Reformatted Output Tensor 0 to Conv_160[Half(1,3,180,180)] -> dim_5[Half(1,3,180,180)]
Layer(CaskConvolution): Conv_163, Tactic: 0x3e7eb35b91b9fa63, Conv_158 + Relu_159 || Conv_161 + Relu_162 || Conv_164 + Relu_165 || Conv_167 + Relu_168[Half(1,64,180,180)] -> Reformatted Output Tensor 0 to Conv_163[Half(1,2,180,180)]
Layer(Reformat): Reformatting CopyNode for Output Tensor 0 to Conv_163, Tactic: 0x0000000000000000, Reformatted Output Tensor 0 to Conv_163[Half(1,2,180,180)] -> rot_5[Half(1,2,180,180)]
Layer(CaskConvolution): Conv_166, Tactic: 0x3e7eb35b91b9fa63, Conv_158 + Relu_159 || Conv_161 + Relu_162 || Conv_164 + Relu_165 || Conv_167 + Relu_168[Half(1,64,180,180)] -> Reformatted Output Tensor 0 to Conv_166[Half(1,2,180,180)]
Layer(Reformat): Reformatting CopyNode for Output Tensor 0 to Conv_166, Tactic: 0x0000000000000000, Reformatted Output Tensor 0 to Conv_166[Half(1,2,180,180)] -> vel_5[Half(1,2,180,180)]
Layer(CaskConvolution): Conv_169, Tactic: 0x3e7eb35b91b9fa63, Conv_158 + Relu_159 || Conv_161 + Relu_162 || Conv_164 + Relu_165 || Conv_167 + Relu_168[Half(1,64,180,180)] -> Reformatted Output Tensor 0 to Conv_169[Half(1,2,180,180)]
Layer(Reformat): Reformatting CopyNode for Output Tensor 0 to Conv_169, Tactic: 0x0000000000000000, Reformatted Output Tensor 0 to Conv_169[Half(1,2,180,180)] -> hm_5[Half(1,2,180,180)]
[08/10/2023-11:18:39] [I] [TRT] [MemUsageChange] TensorRT-managed allocation in building engine: CPU +12, GPU +16, now: CPU 12, GPU 16 (MiB)
[08/10/2023-11:18:39] [W] [TRT] The getMaxBatchSize() function should not be used with an engine built from a network created with NetworkDefinitionCreationFlag::kEXPLICIT_BATCH flag. This function will always return 1.
[08/10/2023-11:18:39] [W] [TRT] The getMaxBatchSize() function should not be used with an engine built from a network created with NetworkDefinitionCreationFlag::kEXPLICIT_BATCH flag. This function will always return 1.
[08/10/2023-11:18:40] [I] Engine built in 144.836 sec.
[08/10/2023-11:18:40] [I] [TRT] [MemUsageChange] Init CUDA: CPU +0, GPU +0, now: CPU 1533, GPU 11595 (MiB)
[08/10/2023-11:18:40] [I] [TRT] Loaded engine size: 12 MiB
[08/10/2023-11:18:40] [V] [TRT] Deserialization required 8656 microseconds.
[08/10/2023-11:18:40] [I] [TRT] [MemUsageChange] TensorRT-managed allocation in engine deserialization: CPU +0, GPU +12, now: CPU 0, GPU 12 (MiB)
[08/10/2023-11:18:40] [I] Engine deserialized in 0.0107561 sec.
[08/10/2023-11:18:40] [V] [TRT] Total per-runner device persistent memory is 134144
[08/10/2023-11:18:40] [V] [TRT] Total per-runner host persistent memory is 155136
[08/10/2023-11:18:40] [V] [TRT] Allocated activation device memory of size 58061824
[08/10/2023-11:18:40] [I] [TRT] [MemUsageChange] TensorRT-managed allocation in IExecutionContext creation: CPU +0, GPU +55, now: CPU 0, GPU 67 (MiB)
[08/10/2023-11:18:40] [I] Using random values for input input
[08/10/2023-11:18:40] [I] Created input binding for input with dimensions 1x256x180x180
[08/10/2023-11:18:40] [I] Using random values for output reg_0
[08/10/2023-11:18:40] [I] Created output binding for reg_0 with dimensions 1x2x180x180
[08/10/2023-11:18:40] [I] Using random values for output height_0
[08/10/2023-11:18:40] [I] Created output binding for height_0 with dimensions 1x1x180x180
[08/10/2023-11:18:40] [I] Using random values for output dim_0
[08/10/2023-11:18:40] [I] Created output binding for dim_0 with dimensions 1x3x180x180
[08/10/2023-11:18:40] [I] Using random values for output rot_0
[08/10/2023-11:18:40] [I] Created output binding for rot_0 with dimensions 1x2x180x180
[08/10/2023-11:18:40] [I] Using random values for output vel_0
[08/10/2023-11:18:40] [I] Created output binding for vel_0 with dimensions 1x2x180x180
[08/10/2023-11:18:40] [I] Using random values for output hm_0
[08/10/2023-11:18:40] [I] Created output binding for hm_0 with dimensions 1x1x180x180
[08/10/2023-11:18:40] [I] Using random values for output reg_1
[08/10/2023-11:18:40] [I] Created output binding for reg_1 with dimensions 1x2x180x180
[08/10/2023-11:18:40] [I] Using random values for output height_1
[08/10/2023-11:18:40] [I] Created output binding for height_1 with dimensions 1x1x180x180
[08/10/2023-11:18:40] [I] Using random values for output dim_1
[08/10/2023-11:18:40] [I] Created output binding for dim_1 with dimensions 1x3x180x180
[08/10/2023-11:18:40] [I] Using random values for output rot_1
[08/10/2023-11:18:40] [I] Created output binding for rot_1 with dimensions 1x2x180x180
[08/10/2023-11:18:40] [I] Using random values for output vel_1
[08/10/2023-11:18:40] [I] Created output binding for vel_1 with dimensions 1x2x180x180
[08/10/2023-11:18:40] [I] Using random values for output hm_1
[08/10/2023-11:18:40] [I] Created output binding for hm_1 with dimensions 1x2x180x180
[08/10/2023-11:18:40] [I] Using random values for output reg_2
[08/10/2023-11:18:40] [I] Created output binding for reg_2 with dimensions 1x2x180x180
[08/10/2023-11:18:40] [I] Using random values for output height_2
[08/10/2023-11:18:40] [I] Created output binding for height_2 with dimensions 1x1x180x180
[08/10/2023-11:18:40] [I] Using random values for output dim_2
[08/10/2023-11:18:40] [I] Created output binding for dim_2 with dimensions 1x3x180x180
[08/10/2023-11:18:40] [I] Using random values for output rot_2
[08/10/2023-11:18:40] [I] Created output binding for rot_2 with dimensions 1x2x180x180
[08/10/2023-11:18:40] [I] Using random values for output vel_2
[08/10/2023-11:18:40] [I] Created output binding for vel_2 with dimensions 1x2x180x180
[08/10/2023-11:18:40] [I] Using random values for output hm_2
[08/10/2023-11:18:40] [I] Created output binding for hm_2 with dimensions 1x2x180x180
[08/10/2023-11:18:40] [I] Using random values for output reg_3
[08/10/2023-11:18:40] [I] Created output binding for reg_3 with dimensions 1x2x180x180
[08/10/2023-11:18:40] [I] Using random values for output height_3
[08/10/2023-11:18:40] [I] Created output binding for height_3 with dimensions 1x1x180x180
[08/10/2023-11:18:40] [I] Using random values for output dim_3
[08/10/2023-11:18:40] [I] Created output binding for dim_3 with dimensions 1x3x180x180
[08/10/2023-11:18:40] [I] Using random values for output rot_3
[08/10/2023-11:18:40] [I] Created output binding for rot_3 with dimensions 1x2x180x180
[08/10/2023-11:18:40] [I] Using random values for output vel_3
[08/10/2023-11:18:40] [I] Created output binding for vel_3 with dimensions 1x2x180x180
[08/10/2023-11:18:40] [I] Using random values for output hm_3
[08/10/2023-11:18:40] [I] Created output binding for hm_3 with dimensions 1x1x180x180
[08/10/2023-11:18:40] [I] Using random values for output reg_4
[08/10/2023-11:18:40] [I] Created output binding for reg_4 with dimensions 1x2x180x180
[08/10/2023-11:18:40] [I] Using random values for output height_4
[08/10/2023-11:18:40] [I] Created output binding for height_4 with dimensions 1x1x180x180
[08/10/2023-11:18:40] [I] Using random values for output dim_4
[08/10/2023-11:18:40] [I] Created output binding for dim_4 with dimensions 1x3x180x180
[08/10/2023-11:18:40] [I] Using random values for output rot_4
[08/10/2023-11:18:40] [I] Created output binding for rot_4 with dimensions 1x2x180x180
[08/10/2023-11:18:40] [I] Using random values for output vel_4
[08/10/2023-11:18:40] [I] Created output binding for vel_4 with dimensions 1x2x180x180
[08/10/2023-11:18:40] [I] Using random values for output hm_4
[08/10/2023-11:18:40] [I] Created output binding for hm_4 with dimensions 1x2x180x180
[08/10/2023-11:18:40] [I] Using random values for output reg_5
[08/10/2023-11:18:40] [I] Created output binding for reg_5 with dimensions 1x2x180x180
[08/10/2023-11:18:40] [I] Using random values for output height_5
[08/10/2023-11:18:40] [I] Created output binding for height_5 with dimensions 1x1x180x180
[08/10/2023-11:18:40] [I] Using random values for output dim_5
[08/10/2023-11:18:40] [I] Created output binding for dim_5 with dimensions 1x3x180x180
[08/10/2023-11:18:40] [I] Using random values for output rot_5
[08/10/2023-11:18:40] [I] Created output binding for rot_5 with dimensions 1x2x180x180
[08/10/2023-11:18:40] [I] Using random values for output vel_5
[08/10/2023-11:18:40] [I] Created output binding for vel_5 with dimensions 1x2x180x180
[08/10/2023-11:18:40] [I] Using random values for output hm_5
[08/10/2023-11:18:40] [I] Created output binding for hm_5 with dimensions 1x2x180x180
[08/10/2023-11:18:40] [I] Layer Information:
[08/10/2023-11:18:40] [I] [TRT] [MemUsageChange] Init CUDA: CPU +0, GPU +0, now: CPU 1535, GPU 11684 (MiB)
[08/10/2023-11:18:40] [I] Layers:
Name: Reformatting CopyNode for Input Tensor 0 to Conv_15 + Relu_16, LayerType: Reformat, Inputs: [ { Name: input, Location: Device, Dimensions: [1,256,180,180], Format/Datatype: Row major linear FP16 format }], Outputs: [ { Name: Reformatted Input Tensor 0 to Conv_15 + Relu_16, Location: Device, Dimensions: [1,256,180,180], Format/Datatype: Channel major FP16 format where channel % 8 == 0 }], ParameterType: Reformat, Origin: REFORMAT, TacticValue: 0x00000000000003ea
Name: Conv_15 + Relu_16, LayerType: CaskConvolution, Inputs: [ { Name: Reformatted Input Tensor 0 to Conv_15 + Relu_16, Location: Device, Dimensions: [1,256,180,180], Format/Datatype: Channel major FP16 format where channel % 8 == 0 }], Outputs: [ { Name: input.12, Location: Device, Dimensions: [1,128,180,180], Format/Datatype: Channel major FP16 format where channel % 8 == 0 }], ParameterType: Convolution, Kernel: [3,3], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [1,1], PostPadding: [1,1], Stride: [1,1], Dilation: [1,1], OutMaps: 128, Groups: 1, Weights: {"Type": "Half", "Count": 294912}, Bias: {"Type": "Half", "Count": 128}, HasSparseWeights: 0, Activation: RELU, HasBias: 1, HasReLU: 1, TacticName: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x128x32_stage4_warpsize2x2x1_g1_tensor16x8x16, TacticValue: 0xdfa020ef435ef810
Name: Conv_17 + Relu_18, LayerType: CaskConvolution, Inputs: [ { Name: input.12, Location: Device, Dimensions: [1,128,180,180], Format/Datatype: Channel major FP16 format where channel % 8 == 0 }], Outputs: [ { Name: input.24, Location: Device, Dimensions: [1,128,180,180], Format/Datatype: Channel major FP16 format where channel % 8 == 0 }], ParameterType: Convolution, Kernel: [3,3], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [1,1], PostPadding: [1,1], Stride: [1,1], Dilation: [1,1], OutMaps: 128, Groups: 1, Weights: {"Type": "Half", "Count": 147456}, Bias: {"Type": "Half", "Count": 128}, HasSparseWeights: 0, Activation: RELU, HasBias: 1, HasReLU: 1, TacticName: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x128x32_stage4_warpsize2x2x1_g1_tensor16x8x16_t1r3s3, TacticValue: 0x60c3421152ef8e10
Name: Conv_19 + Relu_20, LayerType: CaskConvolution, Inputs: [ { Name: input.24, Location: Device, Dimensions: [1,128,180,180], Format/Datatype: Channel major FP16 format where channel % 8 == 0 }], Outputs: [ { Name: input.36, Location: Device, Dimensions: [1,128,180,180], Format/Datatype: Channel major FP16 format where channel % 8 == 0 }], ParameterType: Convolution, Kernel: [3,3], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [1,1], PostPadding: [1,1], Stride: [1,1], Dilation: [1,1], OutMaps: 128, Groups: 1, Weights: {"Type": "Half", "Count": 147456}, Bias: {"Type": "Half", "Count": 128}, HasSparseWeights: 0, Activation: RELU, HasBias: 1, HasReLU: 1, TacticName: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x128x32_stage4_warpsize2x2x1_g1_tensor16x8x16_t1r3s3, TacticValue: 0x60c3421152ef8e10
Name: Conv_21 + Relu_22, LayerType: CaskConvolution, Inputs: [ { Name: input.36, Location: Device, Dimensions: [1,128,180,180], Format/Datatype: Channel major FP16 format where channel % 8 == 0 }], Outputs: [ { Name: input.48, Location: Device, Dimensions: [1,128,180,180], Format/Datatype: Channel major FP16 format where channel % 8 == 0 }], ParameterType: Convolution, Kernel: [3,3], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [1,1], PostPadding: [1,1], Stride: [1,1], Dilation: [1,1], OutMaps: 128, Groups: 1, Weights: {"Type": "Half", "Count": 147456}, Bias: {"Type": "Half", "Count": 128}, HasSparseWeights: 0, Activation: RELU, HasBias: 1, HasReLU: 1, TacticName: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x128x32_stage4_warpsize2x2x1_g1_tensor16x8x16_t1r3s3, TacticValue: 0x60c3421152ef8e10
Name: Conv_23 + Relu_24, LayerType: CaskConvolution, Inputs: [ { Name: input.48, Location: Device, Dimensions: [1,128,180,180], Format/Datatype: Channel major FP16 format where channel % 8 == 0 }], Outputs: [ { Name: input.60, Location: Device, Dimensions: [1,128,180,180], Format/Datatype: Channel major FP16 format where channel % 8 == 0 }], ParameterType: Convolution, Kernel: [3,3], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [1,1], PostPadding: [1,1], Stride: [1,1], Dilation: [1,1], OutMaps: 128, Groups: 1, Weights: {"Type": "Half", "Count": 147456}, Bias: {"Type": "Half", "Count": 128}, HasSparseWeights: 0, Activation: RELU, HasBias: 1, HasReLU: 1, TacticName: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x128x32_stage4_warpsize2x2x1_g1_tensor16x8x16_t1r3s3, TacticValue: 0x60c3421152ef8e10
Name: Conv_25 + Relu_26, LayerType: CaskConvolution, Inputs: [ { Name: input.60, Location: Device, Dimensions: [1,128,180,180], Format/Datatype: Channel major FP16 format where channel % 8 == 0 }], Outputs: [ { Name: input.72, Location: Device, Dimensions: [1,128,180,180], Format/Datatype: Channel major FP16 format where channel % 8 == 0 }], ParameterType: Convolution, Kernel: [3,3], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [1,1], PostPadding: [1,1], Stride: [1,1], Dilation: [1,1], OutMaps: 128, Groups: 1, Weights: {"Type": "Half", "Count": 147456}, Bias: {"Type": "Half", "Count": 128}, HasSparseWeights: 0, Activation: RELU, HasBias: 1, HasReLU: 1, TacticName: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x128x32_stage4_warpsize2x2x1_g1_tensor16x8x16_t1r3s3, TacticValue: 0x60c3421152ef8e10
Name: Conv_27 + Relu_28, LayerType: CaskConvolution, Inputs: [ { Name: input.72, Location: Device, Dimensions: [1,128,180,180], Format/Datatype: Channel major FP16 format where channel % 8 == 0 }], Outputs: [ { Name: input.164, Location: Device, Dimensions: [1,256,180,180], Format/Datatype: Channel major FP16 format where channel % 8 == 0 }], ParameterType: Convolution, Kernel: [1,1], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [0,0], PostPadding: [0,0], Stride: [1,1], Dilation: [1,1], OutMaps: 256, Groups: 1, Weights: {"Type": "Half", "Count": 32768}, Bias: {"Type": "Half", "Count": 256}, HasSparseWeights: 0, Activation: RELU, HasBias: 1, HasReLU: 1, TacticName: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x128x32_stage4_warpsize2x2x1_g1_tensor16x8x16_t1r1s1, TacticValue: 0xea50b6d3d87bf5dd
Name: Reformatting CopyNode for Input Tensor 0 to Conv_44 + Relu_45, LayerType: NoOp, Inputs: [ { Name: input.72, Location: Device, Dimensions: [1,128,180,180], Format/Datatype: Channel major FP16 format where channel % 8 == 0 }], Outputs: [ { Name: Reformatted Input Tensor 0 to Conv_44 + Relu_45, Location: Device, Dimensions: [1,128,180,180], Format/Datatype: Channel major FP16 format where channel % 16 == 0 }], TacticValue: 0x0000000000000000
Name: Conv_44 + Relu_45, LayerType: CaskConvolution, Inputs: [ { Name: Reformatted Input Tensor 0 to Conv_44 + Relu_45, Location: Device, Dimensions: [1,128,180,180], Format/Datatype: Channel major FP16 format where channel % 16 == 0 }], Outputs: [ { Name: input.96, Location: Device, Dimensions: [1,256,90,90], Format/Datatype: Channel major FP16 format where channel % 16 == 0 }], ParameterType: Convolution, Kernel: [3,3], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [1,1], PostPadding: [1,1], Stride: [2,2], Dilation: [1,1], OutMaps: 256, Groups: 1, Weights: {"Type": "Half", "Count": 294912}, Bias: {"Type": "Half", "Count": 256}, HasSparseWeights: 0, Activation: RELU, HasBias: 1, HasReLU: 1, TacticName: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x128x32_stage4_warpsize2x2x1_g1_tensor16x8x16, TacticValue: 0xdfa020ef435ef810
Name: Conv_46 + Relu_47, LayerType: CaskConvolution, Inputs: [ { Name: input.96, Location: Device, Dimensions: [1,256,90,90], Format/Datatype: Channel major FP16 format where channel % 16 == 0 }], Outputs: [ { Name: input.108, Location: Device, Dimensions: [1,256,90,90], Format/Datatype: Channel major FP16 format where channel % 16 == 0 }], ParameterType: Convolution, Kernel: [3,3], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [1,1], PostPadding: [1,1], Stride: [1,1], Dilation: [1,1], OutMaps: 256, Groups: 1, Weights: {"Type": "Half", "Count": 589824}, Bias: {"Type": "Half", "Count": 256}, HasSparseWeights: 0, Activation: RELU, HasBias: 1, HasReLU: 1, TacticName: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x128x32_stage4_warpsize2x2x1_g1_tensor16x8x16, TacticValue: 0xdfa020ef435ef810
Name: Conv_48 + Relu_49, LayerType: CaskConvolution, Inputs: [ { Name: input.108, Location: Device, Dimensions: [1,256,90,90], Format/Datatype: Channel major FP16 format where channel % 16 == 0 }], Outputs: [ { Name: input.120, Location: Device, Dimensions: [1,256,90,90], Format/Datatype: Channel major FP16 format where channel % 16 == 0 }], ParameterType: Convolution, Kernel: [3,3], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [1,1], PostPadding: [1,1], Stride: [1,1], Dilation: [1,1], OutMaps: 256, Groups: 1, Weights: {"Type": "Half", "Count": 589824}, Bias: {"Type": "Half", "Count": 256}, HasSparseWeights: 0, Activation: RELU, HasBias: 1, HasReLU: 1, TacticName: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x128x32_stage4_warpsize2x2x1_g1_tensor16x8x16, TacticValue: 0xdfa020ef435ef810
Name: Conv_50 + Relu_51, LayerType: CaskConvolution, Inputs: [ { Name: input.120, Location: Device, Dimensions: [1,256,90,90], Format/Datatype: Channel major FP16 format where channel % 16 == 0 }], Outputs: [ { Name: input.132, Location: Device, Dimensions: [1,256,90,90], Format/Datatype: Channel major FP16 format where channel % 16 == 0 }], ParameterType: Convolution, Kernel: [3,3], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [1,1], PostPadding: [1,1], Stride: [1,1], Dilation: [1,1], OutMaps: 256, Groups: 1, Weights: {"Type": "Half", "Count": 589824}, Bias: {"Type": "Half", "Count": 256}, HasSparseWeights: 0, Activation: RELU, HasBias: 1, HasReLU: 1, TacticName: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x128x32_stage4_warpsize2x2x1_g1_tensor16x8x16, TacticValue: 0xdfa020ef435ef810
Name: Conv_52 + Relu_53, LayerType: CaskConvolution, Inputs: [ { Name: input.132, Location: Device, Dimensions: [1,256,90,90], Format/Datatype: Channel major FP16 format where channel % 16 == 0 }], Outputs: [ { Name: input.144, Location: Device, Dimensions: [1,256,90,90], Format/Datatype: Channel major FP16 format where channel % 16 == 0 }], ParameterType: Convolution, Kernel: [3,3], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [1,1], PostPadding: [1,1], Stride: [1,1], Dilation: [1,1], OutMaps: 256, Groups: 1, Weights: {"Type": "Half", "Count": 589824}, Bias: {"Type": "Half", "Count": 256}, HasSparseWeights: 0, Activation: RELU, HasBias: 1, HasReLU: 1, TacticName: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x128x32_stage4_warpsize2x2x1_g1_tensor16x8x16, TacticValue: 0xdfa020ef435ef810
Name: Conv_54 + Relu_55, LayerType: CaskConvolution, Inputs: [ { Name: input.144, Location: Device, Dimensions: [1,256,90,90], Format/Datatype: Channel major FP16 format where channel % 16 == 0 }], Outputs: [ { Name: onnx::ConvTranspose_644, Location: Device, Dimensions: [1,256,90,90], Format/Datatype: Channel major FP16 format where channel % 16 == 0 }], ParameterType: Convolution, Kernel: [3,3], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [1,1], PostPadding: [1,1], Stride: [1,1], Dilation: [1,1], OutMaps: 256, Groups: 1, Weights: {"Type": "Half", "Count": 589824}, Bias: {"Type": "Half", "Count": 256}, HasSparseWeights: 0, Activation: RELU, HasBias: 1, HasReLU: 1, TacticName: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x128x32_stage4_warpsize2x2x1_g1_tensor16x8x16, TacticValue: 0xdfa020ef435ef810
Name: Reformatting CopyNode for Input Tensor 0 to ConvTranspose_56 + BatchNormalization_57 + Relu_58, LayerType: NoOp, Inputs: [ { Name: onnx::ConvTranspose_644, Location: Device, Dimensions: [1,256,90,90], Format/Datatype: Channel major FP16 format where channel % 16 == 0 }], Outputs: [ { Name: Reformatted Input Tensor 0 to ConvTranspose_56 + BatchNormalization_57 + Relu_58, Location: Device, Dimensions: [1,256,90,90], Format/Datatype: Channel major FP16 format where channel % 8 == 0 }], TacticValue: 0x0000000000000000
Name: ConvTranspose_56 + BatchNormalization_57 + Relu_58, LayerType: CaskDeconvolutionV2, Inputs: [ { Name: Reformatted Input Tensor 0 to ConvTranspose_56 + BatchNormalization_57 + Relu_58, Location: Device, Dimensions: [1,256,90,90], Format/Datatype: Channel major FP16 format where channel % 8 == 0 }], Outputs: [ { Name: onnx::Concat_647, Location: Device, Dimensions: [1,256,180,180], Format/Datatype: Channel major FP16 format where channel % 8 == 0 }], ParameterType: Convolution, Kernel: [2,2], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [0,0], PostPadding: [0,0], Stride: [2,2], Dilation: [1,1], OutMaps: 256, Groups: 1, Weights: {"Type": "Half", "Count": 262144}, Bias: {"Type": "Half", "Count": 256}, HasSparseWeights: 0, Activation: RELU, HasBias: 1, HasReLU: 1, TacticName: sm80_xmma_deconv_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x128x32_stage4_warpsize2x2x1_g1_tensor16x8x16_strided, TacticValue: 0x611e899dccd3003a
Name: onnx::Concat_647 copy, LayerType: Reformat, Inputs: [ { Name: onnx::Concat_647, Location: Device, Dimensions: [1,256,180,180], Format/Datatype: Channel major FP16 format where channel % 8 == 0 }], Outputs: [ { Name: input.164, Location: Device, Dimensions: [1,256,180,180], Format/Datatype: Channel major FP16 format where channel % 8 == 0 }], ParameterType: Reformat, Origin: CONCAT, TacticValue: 0x00000000000003ea
Name: Conv_60 + Relu_61, LayerType: CaskConvolution, Inputs: [ { Name: input.164, Location: Device, Dimensions: [1,512,180,180], Format/Datatype: Channel major FP16 format where channel % 8 == 0 }], Outputs: [ { Name: Reformatted Output Tensor 0 to Conv_60 + Relu_61, Location: Device, Dimensions: [1,64,180,180], Format/Datatype: Channel major FP16 format where channel % 8 == 0 }], ParameterType: Convolution, Kernel: [3,3], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [1,1], PostPadding: [1,1], Stride: [1,1], Dilation: [1,1], OutMaps: 64, Groups: 1, Weights: {"Type": "Half", "Count": 294912}, Bias: {"Type": "Half", "Count": 64}, HasSparseWeights: 0, Activation: RELU, HasBias: 1, HasReLU: 1, TacticName: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x64x32_stage3_warpsize4x1x1_g1_tensor16x8x16_t1r3s3, TacticValue: 0x529f4431bdae94f5
Name: Reformatting CopyNode for Output Tensor 0 to Conv_60 + Relu_61, LayerType: NoOp, Inputs: [ { Name: Reformatted Output Tensor 0 to Conv_60 + Relu_61, Location: Device, Dimensions: [1,64,180,180], Format/Datatype: Channel major FP16 format where channel % 8 == 0 }], Outputs: [ { Name: onnx::Conv_651, Location: Device, Dimensions: [1,64,180,180], Format/Datatype: Channel major FP16 format where channel % 16 == 0 }], TacticValue: 0x0000000000000000
Name: Conv_62 + Relu_63 || Conv_65 + Relu_66 || Conv_68 + Relu_69 || Conv_71 + Relu_72 || Conv_74 + Relu_75 || Conv_77 + Relu_78 || Conv_80 + Relu_81 || Conv_83 + Relu_84, LayerType: CaskConvolution, Inputs: [ { Name: onnx::Conv_651, Location: Device, Dimensions: [1,64,180,180], Format/Datatype: Channel major FP16 format where channel % 16 == 0 }], Outputs: [ { Name: Reformatted Output Tensor 0 to Conv_62 + Relu_63 || Conv_65 + Relu_66 || Conv_68 + Relu_69 || Conv_71 + Relu_72 || Conv_74 + Relu_75 || Conv_77 + Relu_78 || Conv_80 + Relu_81 || Conv_83 + Relu_84, Location: Device, Dimensions: [1,512,180,180], Format/Datatype: Channel major FP16 format where channel % 16 == 0 }], ParameterType: Convolution, Kernel: [3,3], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [1,1], PostPadding: [1,1], Stride: [1,1], Dilation: [1,1], OutMaps: 512, Groups: 1, Weights: {"Type": "Half", "Count": 294912}, Bias: {"Type": "Half", "Count": 512}, HasSparseWeights: 0, Activation: RELU, HasBias: 1, HasReLU: 1, TacticName: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x128x32_stage4_warpsize2x2x1_g1_tensor16x8x16_t1r3s3, TacticValue: 0x60c3421152ef8e10
Name: Reformatting CopyNode for Output Tensor 0 to Conv_62 + Relu_63 || Conv_65 + Relu_66 || Conv_68 + Relu_69 || Conv_71 + Relu_72 || Conv_74 + Relu_75 || Conv_77 + Relu_78 || Conv_80 + Relu_81 || Conv_83 + Relu_84, LayerType: NoOp, Inputs: [ { Name: Reformatted Output Tensor 0 to Conv_62 + Relu_63 || Conv_65 + Relu_66 || Conv_68 + Relu_69 || Conv_71 + Relu_72 || Conv_74 + Relu_75 || Conv_77 + Relu_78 || Conv_80 + Relu_81 || Conv_83 + Relu_84, Location: Device, Dimensions: [1,512,180,180], Format/Datatype: Channel major FP16 format where channel % 16 == 0 }], Outputs: [ { Name: Conv_62 + Relu_63 || Conv_65 + Relu_66 || Conv_68 + Relu_69 || Conv_71 + Relu_72 || Conv_74 + Relu_75 || Conv_77 + Relu_78 || Conv_80 + Relu_81 || Conv_83 + Relu_84, Location: Device, Dimensions: [1,512,180,180], Format/Datatype: Channel major FP16 format where channel % 8 == 0 }], TacticValue: 0x0000000000000000
Name: Conv_64, LayerType: CaskConvolution, Inputs: [ { Name: Conv_62 + Relu_63 || Conv_65 + Relu_66 || Conv_68 + Relu_69 || Conv_71 + Relu_72 || Conv_74 + Relu_75 || Conv_77 + Relu_78 || Conv_80 + Relu_81 || Conv_83 + Relu_84, Location: Device, Dimensions: [1,64,180,180], Format/Datatype: Channel major FP16 format where channel % 8 == 0 }], Outputs: [ { Name: Reformatted Output Tensor 0 to Conv_64, Location: Device, Dimensions: [1,2,180,180], Format/Datatype: Channel major FP16 format where channel % 8 == 0 }], ParameterType: Convolution, Kernel: [3,3], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [1,1], PostPadding: [1,1], Stride: [1,1], Dilation: [1,1], OutMaps: 2, Groups: 1, Weights: {"Type": "Half", "Count": 1152}, Bias: {"Type": "Half", "Count": 2}, HasSparseWeights: 0, Activation: NONE, HasBias: 1, HasReLU: 0, TacticName: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x32x32_stage4_warpsize4x1x1_g1_tensor16x8x16_t1r3s3, TacticValue: 0x3e7eb35b91b9fa63
Name: Reformatting CopyNode for Output Tensor 0 to Conv_64, LayerType: Reformat, Inputs: [ { Name: Reformatted Output Tensor 0 to Conv_64, Location: Device, Dimensions: [1,2,180,180], Format/Datatype: Channel major FP16 format where channel % 8 == 0 }], Outputs: [ { Name: reg_0, Location: Device, Dimensions: [1,2,180,180], Format/Datatype: Row major linear FP16 format }], ParameterType: Reformat, Origin: REFORMAT, TacticValue: 0x0000000000000000
Name: Conv_67, LayerType: CaskConvolution, Inputs: [ { Name: Conv_62 + Relu_63 || Conv_65 + Relu_66 || Conv_68 + Relu_69 || Conv_71 + Relu_72 || Conv_74 + Relu_75 || Conv_77 + Relu_78 || Conv_80 + Relu_81 || Conv_83 + Relu_84, Location: Device, Dimensions: [1,64,180,180], Format/Datatype: Channel major FP16 format where channel % 8 == 0 }], Outputs: [ { Name: Reformatted Output Tensor 0 to Conv_67, Location: Device, Dimensions: [1,1,180,180], Format/Datatype: Channel major FP16 format where channel % 8 == 0 }], ParameterType: Convolution, Kernel: [3,3], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [1,1], PostPadding: [1,1], Stride: [1,1], Dilation: [1,1], OutMaps: 1, Groups: 1, Weights: {"Type": "Half", "Count": 576}, Bias: {"Type": "Half", "Count": 1}, HasSparseWeights: 0, Activation: NONE, HasBias: 1, HasReLU: 0, TacticName: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x32x32_stage4_warpsize4x1x1_g1_tensor16x8x16_t1r3s3, TacticValue: 0x3e7eb35b91b9fa63
Name: Reformatting CopyNode for Output Tensor 0 to Conv_67, LayerType: Reformat, Inputs: [ { Name: Reformatted Output Tensor 0 to Conv_67, Location: Device, Dimensions: [1,1,180,180], Format/Datatype: Channel major FP16 format where channel % 8 == 0 }], Outputs: [ { Name: height_0, Location: Device, Dimensions: [1,1,180,180], Format/Datatype: Row major linear FP16 format }], ParameterType: Reformat, Origin: REFORMAT, TacticValue: 0x00000000000003e8
Name: Conv_70, LayerType: CaskConvolution, Inputs: [ { Name: Conv_62 + Relu_63 || Conv_65 + Relu_66 || Conv_68 + Relu_69 || Conv_71 + Relu_72 || Conv_74 + Relu_75 || Conv_77 + Relu_78 || Conv_80 + Relu_81 || Conv_83 + Relu_84, Location: Device, Dimensions: [1,64,180,180], Format/Datatype: Channel major FP16 format where channel % 8 == 0 }], Outputs: [ { Name: Reformatted Output Tensor 0 to Conv_70, Location: Device, Dimensions: [1,3,180,180], Format/Datatype: Channel major FP16 format where channel % 8 == 0 }], ParameterType: Convolution, Kernel: [3,3], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [1,1], PostPadding: [1,1], Stride: [1,1], Dilation: [1,1], OutMaps: 3, Groups: 1, Weights: {"Type": "Half", "Count": 1728}, Bias: {"Type": "Half", "Count": 3}, HasSparseWeights: 0, Activation: NONE, HasBias: 1, HasReLU: 0, TacticName: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x32x32_stage4_warpsize4x1x1_g1_tensor16x8x16_t1r3s3, TacticValue: 0x3e7eb35b91b9fa63
Name: Reformatting CopyNode for Output Tensor 0 to Conv_70, LayerType: Reformat, Inputs: [ { Name: Reformatted Output Tensor 0 to Conv_70, Location: Device, Dimensions: [1,3,180,180], Format/Datatype: Channel major FP16 format where channel % 8 == 0 }], Outputs: [ { Name: dim_0, Location: Device, Dimensions: [1,3,180,180], Format/Datatype: Row major linear FP16 format }], ParameterType: Reformat, Origin: REFORMAT, TacticValue: 0x0000000000000000
Name: Conv_73, LayerType: CaskConvolution, Inputs: [ { Name: Conv_62 + Relu_63 || Conv_65 + Relu_66 || Conv_68 + Relu_69 || Conv_71 + Relu_72 || Conv_74 + Relu_75 || Conv_77 + Relu_78 || Conv_80 + Relu_81 || Conv_83 + Relu_84, Location: Device, Dimensions: [1,64,180,180], Format/Datatype: Channel major FP16 format where channel % 8 == 0 }], Outputs: [ { Name: Reformatted Output Tensor 0 to Conv_73, Location: Device, Dimensions: [1,2,180,180], Format/Datatype: Channel major FP16 format where channel % 8 == 0 }], ParameterType: Convolution, Kernel: [3,3], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [1,1], PostPadding: [1,1], Stride: [1,1], Dilation: [1,1], OutMaps: 2, Groups: 1, Weights: {"Type": "Half", "Count": 1152}, Bias: {"Type": "Half", "Count": 2}, HasSparseWeights: 0, Activation: NONE, HasBias: 1, HasReLU: 0, TacticName: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x32x32_stage4_warpsize4x1x1_g1_tensor16x8x16_t1r3s3, TacticValue: 0x3e7eb35b91b9fa63
Name: Reformatting CopyNode for Output Tensor 0 to Conv_73, LayerType: Reformat, Inputs: [ { Name: Reformatted Output Tensor 0 to Conv_73, Location: Device, Dimensions: [1,2,180,180], Format/Datatype: Channel major FP16 format where channel % 8 == 0 }], Outputs: [ { Name: rot_0, Location: Device, Dimensions: [1,2,180,180], Format/Datatype: Row major linear FP16 format }], ParameterType: Reformat, Origin: REFORMAT, TacticValue: 0x0000000000000000
Name: Conv_76, LayerType: CaskConvolution, Inputs: [ { Name: Conv_62 + Relu_63 || Conv_65 + Relu_66 || Conv_68 + Relu_69 || Conv_71 + Relu_72 || Conv_74 + Relu_75 || Conv_77 + Relu_78 || Conv_80 + Relu_81 || Conv_83 + Relu_84, Location: Device, Dimensions: [1,64,180,180], Format/Datatype: Channel major FP16 format where channel % 8 == 0 }], Outputs: [ { Name: Reformatted Output Tensor 0 to Conv_76, Location: Device, Dimensions: [1,2,180,180], Format/Datatype: Channel major FP16 format where channel % 8 == 0 }], ParameterType: Convolution, Kernel: [3,3], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [1,1], PostPadding: [1,1], Stride: [1,1], Dilation: [1,1], OutMaps: 2, Groups: 1, Weights: {"Type": "Half", "Count": 1152}, Bias: {"Type": "Half", "Count": 2}, HasSparseWeights: 0, Activation: NONE, HasBias: 1, HasReLU: 0, TacticName: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x32x32_stage4_warpsize4x1x1_g1_tensor16x8x16_t1r3s3, TacticValue: 0x3e7eb35b91b9fa63
Name: Reformatting CopyNode for Output Tensor 0 to Conv_76, LayerType: Reformat, Inputs: [ { Name: Reformatted Output Tensor 0 to Conv_76, Location: Device, Dimensions: [1,2,180,180], Format/Datatype: Channel major FP16 format where channel % 8 == 0 }], Outputs: [ { Name: vel_0, Location: Device, Dimensions: [1,2,180,180], Format/Datatype: Row major linear FP16 format }], ParameterType: Reformat, Origin: REFORMAT, TacticValue: 0x0000000000000000
Name: Conv_79, LayerType: CaskConvolution, Inputs: [ { Name: Conv_62 + Relu_63 || Conv_65 + Relu_66 || Conv_68 + Relu_69 || Conv_71 + Relu_72 || Conv_74 + Relu_75 || Conv_77 + Relu_78 || Conv_80 + Relu_81 || Conv_83 + Relu_84, Location: Device, Dimensions: [1,64,180,180], Format/Datatype: Channel major FP16 format where channel % 8 == 0 }], Outputs: [ { Name: Reformatted Output Tensor 0 to Conv_79, Location: Device, Dimensions: [1,1,180,180], Format/Datatype: Channel major FP16 format where channel % 8 == 0 }], ParameterType: Convolution, Kernel: [3,3], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [1,1], PostPadding: [1,1], Stride: [1,1], Dilation: [1,1], OutMaps: 1, Groups: 1, Weights: {"Type": "Half", "Count": 576}, Bias: {"Type": "Half", "Count": 1}, HasSparseWeights: 0, Activation: NONE, HasBias: 1, HasReLU: 0, TacticName: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x32x32_stage4_warpsize4x1x1_g1_tensor16x8x16_t1r3s3, TacticValue: 0x3e7eb35b91b9fa63
Name: Reformatting CopyNode for Output Tensor 0 to Conv_79, LayerType: Reformat, Inputs: [ { Name: Reformatted Output Tensor 0 to Conv_79, Location: Device, Dimensions: [1,1,180,180], Format/Datatype: Channel major FP16 format where channel % 8 == 0 }], Outputs: [ { Name: hm_0, Location: Device, Dimensions: [1,1,180,180], Format/Datatype: Row major linear FP16 format }], ParameterType: Reformat, Origin: REFORMAT, TacticValue: 0x00000000000003e8
Name: Conv_82, LayerType: CaskConvolution, Inputs: [ { Name: Conv_62 + Relu_63 || Conv_65 + Relu_66 || Conv_68 + Relu_69 || Conv_71 + Relu_72 || Conv_74 + Relu_75 || Conv_77 + Relu_78 || Conv_80 + Relu_81 || Conv_83 + Relu_84, Location: Device, Dimensions: [1,64,180,180], Format/Datatype: Channel major FP16 format where channel % 8 == 0 }], Outputs: [ { Name: Reformatted Output Tensor 0 to Conv_82, Location: Device, Dimensions: [1,2,180,180], Format/Datatype: Channel major FP16 format where channel % 8 == 0 }], ParameterType: Convolution, Kernel: [3,3], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [1,1], PostPadding: [1,1], Stride: [1,1], Dilation: [1,1], OutMaps: 2, Groups: 1, Weights: {"Type": "Half", "Count": 1152}, Bias: {"Type": "Half", "Count": 2}, HasSparseWeights: 0, Activation: NONE, HasBias: 1, HasReLU: 0, TacticName: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x32x32_stage4_warpsize4x1x1_g1_tensor16x8x16_t1r3s3, TacticValue: 0x3e7eb35b91b9fa63
Name: Reformatting CopyNode for Output Tensor 0 to Conv_82, LayerType: Reformat, Inputs: [ { Name: Reformatted Output Tensor 0 to Conv_82, Location: Device, Dimensions: [1,2,180,180], Format/Datatype: Channel major FP16 format where channel % 8 == 0 }], Outputs: [ { Name: reg_1, Location: Device, Dimensions: [1,2,180,180], Format/Datatype: Row major linear FP16 format }], ParameterType: Reformat, Origin: REFORMAT, TacticValue: 0x0000000000000000
Name: Conv_85, LayerType: CaskConvolution, Inputs: [ { Name: Conv_62 + Relu_63 || Conv_65 + Relu_66 || Conv_68 + Relu_69 || Conv_71 + Relu_72 || Conv_74 + Relu_75 || Conv_77 + Relu_78 || Conv_80 + Relu_81 || Conv_83 + Relu_84, Location: Device, Dimensions: [1,64,180,180], Format/Datatype: Channel major FP16 format where channel % 8 == 0 }], Outputs: [ { Name: Reformatted Output Tensor 0 to Conv_85, Location: Device, Dimensions: [1,1,180,180], Format/Datatype: Channel major FP16 format where channel % 8 == 0 }], ParameterType: Convolution, Kernel: [3,3], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [1,1], PostPadding: [1,1], Stride: [1,1], Dilation: [1,1], OutMaps: 1, Groups: 1, Weights: {"Type": "Half", "Count": 576}, Bias: {"Type": "Half", "Count": 1}, HasSparseWeights: 0, Activation: NONE, HasBias: 1, HasReLU: 0, TacticName: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x32x32_stage4_warpsize4x1x1_g1_tensor16x8x16_t1r3s3, TacticValue: 0x3e7eb35b91b9fa63
Name: Reformatting CopyNode for Output Tensor 0 to Conv_85, LayerType: Reformat, Inputs: [ { Name: Reformatted Output Tensor 0 to Conv_85, Location: Device, Dimensions: [1,1,180,180], Format/Datatype: Channel major FP16 format where channel % 8 == 0 }], Outputs: [ { Name: height_1, Location: Device, Dimensions: [1,1,180,180], Format/Datatype: Row major linear FP16 format }], ParameterType: Reformat, Origin: REFORMAT, TacticValue: 0x00000000000003e8
Name: Conv_86 + Relu_87 || Conv_89 + Relu_90 || Conv_92 + Relu_93 || Conv_95 + Relu_96 || Conv_98 + Relu_99 || Conv_101 + Relu_102 || Conv_104 + Relu_105 || Conv_107 + Relu_108, LayerType: CaskConvolution, Inputs: [ { Name: onnx::Conv_651, Location: Device, Dimensions: [1,64,180,180], Format/Datatype: Channel major FP16 format where channel % 16 == 0 }], Outputs: [ { Name: Reformatted Output Tensor 0 to Conv_86 + Relu_87 || Conv_89 + Relu_90 || Conv_92 + Relu_93 || Conv_95 + Relu_96 || Conv_98 + Relu_99 || Conv_101 + Relu_102 || Conv_104 + Relu_105 || Conv_107 + Relu_108, Location: Device, Dimensions: [1,512,180,180], Format/Datatype: Channel major FP16 format where channel % 16 == 0 }], ParameterType: Convolution, Kernel: [3,3], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [1,1], PostPadding: [1,1], Stride: [1,1], Dilation: [1,1], OutMaps: 512, Groups: 1, Weights: {"Type": "Half", "Count": 294912}, Bias: {"Type": "Half", "Count": 512}, HasSparseWeights: 0, Activation: RELU, HasBias: 1, HasReLU: 1, TacticName: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x128x32_stage4_warpsize2x2x1_g1_tensor16x8x16_t1r3s3, TacticValue: 0x60c3421152ef8e10
Name: Reformatting CopyNode for Output Tensor 0 to Conv_86 + Relu_87 || Conv_89 + Relu_90 || Conv_92 + Relu_93 || Conv_95 + Relu_96 || Conv_98 + Relu_99 || Conv_101 + Relu_102 || Conv_104 + Relu_105 || Conv_107 + Relu_108, LayerType: NoOp, Inputs: [ { Name: Reformatted Output Tensor 0 to Conv_86 + Relu_87 || Conv_89 + Relu_90 || Conv_92 + Relu_93 || Conv_95 + Relu_96 || Conv_98 + Relu_99 || Conv_101 + Relu_102 || Conv_104 + Relu_105 || Conv_107 + Relu_108, Location: Device, Dimensions: [1,512,180,180], Format/Datatype: Channel major FP16 format where channel % 16 == 0 }], Outputs: [ { Name: Conv_86 + Relu_87 || Conv_89 + Relu_90 || Conv_92 + Relu_93 || Conv_95 + Relu_96 || Conv_98 + Relu_99 || Conv_101 + Relu_102 || Conv_104 + Relu_105 || Conv_107 + Relu_108, Location: Device, Dimensions: [1,512,180,180], Format/Datatype: Channel major FP16 format where channel % 8 == 0 }], TacticValue: 0x0000000000000000
Name: Conv_88, LayerType: CaskConvolution, Inputs: [ { Name: Conv_86 + Relu_87 || Conv_89 + Relu_90 || Conv_92 + Relu_93 || Conv_95 + Relu_96 || Conv_98 + Relu_99 || Conv_101 + Relu_102 || Conv_104 + Relu_105 || Conv_107 + Relu_108, Location: Device, Dimensions: [1,64,180,180], Format/Datatype: Channel major FP16 format where channel % 8 == 0 }], Outputs: [ { Name: Reformatted Output Tensor 0 to Conv_88, Location: Device, Dimensions: [1,3,180,180], Format/Datatype: Channel major FP16 format where channel % 8 == 0 }], ParameterType: Convolution, Kernel: [3,3], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [1,1], PostPadding: [1,1], Stride: [1,1], Dilation: [1,1], OutMaps: 3, Groups: 1, Weights: {"Type": "Half", "Count": 1728}, Bias: {"Type": "Half", "Count": 3}, HasSparseWeights: 0, Activation: NONE, HasBias: 1, HasReLU: 0, TacticName: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x32x32_stage4_warpsize4x1x1_g1_tensor16x8x16_t1r3s3, TacticValue: 0x3e7eb35b91b9fa63
Name: Reformatting CopyNode for Output Tensor 0 to Conv_88, LayerType: Reformat, Inputs: [ { Name: Reformatted Output Tensor 0 to Conv_88, Location: Device, Dimensions: [1,3,180,180], Format/Datatype: Channel major FP16 format where channel % 8 == 0 }], Outputs: [ { Name: dim_1, Location: Device, Dimensions: [1,3,180,180], Format/Datatype: Row major linear FP16 format }], ParameterType: Reformat, Origin: REFORMAT, TacticValue: 0x0000000000000000
Name: Conv_91, LayerType: CaskConvolution, Inputs: [ { Name: Conv_86 + Relu_87 || Conv_89 + Relu_90 || Conv_92 + Relu_93 || Conv_95 + Relu_96 || Conv_98 + Relu_99 || Conv_101 + Relu_102 || Conv_104 + Relu_105 || Conv_107 + Relu_108, Location: Device, Dimensions: [1,64,180,180], Format/Datatype: Channel major FP16 format where channel % 8 == 0 }], Outputs: [ { Name: Reformatted Output Tensor 0 to Conv_91, Location: Device, Dimensions: [1,2,180,180], Format/Datatype: Channel major FP16 format where channel % 8 == 0 }], ParameterType: Convolution, Kernel: [3,3], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [1,1], PostPadding: [1,1], Stride: [1,1], Dilation: [1,1], OutMaps: 2, Groups: 1, Weights: {"Type": "Half", "Count": 1152}, Bias: {"Type": "Half", "Count": 2}, HasSparseWeights: 0, Activation: NONE, HasBias: 1, HasReLU: 0, TacticName: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x32x32_stage4_warpsize4x1x1_g1_tensor16x8x16_t1r3s3, TacticValue: 0x3e7eb35b91b9fa63
Name: Reformatting CopyNode for Output Tensor 0 to Conv_91, LayerType: Reformat, Inputs: [ { Name: Reformatted Output Tensor 0 to Conv_91, Location: Device, Dimensions: [1,2,180,180], Format/Datatype: Channel major FP16 format where channel % 8 == 0 }], Outputs: [ { Name: rot_1, Location: Device, Dimensions: [1,2,180,180], Format/Datatype: Row major linear FP16 format }], ParameterType: Reformat, Origin: REFORMAT, TacticValue: 0x0000000000000000
Name: Conv_94, LayerType: CaskConvolution, Inputs: [ { Name: Conv_86 + Relu_87 || Conv_89 + Relu_90 || Conv_92 + Relu_93 || Conv_95 + Relu_96 || Conv_98 + Relu_99 || Conv_101 + Relu_102 || Conv_104 + Relu_105 || Conv_107 + Relu_108, Location: Device, Dimensions: [1,64,180,180], Format/Datatype: Channel major FP16 format where channel % 8 == 0 }], Outputs: [ { Name: Reformatted Output Tensor 0 to Conv_94, Location: Device, Dimensions: [1,2,180,180], Format/Datatype: Channel major FP16 format where channel % 8 == 0 }], ParameterType: Convolution, Kernel: [3,3], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [1,1], PostPadding: [1,1], Stride: [1,1], Dilation: [1,1], OutMaps: 2, Groups: 1, Weights: {"Type": "Half", "Count": 1152}, Bias: {"Type": "Half", "Count": 2}, HasSparseWeights: 0, Activation: NONE, HasBias: 1, HasReLU: 0, TacticName: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x32x32_stage4_warpsize4x1x1_g1_tensor16x8x16_t1r3s3, TacticValue: 0x3e7eb35b91b9fa63
Name: Reformatting CopyNode for Output Tensor 0 to Conv_94, LayerType: Reformat, Inputs: [ { Name: Reformatted Output Tensor 0 to Conv_94, Location: Device, Dimensions: [1,2,180,180], Format/Datatype: Channel major FP16 format where channel % 8 == 0 }], Outputs: [ { Name: vel_1, Location: Device, Dimensions: [1,2,180,180], Format/Datatype: Row major linear FP16 format }], ParameterType: Reformat, Origin: REFORMAT, TacticValue: 0x0000000000000000
Name: Conv_97, LayerType: CaskConvolution, Inputs: [ { Name: Conv_86 + Relu_87 || Conv_89 + Relu_90 || Conv_92 + Relu_93 || Conv_95 + Relu_96 || Conv_98 + Relu_99 || Conv_101 + Relu_102 || Conv_104 + Relu_105 || Conv_107 + Relu_108, Location: Device, Dimensions: [1,64,180,180], Format/Datatype: Channel major FP16 format where channel % 8 == 0 }], Outputs: [ { Name: Reformatted Output Tensor 0 to Conv_97, Location: Device, Dimensions: [1,2,180,180], Format/Datatype: Channel major FP16 format where channel % 8 == 0 }], ParameterType: Convolution, Kernel: [3,3], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [1,1], PostPadding: [1,1], Stride: [1,1], Dilation: [1,1], OutMaps: 2, Groups: 1, Weights: {"Type": "Half", "Count": 1152}, Bias: {"Type": "Half", "Count": 2}, HasSparseWeights: 0, Activation: NONE, HasBias: 1, HasReLU: 0, TacticName: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x32x32_stage4_warpsize4x1x1_g1_tensor16x8x16_t1r3s3, TacticValue: 0x3e7eb35b91b9fa63
Name: Reformatting CopyNode for Output Tensor 0 to Conv_97, LayerType: Reformat, Inputs: [ { Name: Reformatted Output Tensor 0 to Conv_97, Location: Device, Dimensions: [1,2,180,180], Format/Datatype: Channel major FP16 format where channel % 8 == 0 }], Outputs: [ { Name: hm_1, Location: Device, Dimensions: [1,2,180,180], Format/Datatype: Row major linear FP16 format }], ParameterType: Reformat, Origin: REFORMAT, TacticValue: 0x0000000000000000
Name: Conv_100, LayerType: CaskConvolution, Inputs: [ { Name: Conv_86 + Relu_87 || Conv_89 + Relu_90 || Conv_92 + Relu_93 || Conv_95 + Relu_96 || Conv_98 + Relu_99 || Conv_101 + Relu_102 || Conv_104 + Relu_105 || Conv_107 + Relu_108, Location: Device, Dimensions: [1,64,180,180], Format/Datatype: Channel major FP16 format where channel % 8 == 0 }], Outputs: [ { Name: Reformatted Output Tensor 0 to Conv_100, Location: Device, Dimensions: [1,2,180,180], Format/Datatype: Channel major FP16 format where channel % 8 == 0 }], ParameterType: Convolution, Kernel: [3,3], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [1,1], PostPadding: [1,1], Stride: [1,1], Dilation: [1,1], OutMaps: 2, Groups: 1, Weights: {"Type": "Half", "Count": 1152}, Bias: {"Type": "Half", "Count": 2}, HasSparseWeights: 0, Activation: NONE, HasBias: 1, HasReLU: 0, TacticName: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x32x32_stage4_warpsize4x1x1_g1_tensor16x8x16_t1r3s3, TacticValue: 0x3e7eb35b91b9fa63
Name: Reformatting CopyNode for Output Tensor 0 to Conv_100, LayerType: Reformat, Inputs: [ { Name: Reformatted Output Tensor 0 to Conv_100, Location: Device, Dimensions: [1,2,180,180], Format/Datatype: Channel major FP16 format where channel % 8 == 0 }], Outputs: [ { Name: reg_2, Location: Device, Dimensions: [1,2,180,180], Format/Datatype: Row major linear FP16 format }], ParameterType: Reformat, Origin: REFORMAT, TacticValue: 0x0000000000000000
Name: Conv_103, LayerType: CaskConvolution, Inputs: [ { Name: Conv_86 + Relu_87 || Conv_89 + Relu_90 || Conv_92 + Relu_93 || Conv_95 + Relu_96 || Conv_98 + Relu_99 || Conv_101 + Relu_102 || Conv_104 + Relu_105 || Conv_107 + Relu_108, Location: Device, Dimensions: [1,64,180,180], Format/Datatype: Channel major FP16 format where channel % 8 == 0 }], Outputs: [ { Name: Reformatted Output Tensor 0 to Conv_103, Location: Device, Dimensions: [1,1,180,180], Format/Datatype: Channel major FP16 format where channel % 8 == 0 }], ParameterType: Convolution, Kernel: [3,3], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [1,1], PostPadding: [1,1], Stride: [1,1], Dilation: [1,1], OutMaps: 1, Groups: 1, Weights: {"Type": "Half", "Count": 576}, Bias: {"Type": "Half", "Count": 1}, HasSparseWeights: 0, Activation: NONE, HasBias: 1, HasReLU: 0, TacticName: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x32x32_stage4_warpsize4x1x1_g1_tensor16x8x16_t1r3s3, TacticValue: 0x3e7eb35b91b9fa63
Name: Reformatting CopyNode for Output Tensor 0 to Conv_103, LayerType: Reformat, Inputs: [ { Name: Reformatted Output Tensor 0 to Conv_103, Location: Device, Dimensions: [1,1,180,180], Format/Datatype: Channel major FP16 format where channel % 8 == 0 }], Outputs: [ { Name: height_2, Location: Device, Dimensions: [1,1,180,180], Format/Datatype: Row major linear FP16 format }], ParameterType: Reformat, Origin: REFORMAT, TacticValue: 0x00000000000003e8
Name: Conv_106, LayerType: CaskConvolution, Inputs: [ { Name: Conv_86 + Relu_87 || Conv_89 + Relu_90 || Conv_92 + Relu_93 || Conv_95 + Relu_96 || Conv_98 + Relu_99 || Conv_101 + Relu_102 || Conv_104 + Relu_105 || Conv_107 + Relu_108, Location: Device, Dimensions: [1,64,180,180], Format/Datatype: Channel major FP16 format where channel % 8 == 0 }], Outputs: [ { Name: Reformatted Output Tensor 0 to Conv_106, Location: Device, Dimensions: [1,3,180,180], Format/Datatype: Channel major FP16 format where channel % 8 == 0 }], ParameterType: Convolution, Kernel: [3,3], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [1,1], PostPadding: [1,1], Stride: [1,1], Dilation: [1,1], OutMaps: 3, Groups: 1, Weights: {"Type": "Half", "Count": 1728}, Bias: {"Type": "Half", "Count": 3}, HasSparseWeights: 0, Activation: NONE, HasBias: 1, HasReLU: 0, TacticName: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x32x32_stage4_warpsize4x1x1_g1_tensor16x8x16_t1r3s3, TacticValue: 0x3e7eb35b91b9fa63
Name: Reformatting CopyNode for Output Tensor 0 to Conv_106, LayerType: Reformat, Inputs: [ { Name: Reformatted Output Tensor 0 to Conv_106, Location: Device, Dimensions: [1,3,180,180], Format/Datatype: Channel major FP16 format where channel % 8 == 0 }], Outputs: [ { Name: dim_2, Location: Device, Dimensions: [1,3,180,180], Format/Datatype: Row major linear FP16 format }], ParameterType: Reformat, Origin: REFORMAT, TacticValue: 0x0000000000000000
Name: Conv_109, LayerType: CaskConvolution, Inputs: [ { Name: Conv_86 + Relu_87 || Conv_89 + Relu_90 || Conv_92 + Relu_93 || Conv_95 + Relu_96 || Conv_98 + Relu_99 || Conv_101 + Relu_102 || Conv_104 + Relu_105 || Conv_107 + Relu_108, Location: Device, Dimensions: [1,64,180,180], Format/Datatype: Channel major FP16 format where channel % 8 == 0 }], Outputs: [ { Name: Reformatted Output Tensor 0 to Conv_109, Location: Device, Dimensions: [1,2,180,180], Format/Datatype: Channel major FP16 format where channel % 8 == 0 }], ParameterType: Convolution, Kernel: [3,3], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [1,1], PostPadding: [1,1], Stride: [1,1], Dilation: [1,1], OutMaps: 2, Groups: 1, Weights: {"Type": "Half", "Count": 1152}, Bias: {"Type": "Half", "Count": 2}, HasSparseWeights: 0, Activation: NONE, HasBias: 1, HasReLU: 0, TacticName: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x32x32_stage4_warpsize4x1x1_g1_tensor16x8x16_t1r3s3, TacticValue: 0x3e7eb35b91b9fa63
Name: Reformatting CopyNode for Output Tensor 0 to Conv_109, LayerType: Reformat, Inputs: [ { Name: Reformatted Output Tensor 0 to Conv_109, Location: Device, Dimensions: [1,2,180,180], Format/Datatype: Channel major FP16 format where channel % 8 == 0 }], Outputs: [ { Name: rot_2, Location: Device, Dimensions: [1,2,180,180], Format/Datatype: Row major linear FP16 format }], ParameterType: Reformat, Origin: REFORMAT, TacticValue: 0x0000000000000000
Name: Conv_110 + Relu_111 || Conv_113 + Relu_114 || Conv_116 + Relu_117 || Conv_119 + Relu_120 || Conv_122 + Relu_123 || Conv_125 + Relu_126 || Conv_128 + Relu_129 || Conv_131 + Relu_132, LayerType: CaskConvolution, Inputs: [ { Name: onnx::Conv_651, Location: Device, Dimensions: [1,64,180,180], Format/Datatype: Channel major FP16 format where channel % 16 == 0 }], Outputs: [ { Name: Reformatted Output Tensor 0 to Conv_110 + Relu_111 || Conv_113 + Relu_114 || Conv_116 + Relu_117 || Conv_119 + Relu_120 || Conv_122 + Relu_123 || Conv_125 + Relu_126 || Conv_128 + Relu_129 || Conv_131 + Relu_132, Location: Device, Dimensions: [1,512,180,180], Format/Datatype: Channel major FP16 format where channel % 16 == 0 }], ParameterType: Convolution, Kernel: [3,3], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [1,1], PostPadding: [1,1], Stride: [1,1], Dilation: [1,1], OutMaps: 512, Groups: 1, Weights: {"Type": "Half", "Count": 294912}, Bias: {"Type": "Half", "Count": 512}, HasSparseWeights: 0, Activation: RELU, HasBias: 1, HasReLU: 1, TacticName: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x128x32_stage4_warpsize2x2x1_g1_tensor16x8x16_t1r3s3, TacticValue: 0x60c3421152ef8e10
Name: Reformatting CopyNode for Output Tensor 0 to Conv_110 + Relu_111 || Conv_113 + Relu_114 || Conv_116 + Relu_117 || Conv_119 + Relu_120 || Conv_122 + Relu_123 || Conv_125 + Relu_126 || Conv_128 + Relu_129 || Conv_131 + Relu_132, LayerType: NoOp, Inputs: [ { Name: Reformatted Output Tensor 0 to Conv_110 + Relu_111 || Conv_113 + Relu_114 || Conv_116 + Relu_117 || Conv_119 + Relu_120 || Conv_122 + Relu_123 || Conv_125 + Relu_126 || Conv_128 + Relu_129 || Conv_131 + Relu_132, Location: Device, Dimensions: [1,512,180,180], Format/Datatype: Channel major FP16 format where channel % 16 == 0 }], Outputs: [ { Name: Conv_110 + Relu_111 || Conv_113 + Relu_114 || Conv_116 + Relu_117 || Conv_119 + Relu_120 || Conv_122 + Relu_123 || Conv_125 + Relu_126 || Conv_128 + Relu_129 || Conv_131 + Relu_132, Location: Device, Dimensions: [1,512,180,180], Format/Datatype: Channel major FP16 format where channel % 8 == 0 }], TacticValue: 0x0000000000000000
Name: Conv_112, LayerType: CaskConvolution, Inputs: [ { Name: Conv_110 + Relu_111 || Conv_113 + Relu_114 || Conv_116 + Relu_117 || Conv_119 + Relu_120 || Conv_122 + Relu_123 || Conv_125 + Relu_126 || Conv_128 + Relu_129 || Conv_131 + Relu_132, Location: Device, Dimensions: [1,64,180,180], Format/Datatype: Channel major FP16 format where channel % 8 == 0 }], Outputs: [ { Name: Reformatted Output Tensor 0 to Conv_112, Location: Device, Dimensions: [1,2,180,180], Format/Datatype: Channel major FP16 format where channel % 8 == 0 }], ParameterType: Convolution, Kernel: [3,3], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [1,1], PostPadding: [1,1], Stride: [1,1], Dilation: [1,1], OutMaps: 2, Groups: 1, Weights: {"Type": "Half", "Count": 1152}, Bias: {"Type": "Half", "Count": 2}, HasSparseWeights: 0, Activation: NONE, HasBias: 1, HasReLU: 0, TacticName: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x32x32_stage4_warpsize4x1x1_g1_tensor16x8x16_t1r3s3, TacticValue: 0x3e7eb35b91b9fa63
Name: Reformatting CopyNode for Output Tensor 0 to Conv_112, LayerType: Reformat, Inputs: [ { Name: Reformatted Output Tensor 0 to Conv_112, Location: Device, Dimensions: [1,2,180,180], Format/Datatype: Channel major FP16 format where channel % 8 == 0 }], Outputs: [ { Name: vel_2, Location: Device, Dimensions: [1,2,180,180], Format/Datatype: Row major linear FP16 format }], ParameterType: Reformat, Origin: REFORMAT, TacticValue: 0x0000000000000000
Name: Conv_115, LayerType: CaskConvolution, Inputs: [ { Name: Conv_110 + Relu_111 || Conv_113 + Relu_114 || Conv_116 + Relu_117 || Conv_119 + Relu_120 || Conv_122 + Relu_123 || Conv_125 + Relu_126 || Conv_128 + Relu_129 || Conv_131 + Relu_132, Location: Device, Dimensions: [1,64,180,180], Format/Datatype: Channel major FP16 format where channel % 8 == 0 }], Outputs: [ { Name: Reformatted Output Tensor 0 to Conv_115, Location: Device, Dimensions: [1,2,180,180], Format/Datatype: Channel major FP16 format where channel % 8 == 0 }], ParameterType: Convolution, Kernel: [3,3], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [1,1], PostPadding: [1,1], Stride: [1,1], Dilation: [1,1], OutMaps: 2, Groups: 1, Weights: {"Type": "Half", "Count": 1152}, Bias: {"Type": "Half", "Count": 2}, HasSparseWeights: 0, Activation: NONE, HasBias: 1, HasReLU: 0, TacticName: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x32x32_stage4_warpsize4x1x1_g1_tensor16x8x16_t1r3s3, TacticValue: 0x3e7eb35b91b9fa63
Name: Reformatting CopyNode for Output Tensor 0 to Conv_115, LayerType: Reformat, Inputs: [ { Name: Reformatted Output Tensor 0 to Conv_115, Location: Device, Dimensions: [1,2,180,180], Format/Datatype: Channel major FP16 format where channel % 8 == 0 }], Outputs: [ { Name: hm_2, Location: Device, Dimensions: [1,2,180,180], Format/Datatype: Row major linear FP16 format }], ParameterType: Reformat, Origin: REFORMAT, TacticValue: 0x0000000000000000
Name: Conv_118, LayerType: CaskConvolution, Inputs: [ { Name: Conv_110 + Relu_111 || Conv_113 + Relu_114 || Conv_116 + Relu_117 || Conv_119 + Relu_120 || Conv_122 + Relu_123 || Conv_125 + Relu_126 || Conv_128 + Relu_129 || Conv_131 + Relu_132, Location: Device, Dimensions: [1,64,180,180], Format/Datatype: Channel major FP16 format where channel % 8 == 0 }], Outputs: [ { Name: Reformatted Output Tensor 0 to Conv_118, Location: Device, Dimensions: [1,2,180,180], Format/Datatype: Channel major FP16 format where channel % 8 == 0 }], ParameterType: Convolution, Kernel: [3,3], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [1,1], PostPadding: [1,1], Stride: [1,1], Dilation: [1,1], OutMaps: 2, Groups: 1, Weights: {"Type": "Half", "Count": 1152}, Bias: {"Type": "Half", "Count": 2}, HasSparseWeights: 0, Activation: NONE, HasBias: 1, HasReLU: 0, TacticName: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x32x32_stage4_warpsize4x1x1_g1_tensor16x8x16_t1r3s3, TacticValue: 0x3e7eb35b91b9fa63
Name: Reformatting CopyNode for Output Tensor 0 to Conv_118, LayerType: Reformat, Inputs: [ { Name: Reformatted Output Tensor 0 to Conv_118, Location: Device, Dimensions: [1,2,180,180], Format/Datatype: Channel major FP16 format where channel % 8 == 0 }], Outputs: [ { Name: reg_3, Location: Device, Dimensions: [1,2,180,180], Format/Datatype: Row major linear FP16 format }], ParameterType: Reformat, Origin: REFORMAT, TacticValue: 0x0000000000000000
Name: Conv_121, LayerType: CaskConvolution, Inputs: [ { Name: Conv_110 + Relu_111 || Conv_113 + Relu_114 || Conv_116 + Relu_117 || Conv_119 + Relu_120 || Conv_122 + Relu_123 || Conv_125 + Relu_126 || Conv_128 + Relu_129 || Conv_131 + Relu_132, Location: Device, Dimensions: [1,64,180,180], Format/Datatype: Channel major FP16 format where channel % 8 == 0 }], Outputs: [ { Name: Reformatted Output Tensor 0 to Conv_121, Location: Device, Dimensions: [1,1,180,180], Format/Datatype: Channel major FP16 format where channel % 8 == 0 }], ParameterType: Convolution, Kernel: [3,3], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [1,1], PostPadding: [1,1], Stride: [1,1], Dilation: [1,1], OutMaps: 1, Groups: 1, Weights: {"Type": "Half", "Count": 576}, Bias: {"Type": "Half", "Count": 1}, HasSparseWeights: 0, Activation: NONE, HasBias: 1, HasReLU: 0, TacticName: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x32x32_stage4_warpsize4x1x1_g1_tensor16x8x16_t1r3s3, TacticValue: 0x3e7eb35b91b9fa63
Name: Reformatting CopyNode for Output Tensor 0 to Conv_121, LayerType: Reformat, Inputs: [ { Name: Reformatted Output Tensor 0 to Conv_121, Location: Device, Dimensions: [1,1,180,180], Format/Datatype: Channel major FP16 format where channel % 8 == 0 }], Outputs: [ { Name: height_3, Location: Device, Dimensions: [1,1,180,180], Format/Datatype: Row major linear FP16 format }], ParameterType: Reformat, Origin: REFORMAT, TacticValue: 0x00000000000003e8
Name: Conv_124, LayerType: CaskConvolution, Inputs: [ { Name: Conv_110 + Relu_111 || Conv_113 + Relu_114 || Conv_116 + Relu_117 || Conv_119 + Relu_120 || Conv_122 + Relu_123 || Conv_125 + Relu_126 || Conv_128 + Relu_129 || Conv_131 + Relu_132, Location: Device, Dimensions: [1,64,180,180], Format/Datatype: Channel major FP16 format where channel % 8 == 0 }], Outputs: [ { Name: Reformatted Output Tensor 0 to Conv_124, Location: Device, Dimensions: [1,3,180,180], Format/Datatype: Channel major FP16 format where channel % 8 == 0 }], ParameterType: Convolution, Kernel: [3,3], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [1,1], PostPadding: [1,1], Stride: [1,1], Dilation: [1,1], OutMaps: 3, Groups: 1, Weights: {"Type": "Half", "Count": 1728}, Bias: {"Type": "Half", "Count": 3}, HasSparseWeights: 0, Activation: NONE, HasBias: 1, HasReLU: 0, TacticName: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x32x32_stage4_warpsize4x1x1_g1_tensor16x8x16_t1r3s3, TacticValue: 0x3e7eb35b91b9fa63
Name: Reformatting CopyNode for Output Tensor 0 to Conv_124, LayerType: Reformat, Inputs: [ { Name: Reformatted Output Tensor 0 to Conv_124, Location: Device, Dimensions: [1,3,180,180], Format/Datatype: Channel major FP16 format where channel % 8 == 0 }], Outputs: [ { Name: dim_3, Location: Device, Dimensions: [1,3,180,180], Format/Datatype: Row major linear FP16 format }], ParameterType: Reformat, Origin: REFORMAT, TacticValue: 0x0000000000000000
Name: Conv_127, LayerType: CaskConvolution, Inputs: [ { Name: Conv_110 + Relu_111 || Conv_113 + Relu_114 || Conv_116 + Relu_117 || Conv_119 + Relu_120 || Conv_122 + Relu_123 || Conv_125 + Relu_126 || Conv_128 + Relu_129 || Conv_131 + Relu_132, Location: Device, Dimensions: [1,64,180,180], Format/Datatype: Channel major FP16 format where channel % 8 == 0 }], Outputs: [ { Name: Reformatted Output Tensor 0 to Conv_127, Location: Device, Dimensions: [1,2,180,180], Format/Datatype: Channel major FP16 format where channel % 8 == 0 }], ParameterType: Convolution, Kernel: [3,3], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [1,1], PostPadding: [1,1], Stride: [1,1], Dilation: [1,1], OutMaps: 2, Groups: 1, Weights: {"Type": "Half", "Count": 1152}, Bias: {"Type": "Half", "Count": 2}, HasSparseWeights: 0, Activation: NONE, HasBias: 1, HasReLU: 0, TacticName: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x32x32_stage4_warpsize4x1x1_g1_tensor16x8x16_t1r3s3, TacticValue: 0x3e7eb35b91b9fa63
Name: Reformatting CopyNode for Output Tensor 0 to Conv_127, LayerType: Reformat, Inputs: [ { Name: Reformatted Output Tensor 0 to Conv_127, Location: Device, Dimensions: [1,2,180,180], Format/Datatype: Channel major FP16 format where channel % 8 == 0 }], Outputs: [ { Name: rot_3, Location: Device, Dimensions: [1,2,180,180], Format/Datatype: Row major linear FP16 format }], ParameterType: Reformat, Origin: REFORMAT, TacticValue: 0x0000000000000000
Name: Conv_130, LayerType: CaskConvolution, Inputs: [ { Name: Conv_110 + Relu_111 || Conv_113 + Relu_114 || Conv_116 + Relu_117 || Conv_119 + Relu_120 || Conv_122 + Relu_123 || Conv_125 + Relu_126 || Conv_128 + Relu_129 || Conv_131 + Relu_132, Location: Device, Dimensions: [1,64,180,180], Format/Datatype: Channel major FP16 format where channel % 8 == 0 }], Outputs: [ { Name: Reformatted Output Tensor 0 to Conv_130, Location: Device, Dimensions: [1,2,180,180], Format/Datatype: Channel major FP16 format where channel % 8 == 0 }], ParameterType: Convolution, Kernel: [3,3], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [1,1], PostPadding: [1,1], Stride: [1,1], Dilation: [1,1], OutMaps: 2, Groups: 1, Weights: {"Type": "Half", "Count": 1152}, Bias: {"Type": "Half", "Count": 2}, HasSparseWeights: 0, Activation: NONE, HasBias: 1, HasReLU: 0, TacticName: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x32x32_stage4_warpsize4x1x1_g1_tensor16x8x16_t1r3s3, TacticValue: 0x3e7eb35b91b9fa63
Name: Reformatting CopyNode for Output Tensor 0 to Conv_130, LayerType: Reformat, Inputs: [ { Name: Reformatted Output Tensor 0 to Conv_130, Location: Device, Dimensions: [1,2,180,180], Format/Datatype: Channel major FP16 format where channel % 8 == 0 }], Outputs: [ { Name: vel_3, Location: Device, Dimensions: [1,2,180,180], Format/Datatype: Row major linear FP16 format }], ParameterType: Reformat, Origin: REFORMAT, TacticValue: 0x0000000000000000
Name: Conv_133, LayerType: CaskConvolution, Inputs: [ { Name: Conv_110 + Relu_111 || Conv_113 + Relu_114 || Conv_116 + Relu_117 || Conv_119 + Relu_120 || Conv_122 + Relu_123 || Conv_125 + Relu_126 || Conv_128 + Relu_129 || Conv_131 + Relu_132, Location: Device, Dimensions: [1,64,180,180], Format/Datatype: Channel major FP16 format where channel % 8 == 0 }], Outputs: [ { Name: Reformatted Output Tensor 0 to Conv_133, Location: Device, Dimensions: [1,1,180,180], Format/Datatype: Channel major FP16 format where channel % 8 == 0 }], ParameterType: Convolution, Kernel: [3,3], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [1,1], PostPadding: [1,1], Stride: [1,1], Dilation: [1,1], OutMaps: 1, Groups: 1, Weights: {"Type": "Half", "Count": 576}, Bias: {"Type": "Half", "Count": 1}, HasSparseWeights: 0, Activation: NONE, HasBias: 1, HasReLU: 0, TacticName: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x32x32_stage4_warpsize4x1x1_g1_tensor16x8x16_t1r3s3, TacticValue: 0x3e7eb35b91b9fa63
Name: Reformatting CopyNode for Output Tensor 0 to Conv_133, LayerType: Reformat, Inputs: [ { Name: Reformatted Output Tensor 0 to Conv_133, Location: Device, Dimensions: [1,1,180,180], Format/Datatype: Channel major FP16 format where channel % 8 == 0 }], Outputs: [ { Name: hm_3, Location: Device, Dimensions: [1,1,180,180], Format/Datatype: Row major linear FP16 format }], ParameterType: Reformat, Origin: REFORMAT, TacticValue: 0x00000000000003e8
Name: Conv_134 + Relu_135 || Conv_137 + Relu_138 || Conv_140 + Relu_141 || Conv_143 + Relu_144 || Conv_146 + Relu_147 || Conv_149 + Relu_150 || Conv_152 + Relu_153 || Conv_155 + Relu_156, LayerType: CaskConvolution, Inputs: [ { Name: onnx::Conv_651, Location: Device, Dimensions: [1,64,180,180], Format/Datatype: Channel major FP16 format where channel % 16 == 0 }], Outputs: [ { Name: Reformatted Output Tensor 0 to Conv_134 + Relu_135 || Conv_137 + Relu_138 || Conv_140 + Relu_141 || Conv_143 + Relu_144 || Conv_146 + Relu_147 || Conv_149 + Relu_150 || Conv_152 + Relu_153 || Conv_155 + Relu_156, Location: Device, Dimensions: [1,512,180,180], Format/Datatype: Channel major FP16 format where channel % 16 == 0 }], ParameterType: Convolution, Kernel: [3,3], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [1,1], PostPadding: [1,1], Stride: [1,1], Dilation: [1,1], OutMaps: 512, Groups: 1, Weights: {"Type": "Half", "Count": 294912}, Bias: {"Type": "Half", "Count": 512}, HasSparseWeights: 0, Activation: RELU, HasBias: 1, HasReLU: 1, TacticName: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x128x32_stage4_warpsize2x2x1_g1_tensor16x8x16_t1r3s3, TacticValue: 0x60c3421152ef8e10
Name: Reformatting CopyNode for Output Tensor 0 to Conv_134 + Relu_135 || Conv_137 + Relu_138 || Conv_140 + Relu_141 || Conv_143 + Relu_144 || Conv_146 + Relu_147 || Conv_149 + Relu_150 || Conv_152 + Relu_153 || Conv_155 + Relu_156, LayerType: NoOp, Inputs: [ { Name: Reformatted Output Tensor 0 to Conv_134 + Relu_135 || Conv_137 + Relu_138 || Conv_140 + Relu_141 || Conv_143 + Relu_144 || Conv_146 + Relu_147 || Conv_149 + Relu_150 || Conv_152 + Relu_153 || Conv_155 + Relu_156, Location: Device, Dimensions: [1,512,180,180], Format/Datatype: Channel major FP16 format where channel % 16 == 0 }], Outputs: [ { Name: Conv_134 + Relu_135 || Conv_137 + Relu_138 || Conv_140 + Relu_141 || Conv_143 + Relu_144 || Conv_146 + Relu_147 || Conv_149 + Relu_150 || Conv_152 + Relu_153 || Conv_155 + Relu_156, Location: Device, Dimensions: [1,512,180,180], Format/Datatype: Channel major FP16 format where channel % 8 == 0 }], TacticValue: 0x0000000000000000
Name: Conv_136, LayerType: CaskConvolution, Inputs: [ { Name: Conv_134 + Relu_135 || Conv_137 + Relu_138 || Conv_140 + Relu_141 || Conv_143 + Relu_144 || Conv_146 + Relu_147 || Conv_149 + Relu_150 || Conv_152 + Relu_153 || Conv_155 + Relu_156, Location: Device, Dimensions: [1,64,180,180], Format/Datatype: Channel major FP16 format where channel % 8 == 0 }], Outputs: [ { Name: Reformatted Output Tensor 0 to Conv_136, Location: Device, Dimensions: [1,2,180,180], Format/Datatype: Channel major FP16 format where channel % 8 == 0 }], ParameterType: Convolution, Kernel: [3,3], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [1,1], PostPadding: [1,1], Stride: [1,1], Dilation: [1,1], OutMaps: 2, Groups: 1, Weights: {"Type": "Half", "Count": 1152}, Bias: {"Type": "Half", "Count": 2}, HasSparseWeights: 0, Activation: NONE, HasBias: 1, HasReLU: 0, TacticName: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x32x32_stage4_warpsize4x1x1_g1_tensor16x8x16_t1r3s3, TacticValue: 0x3e7eb35b91b9fa63
Name: Reformatting CopyNode for Output Tensor 0 to Conv_136, LayerType: Reformat, Inputs: [ { Name: Reformatted Output Tensor 0 to Conv_136, Location: Device, Dimensions: [1,2,180,180], Format/Datatype: Channel major FP16 format where channel % 8 == 0 }], Outputs: [ { Name: reg_4, Location: Device, Dimensions: [1,2,180,180], Format/Datatype: Row major linear FP16 format }], ParameterType: Reformat, Origin: REFORMAT, TacticValue: 0x0000000000000000
Name: Conv_139, LayerType: CaskConvolution, Inputs: [ { Name: Conv_134 + Relu_135 || Conv_137 + Relu_138 || Conv_140 + Relu_141 || Conv_143 + Relu_144 || Conv_146 + Relu_147 || Conv_149 + Relu_150 || Conv_152 + Relu_153 || Conv_155 + Relu_156, Location: Device, Dimensions: [1,64,180,180], Format/Datatype: Channel major FP16 format where channel % 8 == 0 }], Outputs: [ { Name: Reformatted Output Tensor 0 to Conv_139, Location: Device, Dimensions: [1,1,180,180], Format/Datatype: Channel major FP16 format where channel % 8 == 0 }], ParameterType: Convolution, Kernel: [3,3], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [1,1], PostPadding: [1,1], Stride: [1,1], Dilation: [1,1], OutMaps: 1, Groups: 1, Weights: {"Type": "Half", "Count": 576}, Bias: {"Type": "Half", "Count": 1}, HasSparseWeights: 0, Activation: NONE, HasBias: 1, HasReLU: 0, TacticName: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x32x32_stage4_warpsize4x1x1_g1_tensor16x8x16_t1r3s3, TacticValue: 0x3e7eb35b91b9fa63
Name: Reformatting CopyNode for Output Tensor 0 to Conv_139, LayerType: Reformat, Inputs: [ { Name: Reformatted Output Tensor 0 to Conv_139, Location: Device, Dimensions: [1,1,180,180], Format/Datatype: Channel major FP16 format where channel % 8 == 0 }], Outputs: [ { Name: height_4, Location: Device, Dimensions: [1,1,180,180], Format/Datatype: Row major linear FP16 format }], ParameterType: Reformat, Origin: REFORMAT, TacticValue: 0x00000000000003e8
Name: Conv_142, LayerType: CaskConvolution, Inputs: [ { Name: Conv_134 + Relu_135 || Conv_137 + Relu_138 || Conv_140 + Relu_141 || Conv_143 + Relu_144 || Conv_146 + Relu_147 || Conv_149 + Relu_150 || Conv_152 + Relu_153 || Conv_155 + Relu_156, Location: Device, Dimensions: [1,64,180,180], Format/Datatype: Channel major FP16 format where channel % 8 == 0 }], Outputs: [ { Name: Reformatted Output Tensor 0 to Conv_142, Location: Device, Dimensions: [1,3,180,180], Format/Datatype: Channel major FP16 format where channel % 8 == 0 }], ParameterType: Convolution, Kernel: [3,3], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [1,1], PostPadding: [1,1], Stride: [1,1], Dilation: [1,1], OutMaps: 3, Groups: 1, Weights: {"Type": "Half", "Count": 1728}, Bias: {"Type": "Half", "Count": 3}, HasSparseWeights: 0, Activation: NONE, HasBias: 1, HasReLU: 0, TacticName: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x32x32_stage4_warpsize4x1x1_g1_tensor16x8x16_t1r3s3, TacticValue: 0x3e7eb35b91b9fa63
Name: Reformatting CopyNode for Output Tensor 0 to Conv_142, LayerType: Reformat, Inputs: [ { Name: Reformatted Output Tensor 0 to Conv_142, Location: Device, Dimensions: [1,3,180,180], Format/Datatype: Channel major FP16 format where channel % 8 == 0 }], Outputs: [ { Name: dim_4, Location: Device, Dimensions: [1,3,180,180], Format/Datatype: Row major linear FP16 format }], ParameterType: Reformat, Origin: REFORMAT, TacticValue: 0x0000000000000000
Name: Conv_145, LayerType: CaskConvolution, Inputs: [ { Name: Conv_134 + Relu_135 || Conv_137 + Relu_138 || Conv_140 + Relu_141 || Conv_143 + Relu_144 || Conv_146 + Relu_147 || Conv_149 + Relu_150 || Conv_152 + Relu_153 || Conv_155 + Relu_156, Location: Device, Dimensions: [1,64,180,180], Format/Datatype: Channel major FP16 format where channel % 8 == 0 }], Outputs: [ { Name: Reformatted Output Tensor 0 to Conv_145, Location: Device, Dimensions: [1,2,180,180], Format/Datatype: Channel major FP16 format where channel % 8 == 0 }], ParameterType: Convolution, Kernel: [3,3], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [1,1], PostPadding: [1,1], Stride: [1,1], Dilation: [1,1], OutMaps: 2, Groups: 1, Weights: {"Type": "Half", "Count": 1152}, Bias: {"Type": "Half", "Count": 2}, HasSparseWeights: 0, Activation: NONE, HasBias: 1, HasReLU: 0, TacticName: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x32x32_stage4_warpsize4x1x1_g1_tensor16x8x16_t1r3s3, TacticValue: 0x3e7eb35b91b9fa63
Name: Reformatting CopyNode for Output Tensor 0 to Conv_145, LayerType: Reformat, Inputs: [ { Name: Reformatted Output Tensor 0 to Conv_145, Location: Device, Dimensions: [1,2,180,180], Format/Datatype: Channel major FP16 format where channel % 8 == 0 }], Outputs: [ { Name: rot_4, Location: Device, Dimensions: [1,2,180,180], Format/Datatype: Row major linear FP16 format }], ParameterType: Reformat, Origin: REFORMAT, TacticValue: 0x0000000000000000
Name: Conv_148, LayerType: CaskConvolution, Inputs: [ { Name: Conv_134 + Relu_135 || Conv_137 + Relu_138 || Conv_140 + Relu_141 || Conv_143 + Relu_144 || Conv_146 + Relu_147 || Conv_149 + Relu_150 || Conv_152 + Relu_153 || Conv_155 + Relu_156, Location: Device, Dimensions: [1,64,180,180], Format/Datatype: Channel major FP16 format where channel % 8 == 0 }], Outputs: [ { Name: Reformatted Output Tensor 0 to Conv_148, Location: Device, Dimensions: [1,2,180,180], Format/Datatype: Channel major FP16 format where channel % 8 == 0 }], ParameterType: Convolution, Kernel: [3,3], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [1,1], PostPadding: [1,1], Stride: [1,1], Dilation: [1,1], OutMaps: 2, Groups: 1, Weights: {"Type": "Half", "Count": 1152}, Bias: {"Type": "Half", "Count": 2}, HasSparseWeights: 0, Activation: NONE, HasBias: 1, HasReLU: 0, TacticName: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x32x32_stage4_warpsize4x1x1_g1_tensor16x8x16_t1r3s3, TacticValue: 0x3e7eb35b91b9fa63
Name: Reformatting CopyNode for Output Tensor 0 to Conv_148, LayerType: Reformat, Inputs: [ { Name: Reformatted Output Tensor 0 to Conv_148, Location: Device, Dimensions: [1,2,180,180], Format/Datatype: Channel major FP16 format where channel % 8 == 0 }], Outputs: [ { Name: vel_4, Location: Device, Dimensions: [1,2,180,180], Format/Datatype: Row major linear FP16 format }], ParameterType: Reformat, Origin: REFORMAT, TacticValue: 0x0000000000000000
Name: Conv_151, LayerType: CaskConvolution, Inputs: [ { Name: Conv_134 + Relu_135 || Conv_137 + Relu_138 || Conv_140 + Relu_141 || Conv_143 + Relu_144 || Conv_146 + Relu_147 || Conv_149 + Relu_150 || Conv_152 + Relu_153 || Conv_155 + Relu_156, Location: Device, Dimensions: [1,64,180,180], Format/Datatype: Channel major FP16 format where channel % 8 == 0 }], Outputs: [ { Name: Reformatted Output Tensor 0 to Conv_151, Location: Device, Dimensions: [1,2,180,180], Format/Datatype: Channel major FP16 format where channel % 8 == 0 }], ParameterType: Convolution, Kernel: [3,3], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [1,1], PostPadding: [1,1], Stride: [1,1], Dilation: [1,1], OutMaps: 2, Groups: 1, Weights: {"Type": "Half", "Count": 1152}, Bias: {"Type": "Half", "Count": 2}, HasSparseWeights: 0, Activation: NONE, HasBias: 1, HasReLU: 0, TacticName: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x32x32_stage4_warpsize4x1x1_g1_tensor16x8x16_t1r3s3, TacticValue: 0x3e7eb35b91b9fa63
Name: Reformatting CopyNode for Output Tensor 0 to Conv_151, LayerType: Reformat, Inputs: [ { Name: Reformatted Output Tensor 0 to Conv_151, Location: Device, Dimensions: [1,2,180,180], Format/Datatype: Channel major FP16 format where channel % 8 == 0 }], Outputs: [ { Name: hm_4, Location: Device, Dimensions: [1,2,180,180], Format/Datatype: Row major linear FP16 format }], ParameterType: Reformat, Origin: REFORMAT, TacticValue: 0x0000000000000000
Name: Conv_154, LayerType: CaskConvolution, Inputs: [ { Name: Conv_134 + Relu_135 || Conv_137 + Relu_138 || Conv_140 + Relu_141 || Conv_143 + Relu_144 || Conv_146 + Relu_147 || Conv_149 + Relu_150 || Conv_152 + Relu_153 || Conv_155 + Relu_156, Location: Device, Dimensions: [1,64,180,180], Format/Datatype: Channel major FP16 format where channel % 8 == 0 }], Outputs: [ { Name: Reformatted Output Tensor 0 to Conv_154, Location: Device, Dimensions: [1,2,180,180], Format/Datatype: Channel major FP16 format where channel % 8 == 0 }], ParameterType: Convolution, Kernel: [3,3], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [1,1], PostPadding: [1,1], Stride: [1,1], Dilation: [1,1], OutMaps: 2, Groups: 1, Weights: {"Type": "Half", "Count": 1152}, Bias: {"Type": "Half", "Count": 2}, HasSparseWeights: 0, Activation: NONE, HasBias: 1, HasReLU: 0, TacticName: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x32x32_stage4_warpsize4x1x1_g1_tensor16x8x16_t1r3s3, TacticValue: 0x3e7eb35b91b9fa63
Name: Reformatting CopyNode for Output Tensor 0 to Conv_154, LayerType: Reformat, Inputs: [ { Name: Reformatted Output Tensor 0 to Conv_154, Location: Device, Dimensions: [1,2,180,180], Format/Datatype: Channel major FP16 format where channel % 8 == 0 }], Outputs: [ { Name: reg_5, Location: Device, Dimensions: [1,2,180,180], Format/Datatype: Row major linear FP16 format }], ParameterType: Reformat, Origin: REFORMAT, TacticValue: 0x0000000000000000
Name: Conv_157, LayerType: CaskConvolution, Inputs: [ { Name: Conv_134 + Relu_135 || Conv_137 + Relu_138 || Conv_140 + Relu_141 || Conv_143 + Relu_144 || Conv_146 + Relu_147 || Conv_149 + Relu_150 || Conv_152 + Relu_153 || Conv_155 + Relu_156, Location: Device, Dimensions: [1,64,180,180], Format/Datatype: Channel major FP16 format where channel % 8 == 0 }], Outputs: [ { Name: Reformatted Output Tensor 0 to Conv_157, Location: Device, Dimensions: [1,1,180,180], Format/Datatype: Channel major FP16 format where channel % 8 == 0 }], ParameterType: Convolution, Kernel: [3,3], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [1,1], PostPadding: [1,1], Stride: [1,1], Dilation: [1,1], OutMaps: 1, Groups: 1, Weights: {"Type": "Half", "Count": 576}, Bias: {"Type": "Half", "Count": 1}, HasSparseWeights: 0, Activation: NONE, HasBias: 1, HasReLU: 0, TacticName: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x32x32_stage4_warpsize4x1x1_g1_tensor16x8x16_t1r3s3, TacticValue: 0x3e7eb35b91b9fa63
Name: Reformatting CopyNode for Output Tensor 0 to Conv_157, LayerType: Reformat, Inputs: [ { Name: Reformatted Output Tensor 0 to Conv_157, Location: Device, Dimensions: [1,1,180,180], Format/Datatype: Channel major FP16 format where channel % 8 == 0 }], Outputs: [ { Name: height_5, Location: Device, Dimensions: [1,1,180,180], Format/Datatype: Row major linear FP16 format }], ParameterType: Reformat, Origin: REFORMAT, TacticValue: 0x00000000000003e8
Name: Reformatting CopyNode for Input Tensor 0 to Conv_158 + Relu_159 || Conv_161 + Relu_162 || Conv_164 + Relu_165 || Conv_167 + Relu_168, LayerType: NoOp, Inputs: [ { Name: onnx::Conv_651, Location: Device, Dimensions: [1,64,180,180], Format/Datatype: Channel major FP16 format where channel % 16 == 0 }], Outputs: [ { Name: Reformatted Input Tensor 0 to Conv_158 + Relu_159 || Conv_161 + Relu_162 || Conv_164 + Relu_165 || Conv_167 + Relu_168, Location: Device, Dimensions: [1,64,180,180], Format/Datatype: Channel major FP16 format where channel % 8 == 0 }], TacticValue: 0x0000000000000000
Name: Conv_158 + Relu_159 || Conv_161 + Relu_162 || Conv_164 + Relu_165 || Conv_167 + Relu_168, LayerType: CaskConvolution, Inputs: [ { Name: Reformatted Input Tensor 0 to Conv_158 + Relu_159 || Conv_161 + Relu_162 || Conv_164 + Relu_165 || Conv_167 + Relu_168, Location: Device, Dimensions: [1,64,180,180], Format/Datatype: Channel major FP16 format where channel % 8 == 0 }], Outputs: [ { Name: Conv_158 + Relu_159 || Conv_161 + Relu_162 || Conv_164 + Relu_165 || Conv_167 + Relu_168, Location: Device, Dimensions: [1,256,180,180], Format/Datatype: Channel major FP16 format where channel % 8 == 0 }], ParameterType: Convolution, Kernel: [3,3], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [1,1], PostPadding: [1,1], Stride: [1,1], Dilation: [1,1], OutMaps: 256, Groups: 1, Weights: {"Type": "Half", "Count": 147456}, Bias: {"Type": "Half", "Count": 256}, HasSparseWeights: 0, Activation: RELU, HasBias: 1, HasReLU: 1, TacticName: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x128x32_stage4_warpsize2x2x1_g1_tensor16x8x16_t1r3s3, TacticValue: 0x60c3421152ef8e10
Name: Conv_160, LayerType: CaskConvolution, Inputs: [ { Name: Conv_158 + Relu_159 || Conv_161 + Relu_162 || Conv_164 + Relu_165 || Conv_167 + Relu_168, Location: Device, Dimensions: [1,64,180,180], Format/Datatype: Channel major FP16 format where channel % 8 == 0 }], Outputs: [ { Name: Reformatted Output Tensor 0 to Conv_160, Location: Device, Dimensions: [1,3,180,180], Format/Datatype: Channel major FP16 format where channel % 8 == 0 }], ParameterType: Convolution, Kernel: [3,3], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [1,1], PostPadding: [1,1], Stride: [1,1], Dilation: [1,1], OutMaps: 3, Groups: 1, Weights: {"Type": "Half", "Count": 1728}, Bias: {"Type": "Half", "Count": 3}, HasSparseWeights: 0, Activation: NONE, HasBias: 1, HasReLU: 0, TacticName: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x32x32_stage4_warpsize4x1x1_g1_tensor16x8x16_t1r3s3, TacticValue: 0x3e7eb35b91b9fa63
Name: Reformatting CopyNode for Output Tensor 0 to Conv_160, LayerType: Reformat, Inputs: [ { Name: Reformatted Output Tensor 0 to Conv_160, Location: Device, Dimensions: [1,3,180,180], Format/Datatype: Channel major FP16 format where channel % 8 == 0 }], Outputs: [ { Name: dim_5, Location: Device, Dimensions: [1,3,180,180], Format/Datatype: Row major linear FP16 format }], ParameterType: Reformat, Origin: REFORMAT, TacticValue: 0x0000000000000000
Name: Conv_163, LayerType: CaskConvolution, Inputs: [ { Name: Conv_158 + Relu_159 || Conv_161 + Relu_162 || Conv_164 + Relu_165 || Conv_167 + Relu_168, Location: Device, Dimensions: [1,64,180,180], Format/Datatype: Channel major FP16 format where channel % 8 == 0 }], Outputs: [ { Name: Reformatted Output Tensor 0 to Conv_163, Location: Device, Dimensions: [1,2,180,180], Format/Datatype: Channel major FP16 format where channel % 8 == 0 }], ParameterType: Convolution, Kernel: [3,3], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [1,1], PostPadding: [1,1], Stride: [1,1], Dilation: [1,1], OutMaps: 2, Groups: 1, Weights: {"Type": "Half", "Count": 1152}, Bias: {"Type": "Half", "Count": 2}, HasSparseWeights: 0, Activation: NONE, HasBias: 1, HasReLU: 0, TacticName: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x32x32_stage4_warpsize4x1x1_g1_tensor16x8x16_t1r3s3, TacticValue: 0x3e7eb35b91b9fa63
Name: Reformatting CopyNode for Output Tensor 0 to Conv_163, LayerType: Reformat, Inputs: [ { Name: Reformatted Output Tensor 0 to Conv_163, Location: Device, Dimensions: [1,2,180,180], Format/Datatype: Channel major FP16 format where channel % 8 == 0 }], Outputs: [ { Name: rot_5, Location: Device, Dimensions: [1,2,180,180], Format/Datatype: Row major linear FP16 format }], ParameterType: Reformat, Origin: REFORMAT, TacticValue: 0x0000000000000000
Name: Conv_166, LayerType: CaskConvolution, Inputs: [ { Name: Conv_158 + Relu_159 || Conv_161 + Relu_162 || Conv_164 + Relu_165 || Conv_167 + Relu_168, Location: Device, Dimensions: [1,64,180,180], Format/Datatype: Channel major FP16 format where channel % 8 == 0 }], Outputs: [ { Name: Reformatted Output Tensor 0 to Conv_166, Location: Device, Dimensions: [1,2,180,180], Format/Datatype: Channel major FP16 format where channel % 8 == 0 }], ParameterType: Convolution, Kernel: [3,3], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [1,1], PostPadding: [1,1], Stride: [1,1], Dilation: [1,1], OutMaps: 2, Groups: 1, Weights: {"Type": "Half", "Count": 1152}, Bias: {"Type": "Half", "Count": 2}, HasSparseWeights: 0, Activation: NONE, HasBias: 1, HasReLU: 0, TacticName: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x32x32_stage4_warpsize4x1x1_g1_tensor16x8x16_t1r3s3, TacticValue: 0x3e7eb35b91b9fa63
Name: Reformatting CopyNode for Output Tensor 0 to Conv_166, LayerType: Reformat, Inputs: [ { Name: Reformatted Output Tensor 0 to Conv_166, Location: Device, Dimensions: [1,2,180,180], Format/Datatype: Channel major FP16 format where channel % 8 == 0 }], Outputs: [ { Name: vel_5, Location: Device, Dimensions: [1,2,180,180], Format/Datatype: Row major linear FP16 format }], ParameterType: Reformat, Origin: REFORMAT, TacticValue: 0x0000000000000000
Name: Conv_169, LayerType: CaskConvolution, Inputs: [ { Name: Conv_158 + Relu_159 || Conv_161 + Relu_162 || Conv_164 + Relu_165 || Conv_167 + Relu_168, Location: Device, Dimensions: [1,64,180,180], Format/Datatype: Channel major FP16 format where channel % 8 == 0 }], Outputs: [ { Name: Reformatted Output Tensor 0 to Conv_169, Location: Device, Dimensions: [1,2,180,180], Format/Datatype: Channel major FP16 format where channel % 8 == 0 }], ParameterType: Convolution, Kernel: [3,3], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [1,1], PostPadding: [1,1], Stride: [1,1], Dilation: [1,1], OutMaps: 2, Groups: 1, Weights: {"Type": "Half", "Count": 1152}, Bias: {"Type": "Half", "Count": 2}, HasSparseWeights: 0, Activation: NONE, HasBias: 1, HasReLU: 0, TacticName: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x32x32_stage4_warpsize4x1x1_g1_tensor16x8x16_t1r3s3, TacticValue: 0x3e7eb35b91b9fa63
Name: Reformatting CopyNode for Output Tensor 0 to Conv_169, LayerType: Reformat, Inputs: [ { Name: Reformatted Output Tensor 0 to Conv_169, Location: Device, Dimensions: [1,2,180,180], Format/Datatype: Channel major FP16 format where channel % 8 == 0 }], Outputs: [ { Name: hm_5, Location: Device, Dimensions: [1,2,180,180], Format/Datatype: Row major linear FP16 format }], ParameterType: Reformat, Origin: REFORMAT, TacticValue: 0x0000000000000000

Bindings:
input
reg_0
height_0
dim_0
rot_0
vel_0
hm_0
reg_1
height_1
dim_1
rot_1
vel_1
hm_1
reg_2
height_2
dim_2
rot_2
vel_2
hm_2
reg_3
height_3
dim_3
rot_3
vel_3
hm_3
reg_4
height_4
dim_4
rot_4
vel_4
hm_4
reg_5
height_5
dim_5
rot_5
vel_5
hm_5
[08/10/2023-11:18:40] [I] Starting inference
[08/10/2023-11:18:43] [I] Warmup completed 12 queries over 200 ms
[08/10/2023-11:18:43] [I] Timing trace has 185 queries over 3.05886 s
[08/10/2023-11:18:43] [I] 
[08/10/2023-11:18:43] [I] === Trace details ===
[08/10/2023-11:18:43] [I] Trace averages of 10 runs:
[08/10/2023-11:18:43] [I] Average on 10 runs - GPU latency: 16.4092 ms - Host latency: 17.4959 ms (enqueue 0.825572 ms)
[08/10/2023-11:18:43] [I] Average on 10 runs - GPU latency: 16.7481 ms - Host latency: 17.8157 ms (enqueue 0.760898 ms)
[08/10/2023-11:18:43] [I] Average on 10 runs - GPU latency: 16.4367 ms - Host latency: 17.4972 ms (enqueue 0.632965 ms)
[08/10/2023-11:18:43] [I] Average on 10 runs - GPU latency: 16.3974 ms - Host latency: 17.4468 ms (enqueue 0.715436 ms)
[08/10/2023-11:18:43] [I] Average on 10 runs - GPU latency: 16.4041 ms - Host latency: 17.478 ms (enqueue 0.734723 ms)
[08/10/2023-11:18:43] [I] Average on 10 runs - GPU latency: 16.4651 ms - Host latency: 17.5342 ms (enqueue 0.702936 ms)
[08/10/2023-11:18:43] [I] Average on 10 runs - GPU latency: 16.4451 ms - Host latency: 17.5055 ms (enqueue 0.703979 ms)
[08/10/2023-11:18:43] [I] Average on 10 runs - GPU latency: 16.3306 ms - Host latency: 17.3958 ms (enqueue 0.84386 ms)
[08/10/2023-11:18:43] [I] Average on 10 runs - GPU latency: 16.449 ms - Host latency: 17.5059 ms (enqueue 0.670288 ms)
[08/10/2023-11:18:43] [I] Average on 10 runs - GPU latency: 16.4783 ms - Host latency: 17.5486 ms (enqueue 0.686658 ms)
[08/10/2023-11:18:43] [I] Average on 10 runs - GPU latency: 16.403 ms - Host latency: 17.4524 ms (enqueue 0.72876 ms)
[08/10/2023-11:18:43] [I] Average on 10 runs - GPU latency: 16.3927 ms - Host latency: 17.464 ms (enqueue 0.642737 ms)
[08/10/2023-11:18:43] [I] Average on 10 runs - GPU latency: 16.4771 ms - Host latency: 17.5335 ms (enqueue 0.649951 ms)
[08/10/2023-11:18:43] [I] Average on 10 runs - GPU latency: 16.4528 ms - Host latency: 17.5134 ms (enqueue 0.736694 ms)
[08/10/2023-11:18:43] [I] Average on 10 runs - GPU latency: 16.4302 ms - Host latency: 17.4834 ms (enqueue 0.748511 ms)
[08/10/2023-11:18:43] [I] Average on 10 runs - GPU latency: 16.3778 ms - Host latency: 17.436 ms (enqueue 0.677393 ms)
[08/10/2023-11:18:43] [I] Average on 10 runs - GPU latency: 16.4565 ms - Host latency: 17.5111 ms (enqueue 0.770801 ms)
[08/10/2023-11:18:43] [I] Average on 10 runs - GPU latency: 16.4648 ms - Host latency: 17.526 ms (enqueue 0.592676 ms)
[08/10/2023-11:18:43] [I] 
[08/10/2023-11:18:43] [I] === Performance summary ===
[08/10/2023-11:18:43] [I] Throughput: 60.4801 qps
[08/10/2023-11:18:43] [I] Latency: min = 17.2749 ms, max = 19.3631 ms, mean = 17.5047 ms, median = 17.4156 ms, percentile(99%) = 18.3263 ms
[08/10/2023-11:18:43] [I] Enqueue Time: min = 0.491516 ms, max = 1.35461 ms, mean = 0.712423 ms, median = 0.548096 ms, percentile(99%) = 1.35156 ms
[08/10/2023-11:18:43] [I] H2D Latency: min = 0.649811 ms, max = 0.749344 ms, mean = 0.663748 ms, median = 0.661133 ms, percentile(99%) = 0.737823 ms
[08/10/2023-11:18:43] [I] GPU Compute Time: min = 16.2922 ms, max = 18.3076 ms, mean = 16.443 ms, median = 16.3591 ms, percentile(99%) = 17.161 ms
[08/10/2023-11:18:43] [I] D2H Latency: min = 0.295166 ms, max = 0.535645 ms, mean = 0.397936 ms, median = 0.389893 ms, percentile(99%) = 0.519165 ms
[08/10/2023-11:18:43] [I] Total Host Walltime: 3.05886 s
[08/10/2023-11:18:43] [I] Total GPU Compute Time: 3.04195 s
[08/10/2023-11:18:43] [W] * GPU compute time is unstable, with coefficient of variance = 1.3157%.
[08/10/2023-11:18:43] [W]   If not already in use, locking GPU clock frequency or adding --useSpinWait may improve the stability.
[08/10/2023-11:18:43] [I] Explanations of the performance metrics are printed in the verbose logs.
[08/10/2023-11:18:43] [V] 
[08/10/2023-11:18:43] [V] === Explanations of the performance metrics ===
[08/10/2023-11:18:43] [V] Total Host Walltime: the host walltime from when the first query (after warmups) is enqueued to when the last query is completed.
[08/10/2023-11:18:43] [V] GPU Compute Time: the GPU latency to execute the kernels for a query.
[08/10/2023-11:18:43] [V] Total GPU Compute Time: the summation of the GPU Compute Time of all the queries. If this is significantly shorter than Total Host Walltime, the GPU may be under-utilized because of host-side overheads or data transfers.
[08/10/2023-11:18:43] [V] Throughput: the observed throughput computed by dividing the number of queries by the Total Host Walltime. If this is significantly lower than the reciprocal of GPU Compute Time, the GPU may be under-utilized because of host-side overheads or data transfers.
[08/10/2023-11:18:43] [V] Enqueue Time: the host latency to enqueue a query. If this is longer than GPU Compute Time, the GPU may be under-utilized.
[08/10/2023-11:18:43] [V] H2D Latency: the latency for host-to-device data transfers for input tensors of a single query.
[08/10/2023-11:18:43] [V] D2H Latency: the latency for device-to-host data transfers for output tensors of a single query.
[08/10/2023-11:18:43] [V] Latency: the summation of H2D Latency, GPU Compute Time, and D2H Latency. This is the latency to infer a single query.
[08/10/2023-11:18:43] [I] 
[08/10/2023-11:18:46] [I] 
[08/10/2023-11:18:46] [I] === Profile (188 iterations ) ===
[08/10/2023-11:18:46] [I]                                                                                                                                                                                                                              Layer   Time (ms)   Avg. Time (ms)   Median Time (ms)   Time %
[08/10/2023-11:18:46] [I]                                                                                                                                                                      Reformatting CopyNode for Input Tensor 0 to Conv_15 + Relu_16       66.70           0.3548             0.3531      2.2
[08/10/2023-11:18:46] [I]                                                                                                                                                                                                                  Conv_15 + Relu_16      156.57           0.8328             0.8290      5.1
[08/10/2023-11:18:46] [I]                                                                                                                                                                                                                  Conv_17 + Relu_18       84.16           0.4477             0.4457      2.7
[08/10/2023-11:18:46] [I]                                                                                                                                                                                                                  Conv_19 + Relu_20       83.89           0.4462             0.4444      2.7
[08/10/2023-11:18:46] [I]                                                                                                                                                                                                                  Conv_21 + Relu_22       83.84           0.4459             0.4439      2.7
[08/10/2023-11:18:46] [I]                                                                                                                                                                                                                  Conv_23 + Relu_24       83.70           0.4452             0.4432      2.7
[08/10/2023-11:18:46] [I]                                                                                                                                                                                                                  Conv_25 + Relu_26       83.79           0.4457             0.4437      2.7
[08/10/2023-11:18:46] [I]                                                                                                                                                                                                                  Conv_27 + Relu_28       38.46           0.2046             0.2038      1.3
[08/10/2023-11:18:46] [I]                                                                                                                                                                                                                  Conv_44 + Relu_45       46.25           0.2460             0.2451      1.5
[08/10/2023-11:18:46] [I]                                                                                                                                                                                                                  Conv_46 + Relu_47       83.10           0.4420             0.4398      2.7
[08/10/2023-11:18:46] [I]                                                                                                                                                                                                                  Conv_48 + Relu_49       83.19           0.4425             0.4402      2.7
[08/10/2023-11:18:46] [I]                                                                                                                                                                                                                  Conv_50 + Relu_51       83.14           0.4422             0.4400      2.7
[08/10/2023-11:18:46] [I]                                                                                                                                                                                                                  Conv_52 + Relu_53       83.14           0.4422             0.4402      2.7
[08/10/2023-11:18:46] [I]                                                                                                                                                                                                                  Conv_54 + Relu_55       83.15           0.4423             0.4400      2.7
[08/10/2023-11:18:46] [I]                                                                                                                                                                                 ConvTranspose_56 + BatchNormalization_57 + Relu_58       56.80           0.3021             0.3010      1.9
[08/10/2023-11:18:46] [I]                                                                                                                                                                                                              onnx::Concat_647 copy       73.11           0.3889             0.3870      2.4
[08/10/2023-11:18:46] [I]                                                                                                                                                                                                                  Conv_60 + Relu_61      173.31           0.9219             0.9176      5.6
[08/10/2023-11:18:46] [I]                                                               Conv_62 + Relu_63 || Conv_65 + Relu_66 || Conv_68 + Relu_69 || Conv_71 + Relu_72 || Conv_74 + Relu_75 || Conv_77 + Relu_78 || Conv_80 + Relu_81 || Conv_83 + Relu_84      176.20           0.9372             0.9317      5.7
[08/10/2023-11:18:46] [I]                                                                                                                                                                                                                            Conv_64       21.98           0.1169             0.1165      0.7
[08/10/2023-11:18:46] [I]                                                                                                                                                                               Reformatting CopyNode for Output Tensor 0 to Conv_64        1.93           0.0103             0.0102      0.1
[08/10/2023-11:18:46] [I]                                                                                                                                                                                                                            Conv_67       21.16           0.1126             0.1122      0.7
[08/10/2023-11:18:46] [I]                                                                                                                                                                               Reformatting CopyNode for Output Tensor 0 to Conv_67        1.88           0.0100             0.0100      0.1
[08/10/2023-11:18:46] [I]                                                                                                                                                                                                                            Conv_70       21.06           0.1120             0.1116      0.7
[08/10/2023-11:18:46] [I]                                                                                                                                                                               Reformatting CopyNode for Output Tensor 0 to Conv_70        1.78           0.0095             0.0095      0.1
[08/10/2023-11:18:46] [I]                                                                                                                                                                                                                            Conv_73       21.11           0.1123             0.1120      0.7
[08/10/2023-11:18:46] [I]                                                                                                                                                                               Reformatting CopyNode for Output Tensor 0 to Conv_73        1.86           0.0099             0.0100      0.1
[08/10/2023-11:18:46] [I]                                                                                                                                                                                                                            Conv_76       21.08           0.1121             0.1117      0.7
[08/10/2023-11:18:46] [I]                                                                                                                                                                               Reformatting CopyNode for Output Tensor 0 to Conv_76        1.86           0.0099             0.0100      0.1
[08/10/2023-11:18:46] [I]                                                                                                                                                                                                                            Conv_79       21.11           0.1123             0.1119      0.7
[08/10/2023-11:18:46] [I]                                                                                                                                                                               Reformatting CopyNode for Output Tensor 0 to Conv_79        1.78           0.0095             0.0094      0.1
[08/10/2023-11:18:46] [I]                                                                                                                                                                                                                            Conv_82       21.06           0.1120             0.1117      0.7
[08/10/2023-11:18:46] [I]                                                                                                                                                                               Reformatting CopyNode for Output Tensor 0 to Conv_82        1.85           0.0098             0.0099      0.1
[08/10/2023-11:18:46] [I]                                                                                                                                                                                                                            Conv_85       21.06           0.1120             0.1116      0.7
[08/10/2023-11:18:46] [I]                                                                                                                                                                               Reformatting CopyNode for Output Tensor 0 to Conv_85        1.78           0.0095             0.0094      0.1
[08/10/2023-11:18:46] [I]                                                         Conv_86 + Relu_87 || Conv_89 + Relu_90 || Conv_92 + Relu_93 || Conv_95 + Relu_96 || Conv_98 + Relu_99 || Conv_101 + Relu_102 || Conv_104 + Relu_105 || Conv_107 + Relu_108      175.56           0.9338             0.9288      5.7
[08/10/2023-11:18:46] [I]                                                                                                                                                                                                                            Conv_88       21.73           0.1156             0.1152      0.7
[08/10/2023-11:18:46] [I]                                                                                                                                                                               Reformatting CopyNode for Output Tensor 0 to Conv_88        1.88           0.0100             0.0100      0.1
[08/10/2023-11:18:46] [I]                                                                                                                                                                                                                            Conv_91       21.06           0.1120             0.1116      0.7
[08/10/2023-11:18:46] [I]                                                                                                                                                                               Reformatting CopyNode for Output Tensor 0 to Conv_91        1.85           0.0098             0.0100      0.1
[08/10/2023-11:18:46] [I]                                                                                                                                                                                                                            Conv_94       21.08           0.1121             0.1117      0.7
[08/10/2023-11:18:46] [I]                                                                                                                                                                               Reformatting CopyNode for Output Tensor 0 to Conv_94        1.87           0.0099             0.0100      0.1
[08/10/2023-11:18:46] [I]                                                                                                                                                                                                                            Conv_97       21.09           0.1122             0.1118      0.7
[08/10/2023-11:18:46] [I]                                                                                                                                                                               Reformatting CopyNode for Output Tensor 0 to Conv_97        1.84           0.0098             0.0099      0.1
[08/10/2023-11:18:46] [I]                                                                                                                                                                                                                           Conv_100       21.08           0.1121             0.1117      0.7
[08/10/2023-11:18:46] [I]                                                                                                                                                                              Reformatting CopyNode for Output Tensor 0 to Conv_100        1.87           0.0100             0.0100      0.1
[08/10/2023-11:18:46] [I]                                                                                                                                                                                                                           Conv_103       21.07           0.1121             0.1117      0.7
[08/10/2023-11:18:46] [I]                                                                                                                                                                              Reformatting CopyNode for Output Tensor 0 to Conv_103        1.76           0.0094             0.0094      0.1
[08/10/2023-11:18:46] [I]                                                                                                                                                                                                                           Conv_106       21.04           0.1119             0.1115      0.7
[08/10/2023-11:18:46] [I]                                                                                                                                                                              Reformatting CopyNode for Output Tensor 0 to Conv_106        1.90           0.0101             0.0100      0.1
[08/10/2023-11:18:46] [I]                                                                                                                                                                                                                           Conv_109       21.14           0.1125             0.1121      0.7
[08/10/2023-11:18:46] [I]                                                                                                                                                                              Reformatting CopyNode for Output Tensor 0 to Conv_109        1.85           0.0098             0.0100      0.1
[08/10/2023-11:18:46] [I]                                               Conv_110 + Relu_111 || Conv_113 + Relu_114 || Conv_116 + Relu_117 || Conv_119 + Relu_120 || Conv_122 + Relu_123 || Conv_125 + Relu_126 || Conv_128 + Relu_129 || Conv_131 + Relu_132      175.59           0.9340             0.9289      5.7
[08/10/2023-11:18:46] [I]                                                                                                                                                                                                                           Conv_112       21.69           0.1154             0.1150      0.7
[08/10/2023-11:18:46] [I]                                                                                                                                                                              Reformatting CopyNode for Output Tensor 0 to Conv_112        1.86           0.0099             0.0100      0.1
[08/10/2023-11:18:46] [I]                                                                                                                                                                                                                           Conv_115       21.13           0.1124             0.1117      0.7
[08/10/2023-11:18:46] [I]                                                                                                                                                                              Reformatting CopyNode for Output Tensor 0 to Conv_115        1.81           0.0096             0.0094      0.1
[08/10/2023-11:18:46] [I]                                                                                                                                                                                                                           Conv_118       21.12           0.1123             0.1118      0.7
[08/10/2023-11:18:46] [I]                                                                                                                                                                              Reformatting CopyNode for Output Tensor 0 to Conv_118        1.76           0.0094             0.0094      0.1
[08/10/2023-11:18:46] [I]                                                                                                                                                                                                                           Conv_121       21.09           0.1122             0.1117      0.7
[08/10/2023-11:18:46] [I]                                                                                                                                                                              Reformatting CopyNode for Output Tensor 0 to Conv_121        1.79           0.0095             0.0095      0.1
[08/10/2023-11:18:46] [I]                                                                                                                                                                                                                           Conv_124       21.08           0.1122             0.1117      0.7
[08/10/2023-11:18:46] [I]                                                                                                                                                                              Reformatting CopyNode for Output Tensor 0 to Conv_124        1.88           0.0100             0.0100      0.1
[08/10/2023-11:18:46] [I]                                                                                                                                                                                                                           Conv_127       21.15           0.1125             0.1120      0.7
[08/10/2023-11:18:46] [I]                                                                                                                                                                              Reformatting CopyNode for Output Tensor 0 to Conv_127        1.88           0.0100             0.0100      0.1
[08/10/2023-11:18:46] [I]                                                                                                                                                                                                                           Conv_130       21.06           0.1120             0.1117      0.7
[08/10/2023-11:18:46] [I]                                                                                                                                                                              Reformatting CopyNode for Output Tensor 0 to Conv_130        1.85           0.0098             0.0100      0.1
[08/10/2023-11:18:46] [I]                                                                                                                                                                                                                           Conv_133       21.09           0.1122             0.1118      0.7
[08/10/2023-11:18:46] [I]                                                                                                                                                                              Reformatting CopyNode for Output Tensor 0 to Conv_133        1.79           0.0095             0.0095      0.1
[08/10/2023-11:18:46] [I]                                               Conv_134 + Relu_135 || Conv_137 + Relu_138 || Conv_140 + Relu_141 || Conv_143 + Relu_144 || Conv_146 + Relu_147 || Conv_149 + Relu_150 || Conv_152 + Relu_153 || Conv_155 + Relu_156      175.53           0.9337             0.9290      5.7
[08/10/2023-11:18:46] [I]                                                                                                                                                                                                                           Conv_136       21.70           0.1154             0.1149      0.7
[08/10/2023-11:18:46] [I]                                                                                                                                                                              Reformatting CopyNode for Output Tensor 0 to Conv_136        1.88           0.0100             0.0100      0.1
[08/10/2023-11:18:46] [I]                                                                                                                                                                                                                           Conv_139       21.12           0.1123             0.1117      0.7
[08/10/2023-11:18:46] [I]                                                                                                                                                                              Reformatting CopyNode for Output Tensor 0 to Conv_139        1.77           0.0094             0.0094      0.1
[08/10/2023-11:18:46] [I]                                                                                                                                                                                                                           Conv_142       21.05           0.1120             0.1115      0.7
[08/10/2023-11:18:46] [I]                                                                                                                                                                              Reformatting CopyNode for Output Tensor 0 to Conv_142        1.97           0.0105             0.0100      0.1
[08/10/2023-11:18:46] [I]                                                                                                                                                                                                                           Conv_145       21.25           0.1130             0.1122      0.7
[08/10/2023-11:18:46] [I]                                                                                                                                                                              Reformatting CopyNode for Output Tensor 0 to Conv_145        1.88           0.0100             0.0099      0.1
[08/10/2023-11:18:46] [I]                                                                                                                                                                                                                           Conv_148       21.10           0.1122             0.1118      0.7
[08/10/2023-11:18:46] [I]                                                                                                                                                                              Reformatting CopyNode for Output Tensor 0 to Conv_148        1.88           0.0100             0.0100      0.1
[08/10/2023-11:18:46] [I]                                                                                                                                                                                                                           Conv_151       21.10           0.1122             0.1119      0.7
[08/10/2023-11:18:46] [I]                                                                                                                                                                              Reformatting CopyNode for Output Tensor 0 to Conv_151        1.86           0.0099             0.0100      0.1
[08/10/2023-11:18:46] [I]                                                                                                                                                                                                                           Conv_154       21.10           0.1122             0.1119      0.7
[08/10/2023-11:18:46] [I]                                                                                                                                                                              Reformatting CopyNode for Output Tensor 0 to Conv_154        1.88           0.0100             0.0100      0.1
[08/10/2023-11:18:46] [I]                                                                                                                                                                                                                           Conv_157       21.10           0.1122             0.1119      0.7
[08/10/2023-11:18:46] [I]                                                                                                                                                                              Reformatting CopyNode for Output Tensor 0 to Conv_157        1.77           0.0094             0.0094      0.1
[08/10/2023-11:18:46] [I]                                                                                                                                           Conv_158 + Relu_159 || Conv_161 + Relu_162 || Conv_164 + Relu_165 || Conv_167 + Relu_168       90.04           0.4789             0.4765      2.9
[08/10/2023-11:18:46] [I]                                                                                                                                                                                                                           Conv_160       21.67           0.1153             0.1149      0.7
[08/10/2023-11:18:46] [I]                                                                                                                                                                              Reformatting CopyNode for Output Tensor 0 to Conv_160        1.84           0.0098             0.0098      0.1
[08/10/2023-11:18:46] [I]                                                                                                                                                                                                                           Conv_163       21.08           0.1121             0.1117      0.7
[08/10/2023-11:18:46] [I]                                                                                                                                                                              Reformatting CopyNode for Output Tensor 0 to Conv_163        1.82           0.0097             0.0098      0.1
[08/10/2023-11:18:46] [I]                                                                                                                                                                                                                           Conv_166       21.03           0.1119             0.1115      0.7
[08/10/2023-11:18:46] [I]                                                                                                                                                                              Reformatting CopyNode for Output Tensor 0 to Conv_166        1.81           0.0096             0.0097      0.1
[08/10/2023-11:18:46] [I]                                                                                                                                                                                                                           Conv_169       21.02           0.1118             0.1114      0.7
[08/10/2023-11:18:46] [I]                                                                                                                                                                              Reformatting CopyNode for Output Tensor 0 to Conv_169        1.78           0.0095             0.0093      0.1
[08/10/2023-11:18:46] [I]                                                                                                                                                                                                                              Total     3068.17          16.3201            16.2560    100.0
[08/10/2023-11:18:46] [I] 
&&&& PASSED TensorRT.trtexec [TensorRT v8401] # /usr/src/tensorrt/bin/trtexec --onnx=model/rpn_centerhead_sim.onnx --saveEngine=model/rpn_centerhead_sim.plan.8503 --workspace=4096 --fp16 --outputIOFormats=fp16:chw --inputIOFormats=fp16:chw --verbose --dumpLayerInfo --dumpProfile --separateProfileRun --profilingVerbosity=detailed
